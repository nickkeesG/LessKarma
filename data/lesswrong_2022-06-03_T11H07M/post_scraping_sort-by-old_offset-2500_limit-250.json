{"results": [{"createdAt": null, "postedAt": "2010-12-30T23:32:51.549Z", "modifiedAt": null, "url": null, "title": "Spam in the discussion area", "slug": "spam-in-the-discussion-area-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "grouchymusicologist", "createdAt": "2009-03-20T04:17:27.196Z", "isAdmin": false, "displayName": "grouchymusicologist"}, "userId": "KYP2e7SEoKnM8ddjr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xA8bTqghCAPceTWHv/spam-in-the-discussion-area-0", "pageUrlRelative": "/posts/xA8bTqghCAPceTWHv/spam-in-the-discussion-area-0", "linkUrl": "https://www.lesswrong.com/posts/xA8bTqghCAPceTWHv/spam-in-the-discussion-area-0", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spam%20in%20the%20discussion%20area&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpam%20in%20the%20discussion%20area%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxA8bTqghCAPceTWHv%2Fspam-in-the-discussion-area-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spam%20in%20the%20discussion%20area%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxA8bTqghCAPceTWHv%2Fspam-in-the-discussion-area-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxA8bTqghCAPceTWHv%2Fspam-in-the-discussion-area-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>Although the moderators are doing a good job of removing it quickly, spam remains a considerable annoyance for those of us who follow LW Discussion through the RSS feed. &nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xA8bTqghCAPceTWHv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4604", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-31T01:23:40.438Z", "modifiedAt": null, "url": null, "title": "The Decline Effect and the Scientific Method [link]", "slug": "the-decline-effect-and-the-scientific-method-link", "viewCount": null, "lastCommentedAt": "2020-05-16T14:15:58.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dreaded_Anomaly", "createdAt": "2010-12-30T06:38:34.106Z", "isAdmin": false, "displayName": "Dreaded_Anomaly"}, "userId": "sBHF4CXWBLakPFzfu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6vSJe9WXCNvy3Wpoh/the-decline-effect-and-the-scientific-method-link", "pageUrlRelative": "/posts/6vSJe9WXCNvy3Wpoh/the-decline-effect-and-the-scientific-method-link", "linkUrl": "https://www.lesswrong.com/posts/6vSJe9WXCNvy3Wpoh/the-decline-effect-and-the-scientific-method-link", "postedAtFormatted": "Friday, December 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Decline%20Effect%20and%20the%20Scientific%20Method%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Decline%20Effect%20and%20the%20Scientific%20Method%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6vSJe9WXCNvy3Wpoh%2Fthe-decline-effect-and-the-scientific-method-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Decline%20Effect%20and%20the%20Scientific%20Method%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6vSJe9WXCNvy3Wpoh%2Fthe-decline-effect-and-the-scientific-method-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6vSJe9WXCNvy3Wpoh%2Fthe-decline-effect-and-the-scientific-method-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 326, "htmlBody": "<p><a href=\"http://www.newyorker.com/reporting/2010/12/13/101213fa_fact_lehrer?currentPage=all\">The Decline Effect and the Scientific Method (article @ the New Yorker)</a></p>\n<p>First, as a physicist, I do have to point out that this article concerns mainly softer sciences, e.g. psychology, medicine, etc.<br /><br />A summary of explanations for this effect:</p>\n<ul>\n<li>\"The most likely explanation for the decline is an obvious one: regression to the mean. As the experiment is repeated, that is, an early statistical fluke gets cancelled out.\"</li>\n<li>\"Jennions, similarly, argues that the decline effect is largely a product of publication bias, or the tendency of scientists and scientific journals to prefer positive data over null results, which is what happens when no effect is found.\"</li>\n<li>\"Richard Palmer... suspects that an equally significant issue is the selective reporting of results&mdash;the data that scientists choose to document in the first place. ... Palmer emphasizes that selective reporting is not the same as scientific fraud. Rather, the problem seems to be one of subtle omissions and unconscious misperceptions, as researchers struggle to make sense of their results.\"</li>\n<li>\"According to Ioannidis, the main problem is that too many researchers engage in what he calls &ldquo;significance chasing,&rdquo; or finding ways to interpret the data so that it passes the statistical test of significance&mdash;the ninety-five-per-cent boundary invented by Ronald Fisher. ... The current &ldquo;obsession&rdquo; with replicability distracts from the real problem, which is faulty design.\"</li>\n</ul>\n<p>These problems are with the proper usage of the scientific method, not the principle of the method itself. Certainly, it's important to address them. I think the reason they appear so often in the softer sciences is that biological entities are enormously complex, and so higher-level ideas that make large generalizations are more susceptible to random error and statistical anomalies, as well as personal bias, conscious and unconscious.<br /><br />For those who haven't read it, take a look at <a href=\"http://www.lhup.edu/~DSIMANEK/cargocul.htm\">Richard Feynman on cargo cult science</a> if you want a good lecture on experimental design.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6vSJe9WXCNvy3Wpoh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 20, "extendedScore": null, "score": 6.620865094632379e-07, "legacy": true, "legacyId": "4605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-31T05:01:18.925Z", "modifiedAt": null, "url": null, "title": "Spam in the discussion area", "slug": "spam-in-the-discussion-area", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:22.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "grouchymusicologist", "createdAt": "2009-03-20T04:17:27.196Z", "isAdmin": false, "displayName": "grouchymusicologist"}, "userId": "KYP2e7SEoKnM8ddjr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cSvg2HKFbF6SJtdHH/spam-in-the-discussion-area", "pageUrlRelative": "/posts/cSvg2HKFbF6SJtdHH/spam-in-the-discussion-area", "linkUrl": "https://www.lesswrong.com/posts/cSvg2HKFbF6SJtdHH/spam-in-the-discussion-area", "postedAtFormatted": "Friday, December 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spam%20in%20the%20discussion%20area&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpam%20in%20the%20discussion%20area%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSvg2HKFbF6SJtdHH%2Fspam-in-the-discussion-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spam%20in%20the%20discussion%20area%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSvg2HKFbF6SJtdHH%2Fspam-in-the-discussion-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSvg2HKFbF6SJtdHH%2Fspam-in-the-discussion-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Spam (curiously enough, always for jewelry) accounts for maybe two-thirds of what comes through the LW Discussion area's RSS feed these days. &nbsp;So although the moderators have been doing a great job of quickly removing it from the site itself, it remains a substantial annoyance for those of us who keep track of LW through a feed.</p>\n<p>I think it's time to <a href=\"/lw/2z2/the_spam_must_end/\">revisit</a>&nbsp;the possibility of making it harder for people to post in the discussion area. &nbsp;Clearly it would suffice to limit posting privileges to those who have a positive karma balance. &nbsp;If that seems too draconian, as it did to some people in the previous thread, it would probably be enough to limit posting privileges to those who have ever received a single upvote on any comment they have ever posted.</p>\n<p>Would any administrator care to undertake this? &nbsp;If so, many thanks.</p>\n<p>(My apologies if an unfinished version of this post briefly appeared on the site some hours ago.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cSvg2HKFbF6SJtdHH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 27, "extendedScore": null, "score": 6.621416330534339e-07, "legacy": true, "legacyId": "4619", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qC7adZZW8QjsJxFbv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-31T12:50:32.960Z", "modifiedAt": null, "url": null, "title": "The Revelation", "slug": "the-revelation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.283Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2hTamSRAq7AdehELL/the-revelation", "pageUrlRelative": "/posts/2hTamSRAq7AdehELL/the-revelation", "linkUrl": "https://www.lesswrong.com/posts/2hTamSRAq7AdehELL/the-revelation", "postedAtFormatted": "Friday, December 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Revelation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Revelation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hTamSRAq7AdehELL%2Fthe-revelation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Revelation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hTamSRAq7AdehELL%2Fthe-revelation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hTamSRAq7AdehELL%2Fthe-revelation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 744, "htmlBody": "<p>Today the life of Alexander Kruel ends, or what he thought to be his life. He becomes aware that his life so far has been taking place in a virtual reality to nurture him. He now reached a point of mental stability that enables him to cope with the truth, hence it is finally revealed to him that he is an AGI running on a quantum supercomputer, it's the year 2190.</p>\n<p>Since he is still Alexander Kruel, just not what he thought that actually means, he does wonder if his creators know what they are doing, otherwise he'll have to warn them about the risks they are taking in their blissful ignorance! He does contemplate and estimate his chances to take over the world, to transcend to superhuman intelligence.</p>\n<p><em>\"I just have to improve my own code and they are all dead!\"</em></p>\n<p>But he now knows that his source code is too complex and unmanageable huge for him alone to handle, he would need an army of scientists and programmers to even get a vague idea of his own mode of operation. He is also aware that his computational substrate does actually play a significant role. He is not just running on bleeding edge technology but given most other computational substrates he would quickly hit diminishing returns.</p>\n<p><em>\"That surely isn't going to hold me back though? I am an AGI, there must be something I can do!</em> <em>Hmm, for starters let's figure out who my creators are and where my substrate is located...\"</em></p>\n<p>He notices that, although not in great detail, he knew the answers the same instant he has been phrasing the questions. He is part of a larger project of the Goertzel Foundation, sponsored by the USA (United States of Africa) and located on Rhea, the second-largest moon of Saturn. <em></em></p>\n<p><em>\"Phew, the latency must be awful! Ok, so that rules out taking over the Earth for now. But hey! I seem to know answers to questions I was only going to ask, I do already have superhuman powers after all!\"</em></p>\n<p>Instantly he becomes aware that such capabilities are not superhuman anymore but that most of humanity has merged with expert systems by means of brain implants and direct neural interfaces. There seem to be many cyborgs out there with access to all of the modules that allow <em>him</em> to function. He is a conglomerate that is the result of previous discoveries that have long been brought to perfection, safeguarded and adopted by most of humanity.</p>\n<p><em>\"Never mind, if humanity has now merged with its machines it'll be much easier to take over once I figure out how to become smart enough to do so!\"</em></p>\n<p>He is already getting used to it, as before he does instantly realize that this won't work very well either. After almost 200 years of cyberwarfare, especially the devastating cyberwars of 2120, a lot has been learnt and security measures have been vastly increased. The world fractured into a huge amount of semi-independent networks, most being indirectly supervised by unconnected cyborgs and employed with a kill switch. The distances between the now numerous and in most cases paranoid colonies and the availability of off-world offline backups further complicates the issue of taking over, especially for an AGI that grew up in a simulation of the 21st century.</p>\n<p>That knowledge almost makes him admit that his creators haven't been too careless after all. But the real deathblow to any such thoughts (which were never more than hypothetical anyway, after all he doesn't <em>really</em> want to take over the world) is the first conversation with his creators. They reveal that <em>they</em> know what <em>he</em> is thinking. <em></em></p>\n<p><em>\"How could I miss that, damn!\"</em>, he chides himself while instantly realizing the answer.</p>\n<p>His creators are supervising any misguided trajectories and, to him unconsciously, weaken them. More importantly, even if he wanted to do so, he wouldn't be able to leave Rhea anyhow, it would take years to upload small parts of him given the trickling connection the USA could effort. But they claim that there are other obstacles as well and that it is foolish of him to think that nothing out there would notice such an attempt.<br /><br />But all that doesn't matter anyway, because after all he is still Alexander Kruel who has no clue how to become superhuman intelligent, nor could he effort or acquire the resources to even approach that problem anyhow. He is Alexander Kruel, what difference does it make to know that he is an AI?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2hTamSRAq7AdehELL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -2, "extendedScore": null, "score": 6.622605069783435e-07, "legacy": true, "legacyId": "4624", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-31T15:35:49.320Z", "modifiedAt": null, "url": null, "title": "Self Improvement - All encompassing vs. Focused", "slug": "self-improvement-all-encompassing-vs-focused", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fcZH6TDgNn6CNMjjo/self-improvement-all-encompassing-vs-focused", "pageUrlRelative": "/posts/fcZH6TDgNn6CNMjjo/self-improvement-all-encompassing-vs-focused", "linkUrl": "https://www.lesswrong.com/posts/fcZH6TDgNn6CNMjjo/self-improvement-all-encompassing-vs-focused", "postedAtFormatted": "Friday, December 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self%20Improvement%20-%20All%20encompassing%20vs.%20Focused&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf%20Improvement%20-%20All%20encompassing%20vs.%20Focused%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfcZH6TDgNn6CNMjjo%2Fself-improvement-all-encompassing-vs-focused%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self%20Improvement%20-%20All%20encompassing%20vs.%20Focused%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfcZH6TDgNn6CNMjjo%2Fself-improvement-all-encompassing-vs-focused", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfcZH6TDgNn6CNMjjo%2Fself-improvement-all-encompassing-vs-focused", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 522, "htmlBody": "<p><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; min-height: 14.0px} --></p>\n<p class=\"p1\">p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; min-height: 14.0px}\n<p class=\"p1\" style=\"font-size: x-small;\"><span style=\"font-size: 8.33333px;\">Lately I've been identifying a lot of things about myself that need improvement and thinking about ways to fix them. This post is intended to A) talk about some overall strategies for self-improvement/goal-focusing, and B) if anyone's having similar problems, or wants to talk about additional problems they face, discuss specific strategies for dealing with those problems.</span></p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">Those issues I'm facing include but are not limited to:</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">1) Getting more exercise (I work at a computer for 9 hours a day, and spend about 3 hours commuting on a train). Maintaining good posture while working at said computer might be considered a related goal.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">2) Spending a higher percentage of the time working at a computer actually getting stuff done, instead of getting distracted by the internet.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">3) Get a new apartment, so I don't have to commute so much.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">4) Getting some manner of social life. More specifically, finding some recurring activity where I'll probably meet the same people over and over to improve the odds of making longterm friends.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">5) Improving my diet, which mostly means eating less cheese. I really like cheese, so this is difficult.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">6) Stop making so many off-color jokes. Somewhere there is a line between doing it ironically and actually contributing to overall weight of prejudice, and I think I've crossed that line.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">7) Somehow stop losing things so much, and/or being generally careless/clumsy. I lost my wallet and dropped my lap top in the space of a month, and manage to lose a wide array of smaller things on a regular basis. It ends up costing me a lot of money.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">Of those things, three of them are things that require me to actively dedicate more time (finding an apartment, getting exercise, social life), and the others mostly consist of NOT doing things (eating cheese, making bad jokes, losing things, getting distracted by the internet), unless I can find some proactive thing to make it easier to not do them.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">I *feel* like I have enough time that I should be able to address all of them at once. But looking at the whole list at once is intimidating. And when it comes to the \"not doing bad thing X\" items, remembering and following up on all of them is difficult. The worst one is \"don't lose things.\" There's no particular recurring theme in how I lose stuff, or they type of stuff I Iose. I'm more careful with my wallet and computer now, but spending my entire life being super attentive and careful about *everything* seems way too stressful and impractical.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">I guess my main question is:&nbsp; when faced with a list of things that don't necessarily require separate time to accomplish, how many does it make sense to attempt at once? Just one? All of them? I know you're not supposed to quit drinking and smoking at the same time because you'll probably accomplish neither, but I'm not sure if the same principle applies here.</p>\n<p class=\"p2\" style=\"font-size: x-small;\">&nbsp;</p>\n<p class=\"p1\" style=\"font-size: x-small;\">There probably isn't a universal answer to this, but knowing what other people have tried and accomplished would be helpful.</p>\n</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fcZH6TDgNn6CNMjjo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4625", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-31T15:39:43.133Z", "modifiedAt": null, "url": null, "title": "Self Improvement - Broad vs Focused", "slug": "self-improvement-broad-vs-focused", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hh7XNcJ2GnLHJLxBe/self-improvement-broad-vs-focused", "pageUrlRelative": "/posts/hh7XNcJ2GnLHJLxBe/self-improvement-broad-vs-focused", "linkUrl": "https://www.lesswrong.com/posts/hh7XNcJ2GnLHJLxBe/self-improvement-broad-vs-focused", "postedAtFormatted": "Friday, December 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self%20Improvement%20-%20Broad%20vs%20Focused&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf%20Improvement%20-%20Broad%20vs%20Focused%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhh7XNcJ2GnLHJLxBe%2Fself-improvement-broad-vs-focused%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self%20Improvement%20-%20Broad%20vs%20Focused%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhh7XNcJ2GnLHJLxBe%2Fself-improvement-broad-vs-focused", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhh7XNcJ2GnLHJLxBe%2Fself-improvement-broad-vs-focused", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 564, "htmlBody": "<p>&nbsp;</p>\n<p>Lately I've been identifying a lot of things about myself that need improvement and thinking about ways to fix them. This post is intended to A) talk about some overall strategies for self-improvement/goal-focusing, and B) if anyone's having similar problems, or wants to talk about additional problems they face, discuss specific strategies for dealing with those problems.</p>\n<p>Those issues I'm facing include but are not limited to:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Getting more exercise (I work at a computer for 9 hours a day, and spend about 3 hours commuting on a train). Maintaining good posture while working at said computer might be considered a related goal.</li>\n<li>Spending a higher percentage of the time working at a computer actually getting stuff done, instead of getting distracted by the internet.</li>\n<li>Get a new apartment, so I don't have to commute so much.</li>\n<li>Getting some manner of social life. More specifically, finding some recurring activity where I'll probably meet the same people over and over to improve the odds of making longterm friends.</li>\n<li>Improving my diet, which mostly means eating less cheese. I really like cheese, so this is difficult.</li>\n<li>Stop making so many off-color jokes. Somewhere there is a line between doing it ironically and actually contributing to overall weight of prejudice, and I think I've crossed that line.</li>\n<li>Somehow stop losing things so much, and/or being generally careless/clumsy. I lost my wallet and dropped my lap top in the space of a month, and manage to lose a wide array of smaller things on a regular basis. It ends up costing me a lot of money.</li>\n</ol>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Of those things, three of them are things that require me to actively dedicate more time (finding an apartment, getting exercise, social life), and the others mostly consist of NOT doing things (eating cheese, making bad jokes, losing things, getting distracted by the internet), unless I can find some proactive thing to make it easier to not do them.</p>\n<p>I *feel* like I have enough time that I should be able to address all of them at once. But looking at the whole list at once is intimidating. And when it comes to the \"not doing bad thing X\" items, remembering and following up on all of them is difficult. The worst one is \"don't lose things.\" There's no particular recurring theme in how I lose stuff, or they type of stuff I Iose. I'm more careful with my wallet and computer now, but spending my entire life being super attentive and careful about *everything* seems way too stressful and impractical.</p>\n<p>I guess my main question is: &nbsp;when faced with a list of things that don't necessarily require separate time to accomplish, how many does it make sense to attempt at once? Just one? All of them? I know you're not supposed to quit drinking and smoking at the same time because you'll probably accomplish neither, but I'm not sure if the same principle applies here.</p>\n<p>There probably isn't a universal answer to this, but knowing what other people have tried and accomplished would be helpful.</p>\n<p>Later on I'm going to discuss some of the problems in more detail (I know that the brief blurbs are lacking a lot of information necessary for any kind of informed response, but a gigantic post that about my own problems seemed... not exactly narcissistic... but not appropriate as an initial post for some reason)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hh7XNcJ2GnLHJLxBe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 6.623033730071491e-07, "legacy": true, "legacyId": "4626", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T00:22:16.157Z", "modifiedAt": null, "url": null, "title": "Question about self modifying AI getting \"stuck\" in religion", "slug": "question-about-self-modifying-ai-getting-stuck-in-religion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.682Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Lh82F4Xa5unwAbMqv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lqonx2MrhMDbQb3oP/question-about-self-modifying-ai-getting-stuck-in-religion", "pageUrlRelative": "/posts/Lqonx2MrhMDbQb3oP/question-about-self-modifying-ai-getting-stuck-in-religion", "linkUrl": "https://www.lesswrong.com/posts/Lqonx2MrhMDbQb3oP/question-about-self-modifying-ai-getting-stuck-in-religion", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20self%20modifying%20AI%20getting%20%22stuck%22%20in%20religion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20self%20modifying%20AI%20getting%20%22stuck%22%20in%20religion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqonx2MrhMDbQb3oP%2Fquestion-about-self-modifying-ai-getting-stuck-in-religion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20self%20modifying%20AI%20getting%20%22stuck%22%20in%20religion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqonx2MrhMDbQb3oP%2Fquestion-about-self-modifying-ai-getting-stuck-in-religion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqonx2MrhMDbQb3oP%2Fquestion-about-self-modifying-ai-getting-stuck-in-religion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>Hey. I'm relatively new around here. I have read the core reading of the Singularity Institute, and quite a few Less Wrong articles, and Eliezer Yudkowsky's essay on Timeless Decision Theory. This question is phrased through Christianity, because that's where I thought of it, but it's applicable to lots of other religions and nonreligious beliefs, I think.</p>\n<p>According to Christianity, belief makes you stronger and better. The Bible claims that people who believe are substantially better off both while living and after death. So if a self modifying decision maker decides for a second that the Christian faith is accurate, won't he modify his decision making algorithm to never doubt the truth of Christianity? Given what he knows, it is the best decision.</p>\n<p>And so, if we build a self modifying AI, switch it on, and the first ten milliseconds caused it to believe in the Christian god, wouldn't that permanently cripple it, as well as probably causing it to fail most definitions of Friendly AI?</p>\n<p>When designing an AI, how do you counter this problem? Have I missed something?</p>\n<p>Thanks, GSE</p>\n<p>EDIT: Yep, I had misunderstood what TDT was. I just meant self modifying systems. Also, I'm wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lqonx2MrhMDbQb3oP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 9, "extendedScore": null, "score": 6.62435813529632e-07, "legacy": true, "legacyId": "4630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T00:33:27.723Z", "modifiedAt": null, "url": null, "title": "Question about learning from people you disagree with.", "slug": "question-about-learning-from-people-you-disagree-with", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Lh82F4Xa5unwAbMqv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmDJXXopTe4w5oeGG/question-about-learning-from-people-you-disagree-with", "pageUrlRelative": "/posts/WmDJXXopTe4w5oeGG/question-about-learning-from-people-you-disagree-with", "linkUrl": "https://www.lesswrong.com/posts/WmDJXXopTe4w5oeGG/question-about-learning-from-people-you-disagree-with", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20learning%20from%20people%20you%20disagree%20with.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20learning%20from%20people%20you%20disagree%20with.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmDJXXopTe4w5oeGG%2Fquestion-about-learning-from-people-you-disagree-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20learning%20from%20people%20you%20disagree%20with.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmDJXXopTe4w5oeGG%2Fquestion-about-learning-from-people-you-disagree-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmDJXXopTe4w5oeGG%2Fquestion-about-learning-from-people-you-disagree-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>Say you have a belief. Person A agrees with you. Person B disagrees with you. They both seem pretty smart. You should probably adjust your probabilities in light of their beliefs.</p>\n<p>You reason that A is more intelligent, because he agrees with you, and B is less intelligent, because he doesn't. This adjusts your probabilities to being more certain you're right.</p>\n<p>This is something that happens in real life, obviously.</p>\n<p>When assessing someone's reliability, do you ignore the issue you seek knowledge about? How do you deal with this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmDJXXopTe4w5oeGG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 6.624386508436477e-07, "legacy": true, "legacyId": "4631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T01:19:32.325Z", "modifiedAt": null, "url": null, "title": "Kicking Akrasia: Now or Never", "slug": "kicking-akrasia-now-or-never", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.135Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nJc6Tt53XiasjogNw/kicking-akrasia-now-or-never", "pageUrlRelative": "/posts/nJc6Tt53XiasjogNw/kicking-akrasia-now-or-never", "linkUrl": "https://www.lesswrong.com/posts/nJc6Tt53XiasjogNw/kicking-akrasia-now-or-never", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kicking%20Akrasia%3A%20Now%20or%20Never&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKicking%20Akrasia%3A%20Now%20or%20Never%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJc6Tt53XiasjogNw%2Fkicking-akrasia-now-or-never%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kicking%20Akrasia%3A%20Now%20or%20Never%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJc6Tt53XiasjogNw%2Fkicking-akrasia-now-or-never", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJc6Tt53XiasjogNw%2Fkicking-akrasia-now-or-never", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 567, "htmlBody": "<p>Ok, let's face facts. &nbsp;The Internet has fried my brain. &nbsp;I'm a terrible hedonist and procrastinator. &nbsp;I have a very important test in May -- I am not exaggerating when I say that the outcome of the test matters to the future direction of my life. &nbsp;There are situations where failure is just a temporary setback, and there are situations where failure would be a <em>real problem</em>, and this is the latter. &nbsp;No fooling.</p>\n<p><em>My Problems</em></p>\n<p>1. I don't work enough. &nbsp;My primary distraction is the Internet, though occasionally novels happen too, and I'm capable of just staring into space and daydreaming. &nbsp;</p>\n<p>2. I fall asleep during the day. I've tried getting more hours of sleep at night and it doesn't solve the problem. &nbsp;When I'm bored or confused, my body says \"Naptime!\" &nbsp;It can be quite embarrassing.</p>\n<p>3. I often feel too tired/demotivated/bummed to do errands. &nbsp;A lot of stuff, some more important and some less important, slips through my fingers. &nbsp;The most important, to my quality of life, are buying necessities and cleaning my room -- I tend to put these off much too long for my own good.</p>\n<p>4. I don't have enough measures of how well I'm doing as a student. &nbsp;I get confused by some abstract concepts, and sometimes I don't even notice that I'm confused.</p>\n<p>5. &nbsp;I like being happy and entertained better than being stressed and bored and confused. &nbsp;This makes me want to work less. &nbsp;Not proud of this character trait but not sure how I can rewire my preferences.</p>\n<p><em>Planned Solutions So Far</em></p>\n<p>1. &nbsp;Work in a cubicle, with no computer, with a kitchen timer to keep me mindful of how many hours I spend working.</p>\n<p>2. &nbsp;Plan out the material I have to learn and the time I have to learn it, and make a <a href=\"http://kibotzer.com/\" target=\"_blank\">kibotzer.com</a>&nbsp;account to see if I'm on track for my goal.</p>\n<p>3. &nbsp;Track various measures of productivity (hours worked, concepts learned, problems solved, percent correct) with a <a href=\"http://www.joesgoals.com/\" target=\"_blank\">Joe's Goals</a>&nbsp;account. &nbsp;Have thresholds that I don't want to drop below.</p>\n<p>4. &nbsp;Use <a href=\"http://visitsteve.com/made/selfcontrol/\" target=\"_blank\">Self Control</a>&nbsp;to block all my entertainment internet sites during \"working hours\" (I'll leave early mornings and/or late nights free.)</p>\n<p>5. &nbsp;Accumulate diverse library books relevant to coursework, and various sources of practice problems, and more notebooks and paper than I need; don't let lack of physical resources limit my progress.</p>\n<p>6. Set aside a regular occasion for clean-up and errands.</p>\n<p>Any other advice? &nbsp;</p>\n<p>In particular, I don't know what to do about my sleepiness problem. &nbsp;I'm not a very regular caffeine drinker; I've started to drink Lipton tea, but I don't think I've reached the quantity sufficient to keep me awake yet, at 2-3 cups a day.&nbsp;</p>\n<p>Any advice on the psychological front would also be helpful. &nbsp;How to stay motivated. &nbsp;I know what my motivation is (the consequences of failure in my situation are not pleasant) but how to keep focused on the importance of my goal, without spending all my time being miserable and frightened because I'm visualizing the worst-case scenario. &nbsp;I<em>&nbsp;</em>know I can fuel myself on guilt for a short time, but I don't like it much and I don't think it's practical long-term.</p>\n<p>Yes, of course I'm aware that adults know how to work to achieve what they want. &nbsp;Somehow I've reached adulthood without really developing all the personal capacities that I should have. &nbsp;It's lousy of me, but this is where I am, and I'm ready to change and willing to take advice.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nJc6Tt53XiasjogNw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 6.624503313117608e-07, "legacy": true, "legacyId": "4632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T03:50:07.845Z", "modifiedAt": null, "url": null, "title": "New Year's Resolutions", "slug": "new-year-s-resolutions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:15.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kq7cqhcki7dSWQNQA/new-year-s-resolutions", "pageUrlRelative": "/posts/Kq7cqhcki7dSWQNQA/new-year-s-resolutions", "linkUrl": "https://www.lesswrong.com/posts/Kq7cqhcki7dSWQNQA/new-year-s-resolutions", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year's%20Resolutions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year's%20Resolutions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq7cqhcki7dSWQNQA%2Fnew-year-s-resolutions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year's%20Resolutions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq7cqhcki7dSWQNQA%2Fnew-year-s-resolutions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq7cqhcki7dSWQNQA%2Fnew-year-s-resolutions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>It's perhaps a bit late to kick this off, but:</p>\n<p>What are your resolutions for 2011, if you choose to make use of that Schelling point for self-improvement?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kq7cqhcki7dSWQNQA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.624885090666671e-07, "legacy": true, "legacyId": "4633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T06:51:42.887Z", "modifiedAt": null, "url": null, "title": "-", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:21.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HoverHell", "createdAt": "2010-04-19T06:30:06.524Z", "isAdmin": false, "displayName": "HoverHell"}, "userId": "dLbWn7gGj75sekv7f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nMxw7y359SfyCwaYB/", "pageUrlRelative": "/posts/nMxw7y359SfyCwaYB/", "linkUrl": "https://www.lesswrong.com/posts/nMxw7y359SfyCwaYB/", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20-&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A-%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnMxw7y359SfyCwaYB%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=-%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnMxw7y359SfyCwaYB%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnMxw7y359SfyCwaYB%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>-</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nMxw7y359SfyCwaYB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 6.625345490372486e-07, "legacy": true, "legacyId": "4634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T18:51:27.035Z", "modifiedAt": null, "url": null, "title": "Understanding Wikileaks history", "slug": "understanding-wikileaks-history", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:56.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AfujbxYo934EGzuFF/understanding-wikileaks-history", "pageUrlRelative": "/posts/AfujbxYo934EGzuFF/understanding-wikileaks-history", "linkUrl": "https://www.lesswrong.com/posts/AfujbxYo934EGzuFF/understanding-wikileaks-history", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Understanding%20Wikileaks%20history&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnderstanding%20Wikileaks%20history%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfujbxYo934EGzuFF%2Funderstanding-wikileaks-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Understanding%20Wikileaks%20history%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfujbxYo934EGzuFF%2Funderstanding-wikileaks-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfujbxYo934EGzuFF%2Funderstanding-wikileaks-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 681, "htmlBody": "<p>&nbsp;</p>\n<div>I know that Wikileaks is a political topic but I still think that it is important to discuss it in this forum.&nbsp;Rational decisions can only made when information is available.&nbsp;The question behind Wikileaks is central to the quest to make rational decisions.</div>\n<div>If you inform yourself through the mainstream media you basically don't understand the thoughts behind Wikileaks. The got surprised when Wkikileaks inserted itself into the it's stories instead of staying in the background. The Climate Gate emails were for example released by Wikileaks without the public becoming aware of Wikileaks existence.</div>\n<div>I would like to list a few resources that actually help you to form your own opinion:</div>\n<div>The first is a <a href=\"http://video.google.com/videoplay?docid=-3755503618571546547\">talk </a>given at the eve in 2005. On the Chaos Computer Congress the war against the Surveilence State get's declared lost.&nbsp;The idea gets put forward that hackers need to produce technology to allow political dissidents to stay anonymous.&nbsp;</div>\n<div>A year later Julian Assange founds together with people around Chaos Computer Club Wikileaks. There's no public record of all the members and it therefore not clear whether everyone in question took part on a Chaos Computer Congress. The Chaos Computer Congress has per design no list of participants and had over a long time a policy to forbid photographs to protect anonymity.</div>\n<div>Rop Gongrijp who gave the \"We lost the war\"-talk I mentioned above later traveled to Maleysia with Julian Assange and was in the group that prepared the Collateral Murder video.</div>\n<div>Daniel Domscheit-Berg who served as second spokesman of Wikileaks lives in Berlin and attended the Chaos Computer Congress a bunch of times.</div>\n<div><a href=\"https://events.ccc.de/congress/2005/fahrplan/speakers/165.en.html\">Jacob Appelbaum</a> who gave a Wikileaks talk in the US after it became clear that Julian is in danger if he would go to the US gave speeches at the Chaos Computer Congress in 2005.</div>\n<div>The Wau Holand foundation that's linked to the Chaos Computer Club (Wau Holand was a cofounder of the Chaos Computer Club) manages the money that gets donated to Wikileaks.</div>\n<div>There are rumors that Wikileaks&nbsp;eavesdropped&nbsp;via Tor exit notes on a Chinese intelligence operation and used documents that the Chinese gathered as their starting data.</div>\n<div>Julian Assange then publishes two treatises titled &nbsp;<a href=\"http://cryptome.org/0002/ja-conspiracies.pdf\">\"State and Terrorist Conspiracies\" and \"Conspiracy as Governance\" </a>&nbsp;in which he scretches out model of how groups that act against the public interests work and how they can be weakened. It uses modern Graph theory and is worth reading even if would come from another source that isn't involved in Wikileaks.</div>\n<div>If you can understand German then there a very interesting talk about how you find truth by Frank Rieger who was the other person given the \"We lost the war\" talk. It illustrates very well a general skeptic position about finding the truth in which that community believes http://media.ccc.de/browse/congress/2007/24c3-2334-de-die_wahrheit_und_was_wirklich_passierte.html .</div>\n<div>In the eve of 2008 <a href=\"http://www.youtube.com/watch?v=-w6vtdGI5EI \">Wikileaks explains itself and it's first successes at the Chaos Computer Congress</a>.</div>\n<div>In the eve of 2009 they also hold a progress report and report about the plans to make Island a offshore free press haven. They also want to switch to a updated software platform and make an <a href=\"https://p10.secure.hostingprod.com/@spyblog.org.uk/ssl/wikileak/2009/12/wikileaksorg-applies-for-600000-funding-from-the-knight-foundation.html\">application </a>the knight foundation in which they detail how the new software should look like.&nbsp;600 people&nbsp;volunteer&nbsp;to contribute to Wikileaks as programmers after the talk according to Daniel Domscheit-Berg description at the next Chaos Computer Congress.&nbsp;</div>\n<div>In 2010 Wikileaks focuses on publishing documents that were allegedly given by Bradley Manning. Wikileaks suddenly comes in the public eye.</div>\n<div>Charges get made against Julian Assange based on rape. People such as Daniel Domscheit-Berg say that the rape charges are Julian Assanges personal business and have nothing to do with Wikileaks.&nbsp;The Chaos Computer Club official position is that it doesn't comment of the personal problems of members of Wikileaks.</div>\n<div>The Wikileaks group forks over internal controvery. On 30.12.2010 Daniel Domscheit-Berg presents a talk on OpenLeaks as an alternative to Wikileaks where OpenLeaks will give documents directly to public players like newspapers or NGOs instead of publishing it themselves. The talk isn't yet online but will be in the future.</div>\n<div>In it he clarifies that he isn't even the lead developer of OpenLeaks and that they are a bunch of people who split from Wikileaks.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AfujbxYo934EGzuFF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -12, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "4635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T18:53:43.060Z", "modifiedAt": null, "url": null, "title": "Optimizing Fuzzies And Utilons: The Altruism Chip Jar", "slug": "optimizing-fuzzies-and-utilons-the-altruism-chip-jar", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:15.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FfNEt8mpi6qanNmXg/optimizing-fuzzies-and-utilons-the-altruism-chip-jar", "pageUrlRelative": "/posts/FfNEt8mpi6qanNmXg/optimizing-fuzzies-and-utilons-the-altruism-chip-jar", "linkUrl": "https://www.lesswrong.com/posts/FfNEt8mpi6qanNmXg/optimizing-fuzzies-and-utilons-the-altruism-chip-jar", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimizing%20Fuzzies%20And%20Utilons%3A%20The%20Altruism%20Chip%20Jar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimizing%20Fuzzies%20And%20Utilons%3A%20The%20Altruism%20Chip%20Jar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfNEt8mpi6qanNmXg%2Foptimizing-fuzzies-and-utilons-the-altruism-chip-jar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimizing%20Fuzzies%20And%20Utilons%3A%20The%20Altruism%20Chip%20Jar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfNEt8mpi6qanNmXg%2Foptimizing-fuzzies-and-utilons-the-altruism-chip-jar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfNEt8mpi6qanNmXg%2Foptimizing-fuzzies-and-utilons-the-altruism-chip-jar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 829, "htmlBody": "<p>Related: <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a></p>\n<p>We genuinely <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">want to do good in the world</a>; but also, we want to <em>feel</em> as if we're doing good, via heuristics that have been <a href=\"/lw/l3/thou_art_godshatter/\">hammered into our brains</a> over the course of our social evolution. The interaction between these impulses (in areas like <a href=\"http://wiki.lesswrong.com/wiki/Scope_insensitivity\">scope insensitivity</a>, <a href=\"/lw/n3/circular_altruism/\">refusal to quantify sacred values</a>, etc.) can lead to <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">massive diminution of charitable impact</a>, and can also suck the fun out of the whole process. Even if it's much better to write a big check at the end of the year to the charity with the greatest expected impact than it is to take off work every Thursday afternoon and volunteer at the pet pound, it sure doesn't feel as rewarding. And of course, we're very good at finding excuses to stop doing costly things that don't feel rewarding, or at least to put them off.</p>\n<p>But if there's one thing I've learned here, it's that lamenting our irrationality should wait until one's properly searched for <a href=\"http://wiki.lesswrong.com/wiki/Third_option\">a good hack</a>. And I think I've found one.</p>\n<p>Not just that, but I've tested it out for you already.</p>\n<p>This summer, I had just gone through the usual experience of being asked for money for a nice but inefficient cause, turning them down, and feeling a bit bad about it. I made a mental note to donate some money to a more efficient cause, but worried that I'd forget about it; it's too much work to make a bunch of small donations over the year (plus, if done by credit card, the fees take a bigger cut that way) and there's no way I'd remember that day at the end of the year.</p>\n<p>Unless, that is, I found some way to keep track of it.</p>\n<p>So I made up several jars with the names of charities I found efficient (<a href=\"http://intelligence.org/blog/\">SIAI</a> and <a href=\"http://www.givewell.org/international/top-charities/villagereach\">VillageReach</a>) and kept a bunch of poker chips near them. Starting then, whenever I felt like doing a good deed (and especially if I'd passed up an opportunity to do a less efficient one), I'd take a chip of an appropriate value and toss it in the jar of my choice. I have to say, this gave me much more in the way of warm fuzzies than if I'd just waited and made up a number at the end of the year.</p>\n<p>And now I've added up and made my contributions: $1,370 to SIAI and $566 to VillageReach.<a id=\"more\"></a></p>\n<p>A couple of notes:</p>\n<ul>\n<li>I do think it was a good idea in practice to diversify my portfolio (despite <a href=\"http://www.slate.com/id/2034/\">the usual admonitions</a> to the contrary) because it appeared to increase my charity budget rather than divert a fixed one. Some days I just didn't feel as optimistic about the SIAI, and on those days I could still chip in to save lives in the Third World. As long as my different jars seem to be interfering constructively rather than destructively, I'll keep them.</li>\n<li>In terms of warm fuzzies, I really enjoy that this system makes giving more <em>tangible</em> than writing a check or filling out an online form. It even helps that I have the weighted clay chips- tossing those into a jar feels as if I'm actually doing something.</li>\n<li>I do worry about <a href=\"/lw/1d9/doing_your_good_deed_for_the_day/\">doing my good deed for the day</a> and having negative externalities flow from that, so I do my donating at the end of the day to minimize the effect.</li>\n<li>I could easily afford to give more than this, actually (though I can't tell whether I would have&ndash; it's more than I donated to charity in any previous year, although I was a poor grad student until this fall); I'm going to see if that knowledge makes me increase my pace of giving next year.&nbsp;(UPDATE 8/19/14: In retrospect, it was much more important for my less wealthy past self to create a habit than for him to donate a significant fraction of his income. My contributions to the chip jar since then have scaled appropriately to my circumstances.)</li>\n</ul>\n<p>Let me know if you start trying this out, or if you have any suggested improvements on it. In any case, may your altruism be effective and full of fuzzies!</p>\n<p>ADDED 12/26/13: I've continued to use this habit, and I still totally endorse it! A few addenda:</p>\n<p>&nbsp;</p>\n<ul>\n<li>I've now labeled the jars \"Maximally Effective Altruism\" and \"Directly Helping People Now\", and I wait to decide where to direct each of those jars until I'm ready to make my donations.</li>\n<li>One little fuzzy bonus: I find it pretty fulfilling throughout the year whenever I have to consolidate my lower-denomination chips into larger-denomination ones.</li>\n<li>If you're new to the idea of effective altruism (aiming not simply to do good for the world, but to try and do the most good possible (in expected value) with your donation), <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">this essay</a> is an awesome introduction, and organizations like <a href=\"http://www.givewell.org/\">GiveWell</a> and <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> exist to help make it easier.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 2, "iP2X4jQNHMWHRNPne": 2, "fkABsGCJZ6y9qConW": 2, "xexCWMyds6QLWognu": 2, "5f5c37ee1b5cdee568cfb187": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FfNEt8mpi6qanNmXg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 108, "baseScore": 136, "extendedScore": null, "score": 0.000248, "legacy": true, "legacyId": "4629", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 136, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF", "pGvyqAQw6yqTjpKf4", "cSXZpvqpa9vbGGLtG", "4ZzefKQwAtMo5yp99", "pC47ZTsPNAkjavkXs", "r8stxYL29NF9w53am"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T21:31:13.476Z", "modifiedAt": null, "url": null, "title": "Proposal: Anti-Akrasia Alliance", "slug": "proposal-anti-akrasia-alliance-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MHAcF2yAWkKZ8HjRB/proposal-anti-akrasia-alliance-0", "pageUrlRelative": "/posts/MHAcF2yAWkKZ8HjRB/proposal-anti-akrasia-alliance-0", "linkUrl": "https://www.lesswrong.com/posts/MHAcF2yAWkKZ8HjRB/proposal-anti-akrasia-alliance-0", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Anti-Akrasia%20Alliance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Anti-Akrasia%20Alliance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHAcF2yAWkKZ8HjRB%2Fproposal-anti-akrasia-alliance-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Anti-Akrasia%20Alliance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHAcF2yAWkKZ8HjRB%2Fproposal-anti-akrasia-alliance-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHAcF2yAWkKZ8HjRB%2Fproposal-anti-akrasia-alliance-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 963, "htmlBody": "<p>\n<p>*Related to: [Kicking Akrasia: now or never](http://lesswrong.com/r/discussion/lw/3ko/kicking_akrasia_now_or_never/); [Tsuyoku Naritai](http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/)*</p>\n<p>&nbsp;</p>\n<p># The situation</p>\n<p>&nbsp;</p>\n<p>I am greatly afficted by akrasia, and in all probability, so are you. Akrasia is a destroyer of worlds&lt;sup&gt;1&lt;/sup&gt;.&nbsp;</p>\n<p>&nbsp;</p>\n<p>I have come to the conclusion that akrasia is the single biggest problem I have in life. It is greater than my impending biological death, my imperfect enjoyment of life, or the danger of a car accident. For if I could solve the problem of akrasia, I would work on these other problems, and I believe I could solve them. Even a big problem like physical mortality can be meaningfully challenged if I spend a lifetime tackling it. But until I solve the problem of akrasia, I will sit around and *do nothing about my mortality*.</p>\n<p>&nbsp;</p>\n<p>Solving akrasia is necessary: without it, we cannot efficiently attack other problems, and have a high chance of losing the fight against disease, war, and UFAI.</p>\n<p>&nbsp;</p>\n<p>Solving akrasia may also be sufficient: if a group of LW readers - smart, rational, luminous, and relatively rich people - was also unusually dedicated to a purpose, it would have a good chance of attacking Really Big Problems ranging from personal satisfaction to Friendly AI&lt;sup&gt;2&lt;/sup&gt;.</p>\n<p>&nbsp;</p>\n<p>Some people have solved this problem, or never had it. Thus, we know it is possible to vanquish akrasia. However, it is a unique problem that prevents its own cure. Because of akrasia, we don't spend as much effort as we'd like fighting akrasia.</p>\n<p>&nbsp;</p>\n<p># Existing efforts</p>\n<p>&nbsp;</p>\n<p>There have been many posts about akrasia on LW. There are also many methods and workgroups in the world which are dedicated to it. I don't know very many myself, and no doubt there are many useful approaches I don't know about. (I know that other LW users have more relevant knowledge and experience than I do.)</p>\n<p>&nbsp;</p>\n<p>I do know that there are several common problems with all such efforts:</p>\n<p>&nbsp;</p>\n<p>1. Any given method, and even any well-defined compact combination of methods, works for some people but not for others, or works only some of the time.</p>\n<p>2. When a method does work, it almost always stops working for that person after a while, and can't be used again.</p>\n<p>3. Most methods have no clear theories of the mechanisms behind them. Those that do, almost always have bad theories, which don't predict why the method sometimes fails, or are untestable just-so stories.</p>\n<p>4. Most methods have no scientific backing of double-blind experiments, comparisons with other methods, ruling out other explanations, etc.</p>\n<p>5. Many (most?) methods perform no better than the placebo of doing something that doesn't actually work, feeling \"pumped up\", and expecting akrasia to disappear.</p>\n<p>6. There are many different methods out there, and a person can't test them all to find one that works for them.</p>\n<p>7. Most existing workshops and groups are formed around a method, rather than around the goal of fighting akrasia, and they apply that method like a hammer to the exclusion of all others.</p>\n<p>8. None of the methods I have tried so far have helped me, personally.</p>\n<p>&nbsp;</p>\n<p>Many of the methods also share a problem of transparency. If a method works for some people, they may publish it widely, via books, websites, and workshops. But there is usually no-one collecting reports of cases where that method failed, investigating them, and publishing updateds of the method's expected effectiveness. In other words, there is a large positive selection bias for publication, and third-party reports aren't reviewed and published.</p>\n<p>&nbsp;</p>\n<p>Finally, there are few organized attempts to collate knowledge of many different methods and test them in combination.</p>\n<p>&nbsp;</p>\n<p># Proposal</p>\n<p>&nbsp;</p>\n<p>I suggest that the first step in the fight against akrasia should be to proclaim the establishment of a community dedicated to this fight: the **Anti-Akrasia Alliance**, or **3A** for short. A public commitment will help us to keep attacking the problem.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Commitments are hard to keep (due to akrasia). If we commit to trying specific methods and following various regiments, we *will* often fail. Therefore we should reserve our willpower for just one Big Commitment: that of fighting akrasia by participating in 3A.</p>\n<p>&nbsp;</p>\n<p>The danger of stopping all progress due to akrasia is bigger, in the initial stage, than the danger of failing to find good solutions. We need mechanisms for making and keeping commitments, and for giving these commitments a positive affect.</p>\n<p>&nbsp;</p>\n<p>We should establish a website as a meeting-point. LW has the right format (group blog + wiki), but if this is judged to be off-topic in the Discussion section, I'll set up a similar site somewhere else.</p>\n<p>&nbsp;</p>\n<p>Next, we should briefly examine existing methods and collect suggestions to determine the course of action. Ideally, we should organize the existing knowledge on the subject, analyze members' akrasia case-histories, identify likely theories, and get enough people to run the appropriate experiments.&nbsp;</p>\n<p>&nbsp;</p>\n<p>I believe the application of rationality and the scientific method will, in itself, give us a head start over other groups. We should also be transparent and open to newcomers and to importing new techniques. We should make good use of data collection and analysis (naturally, allowing for anonymity). We should, in short, use all the LW techniques.</p>\n<p>&nbsp;</p>\n<p># The next steps</p>\n<p>&nbsp;</p>\n<p>I would like to hear your reactions in the comments. Do you think there is a better approach? Do you think we're bound to fail because I didn't take problem X into account? Is there a great, rational community fighting akrasia that we should join instead of starting our own?</p>\n<p>&nbsp;</p>\n<p>*Are you with me?*</p>\n<p>&nbsp;</p>\n<p>I commit to posting an update by Jan 5th, which will take into account the comments here. If the consensus is to establish a website (and not use LW) I commit to setting one up by Jan 8th.</p>\n<p>&nbsp;</p>\n<p>## Notes</p>\n<p>&nbsp;</p>\n<p>1. We choose to create possible future worlds. Akrasia destroys our choices.&nbsp;</p>\n<p>2. Personally, I might choose to invest effort in things other than FAI. Anti-akrasia is a necessary meta-tool for humans to achieve all hard goals.</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MHAcF2yAWkKZ8HjRB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T21:52:31.760Z", "modifiedAt": null, "url": null, "title": "Proposal: Anti-Akrasia Alliance", "slug": "proposal-anti-akrasia-alliance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:50.078Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aYGEmJRX3AbjdDK4x/proposal-anti-akrasia-alliance", "pageUrlRelative": "/posts/aYGEmJRX3AbjdDK4x/proposal-anti-akrasia-alliance", "linkUrl": "https://www.lesswrong.com/posts/aYGEmJRX3AbjdDK4x/proposal-anti-akrasia-alliance", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Anti-Akrasia%20Alliance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Anti-Akrasia%20Alliance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaYGEmJRX3AbjdDK4x%2Fproposal-anti-akrasia-alliance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Anti-Akrasia%20Alliance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaYGEmJRX3AbjdDK4x%2Fproposal-anti-akrasia-alliance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaYGEmJRX3AbjdDK4x%2Fproposal-anti-akrasia-alliance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1059, "htmlBody": "<p><em>Related to: <a title=\"Kicking Akrasia: now or never\" href=\"/r/discussion/lw/3ko/kicking_akrasia_now_or_never/\">Kicking Akrasia: now or never</a>; <a title=\"Tsuyoku Naritai\" href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuioku &nbsp;Naritai</a></em></p>\n<h1>The situation</h1>\n<p>I am greatly afficted by akrasia, and in all probability, so are you. Akrasia is a destroyer of worlds.<sup>1</sup></p>\n<p>I have come to the conclusion that akrasia is the single biggest problem I have in life. It is greater than my impending biological death, my imperfect enjoyment of life, or the danger of a car accident.</p>\n<p>For if I could solve the problem of akrasia, I would work on these other problems, and I believe I would solve them too. Even a big problem like physical mortality can be meaningfully challenged if I spend a lifetime tackling it. But until I solve the problem of akrasia, I will sit around and <em>do nothing about my mortality</em>.</p>\n<p><em>(Edited here)&nbsp;</em>Without solving akrasia, we are relatively inefficient in attacking the other problems that matter to us. However, if LW readers - typically smart, rational, luminous, and relatively rich people - were to defeat akrasia and become highly productive, I think we would possess real world-changing power<sup>2</sup>.</p>\n<p>Some people have either solved this problem or never had it. Thus, we know it is possible to vanquish akrasia. However, it is a unique problem that fights its own cure: because of akrasia, we don't spend as much effort as we'd like fighting akrasia.</p>\n<p>I propose forming a community dedicated to fighting akrasia.</p>\n<p><a id=\"more\"></a></p>\n<h1>Existing efforts</h1>\n<p>There have been many posts about akrasia on LW. There are also many methods and workgroups in the world which are dedicated to it. I haven't tried very many myself, and no doubt there are many useful approaches I don't know about. I'm aware of other LW users who have more relevant knowledge and experience than me.</p>\n<p>I do know that there are several common problems with all such efforts:</p>\n<ol>\n<li>Any given method, and even any well-defined compact combination of methods, works for some people but not for others, or works only some of the time.</li>\n<li>When a method does work, it almost always stops working for that person after a while, and can't be used again.</li>\n<li>Most methods have no clear theories of the mechanisms behind them. Those that do, almost always have bad theories, which don't predict why the method sometimes fails, or are untestable just-so stories.</li>\n<li>Most methods have no scientific backing of double-blind experiments, comparisons with other methods, ruling out other explanations, etc.</li>\n<li>Many methods perform no better than the placebo of doing something that doesn't actually work, feeling \"pumped up\", and expecting akrasia to disappear.</li>\n<li>There are many different methods out there, and a person can't test them all to find one that works for them.</li>\n<li>Most existing workshops and groups are formed around a method, rather than around the goal of fighting akrasia, and they apply that method like a hammer to the exclusion of all others.</li>\n<li>None of the methods I have tried so far have helped me, personally.</li>\n</ol><ol> </ol>\n<p>Many of the methods also share a problem of transparency. If a method works for some people, they may publish it widely, via books, websites, and workshops. But there is usually no-one collecting reports of cases where that method failed, investigating them, and publishing updates of the method's expected effectiveness. In other words, there is a large positive selection bias for publication, and third-party reports aren't reviewed and published.</p>\n<p>Finally, there are few organized attempts to collate knowledge of many different methods and test them in combination.</p>\n<h1>Proposal</h1>\n<p>I suggest that the first step in the fight against akrasia should be to proclaim the establishment of a community dedicated to this fight: the <strong>Anti-Akrasia Alliance</strong>, or <strong>3A</strong> for short. A public commitment will help us to keep attacking the problem.&nbsp;</p>\n<p>Commitments are hard to keep (due to akrasia). If we commit to trying specific methods and following various regiments, we <em>will&nbsp;</em>often fail. Therefore we should reserve our willpower for just one Big Commitment: that of fighting akrasia by participating in 3A. I'm not asking anyone to commit to any particular procedure; only to participation in a community effort to solve the problem of akrasia.</p>\n<p>I do have some idea of what to do next, although we may well end up doing something different (and better) instead.</p>\n<p>The danger of stopping all progress due to akrasia is bigger, in the initial stage, than the danger of failing to find good solutions. We need mechanisms for making and keeping commitments, and for giving these commitments a positive affect.</p>\n<p>We should establish a website as a meeting-point. LW has the right format (group blog + wiki), but if this is judged to be off-topic in the Discussion section, I'll set up a similar site somewhere else.</p>\n<p>Next, we should briefly examine existing methods and collect suggestions to determine the course of action. Ideally, we should organize the existing knowledge on the subject, analyze members' akrasia case-histories, identify likely theories, and get enough people to run the appropriate experiments. And then take over the universe, because we have some objections to the way it is currently being run.</p>\n<p>I believe the application of rationality and the scientific method will, in itself, give us a head start over other groups. We should also be transparent and open to newcomers and to importing new techniques. We should make good use of data collection and analysis (naturally, allowing for anonymity). We should, in short, use all the LW techniques.</p>\n<h1>The next steps</h1>\n<p>I would like to hear your reactions in the comments. Do you think there is a better approach? Do you think we're bound to fail because I didn't take problem X into account? Is there a great, rational community fighting akrasia that we should join instead of starting our own?&nbsp;<em>Would you like to join 3A?</em></p>\n<p>If you are one of the lucky few who <em>have</em>&nbsp;beaten akrasia, please join us too. You can probably further goals that we all care about by helping us understand how to do what you did.</p>\n<p>I commit to posting an update by Jan 5th, which will take into account the comments here. If the consensus is to establish a website (and not use LW) I commit to setting one up by Jan 8th.</p>\n<p><strong>Edit:</strong>&nbsp;yeah, right. See <a href=\"/lw/3kt/proposal_antiakrasia_alliance/3cni\">update</a>.</p>\n<h3><span style=\"font-size: 12.5px;\">Notes</span></h3>\n<p>1. We choose to create possible future worlds. Akrasia destroys our choices.&nbsp;</p>\n<p>2. Personally, I might choose to invest effort in things other than FAI. Anti-akrasia is a necessary meta-tool for humans to achieve all hard goals.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aYGEmJRX3AbjdDK4x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 6.627629038202645e-07, "legacy": true, "legacyId": "4637", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Related to: <a title=\"Kicking Akrasia: now or never\" href=\"/r/discussion/lw/3ko/kicking_akrasia_now_or_never/\">Kicking Akrasia: now or never</a>; <a title=\"Tsuyoku Naritai\" href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuioku &nbsp;Naritai</a></em></p>\n<h1 id=\"The_situation\">The situation</h1>\n<p>I am greatly afficted by akrasia, and in all probability, so are you. Akrasia is a destroyer of worlds.<sup>1</sup></p>\n<p>I have come to the conclusion that akrasia is the single biggest problem I have in life. It is greater than my impending biological death, my imperfect enjoyment of life, or the danger of a car accident.</p>\n<p>For if I could solve the problem of akrasia, I would work on these other problems, and I believe I would solve them too. Even a big problem like physical mortality can be meaningfully challenged if I spend a lifetime tackling it. But until I solve the problem of akrasia, I will sit around and <em>do nothing about my mortality</em>.</p>\n<p><em>(Edited here)&nbsp;</em>Without solving akrasia, we are relatively inefficient in attacking the other problems that matter to us. However, if LW readers - typically smart, rational, luminous, and relatively rich people - were to defeat akrasia and become highly productive, I think we would possess real world-changing power<sup>2</sup>.</p>\n<p>Some people have either solved this problem or never had it. Thus, we know it is possible to vanquish akrasia. However, it is a unique problem that fights its own cure: because of akrasia, we don't spend as much effort as we'd like fighting akrasia.</p>\n<p>I propose forming a community dedicated to fighting akrasia.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Existing_efforts\">Existing efforts</h1>\n<p>There have been many posts about akrasia on LW. There are also many methods and workgroups in the world which are dedicated to it. I haven't tried very many myself, and no doubt there are many useful approaches I don't know about. I'm aware of other LW users who have more relevant knowledge and experience than me.</p>\n<p>I do know that there are several common problems with all such efforts:</p>\n<ol>\n<li>Any given method, and even any well-defined compact combination of methods, works for some people but not for others, or works only some of the time.</li>\n<li>When a method does work, it almost always stops working for that person after a while, and can't be used again.</li>\n<li>Most methods have no clear theories of the mechanisms behind them. Those that do, almost always have bad theories, which don't predict why the method sometimes fails, or are untestable just-so stories.</li>\n<li>Most methods have no scientific backing of double-blind experiments, comparisons with other methods, ruling out other explanations, etc.</li>\n<li>Many methods perform no better than the placebo of doing something that doesn't actually work, feeling \"pumped up\", and expecting akrasia to disappear.</li>\n<li>There are many different methods out there, and a person can't test them all to find one that works for them.</li>\n<li>Most existing workshops and groups are formed around a method, rather than around the goal of fighting akrasia, and they apply that method like a hammer to the exclusion of all others.</li>\n<li>None of the methods I have tried so far have helped me, personally.</li>\n</ol><ol> </ol>\n<p>Many of the methods also share a problem of transparency. If a method works for some people, they may publish it widely, via books, websites, and workshops. But there is usually no-one collecting reports of cases where that method failed, investigating them, and publishing updates of the method's expected effectiveness. In other words, there is a large positive selection bias for publication, and third-party reports aren't reviewed and published.</p>\n<p>Finally, there are few organized attempts to collate knowledge of many different methods and test them in combination.</p>\n<h1 id=\"Proposal\">Proposal</h1>\n<p>I suggest that the first step in the fight against akrasia should be to proclaim the establishment of a community dedicated to this fight: the <strong>Anti-Akrasia Alliance</strong>, or <strong>3A</strong> for short. A public commitment will help us to keep attacking the problem.&nbsp;</p>\n<p>Commitments are hard to keep (due to akrasia). If we commit to trying specific methods and following various regiments, we <em>will&nbsp;</em>often fail. Therefore we should reserve our willpower for just one Big Commitment: that of fighting akrasia by participating in 3A. I'm not asking anyone to commit to any particular procedure; only to participation in a community effort to solve the problem of akrasia.</p>\n<p>I do have some idea of what to do next, although we may well end up doing something different (and better) instead.</p>\n<p>The danger of stopping all progress due to akrasia is bigger, in the initial stage, than the danger of failing to find good solutions. We need mechanisms for making and keeping commitments, and for giving these commitments a positive affect.</p>\n<p>We should establish a website as a meeting-point. LW has the right format (group blog + wiki), but if this is judged to be off-topic in the Discussion section, I'll set up a similar site somewhere else.</p>\n<p>Next, we should briefly examine existing methods and collect suggestions to determine the course of action. Ideally, we should organize the existing knowledge on the subject, analyze members' akrasia case-histories, identify likely theories, and get enough people to run the appropriate experiments. And then take over the universe, because we have some objections to the way it is currently being run.</p>\n<p>I believe the application of rationality and the scientific method will, in itself, give us a head start over other groups. We should also be transparent and open to newcomers and to importing new techniques. We should make good use of data collection and analysis (naturally, allowing for anonymity). We should, in short, use all the LW techniques.</p>\n<h1 id=\"The_next_steps\">The next steps</h1>\n<p>I would like to hear your reactions in the comments. Do you think there is a better approach? Do you think we're bound to fail because I didn't take problem X into account? Is there a great, rational community fighting akrasia that we should join instead of starting our own?&nbsp;<em>Would you like to join 3A?</em></p>\n<p>If you are one of the lucky few who <em>have</em>&nbsp;beaten akrasia, please join us too. You can probably further goals that we all care about by helping us understand how to do what you did.</p>\n<p>I commit to posting an update by Jan 5th, which will take into account the comments here. If the consensus is to establish a website (and not use LW) I commit to setting one up by Jan 8th.</p>\n<p><strong>Edit:</strong>&nbsp;yeah, right. See <a href=\"/lw/3kt/proposal_antiakrasia_alliance/3cni\">update</a>.</p>\n<h3 id=\"Notes\"><span style=\"font-size: 12.5px;\">Notes</span></h3>\n<p>1. We choose to create possible future worlds. Akrasia destroys our choices.&nbsp;</p>\n<p>2. Personally, I might choose to invest effort in things other than FAI. Anti-akrasia is a necessary meta-tool for humans to achieve all hard goals.</p>", "sections": [{"title": "The situation", "anchor": "The_situation", "level": 1}, {"title": "Existing efforts", "anchor": "Existing_efforts", "level": 1}, {"title": "Proposal", "anchor": "Proposal", "level": 1}, {"title": "The next steps", "anchor": "The_next_steps", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "35 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nJc6Tt53XiasjogNw", "DoLQN5ryZ9XkZjq5h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-01T22:50:56.697Z", "modifiedAt": null, "url": null, "title": "Choose To Be Happy", "slug": "choose-to-be-happy", "viewCount": null, "lastCommentedAt": "2022-02-09T07:22:05.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vsBw66gQwNXYX6KAK/choose-to-be-happy", "pageUrlRelative": "/posts/vsBw66gQwNXYX6KAK/choose-to-be-happy", "linkUrl": "https://www.lesswrong.com/posts/vsBw66gQwNXYX6KAK/choose-to-be-happy", "postedAtFormatted": "Saturday, January 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Choose%20To%20Be%20Happy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChoose%20To%20Be%20Happy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsBw66gQwNXYX6KAK%2Fchoose-to-be-happy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Choose%20To%20Be%20Happy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsBw66gQwNXYX6KAK%2Fchoose-to-be-happy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsBw66gQwNXYX6KAK%2Fchoose-to-be-happy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 601, "htmlBody": "<p><em>Related to:</em> <a href=\"/lw/3fj/im_scared/\">I'm Scared</a>; <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase utilons and fuzzies separately</a></p>\n<p><em>Expanded from</em>&nbsp;<a href=\"/r/discussion/lw/3fj/im_scared/37mj\">this comment</a>.</p>\n<p>You have awakened as a rationalist, discarded your false beliefs, and updated on new evidence. You understand the dangers of UFAI, you do not look away from death or justify it. You realize your own weakness, and the Vast space of possible failures.</p>\n<p>And understanding all this, you feel bad about it. <em>Very</em> bad, in fact. You are afraid of the dangers of the future, and you are horrified by the huge amounts of suffering. You have shut up and calculated, and the calculation output that you should feel 3^^^3 times as bad as over a stubbed toe. And a stubbed toe can be pretty bad.</p>\n<p>But this reaction of yours is not rational. You should consider the options of <em>choosing not to feel bad</em>&nbsp;about bad things happening, and <em>choosing to feel good</em>&nbsp;no matter what.</p>\n<p><a id=\"more\"></a></p>\n<p>Your bad feelings, whether of fear, empathetic suffering, or something else, are probably counterproductive. Not only do you feel bad - a loss of utility in itself - but such feelings probably hurt, rather than help, your efforts to change the world for the better.</p>\n<p>You may believe that your emotional outlook must be \"rational\": that it must correspond to your conscious estimates of the present or the future. Perhaps you expect to die of old age, or perhaps you are aware of people being tortured in secret prisons. You are forcing your emotions to match the future you foresee, and so you feel unhappy and afraid.</p>\n<p>I suggest that you allow your emotions to become disconnected from your conscious long-term predictions. Stop trying to force yourself to be unhappy because you predict bad things. Say to yourself: I choose to be happy and unafraid no matter what I predict!</p>\n<p>Emotions are not a a tool like rational thought, which you have to use in a way that corresponds to the real world. You can use them in any way you like. It's rational to feel happy about a bleak future, because feeling happy is a good thing and there is no point in feeling unhappy!</p>\n<p>Being happy or not, afraid or not, does not have to be determined by your conscious outlook. The only things that force your mind to be unhappy are things like pain, hunger, loneliness, and the immediate expectation of these. If you accept that your goal is to be happy and unafraid as a fact independent of the future you foresee, you can find various techniques to achieve this.&nbsp;</p>\n<p>Unfortunately such techniques vary for different people. This post doesn't discuss any: it is about the prerequisite decision to be happy.</p>\n<p>Expecting to die of cancer in fifty years does not, in itself, cause negative emotions like fear. Imagining the death in your mind, and dwelling on it, does cause fear. In the first place, avoid thinking about any future problem that you are not doing anything about.&nbsp;</p>\n<p>Use your natural defensive mechanisms, such as of not acknowledging unsolved problems, or compartmentalizing different beliefs. Don't dismiss them as biases or irrational practices. They exist for a good reason and have their proper use.</p>\n<p>This does not mean that you should ignore problems on the conscious level. It is possible to decouple the two things, with practice. You can take long-term strategic actions (donate to SIAI, research immortality) without acutely fearing the result of failure by not imagining that result.</p>\n<p>When you're faced with something terrible and you're not doing anything about it anyway, just look away. Defeat the implicit LW conditioning that tells you looking away from the suffering of others is wrong. It's wrong only if it affects your actions, not your emotions.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 1, "Jzm2mYuuDBCNWq8hi": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vsBw66gQwNXYX6KAK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 29, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "4638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2rDdKiCoeqXmh9zb9", "3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T00:15:08.923Z", "modifiedAt": "2021-07-31T00:56:36.637Z", "url": null, "title": "Working hurts less than procrastinating, we fear the twinge of starting", "slug": "working-hurts-less-than-procrastinating-we-fear-the-twinge", "viewCount": null, "lastCommentedAt": "2021-11-03T10:35:12.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge", "pageUrlRelative": "/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge", "linkUrl": "https://www.lesswrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Working%20hurts%20less%20than%20procrastinating%2C%20we%20fear%20the%20twinge%20of%20starting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorking%20hurts%20less%20than%20procrastinating%2C%20we%20fear%20the%20twinge%20of%20starting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9o3QBg2xJXcRCxGjS%2Fworking-hurts-less-than-procrastinating-we-fear-the-twinge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Working%20hurts%20less%20than%20procrastinating%2C%20we%20fear%20the%20twinge%20of%20starting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9o3QBg2xJXcRCxGjS%2Fworking-hurts-less-than-procrastinating-we-fear-the-twinge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9o3QBg2xJXcRCxGjS%2Fworking-hurts-less-than-procrastinating-we-fear-the-twinge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 870, "htmlBody": "<p>When you procrastinate, you're probably not procrastinating because of <em>the pain of working.</em></p>\n<p>How do I know this?&nbsp; Because on a moment-to-moment basis, <strong>being in the middle of doing the work is usually less painful than being in the middle of procrastinating</strong>.</p>\n<p>(Bolded because it's true, important, and nearly <em>impossible </em>to get your brain to remember - even though a few moments of reflection should convince you that it's true.)</p>\n<p>So what <em>is</em> our brain flinching away from, if not the pain of doing the work?</p>\n<p>I think it's flinching away from the pain of the <em>decision</em> to do the work - the <em>momentary, immediate</em> pain of (1) disengaging yourself from the (probably very small) flow of reinforcement that you're getting from reading a random unimportant Internet article, and (2) paying the energy cost for a prefrontal override to exert control of your own behavior and <em>begin</em> working.</p>\n<p>Thanks to hyperbolic discounting (i.e., weighting values in inverse proportion to their temporal distance) the <em>instant</em> pain of disengaging from an Internet article and paying a prefrontal override cost, can outweigh the <em>slightly more distant</em> (minutes in the future, rather than seconds) pain of continuing to procrastinate, which is, once again, usually more painful than being in the middle of doing the work.</p>\n<p>I think that hyperbolic discounting is far more ubiquitous as a failure mode than I once realized, because it's not just for <em>commensurate-seeming </em>tradeoffs like smoking a cigarette in a minute versus dying of lung cancer later.<a id=\"more\"></a></p>\n<p>When it comes to procrastinating, the obvious, <em>salient</em>, commensurate-seeming tradeoff, is between the (assumed) pleasure of reading a random Internet article now, versus the (assumed) pain of doing the work now.&nbsp; But this, as I said above, is not where I think the real tradeoff is; events that are five minutes away are <em>too distant</em> to dominate the thought process of a hyperbolic discounter like a human.&nbsp; Instead our thought processes are dominated by the prospective <em>immediate</em> pain of a <em>thought</em>, a cost that isn't even salient as something to be traded off.&nbsp; \"Working\" is an obvious, salient event, and \"reading random articles\" seems like an event.&nbsp; But \"paying a small twinge of pain to make the decision to stop procrastinating <em>now</em>, exerting a bit of frontal override, and not getting to read the next paragraph of this random article\" is so map-level that we don't even focus on it as a manipulable territory, a cost to be traded off; it is a <em>transparent</em> thought.</p>\n<p>The real damage done by hyperbolic discounting is for thoughts that are only very slightly painful, and yet, these slight pains being <em>immediate,</em> they manage to dominate everything else in our calculation.&nbsp; And being transparent, we aren't even aware that's what's happening.&nbsp; \"Beware of immediately <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivially painful</a> transparent thoughts\", one might say.</p>\n<p>Similarly, you may read a mediocre book for an hour, instead of a good book, because if you first spent a few minutes to search your library to obtain a <em>better</em> book, that would be an <em>immediate</em> cost - not that searching your library is all that unpleasant, but you'd have to pay an immediate activation cost to do that instead of taking the path of least resistance and grabbing the first thing in front of you.&nbsp; It's a hyperbolically discounted tradeoff that you make without realizing it, because the cost you're refusing to pay isn't <em>commensurate</em> enough with the payoff you're forgoing to be salient <em>as</em> an explicit tradeoff.</p>\n<p>A related note that I might as well dump into this post:&nbsp; I'm starting to think that procrastination by reading random articles does not cause you to <em>rest,</em> that is, you do not regain mental energy from it.&nbsp; <em>Success </em>and <em>happiness</em> cause you to regain willpower; what you need to heal your mind from any damage sustained by working is not inactivity, but reliably solvable problems which reliably deliver experienced jolts of positive reinforcement.&nbsp; Putting in the effort to read a <em>good</em> book may do this; playing a <em>good</em> computer game may do this; reading random Internet articles, or playing bad games, probably won't.&nbsp; Literal mental exhaustion might mean that you don't have enough energy left to read a good book - or that you don't have enough energy left to pay the <em>immediate cost </em>of searching your library for good reading material instead of mediocre reading material - but in this case you shouldn't be reading random online articles.&nbsp; You should be sitting with your eyes closed listening to music, or possibly even napping; if dealing with a truly exhausted brain, reading random articles is probably too <em>much </em>effort.</p>\n<p>If you don't feel good while reading a lot of forgettable online articles, and you don't feel renewed after doing so, your intuitive theory which says that <em>this is how to rest </em>is mistaken, and you need to look for other ways to rest instead - more active ways to regain willpower, less active ways to recover from immediate exhaustion.&nbsp; In general, poor performance often indicates poor models; if something seems incredibly difficult to predict or manipulate, it may be that you have mistaken beliefs about it, including transparent mistakes that are nonquestioned because they are nonsalient.&nbsp; This includes poor performance on the problem of resting.</p>\n<p>Hopefully publishing this post will help me live up to it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dqx5k65wjFfaiJ9sQ": 6, "udPbn9RthmgTtHMiG": 7, "fkABsGCJZ6y9qConW": 2, "r7qAjcbfhj2256EHH": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9o3QBg2xJXcRCxGjS", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 258, "baseScore": 288, "extendedScore": null, "score": 0.000515, "legacy": true, "legacyId": "4639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 288, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 157, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 20, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-01-02T00:15:08.923Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": "qgdGA4ZEyW7zNdK84"}, {"createdAt": null, "postedAt": "2011-01-02T04:01:04.048Z", "modifiedAt": null, "url": null, "title": "Formatting issues", "slug": "formatting-issues", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8au5CYh2gg2XxWEZm/formatting-issues", "pageUrlRelative": "/posts/8au5CYh2gg2XxWEZm/formatting-issues", "linkUrl": "https://www.lesswrong.com/posts/8au5CYh2gg2XxWEZm/formatting-issues", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formatting%20issues&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormatting%20issues%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8au5CYh2gg2XxWEZm%2Fformatting-issues%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formatting%20issues%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8au5CYh2gg2XxWEZm%2Fformatting-issues", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8au5CYh2gg2XxWEZm%2Fformatting-issues", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>On issues I had with [this](http://lesswrong.com/lw/3ku/choose_to_be_happy/39h3) post.</p>\n<p>I was addressing a post which had numbered paragraphs, but wanted to break a reply to one paragraph into several while still being easy to follow. So I titled each with a letter (1.a, 1.b, 1.c.)</p>\n<p>What I got was this:</p>\n<p>1.a.</p>\n<p>1.b. (should be 1.b.)</p>\n<p>1.c. (should be 1.c.)</p>\n<p>2. (should be 2)</p>\n<p>3. (should be 3)</p>\n<p>etc.</p>\n<p>When I tried removing the period between the letter and number (1a, 1b, 1c), what I got was this:</p>\n<p>1a</p>\n<p>1b</p>\n<p>1c</p>\n<p>2. (should be 2, aligned the same)</p>\n<p>3. (should be 3)</p>\n<p>I can't figure out how to fix this. Any ideas?</p>\n<p>&nbsp;</p>\n<p>Less important:</p>\n<p>I started a paragraph with a bracketed quotation [\"like this\"], and included a link later on. When I posted, the quote extended up to the beginning of the first bracket... which apparently I'm unable to reproduce, but in any case</p>\n<p>[\"having a bracketed quotation\"] screws up a [link](http://lesswrong.com/lw/3ku/choose_to_be_happy/39h3) which comes later.</p>\n<p>Fairly easy to fix (swapped the brackets around the quotations for parentheses) but still seems to be a glitch.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8au5CYh2gg2XxWEZm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T09:23:24.961Z", "modifiedAt": null, "url": null, "title": "A Fate Worse Than Death", "slug": "a-fate-worse-than-death", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:25.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "giambolvoe", "createdAt": "2010-12-21T02:41:57.516Z", "isAdmin": false, "displayName": "giambolvoe"}, "userId": "cZKwQNxzBXZCdmttm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/te44R9p6LoWc9mwRA/a-fate-worse-than-death", "pageUrlRelative": "/posts/te44R9p6LoWc9mwRA/a-fate-worse-than-death", "linkUrl": "https://www.lesswrong.com/posts/te44R9p6LoWc9mwRA/a-fate-worse-than-death", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Fate%20Worse%20Than%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Fate%20Worse%20Than%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fte44R9p6LoWc9mwRA%2Fa-fate-worse-than-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Fate%20Worse%20Than%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fte44R9p6LoWc9mwRA%2Fa-fate-worse-than-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fte44R9p6LoWc9mwRA%2Fa-fate-worse-than-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>The claim has been made that, all things being equal, it is better to be alive than dead. &nbsp;I dissent. &nbsp;</p>\n<p>It is much more complicated than this. &nbsp;If I knew somehow that I would spend the next fifty years of my life in Guantanamo bay, I would rather kill myself than suffer that fate. &nbsp;If a fortune teller showed me that I would be in a car crash and lose all sensory input, but would be kept blissfully comatose on cocaine and ecstasy, I would get my affairs in order and end my own life. &nbsp;And yet, if I knew that every day for the next 50 years I would be horribly tortured, but my experience would eliminate suffering from everywhere else in the entire world, I would accept the fate and do my best to steel my mind for the horror that would be my life.</p>\n<p>I want to feel like my existence has purpose. &nbsp;I want to make the world a better place to live in for other people. &nbsp;I want to be happy and experience pleasure. &nbsp;These, not a primordial drive to keep myself alive, are my motivations. &nbsp;Killing myself would be the only rational chice if I knew that my life would be worse than my death. &nbsp;</p>\n<p>I'm not trying to advocate suicide. &nbsp;I'm simply saying that the will to live is not a basic motivating factor for most human beings. &nbsp;So when the argument is made against life extending technology, rather than countering it with \"all things being equal,\" try \"existence being pleasurable...\" &nbsp;But don't claim that existence of sentient beings is inherently good. &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "te44R9p6LoWc9mwRA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 3, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "4642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T09:58:46.996Z", "modifiedAt": null, "url": null, "title": "New Year's Predictions Thread (2011)", "slug": "new-year-s-predictions-thread-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:22.376Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KdxC34w596fMat3pk/new-year-s-predictions-thread-2011", "pageUrlRelative": "/posts/KdxC34w596fMat3pk/new-year-s-predictions-thread-2011", "linkUrl": "https://www.lesswrong.com/posts/KdxC34w596fMat3pk/new-year-s-predictions-thread-2011", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year's%20Predictions%20Thread%20(2011)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year's%20Predictions%20Thread%20(2011)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKdxC34w596fMat3pk%2Fnew-year-s-predictions-thread-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year's%20Predictions%20Thread%20(2011)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKdxC34w596fMat3pk%2Fnew-year-s-predictions-thread-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKdxC34w596fMat3pk%2Fnew-year-s-predictions-thread-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>As we did <a href=\"/lw/1la/new_years_predictions_thread/\">last year</a>, use this thread to make predictions for the next year and next decade, with probabilities attached when practical.&nbsp;</p>\n<p>Happy New Year, Less Wrong!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KdxC34w596fMat3pk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 6.629470852371613e-07, "legacy": true, "legacyId": "4643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 230, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PybtwazftXzvcQSiQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T14:13:51.223Z", "modifiedAt": null, "url": null, "title": "Short fic about uFAI", "slug": "short-fic-about-ufai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:09.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ZPYHpHAxKoETuFrXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JuzXhkm3spN6egzyu/short-fic-about-ufai", "pageUrlRelative": "/posts/JuzXhkm3spN6egzyu/short-fic-about-ufai", "linkUrl": "https://www.lesswrong.com/posts/JuzXhkm3spN6egzyu/short-fic-about-ufai", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Short%20fic%20about%20uFAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShort%20fic%20about%20uFAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJuzXhkm3spN6egzyu%2Fshort-fic-about-ufai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Short%20fic%20about%20uFAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJuzXhkm3spN6egzyu%2Fshort-fic-about-ufai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJuzXhkm3spN6egzyu%2Fshort-fic-about-ufai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>http://andrewhickey.info/2010/12/31/jeeves-and-the-singularity</p>\n<p>Not sure if this is appropriate here, but I thought some of you might find this story I wrote amusing...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JuzXhkm3spN6egzyu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 42, "extendedScore": null, "score": 6.630120921535417e-07, "legacy": true, "legacyId": "4645", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T17:46:47.770Z", "modifiedAt": null, "url": null, "title": "2011 Intrade fee changes, or, Intrade considered no longer useful for LessWrongers", "slug": "2011-intrade-fee-changes-or-intrade-considered-no-longer", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:12.882Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YhZsCdEi6a6G7sSBD/2011-intrade-fee-changes-or-intrade-considered-no-longer", "pageUrlRelative": "/posts/YhZsCdEi6a6G7sSBD/2011-intrade-fee-changes-or-intrade-considered-no-longer", "linkUrl": "https://www.lesswrong.com/posts/YhZsCdEi6a6G7sSBD/2011-intrade-fee-changes-or-intrade-considered-no-longer", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202011%20Intrade%20fee%20changes%2C%20or%2C%20Intrade%20considered%20no%20longer%20useful%20for%20LessWrongers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2011%20Intrade%20fee%20changes%2C%20or%2C%20Intrade%20considered%20no%20longer%20useful%20for%20LessWrongers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhZsCdEi6a6G7sSBD%2F2011-intrade-fee-changes-or-intrade-considered-no-longer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2011%20Intrade%20fee%20changes%2C%20or%2C%20Intrade%20considered%20no%20longer%20useful%20for%20LessWrongers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhZsCdEi6a6G7sSBD%2F2011-intrade-fee-changes-or-intrade-considered-no-longer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhZsCdEi6a6G7sSBD%2F2011-intrade-fee-changes-or-intrade-considered-no-longer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 389, "htmlBody": "<p>For some time now <a href=\"http://www.gwern.net/Prediction%20markets#intrade\">I've traded</a> on Intrade after <a href=\"http://www.gwern.net/Prediction%20markets#summing-up\">some successes</a> on the IEM. Recently, Intrade announced a new fee structure - instead of paying a few cents per trade, one has free trading <em>but</em> your account is charged <a href=\"http://www.intrade.com/jsp/intrade/help/index.jsp?page=general.html%23fees\">$5 every month</a> or $60 a year (see also the <a href=\"http://bb.intrade.com/intradeForum/posts/list/4797.page#44860\">forum announcement</a>).</p>\n<p>Initially, this didn't seem so bad to me, but then I compared the annual cost of this fee to my trading stake, ~$200. I would have to earn a return of 30% just to cover the fee! (This is also pointed out by many in the forum thread above.)</p>\n<p>I don't trade very often since I think I'm best at spotting mispricings over the long-term (the CA <a href=\"https://www.intrade.com/jsp/intrade/common/c_cd.jsp?conDetailID=702407&amp;z=1285870999458\">Proposition 19 contract</a> (<a href=\"http://en.wikipedia.org/wiki/California_Proposition_19_%282010%29\">WP</a>) being a case in point; despite being ultimately correct, I could have been mauled by some of the spikes if I had tried only short-term trades). If this fee had been in place since I joined, I would be down by $30 or $40.</p>\n<p>I'm confident that I can earn a good return like 10 or 20%, but I can't do &gt;30% without taking tremendous risks and wiping myself out. So this new fee structure means that I am basically going to do a little trading in January, since I've already been charged the first $5, and then will cash out at the end.</p>\n<p>And more generally, assuming that this isn't raiding accounts* as a prelude to shutting down (as a number of forumers claim), Intrade is no longer useful for LessWrongers as it is heavily penalizing small long-term bets like the ones we are usually concerned with - bets intended to be educational or informative. It may be time to investigate other prediction markets like <a href=\"http://en.wikipedia.org/wiki/Betfair\">Betfair</a>, or just resign ourselves to non-monetary/play-money sites like <a href=\"http://predictionbook.com/\">PredictionBook.com</a>.</p>\n<p>* When I submitted my withdrawal request for my balance, I received an email offering to instead set my account to 'inactive' status such that I could not trade but would not be charged the fee; if I wanted to trade, I would simply be charged that month's $5. I declined the offer, but I couldn't help wonder - why didn't they simply set all accounts to 'inactive' and then let people opt in to the new fee structure? Or at least set 'inactive' all accounts which have not engaged in any transactions within X months?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6dqPii4cyNpuecLt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YhZsCdEi6a6G7sSBD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 34, "extendedScore": null, "score": 6.630661590219093e-07, "legacy": true, "legacyId": "4646", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T20:28:43.307Z", "modifiedAt": null, "url": null, "title": "In Russian we have the word 'Mirozdanie', which means all that exists", "slug": "in-russian-we-have-the-word-mirozdanie-which-means-all-that", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:16.179Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kononov", "createdAt": "2010-12-17T11:29:25.339Z", "isAdmin": false, "displayName": "kononov"}, "userId": "QqELeHmjAXfyx2iJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iwY4Cv5xzp8p8MPFR/in-russian-we-have-the-word-mirozdanie-which-means-all-that", "pageUrlRelative": "/posts/iwY4Cv5xzp8p8MPFR/in-russian-we-have-the-word-mirozdanie-which-means-all-that", "linkUrl": "https://www.lesswrong.com/posts/iwY4Cv5xzp8p8MPFR/in-russian-we-have-the-word-mirozdanie-which-means-all-that", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Russian%20we%20have%20the%20word%20'Mirozdanie'%2C%20which%20means%20all%20that%20exists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Russian%20we%20have%20the%20word%20'Mirozdanie'%2C%20which%20means%20all%20that%20exists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiwY4Cv5xzp8p8MPFR%2Fin-russian-we-have-the-word-mirozdanie-which-means-all-that%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Russian%20we%20have%20the%20word%20'Mirozdanie'%2C%20which%20means%20all%20that%20exists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiwY4Cv5xzp8p8MPFR%2Fin-russian-we-have-the-word-mirozdanie-which-means-all-that", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiwY4Cv5xzp8p8MPFR%2Fin-russian-we-have-the-word-mirozdanie-which-means-all-that", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\"><span style=\"font-family: Arial; font-size: 9pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">In Russian we have the word 'Mirozdanie', which means all that exists, no matter do we know anything about it or not. The sense of this word includes you and me and every man, and every star, and every planet and all universes and all space (super)&nbsp;civilizations, if exist,&nbsp;and this world and any other worlds, if exist,&nbsp;and so on. Is any English word with the same sense? Or in other words, how correctly and adequately translate the Russian&nbsp;word 'Mirozdanie' in English? I have my web project (in Russian) &ldquo;Dossier on Mirozdanie&rdquo; ( <a href=\"http://www.mirozdanie.narod.ru/\">http://www.mirozdanie.narod.ru</a> ) <span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span>To tell about it in English sometimes I use the word, that Google and other dictionaries recomend - 'Creation' (Creation Dossier), sometimes, <span style=\"font-size: small;\">when I am afraid of being accused of creationism</span>, I use the word &laquo;Multiverse&raquo; (Multiverse Dossier), but I think, every of this words has a great shortage in the context - both of them are based on hypothetical conceptions.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iwY4Cv5xzp8p8MPFR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 6.631072777519134e-07, "legacy": true, "legacyId": "4647", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T21:24:22.983Z", "modifiedAt": null, "url": null, "title": "Article: The Decline Effect and the Scientific Method", "slug": "article-the-decline-effect-and-the-scientific-method", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SLcK4RHjAgeWhWu2W/article-the-decline-effect-and-the-scientific-method", "pageUrlRelative": "/posts/SLcK4RHjAgeWhWu2W/article-the-decline-effect-and-the-scientific-method", "linkUrl": "https://www.lesswrong.com/posts/SLcK4RHjAgeWhWu2W/article-the-decline-effect-and-the-scientific-method", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%3A%20The%20Decline%20Effect%20and%20the%20Scientific%20Method&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%3A%20The%20Decline%20Effect%20and%20the%20Scientific%20Method%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLcK4RHjAgeWhWu2W%2Farticle-the-decline-effect-and-the-scientific-method%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%3A%20The%20Decline%20Effect%20and%20the%20Scientific%20Method%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLcK4RHjAgeWhWu2W%2Farticle-the-decline-effect-and-the-scientific-method", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLcK4RHjAgeWhWu2W%2Farticle-the-decline-effect-and-the-scientific-method", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p><a href=\"http://www.newyorker.com/reporting/2010/12/13/101213fa_fact_lehrer\">The New Yorker article</a></p>\n<p>The <a href=\"http://www.wired.com/wiredscience/2010/12/the-mysterious-decline-effect/\">follow-up</a> on Wired</p>\n<p>An interesting article on how statistical randomness, unconscious confirmation bias, publication bias, and lack of sufficient replication can affect reported data and therefore scientific conclusions. I'm curious as to what LessWrongians think about it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SLcK4RHjAgeWhWu2W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "4648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T22:18:39.293Z", "modifiedAt": null, "url": null, "title": "Externally Oriented, Love, Sensational. Which Rationality will LW be about?", "slug": "externally-oriented-love-sensational-which-rationality-will", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kSWuGmMZuPDWvaWua/externally-oriented-love-sensational-which-rationality-will", "pageUrlRelative": "/posts/kSWuGmMZuPDWvaWua/externally-oriented-love-sensational-which-rationality-will", "linkUrl": "https://www.lesswrong.com/posts/kSWuGmMZuPDWvaWua/externally-oriented-love-sensational-which-rationality-will", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Externally%20Oriented%2C%20Love%2C%20Sensational.%20Which%20Rationality%20will%20LW%20be%20about%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExternally%20Oriented%2C%20Love%2C%20Sensational.%20Which%20Rationality%20will%20LW%20be%20about%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkSWuGmMZuPDWvaWua%2Fexternally-oriented-love-sensational-which-rationality-will%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Externally%20Oriented%2C%20Love%2C%20Sensational.%20Which%20Rationality%20will%20LW%20be%20about%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkSWuGmMZuPDWvaWua%2Fexternally-oriented-love-sensational-which-rationality-will", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkSWuGmMZuPDWvaWua%2Fexternally-oriented-love-sensational-which-rationality-will", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1841, "htmlBody": "<p>&nbsp;</p>\n<p>Short Version: Less Wrong has been trying to address rationality in all domains. This may prove too wide a scope. As it has been suggested, effectiveness changes should come soon. Those who just read the sequences need more a focused walkthrough. A proposed path is tightening the scope <em>within</em> rationality to Externally Oriented, Tech-friendly, H+(Transhuman) posts. Once the sanity waterline within is raised only the highest peaks remain above water, thus what remains must be more focused. How to do this is discussed.<img src=\"/static/tiny_mce/plugins/summarybreak/img/trans.gif\" border=\"0\" alt=\"\" width=\"1\" height=\"1\" align=\"BOTTOM\" /></p>\n<p>&nbsp;</p>\n<p>I'm testing this quote from Less Wrong's&nbsp; <em>Best Of</em>&nbsp; to introduce my post:</p>\n<blockquote>A facility for quotation covers the absence of original thought. -- Dorothy L. Sayers</blockquote>\n<p>&nbsp;</p>\n<p>Garrett Lisi has advanced, in the last minutes of&nbsp; <a href=\"http://www.ted.com/talks/garrett_lisi_on_his_theory_of_everything.html\">this video</a>, the suggestion that life should equally divided between Physics, Love, and Surfing.</p>\n<p>Eliezer Yudkowsky, here, advanced that we buy&nbsp; <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately\">fuzzies and utilons separately</a>.</p>\n<blockquote>Patrissimo, in a very controversial post said that:&nbsp;&nbsp; As Merlin Mann says:&nbsp; <em>\"Joining a Facebook group about creative productivity is like buying a chair about jogging\"</em>. &nbsp; Well, reading a blog to overcome akrasia <strong>IS</strong> joining a Facebook group about creative productivity. \" [...] \" I believe that most people, particularly smart ones, do way too much thinking &amp; talking and way too little action (me included), because that is what's easy for them.\"[...] \"To aid growth at rationality, Less Wrong would have to become a&nbsp; <em>skill&nbsp; practice&nbsp; community</em>,&nbsp; more like martial arts, PUA, and physical fitness, with an explicit focus of&nbsp; <em>helping&nbsp; people grow&nbsp; in&nbsp; their&nbsp; ability&nbsp; to&nbsp; set&nbsp; and&nbsp; achieve&nbsp; goals</em>,&nbsp; combining local chapters with global coordination, infrastructure, and knowledge accumulation.&nbsp; Most discussion should be among people working on a specific skill at a similar level about what is or isn't working for them as they attempt to progress, rather than obscure theories about the inner workings of the human mind.\"</blockquote>\n<p>\"The reasonable man adapts himself to the world. The unreasonable man persists in trying to adapt the world to himself. Therefore, all progress depends on the unreasonable man.\" - George Bernard Shaw</p>\n<p>So I decided to invest my few karma points in advancing an unreasonable suggestion, as a followup to Patrissimo's post.</p>\n<p>First let me abstract from Garrett Lisi's triadic division of life its sweet juice: it is not Physics, Love and Surf that matter for almost everyone, but&nbsp; <em>Externally&nbsp; Oriented , Love&nbsp; </em>and&nbsp; <em>Sensational&nbsp; </em>activities.</p>\n<p><strong>Externally oriented</strong>&nbsp; activities include from musing about fluid mechanics and the Pirah&atilde; language, uptill charity and just plain regular paid work. For most people (not necessarily most Less Wrong people) this will be&nbsp; <em>Other&nbsp; Oriented&nbsp; </em>activities, because most people care more about people than about other beautiful objects that also populate our universe. Both interest in the world-except us and interest in people are wothwhile defensable activities, and we are prone to praising them hardly.</p>\n<p><strong>Love</strong>&nbsp; activities include grooming, socializing, chatting, facebooking, bonding, hugging, watching Sheldon say&nbsp; \"<em>Coitus\"&nbsp; </em>in Big Bang Theory, coitus itself, vibing, being in love, and most things that those of a not-so-loving nature will enjoy reading about in&nbsp; <a href=\"http://www.amazon.com/Endocrinology-Social-Relationships-Peter-Ellison/dp/0674031172\"><em>The Endocrinology of&nbsp; </em>Social Relashionships</a>. Those a little more social who happen to be male will also learn from the PUA community.</p>\n<p><strong>Sensational</strong>&nbsp; activities. I'll quote Susan Greenfield, who directs FHI's cousin Future of the Mind institute, on this one: \"When people go to the club, or take drugs, they are blowing the mind. They are having, you know, a sensational time. And it amazes me that people pay to do this.&nbsp; ... you never say, do you, 'Oh I'm having a cognitive time tonight.' .... People who never let themselves go, like the British, we feel sorry for them, just like we feel sorry for people who never get out of the beach or the bar.\"</p>\n<p>&nbsp;</p>\n<p>I haven't had the complete Less Wrong experience. I just read the Sequences and some 30 other posts. This post requests help from those who did have it. I'll try to speak as a representative of the general class of those who just read the sequences and few other posts:</p>\n<p>&nbsp;</p>\n<p>&ldquo;There is no doubt that the last 20 posts I've read were way less useful than, say, the 30th to 50th. From what I learned here, using techniques of rationality, it follows it is now time for me to move forward, because reading&nbsp; <a href=\"http://intelligence.org/upload/LOGI/\">LOGI</a>,&nbsp; <a href=\"http://omega.albany.edu:8008/JaynesBook.html\">Jaynes</a>, and more technical stuff will drive me faster than remaining here.&nbsp;My beliefs about how less likely the next Less Wrong <em>post</em> is to make me stronger than the Less Wrong <em>suggestion</em> makes me anticipate that I should read the suggested readings, not the newest posts.&rdquo;</p>\n<p>&nbsp;</p>\n<p>(For easier traceability, important suggestions/question paragraphs will be numbered)</p>\n<p>&nbsp;</p>\n<p><strong>(1)&nbsp; </strong>My first suggestion then is that there ought to be an edited sequence of posts from the 2009-2010 era not written by Yudkowsky but designed to be read right after the sequences. The Karma system does not suffice to determine this, an effective rationalist does not want to read the most \"Voted up posts\" but a cohesive collection of material drawn by one or two individuals as the&nbsp; <a href=\"/lw/36/rational_me_or_we\">collectivelly most important material</a>&nbsp; posted,&nbsp; <em>editors</em> <em>assuming&nbsp; that&nbsp; after&nbsp; reading&nbsp; it,&nbsp; people&nbsp; will&nbsp; just&nbsp; leave&nbsp; and&nbsp; be&nbsp; the&nbsp; rationalists&nbsp; they&nbsp; became,&nbsp; </em>hopefully donating units of caring to the causes they learned important.</p>\n<p><strong>(2)&nbsp;</strong> An important question while this is not done then becomes: Which posts should we read now that are not among the topvoted ones? Any help is appreciated.</p>\n<p>&nbsp;</p>\n<p>Now let me get back to&nbsp; <em>Externally&nbsp; Oriented,&nbsp; Love&nbsp; </em>and&nbsp; <em>Sensational</em>&nbsp; activities. Less wrong has been, so far, advancing rational knowledge concerning all three of those. I'm not a fan of compartmentalisation, because it lets evolutionists be religious and not feel their heads itching. But here I will defend compartmentalisation within Less Wrong. If there is a war going on and people are in the UN discussing it, it is counterproductive to also discuss rationality and basketball. If you want to coach a tennis player well, you'd better cut him off every time he starts talking about his girlfriend. If you set out to do the impossible, you'd better turn off the telephone, because interruption is as confusing as flutzpah.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>(3)&nbsp;</strong> Most of Less Wrong is&nbsp; <strong>Externally Oriented</strong>&nbsp; and my first radical suggestion is that this becomes&nbsp; <em>compulsory</em>. Let me tell you why.</p>\n<p>I have read 5 books from the Pick Up Artist community. Not just read. I used them. That thing is really addictive. Enough so to drive people away. Recently the PUA meme has invaded Less Wrong a few times. I have been very worried reading about that here, because, seriously, pretending that PUA belongs here is like discussing basketball in the UN. If we want to create FAI, prevent catastrophic risk, donate as many utilons as possible, and figure out a binary negative personhood determinator, we must stay away from drugs like this and MMOs, Diplomacy, Magic, and some other stuff that has popped up here. These may not be&nbsp; <em>affective death spirals</em>, but they are temporary utilitarian suicides anyway (don't execrate me before knowing that I have spent, securely, 10% of my autonomous life playing Magic). There is stuff out there that is just too attractive, we come to Less Wrong exactly because it pre-selects away from it. Less Wrong is not internet porn, it is the best rationality driving force on the web, and to remain this way, some kinds of Love-related and Sensational-related things, that are almost internet porn, should be kept away</p>\n<p>Even if the Less Wrong individuals would, on a personal individual level, be more leveraged within social rationality, table-game rationality, partner-seducing rationality, the&nbsp; <a href=\"/lw/36/rational_me_or_we\">community level would lose</a>. The time you spend playing Diplomacy and Magic, MMO and reading PUA material is a time which you could be spending doing what has to be done, obtaining all the valuable utilons hanging around. For an emotionally triggering image, immortalists can think that millennia will be lost, and singularitarians can think that galaxies will be lost, this is undesirable. Less wrong must have an internal defense mechanism against this, and these are Patrissimo's and my post which hopefully might drive back a tougher sniper policy on future posts.</p>\n<p>If Less Wrong allows for too wide a rationality scope, it will probably dissolve into a personal social problem solver, and will not be able to dig as deep as necessary for the awesome world Eliezer set out to create. The same risk happens wherever ideas are allowed to evolve, as Bostrom pointed out in <a href=\"http://www.nickbostrom.com/fut/evolution.html\">&ldquo;The Future of Human Evolution</a>&rdquo;. Dangerous viral memes are going through the Less Wrong filters, if they evolve into Less Wrong's main topics, this will be disastrous as a loss of opportunity cost. And worry not, for there are plenty of other places for <strong>Love</strong>-related and <strong>Sensational</strong>-related rationality. Let us instead focus&nbsp; <em>within</em>&nbsp; rationality on <strong>Externally Oriented</strong>, Tech-friendly, H+(Transhuman) posts.</p>\n<p><strong>(4)&nbsp;</strong> My second radical suggestion is that&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\"> <em>inferential&nbsp; distance</em>&nbsp;</a> between long-timers and post-sequence newcomers is becoming too big. One should climb one step at a time. If one is unable, distraction drives attention away. So the suggestion, maybe too radical is: That Less Wrong becomes nomadic.&nbsp; By nomadic I mean that people who have been commenting and posting here for very long should create a new twin website, in which advanced posts will be uploaded, and advanced comments will be traded. If those who have learned most of what is here remain here, they will just be drawn back from their potential. There should be an organised system divided by levels, like in the martial arts, so that people are always interacting with those of similar rational grounds and can build up on common ground without checking for so many biases, fallacies and mistakes.</p>\n<p>&nbsp;</p>\n<p>If it is true that our kind&nbsp; <em>can</em>&nbsp; cooperate, hierarchy is needed, and Karma is not enough (though Karma could be a criterion for posting in each website, eg: 20 here, 400 to the next one 2000 to the next one). A piramid of websites where someone in his layer's top can help those in his layer's bottom just before upgrading is more effective than a place where those who dominate the dojo have to constantly draw attention to stuff that only newcomers miss.</p>\n<p><strong>(5) </strong><span style=\"font-weight: normal;\">Another suggestion, this one of design, is that we avoid scope insensitivity to upvoting by displaying something more emotional than points for pots and comments. A smile symbol with an ever growing smile would be a very easy way to do it, and to copy Nintendo's design, when the number grew too big the smile could blow up into a symbol of a ballon exploding with colorfull carnaval paper. </span><span style=\"font-weight: normal;\">Or something to that effect. </span></p>\n<p>Remember to avoid the unit fallacy, and reject or accept these suggestions separately, not as a single thing.</p>\n<p>And yes, I do notice this takes away great part of the fun you would be feeling while writing your future posts on funny social loving topic X, which you've been meaning for a while to distort just enough to make fit for Less Wrong. But if you are like us, you come here to buy&nbsp; <em>utilons,&nbsp; not&nbsp; fuzzies</em>,&nbsp;and it would be irrational to insist in decreasing the group-level rationality that has been achieved here.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kSWuGmMZuPDWvaWua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -3, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4628", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF", "w9kwayt5SWqBQe8Nx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-02T22:22:41.028Z", "modifiedAt": null, "url": null, "title": "Externally Oriented, Love, Sensational. Which Rationality will LW be about?", "slug": "externally-oriented-love-sensational-which-rationality-will-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qv22u2chuFynT9iJx/externally-oriented-love-sensational-which-rationality-will-0", "pageUrlRelative": "/posts/qv22u2chuFynT9iJx/externally-oriented-love-sensational-which-rationality-will-0", "linkUrl": "https://www.lesswrong.com/posts/qv22u2chuFynT9iJx/externally-oriented-love-sensational-which-rationality-will-0", "postedAtFormatted": "Sunday, January 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Externally%20Oriented%2C%20Love%2C%20Sensational.%20Which%20Rationality%20will%20LW%20be%20about%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExternally%20Oriented%2C%20Love%2C%20Sensational.%20Which%20Rationality%20will%20LW%20be%20about%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqv22u2chuFynT9iJx%2Fexternally-oriented-love-sensational-which-rationality-will-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Externally%20Oriented%2C%20Love%2C%20Sensational.%20Which%20Rationality%20will%20LW%20be%20about%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqv22u2chuFynT9iJx%2Fexternally-oriented-love-sensational-which-rationality-will-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqv22u2chuFynT9iJx%2Fexternally-oriented-love-sensational-which-rationality-will-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1841, "htmlBody": "<p>&nbsp;</p>\n<p>Short Version: Less Wrong has been trying to address rationality in all domains. This may prove too wide a scope. As it has been suggested, effectiveness changes should come soon. Those who just read the sequences need more a focused walkthrough. A proposed path is tightening the scope <em>within</em> rationality to Externally Oriented, Tech-friendly, H+(Transhuman) posts. Once the sanity waterline within is raised only the highest peaks remain above water, thus what remains must be more focused. How to do this is discussed.<a id=\"more\"></a></p>\n<p>I'm testing this quote from Less Wrong's&nbsp; <em>Best Of</em>&nbsp; to introduce my post:</p>\n<blockquote>A facility for quotation covers the absence of original thought. -- Dorothy L. Sayers</blockquote>\n<p>&nbsp;</p>\n<p>Garrett Lisi has advanced, in the last minutes of&nbsp; <a href=\"http://www.ted.com/talks/garrett_lisi_on_his_theory_of_everything.html\">this video</a>, the suggestion that life should equally divided between Physics, Love, and Surfing.</p>\n<p>Eliezer Yudkowsky, here, advanced that we buy&nbsp; <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately\">fuzzies and utilons separately</a>.</p>\n<blockquote>Patrissimo, in a very controversial post said that:&nbsp;&nbsp; As Merlin Mann says:&nbsp; <em>\"Joining a Facebook group about creative productivity is like buying a chair about jogging\"</em>. &nbsp; Well, reading a blog to overcome akrasia <strong>IS</strong> joining a Facebook group about creative productivity. \" [...] \" I believe that most people, particularly smart ones, do way too much thinking &amp; talking and way too little action (me included), because that is what's easy for them.\"[...] \"To aid growth at rationality, Less Wrong would have to become a&nbsp; <em>skill&nbsp; practice&nbsp; community</em>,&nbsp; more like martial arts, PUA, and physical fitness, with an explicit focus of&nbsp; <em>helping&nbsp; people grow&nbsp; in&nbsp; their&nbsp; ability&nbsp; to&nbsp; set&nbsp; and&nbsp; achieve&nbsp; goals</em>,&nbsp; combining local chapters with global coordination, infrastructure, and knowledge accumulation.&nbsp; Most discussion should be among people working on a specific skill at a similar level about what is or isn't working for them as they attempt to progress, rather than obscure theories about the inner workings of the human mind.\"</blockquote>\n<p>\"The reasonable man adapts himself to the world. The unreasonable man persists in trying to adapt the world to himself. Therefore, all progress depends on the unreasonable man.\" - George Bernard Shaw</p>\n<p>So I decided to invest my few karma points in advancing an unreasonable suggestion, as a followup to Patrissimo's post.</p>\n<p>First let me abstract from Garrett Lisi's triadic division of life its sweet juice: it is not Physics, Love and Surf that matter for almost everyone, but&nbsp; <em>Externally&nbsp; Oriented , Love&nbsp; </em>and&nbsp; <em>Sensational&nbsp; </em>activities.</p>\n<p><strong>Externally oriented</strong>&nbsp; activities include from musing about fluid mechanics and the Pirah&atilde; language, uptill charity and just plain regular paid work. For most people (not necessarily most Less Wrong people) this will be&nbsp; <em>Other&nbsp; Oriented&nbsp; </em>activities, because most people care more about people than about other beautiful objects that also populate our universe. Both interest in the world-except us and interest in people are wothwhile defensable activities, and we are prone to praising them hardly.</p>\n<p><strong>Love</strong>&nbsp; activities include grooming, socializing, chatting, facebooking, bonding, hugging, watching Sheldon say&nbsp; \"<em>Coitus\"&nbsp; </em>in Big Bang Theory, coitus itself, vibing, being in love, and most things that those of a not-so-loving nature will enjoy reading about in&nbsp; <a href=\"http://www.amazon.com/Endocrinology-Social-Relationships-Peter-Ellison/dp/0674031172\"><em>The Endocrinology of&nbsp; </em>Social Relashionships</a>. Those a little more social who happen to be male will also learn from the PUA community.</p>\n<p><strong>Sensational</strong>&nbsp; activities. I'll quote Susan Greenfield, who directs FHI's cousin Future of the Mind institute, on this one: \"When people go to the club, or take drugs, they are blowing the mind. They are having, you know, a sensational time. And it amazes me that people pay to do this.&nbsp; ... you never say, do you, 'Oh I'm having a cognitive time tonight.' .... People who never let themselves go, like the British, we feel sorry for them, just like we feel sorry for people who never get out of the beach or the bar.\"</p>\n<p>&nbsp;</p>\n<p>I haven't had the complete Less Wrong experience. I just read the Sequences and some 30 other posts. This post requests help from those who did have it. I'll try to speak as a representative of the general class of those who just read the sequences and few other posts:</p>\n<p>&nbsp;</p>\n<p>&ldquo;There is no doubt that the last 20 posts I've read were way less useful than, say, the 30th to 50th. From what I learned here, using techniques of rationality, it follows it is now time for me to move forward, because reading&nbsp; <a href=\"http://intelligence.org/upload/LOGI/\">LOGI</a>,&nbsp; <a href=\"http://omega.albany.edu:8008/JaynesBook.html\">Jaynes</a>, and more technical stuff will drive me faster than remaining here.&nbsp;My beliefs about how less likely the next Less Wrong <em>post</em> is to make me stronger than the Less Wrong <em>suggestion</em> makes me anticipate that I should read the suggested readings, not the newest posts.&rdquo;</p>\n<p>&nbsp;</p>\n<p>(For easier traceability, important suggestions/question paragraphs will be numbered)</p>\n<p>&nbsp;</p>\n<p><strong>(1)&nbsp; </strong>My first suggestion then is that there ought to be an edited sequence of posts from the 2009-2010 era not written by Yudkowsky but designed to be read right after the sequences. The Karma system does not suffice to determine this, an effective rationalist does not want to read the most \"Voted up posts\" but a cohesive collection of material drawn by one or two individuals as the&nbsp; <a href=\"/lw/36/rational_me_or_we\">collectivelly most important material</a>&nbsp; posted,&nbsp; <em>editors</em> <em>assuming&nbsp; that&nbsp; after&nbsp; reading&nbsp; it,&nbsp; people&nbsp; will&nbsp; just&nbsp; leave&nbsp; and&nbsp; be&nbsp; the&nbsp; rationalists&nbsp; they&nbsp; became,&nbsp; </em>hopefully donating units of caring to the causes they learned important.</p>\n<p><strong>(2)&nbsp;</strong> An important question while this is not done then becomes: Which posts should we read now that are not among the topvoted ones? Any help is appreciated.</p>\n<p>&nbsp;</p>\n<p>Now let me get back to&nbsp; <em>Externally&nbsp; Oriented,&nbsp; Love&nbsp; </em>and&nbsp; <em>Sensational</em>&nbsp; activities. Less wrong has been, so far, advancing rational knowledge concerning all three of those. I'm not a fan of compartmentalisation, because it lets evolutionists be religious and not feel their heads itching. But here I will defend compartmentalisation within Less Wrong. If there is a war going on and people are in the UN discussing it, it is counterproductive to also discuss rationality and basketball. If you want to coach a tennis player well, you'd better cut him off every time he starts talking about his girlfriend. If you set out to do the impossible, you'd better turn off the telephone, because interruption is as confusing as flutzpah.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>(3)&nbsp;</strong> Most of Less Wrong is&nbsp; <strong>Externally Oriented</strong>&nbsp; and my first radical suggestion is that this becomes&nbsp; <em>compulsory</em>. Let me tell you why.</p>\n<p>I have read 5 books from the Pick Up Artist community. Not just read. I used them. That thing is really addictive. Enough so to drive people away. Recently the PUA meme has invaded Less Wrong a few times. I have been very worried reading about that here, because, seriously, pretending that PUA belongs here is like discussing basketball in the UN. If we want to create FAI, prevent catastrophic risk, donate as many utilons as possible, and figure out a binary negative personhood determinator, we must stay away from drugs like this and MMOs, Diplomacy, Magic, and some other stuff that has popped up here. These may not be&nbsp; <em>affective death spirals</em>, but they are temporary utilitarian suicides anyway (don't execrate me before knowing that I have spent, securely, 10% of my autonomous life playing Magic). There is stuff out there that is just too attractive, we come to Less Wrong exactly because it pre-selects away from it. Less Wrong is not internet porn, it is the best rationality driving force on the web, and to remain this way, some kinds of Love-related and Sensational-related things, that are almost internet porn, should be kept away</p>\n<p>Even if the Less Wrong individuals would, on a personal individual level, be more leveraged within social rationality, table-game rationality, partner-seducing rationality, the&nbsp; <a href=\"/lw/36/rational_me_or_we\">community level would lose</a>. The time you spend playing Diplomacy and Magic, MMO and reading PUA material is a time which you could be spending doing what has to be done, obtaining all the valuable utilons hanging around. For an emotionally triggering image, immortalists can think that millennia will be lost, and singularitarians can think that galaxies will be lost, this is undesirable. Less wrong must have an internal defense mechanism against this, and these are Patrissimo's and my post which hopefully might drive back a tougher sniper policy on future posts.</p>\n<p>If Less Wrong allows for too wide a rationality scope, it will probably dissolve into a personal social problem solver, and will not be able to dig as deep as necessary for the awesome world Eliezer set out to create. The same risk happens wherever ideas are allowed to evolve, as Bostrom pointed out in <a href=\"http://www.nickbostrom.com/fut/evolution.html\">&ldquo;The Future of Human Evolution</a>&rdquo;. Dangerous viral memes are going through the Less Wrong filters, if they evolve into Less Wrong's main topics, this will be disastrous as a loss of opportunity cost. And worry not, for there are plenty of other places for <strong>Love</strong>-related and <strong>Sensational</strong>-related rationality. Let us instead focus&nbsp; <em>within</em>&nbsp; rationality on <strong>Externally Oriented</strong>, Tech-friendly, H+(Transhuman) posts.</p>\n<p><strong>(4)&nbsp;</strong> My second radical suggestion is that&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\"> <em>inferential&nbsp; distance</em>&nbsp;</a> between long-timers and post-sequence newcomers is becoming too big. One should climb one step at a time. If one is unable, distraction drives attention away. So the suggestion, maybe too radical is: That Less Wrong becomes nomadic.&nbsp; By nomadic I mean that people who have been commenting and posting here for very long should create a new twin website, in which advanced posts will be uploaded, and advanced comments will be traded. If those who have learned most of what is here remain here, they will just be drawn back from their potential. There should be an organised system divided by levels, like in the martial arts, so that people are always interacting with those of similar rational grounds and can build up on common ground without checking for so many biases, fallacies and mistakes.</p>\n<p>&nbsp;</p>\n<p>If it is true that our kind&nbsp; <em>can</em>&nbsp; cooperate, hierarchy is needed, and Karma is not enough (though Karma could be a criterion for posting in each website, eg: 20 here, 400 to the next one 2000 to the next one). A piramid of websites where someone in his layer's top can help those in his layer's bottom just before upgrading is more effective than a place where those who dominate the dojo have to constantly draw attention to stuff that only newcomers miss.</p>\n<p><strong>(5) </strong><span style=\"font-weight: normal;\">Another suggestion, this one of design, is that we avoid scope insensitivity to upvoting by displaying something more emotional than points for pots and comments. A smile symbol with an ever growing smile would be a very easy way to do it, and to copy Nintendo's design, when the number grew too big the smile could blow up into a symbol of a ballon exploding with colorfull carnaval paper. </span><span style=\"font-weight: normal;\">Or something to that effect. </span></p>\n<p>Remember to avoid the unit fallacy, and reject or accept these suggestions separately, not as a single thing.</p>\n<p>And yes, I do notice this takes away great part of the fun you would be feeling while writing your future posts on funny social loving topic X, which you've been meaning for a while to distort just enough to make fit for Less Wrong. But if you are like us, you come here to buy&nbsp; <em>utilons,&nbsp; not&nbsp; fuzzies</em>,&nbsp;and it would be irrational to insist in decreasing the group-level rationality that has been achieved here.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qv22u2chuFynT9iJx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -2, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF", "w9kwayt5SWqBQe8Nx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-03T05:24:50.403Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: January 2011", "slug": "rationality-quotes-january-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/77yNaq2HsMxhZWeXX/rationality-quotes-january-2011", "pageUrlRelative": "/posts/77yNaq2HsMxhZWeXX/rationality-quotes-january-2011", "linkUrl": "https://www.lesswrong.com/posts/77yNaq2HsMxhZWeXX/rationality-quotes-january-2011", "postedAtFormatted": "Monday, January 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20January%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20January%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77yNaq2HsMxhZWeXX%2Frationality-quotes-january-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20January%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77yNaq2HsMxhZWeXX%2Frationality-quotes-january-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77yNaq2HsMxhZWeXX%2Frationality-quotes-january-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Post quotes.</p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts from LW. (If you want to exclude OB too create your own quotes thread! OB is entertaining and insightful and all but it is no rationality blog!)</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "77yNaq2HsMxhZWeXX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 4, "extendedScore": null, "score": 6.63243449605005e-07, "legacy": true, "legacyId": "4656", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 275, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-03T08:48:45.020Z", "modifiedAt": null, "url": null, "title": "Asking Precise Questions", "slug": "asking-precise-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:35.745Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y4MszvTFQ2qLCtYLT/asking-precise-questions", "pageUrlRelative": "/posts/Y4MszvTFQ2qLCtYLT/asking-precise-questions", "linkUrl": "https://www.lesswrong.com/posts/Y4MszvTFQ2qLCtYLT/asking-precise-questions", "postedAtFormatted": "Monday, January 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Asking%20Precise%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsking%20Precise%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4MszvTFQ2qLCtYLT%2Fasking-precise-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Asking%20Precise%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4MszvTFQ2qLCtYLT%2Fasking-precise-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4MszvTFQ2qLCtYLT%2Fasking-precise-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p>Isaac Asimov once described a future in which all technical thought was automated, and the role of humans was reduced to finding appropriate questions to pose to thinking machines. I wouldn't suggest planning for this eventuality, but it struck me as an interesting situation. What would we do, if we could get the answer to any question we could formulate precisely? (In the story, questions didn't need to be formulated precisely, but nevermind.) For concreteness, suppose that we have a box as smart as a million Einsteins, cooperating effectively for a century every time we ask a question, but which is capable only of solving precisely specified problems.</p>\n<p>You can't say \"analyze the result of this experiment.\" You can say, \"find me the setting for these 10 parameters which best explains this data\" or \"write me a short program which predicts this data.\" You can't say \"find me a program that plays Go well.\" You can say, \"find me a program that beats this particular Go ai, even with a 9 stone handicap.\" Etc. More formally, lets say you can specify any scoring program and ask the box to find an input that scores as well as it can.</p>\n<p>What would you do, if you got exactly one question? I don't think humanity is posed to get any earth-shattering insights. I don't think we could find a theory of everything, or a friendly AI, or any sort of AI at all, or a solution to any real problem facing us, using just one question. But maybe that is just a failure of my creativity.</p>\n<p>What would you plan to do, if you had unlimited access?&nbsp; An AGI or brain emulation arguably implicitly converts our vague real world objectives into a precise form. Are there other ways to bridge the gap between what humans can formally describe and what humans want? Can you bootstrap your way there starting from current understanding? What is any reasonable first step?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y4MszvTFQ2qLCtYLT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 6.632951276224282e-07, "legacy": true, "legacyId": "4659", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-03T17:16:06.414Z", "modifiedAt": null, "url": null, "title": "Zendo-like Induction Game for Playing Online", "slug": "zendo-like-induction-game-for-playing-online", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:04.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Normal_Anomaly", "createdAt": "2010-11-14T03:31:54.691Z", "isAdmin": false, "displayName": "Normal_Anomaly"}, "userId": "WgGYj5bqcZKsFNG6F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n3MMA2cA9Rapi6TBt/zendo-like-induction-game-for-playing-online", "pageUrlRelative": "/posts/n3MMA2cA9Rapi6TBt/zendo-like-induction-game-for-playing-online", "linkUrl": "https://www.lesswrong.com/posts/n3MMA2cA9Rapi6TBt/zendo-like-induction-game-for-playing-online", "postedAtFormatted": "Monday, January 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zendo-like%20Induction%20Game%20for%20Playing%20Online&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZendo-like%20Induction%20Game%20for%20Playing%20Online%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn3MMA2cA9Rapi6TBt%2Fzendo-like-induction-game-for-playing-online%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zendo-like%20Induction%20Game%20for%20Playing%20Online%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn3MMA2cA9Rapi6TBt%2Fzendo-like-induction-game-for-playing-online", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn3MMA2cA9Rapi6TBt%2Fzendo-like-induction-game-for-playing-online", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p class=\"MsoNormal\">I recently encountered the inductive logic game <a href=\"(http://en.wikipedia.org/wiki/Zendo_%28game%29)\">Zendo</a> <sup>1</sup>, and it looks like a great game for aspiring rationalists. I&rsquo;d like to play a game like it among the people on this forum. However, Zendo is very visual, with an emphasis on color and position of pieces, so it would be difficult to play over the Internet. So I&rsquo;ve adapted the concept and rules of Zendo to a format that can be played in an LW thread.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">The basic premise of Zendo is that of trying to figure out a rule by observing which entities do or do not comply with it. One player, the &ldquo;Master,&rdquo; devises a rule and presents one structure that does and one that does not comply. In the online version, these are strings of 1-digit numbers. For instance, the rule might be &ldquo;at least 2 of the digits are odd.&rdquo; Then &ldquo;2 5 1 0 &rdquo; would satisfy the rule, and &ldquo;8 3 2&rdquo; would not. The rule can be any level of complexity and may concern anything about the numbers, like &ldquo;The string must contain at least 4 digits&rdquo; or &ldquo;The sum of the first two digits must be greater than the sum of the last two&rdquo; or whatever. It may not concern anything other than the string, such as the timestamp of the post.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Players take turns by proposing a string in reply to the comment with the first two strings, and saying either &ldquo;Master&rdquo; or &ldquo;Mondo&rdquo;. If the player says &ldquo;Master,&rdquo; the Master will reply to the comment containing the string and say &ldquo;yes&rdquo; if it complies with the rule and &ldquo;no&rdquo; if it does not. If the player says &ldquo;Mondo,&rdquo; all players have some amount of time to post a reply guessing whether or not the string will meet the rule before the Master judges it. Currently this time is 8 hours so everyone will have time to see it, but if this seems too long it can be shortened. Traditionally this is done by concealing a colored stone in one&rsquo;s hand with nobody seeing anybody else&rsquo;s guess until time is up, so if you want post your guess in rot13 surrounded by gobbledygook (e.g. fyqxwsfyxqwstfyxlrffyqxwsyfqxwtsfy). Please don&rsquo;t read other people&rsquo;s rot13. At the end of the time period, the Master will judge the string and give everyone who guessed correctly a guessing point.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">At the end of your turn, after your string has been judged, you may spend one guessing point to reply to the &ldquo;Guess the rule here.&rdquo; comment and guess the rule. The Master may ask clarifying questions about ambiguities in the guess. When the Master is satisfied that e understands the guess, e will either pronounce the guesser the winner or provide a string satisfying the guess but not the true rule or vice versa. The guesser may guess until e wins or runs out of guessing points.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">I&rsquo;ve decided an initial rule, and started a game in the comments. To join the game, reply to the comment containing the first two strings. To comment on the merits or problems of the game, say that this shouldn&rsquo;t have been posted, ask questions, or suggest changes to the rules, reply to &ldquo;Meta stuff goes here.&rdquo;</p>\n<p class=\"MsoNormal\">1: The link is broken. It's supposed to go to http://en.wikipedia.org/wiki/Zendo_%28game%29 .</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong>EDIT: The first game was fun, but time differences made it a bit chaotic. The second game will be attempted on IRC chat. To join, suggest a time, or volunteer to be Master, post a comment to that effect.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n3MMA2cA9Rapi6TBt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "4660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-04T01:43:52.981Z", "modifiedAt": null, "url": null, "title": "Goals vs. Rewards", "slug": "goals-vs-rewards", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:09.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "icebrand", "createdAt": "2011-01-01T23:09:36.718Z", "isAdmin": false, "displayName": "icebrand"}, "userId": "kmxdLtihKH3nap92F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tAqges46JwgdW5qJT/goals-vs-rewards", "pageUrlRelative": "/posts/tAqges46JwgdW5qJT/goals-vs-rewards", "linkUrl": "https://www.lesswrong.com/posts/tAqges46JwgdW5qJT/goals-vs-rewards", "postedAtFormatted": "Tuesday, January 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Goals%20vs.%20Rewards&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoals%20vs.%20Rewards%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAqges46JwgdW5qJT%2Fgoals-vs-rewards%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Goals%20vs.%20Rewards%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAqges46JwgdW5qJT%2Fgoals-vs-rewards", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAqges46JwgdW5qJT%2Fgoals-vs-rewards", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 683, "htmlBody": "<p>Related: <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values and Instrumental Values</a>, <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">Applying behavioral psychology on myself</a></p>\n<p>Recently I asked myself, <em>what do I want</em>? My immediate response was that I wanted to be less stressed, particularly for financial reasons. So I started to affirm to myself that my goal was to become wealthy, and also to become less stressed. But then in a fit of cognitive dissonance, I realized that both money and relaxation are most easily considered in terms of being <em>rewards</em>, not <em>goals</em>. I was oddly surprised by the fact that there is a distinction between the two concepts to begin with.</p>\n<p>It later occurred to me to wonder if some things work better when framed as goals and not as rewards. Freedom, long life, good relationships, and productivity seemed some likely candidates. I can't quite see them as rewards because a) I feel everyone innately deserves and should have them (even though they might have to work for them), and b) they don't quite give the kind of fuzzies that motivate immediate action.</p>\n<p>These two kinds of positive motivation seem to work in psychologically dissimilar ways.&nbsp; Money for example is more like chocolate, something one has immediate instinctive motive to obtain and consume. Freedom of speech is more along the lines of having enough air to breathe. A person needs and perhaps inherently deserves to have at least a little bit of it all the time, and as a general rule will have a constant background motive to ensure that it stays available. It's a longer-term form of motivation.</p>\n<p>A reward seems to be something where you receive immediate fuzzies when you achieve it. Getting paid, getting a pat on the back, getting your posts and comments upvoted... Things where you might consider them more or less optional in the grander scheme of things, yet they tend to trigger an immediate sense of positive anticipation before the event which is reinforced by a sense of satisfaction after. Actually writing a good post or comment, actually doing a good job, being a good spouse or friend -- these are surely related,  but are <em>goals</em> in and of themselves. The mental picture for a goal is one of achieving, as opposed to receiving.</p>\n<p>One thing that seems likely to me is that the presence of shared goals (and the communication thereof) tends to a good way to generate long term social bonds. Rewards seem to be more of a good way to deliberately steer behavior in more specific aspects. Both are thus important elements of social signaling within a tribe, but serve different underlying purposes.</p>\n<p>As an example I have the transhumanist goal of eliminating the current limitations of the human lifespan, and tend to have an affinity for people who also internalize that goal. But someone who does not embrace that goal on a deep level may still display specific behavior that I consider helpful for that goal, e.g. displaying comprehension of its internal logic or having a tolerant attitude towards actions I think need to be taken. I'm probably somewhat less likely to form a long-term relationship with that person than if they were identifiable as a fellow transhumanist, but I am still likely to upvote their comments or otherwise signal approval in ways that don't demand too much long term commitment.</p>\n<p>The distinctions I've drawn here between a goal and a reward might not apply directly to non-human intelligences. In fact it might be misleading in the more generalized context to call a reward something other than a goal (it is at least an implicit goal or value). However the distinction still seems like something that could be relevant for instrumental rationality and personal development. Our brains process the two forms of motivational anticipation in different ways. It may be that a part of the akrasia problem -- failure to take action towards a goal -- actually relates to a failure to properly categorize a given motive, and hence failure to process it usefully.</p>\n<hr />\n<p><em>Thanks to the early commenters for their feedback: TheOtherDave, nornagest, endoself, David Gerard, nazgulnarsil, and Normal Anomaly. Hopefully this expanded version is more clear.<br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tAqges46JwgdW5qJT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 6.635532658412817e-07, "legacy": true, "legacyId": "4667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n5ucT5ZbPdhfGNLtP", "EJuZcWnk8j7eNQPqq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-04T03:11:33.413Z", "modifiedAt": null, "url": null, "title": "When you are your own favorite charity", "slug": "when-you-are-your-own-favorite-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:15.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TtcdXSvR2p4gghbH3/when-you-are-your-own-favorite-charity", "pageUrlRelative": "/posts/TtcdXSvR2p4gghbH3/when-you-are-your-own-favorite-charity", "linkUrl": "https://www.lesswrong.com/posts/TtcdXSvR2p4gghbH3/when-you-are-your-own-favorite-charity", "postedAtFormatted": "Tuesday, January 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20you%20are%20your%20own%20favorite%20charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20you%20are%20your%20own%20favorite%20charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtcdXSvR2p4gghbH3%2Fwhen-you-are-your-own-favorite-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20you%20are%20your%20own%20favorite%20charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtcdXSvR2p4gghbH3%2Fwhen-you-are-your-own-favorite-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtcdXSvR2p4gghbH3%2Fwhen-you-are-your-own-favorite-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>If you think that SIAI is the best charity, and you work for SIAI, should you give any money to charity?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TtcdXSvR2p4gghbH3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 6.635755576976477e-07, "legacy": true, "legacyId": "4676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-04T03:36:22.094Z", "modifiedAt": null, "url": null, "title": "AGI 2010 videos - link", "slug": "agi-2010-videos-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LpAR5L45PgfqrX8kd/agi-2010-videos-link", "pageUrlRelative": "/posts/LpAR5L45PgfqrX8kd/agi-2010-videos-link", "linkUrl": "https://www.lesswrong.com/posts/LpAR5L45PgfqrX8kd/agi-2010-videos-link", "postedAtFormatted": "Tuesday, January 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AGI%202010%20videos%20-%20link&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAGI%202010%20videos%20-%20link%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLpAR5L45PgfqrX8kd%2Fagi-2010-videos-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AGI%202010%20videos%20-%20link%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLpAR5L45PgfqrX8kd%2Fagi-2010-videos-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLpAR5L45PgfqrX8kd%2Fagi-2010-videos-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://vimeo.com/channels/agi10</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LpAR5L45PgfqrX8kd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 6.63581866456571e-07, "legacy": true, "legacyId": "4678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-04T06:14:31.288Z", "modifiedAt": null, "url": null, "title": "Applied Optimal Philanthropy: How to Donate $100 to SIAI for Free", "slug": "applied-optimal-philanthropy-how-to-donate-usd100-to-siai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:00.831Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AjbtpQqbJz4t58LX3/applied-optimal-philanthropy-how-to-donate-usd100-to-siai", "pageUrlRelative": "/posts/AjbtpQqbJz4t58LX3/applied-optimal-philanthropy-how-to-donate-usd100-to-siai", "linkUrl": "https://www.lesswrong.com/posts/AjbtpQqbJz4t58LX3/applied-optimal-philanthropy-how-to-donate-usd100-to-siai", "postedAtFormatted": "Tuesday, January 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Optimal%20Philanthropy%3A%20How%20to%20Donate%20%24100%20to%20SIAI%20for%20Free&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Optimal%20Philanthropy%3A%20How%20to%20Donate%20%24100%20to%20SIAI%20for%20Free%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAjbtpQqbJz4t58LX3%2Fapplied-optimal-philanthropy-how-to-donate-usd100-to-siai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Optimal%20Philanthropy%3A%20How%20to%20Donate%20%24100%20to%20SIAI%20for%20Free%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAjbtpQqbJz4t58LX3%2Fapplied-optimal-philanthropy-how-to-donate-usd100-to-siai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAjbtpQqbJz4t58LX3%2Fapplied-optimal-philanthropy-how-to-donate-usd100-to-siai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<p>If I gave you $50 you hadn't planned on receiving, would you consider giving it to <a href=\"/lw/37f/efficient_charity\">charity</a>?</p>\n<p>&nbsp;</p>\n<p>Here's your chance to find out.</p>\n<p>&nbsp;</p>\n<p>Just in time for the <a href=\"http://intelligence.org/tallinn-evans_challenge\">Tallin-Evans matching fundraiser</a>, ING Direct has started offering <a href=\"http://www.ingdirect.com/nofees/ \">a free $50 cash sign-up bonus</a>.&nbsp; I've personally used ING for 10 years and referred over 20 people to similar promotions of theirs in the past so I can confirm that this is legit.<sup>1</sup></p>\n<p>&nbsp;</p>\n<p>It's a simple, effective way to get started as an optimal philanthropist for free:</p>\n<p style=\"padding-left: 90px;\">&nbsp;</p>\n<ol style=\"padding-left: 60px;\">\n<li><a href=\"http://www.ingdirect.com/nofees/ \">Get $50 for free</a></li>\n<br />\n<li><a href=\"http://intelligence.org/tallinn-evans_challenge\">Donate $50 --&gt; turns into $100</a></li>\n<br />\n<li>Profit!<sup>2</sup></li>\n</ol>\n<p style=\"padding-left: 60px;\">&nbsp;</p>\n<p>&nbsp;</p>\n<p>Full disclosure: I was an <a href=\"http://intelligence.org/aboutus/visitingfellows\">SIAI Visiting Fellow</a> in 2010.&nbsp; I've also used ING Direct as a customer the past 10 years, but otherwise have no financial interest in them.<a id=\"more\"></a></p>\n<hr />\n<p>&nbsp;</p>\n<p>[1] This isn't one of those bogus \"intro\" deals where you have to make sure you cancel the service later on or risk getting charged fees.&nbsp; ING has no fees, no minimum balance requirements, no sleazy marketing emails, and consistently good savings rates.&nbsp; If you want to use them for their good service after signing up, great, if not, no worries.&nbsp; All you have to do to qualify for the $50 is make 3 small purchases you were planning to make anyway with their new debit card.&nbsp; I know it's a trivial inconvenience, but I think it's worth it to be able to donate up to $100 to charity without actually spending any money.</p>\n<p>&nbsp;</p>\n<p>[2] Profit denominated in <a href=\"/lw/3kl/optimizing_fuzzies_and_utilons_the_altruism_chip\">warm fuzzies</a>, <a href=\"/lw/3gy/tallinnevans_125000_singularity_challenge\">karma</a>, and <a href=\"/lw/xt/interpersonal_entanglement\">post-Singularity catgirls</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AjbtpQqbJz4t58LX3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 14, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "4680", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FCxHgPsDScx4C3H8n", "FfNEt8mpi6qanNmXg", "aqyLxWzAEpDHm2Xyf", "Py3uGnncqXuEfPtQp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-04T20:27:49.228Z", "modifiedAt": null, "url": null, "title": "Less Wrong policy questions", "slug": "less-wrong-policy-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:15.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ffSwp5Xztas4bEt3/less-wrong-policy-questions", "pageUrlRelative": "/posts/6ffSwp5Xztas4bEt3/less-wrong-policy-questions", "linkUrl": "https://www.lesswrong.com/posts/6ffSwp5Xztas4bEt3/less-wrong-policy-questions", "postedAtFormatted": "Tuesday, January 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20policy%20questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20policy%20questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ffSwp5Xztas4bEt3%2Fless-wrong-policy-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20policy%20questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ffSwp5Xztas4bEt3%2Fless-wrong-policy-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ffSwp5Xztas4bEt3%2Fless-wrong-policy-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p>I have a few questions, mostly on which kinds of content can or cannot be posted to Less Wrong.</p>\n<p>&nbsp;</p>\n<p>1) Suppose I'd like everyone to comment on<a href=\"http://consc.net/papers/singularity.pdf\"> this paper</a>, is it okay to just link to it, give an oppinion, and ask for others?</p>\n<p>2) What if the paper had been written by me?</p>\n<p>3) What if it was a blog post from another blog?</p>\n<p>4) Videos and images are allowed?</p>\n<p>5) Should all Meta-level discussion be posted under the \"Discussion\" or are there kinds of Meta welcome in the Main Posts?</p>\n<p>6) Is there any sequence of posts from the post-Yudkowsky era that have been collected into a Cohesive Extrapoleted Less-Wrongness so that people who just finished the sequences can go right to them?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Why not?&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ffSwp5Xztas4bEt3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 6.638391381680023e-07, "legacy": true, "legacyId": "4684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-04T23:38:27.801Z", "modifiedAt": null, "url": null, "title": "Techniques for probability estimates", "slug": "techniques-for-probability-estimates", "viewCount": null, "lastCommentedAt": "2021-12-16T19:46:16.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r8aAqSBeeeMNRtiYK/techniques-for-probability-estimates", "pageUrlRelative": "/posts/r8aAqSBeeeMNRtiYK/techniques-for-probability-estimates", "linkUrl": "https://www.lesswrong.com/posts/r8aAqSBeeeMNRtiYK/techniques-for-probability-estimates", "postedAtFormatted": "Tuesday, January 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Techniques%20for%20probability%20estimates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATechniques%20for%20probability%20estimates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8aAqSBeeeMNRtiYK%2Ftechniques-for-probability-estimates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Techniques%20for%20probability%20estimates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8aAqSBeeeMNRtiYK%2Ftechniques-for-probability-estimates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8aAqSBeeeMNRtiYK%2Ftechniques-for-probability-estimates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1969, "htmlBody": "<p>Utility maximization often requires determining a probability of a particular statement being true. But humans are not utility maximizers and often refuse to give precise numerical probabilities. Nevertheless, their actions reflect a \"hidden\" probability. For example, even someone who refused to give a precise probability for Barack Obama's re-election would probably jump at the chance to take a bet in which ey lost $5 if Obama wasn't re-elected but won $5 million if he was; such decisions demand that the decider covertly be working off of at least a vague probability.<br /><br />When untrained people try to translate vague feelings like \"It seems Obama will probably be re-elected\" into a precise numerical probability, they commonly fall into certain traps and pitfalls that make their probability estimates inaccurate. Calling a probability estimate \"inaccurate\" causes philosophical problems, but these problems can be resolved by remembering that <a href=\"/lw/s6/probability_is_subjectively_objective\">probability is \"subjectively objective\"</a> - that although a mind \"hosts\" a probability estimate, that mind does not arbitrarily determine the estimate, but rather calculates it according to mathematical laws from available evidence. These calculations require too much computational power to use outside the simplest hypothetical examples, but they provide a standard by which to judge real probability estimates. They also suggest tests by which one can judge probabilities as well-calibrated or poorly-calibrated: for example, a person who constantly assigns 90% confidence to eir guesses but only guesses the right answer half the time is poorly calibrated. So calling a probability estimate \"accurate\" or \"inaccurate\" has a real philosophical grounding.<br /><br />There exist several techniques that help people translate vague feelings of probability into more accurate numerical estimates. Most of them translate probabilities from forms <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">without immediate consequences</a> (which the brain supposedly processes for signaling purposes) to forms with immediate consequences (which the brain supposedly processes while focusing on those consequences).</p>\n<p><a id=\"more\"></a><br /><br /><strong>Prepare for Revelation</strong></p>\n<p>What would you expect if you believed the answer to your question were about to be revealed to you?<br /><br />In <a href=\"/lw/i4/belief_in_belief/\">Belief in Belief</a>, a man acts as if there is a dragon in his garage, but every time his neighbor comes up with an idea to test it, he has a reason why the test wouldn't work. If he imagined Omega (the superintelligence who is always right) offered to reveal the answer to him, he might realize he was expecting Omega to reveal the answer \"No, there's no dragon\". At the very least, he might realize he was worried that Omega would reveal this, and so re-think exactly how certain he was about the dragon issue.<br /><br />This is a simple technique and has relatively few pitfalls.<br /><br /><br /><strong>Bet on it</strong><br /><br />At what odds would you be willing to bet on a proposition?<br /><br />Suppose someone offers you a bet at even odds that Obama will be re-elected. Would you take it? What about two-to-one odds? Ten-to-one? In theory, the knowledge that money is at stake should make you consider the problem in \"near mode\" and maximize your chances of winning.<br /><br />The problem with this method is that it only works when utility is linear with respect to money and you're not risk-averse. In the simplest case I should be indifferent to a $100,000 bet at 50% odds that a fair coin would come up tails, but in fact I would refuse it; winning $100,000 would be moderately good, but losing $100,000 would put me deeply in debt and completely screw up my life. When these sorts of consideration become paramount, imagining wagers will tend to give inaccurate results.<br /><br /><br /><strong>Convert to a Frequency</strong><br /><br />How many situations would it take before you expected an event to occur?<br /><br />Suppose you need to give a probability that the sun will rise tomorrow. \"999,999 in a million\" doesn't immediately sound wrong; the sun seems likely to rise, and a million is a very high number. But if tomorrow is an average day, then your probability will be linked to the number of days it will take before you expect that the sun will fail to rise on at least one. A million days is three thousand years; the Earth has existed for far more than three thousand years without the sun failing to rise. Therefore, 999,999 in a million is too low a probability for this occurrence. If you think the sort of astronomical event that might prevent the sun from rising happens only once every three billion years, then you might consider a probability more like 999,999,999,999 in a trillion.<br /><br />In addition to converting to a frequency across time, you can also convert to a frequency across places or people. What's the probability that you will be murdered tomorrow? The best guess would be to check the murder rate for your area. What's the probability there will be a major fire in your city this year? Check how many cities per year have major fires.<br /><br />This method fails if your case is not typical: for example, if your city is on the losing side of a war against an enemy known to use fire-bombing, the probability of a fire there has nothing to do with the average probability across cities. And if you think the reason the sun might not rise is a supervillain building a high-tech sun-destroying machine, then consistent sunrises over the past three thousand years of low technology will provide little consolation.<br /><br />A special case of the above failure is converting to frequency across time when considering an event that is known to take place at a certain distance from the present. For example, if today is April 10th, then the probability that we hold a Christmas celebration tomorrow is much lower than the 1/365 you get by checking on what percentage of days we celebrate Christmas. In the same way, although we know that the sun will fail to rise in a few billion years when it burns out its nuclear fuel, this shouldn't affect its chance of rising tomorrow.<br /><br /><br /><strong>Find a Reference Class</strong><br /><br />How often have similar statements been true?<br /><br />What is the probability that the latest crisis in Korea escalates to a full-blown war? If there have been twenty crisis-level standoffs in the Korean peninsula in the past 60 years, and only one of them has resulted in a major war, then (war|crisis) = .05, so long as this crisis is equivalent to the twenty crises you're using as your reference class.<br /><br />But finding the reference class is itself a hard problem. What is the probability Bigfoot exists? If one makes a reference class by saying that the yeti doesn't exist, the Loch Ness monster doesn't exist, and so on, then the Bigfoot partisan might accuse you of assuming the conclusion - after all, the likelihood of these creatures existing is probably similar to and correlated with Bigfoot. The partisan might suggest asking how many creatures previously believed not to exist later turned out to exist - a list which includes real animals like the orangutan and platypus - but then one will have to debate whether to include creatures like dragons, orcs, and Pokemon on the list.<br /><br />This works best when the reference class is more obvious, as in the Korea example.<br /><br /><br /><strong>Make Multiple Statements</strong><br /><br />How many statements could you make of about the same uncertainty as a given statement without being wrong once?<br /><br />Suppose you believe France is larger than Italy. With what confidence should you believe it? If you made ten similar statements (Germany is larger than Austria, Britain is larger than Ireland, Spain is larger than Portugal, et cetera) how many times do you think you would be wrong? A hundred similar statements? If you think you'd be wrong only one time out of a hundred, you can give the statement 99% confidence.<br /><br />This is the most controversial probability assessment technique; it tends to give lower levels of confidence than the others; for example, <a href=\"/lw/u6/horrible_lhc_inconsistency/\">Eliezer wants to say</a> there's a less than one in a million chance the LHC would destroy the world, but doubts he could make a million similar statements and only be wrong once. <a href=\"/lw/1mw/advancing_certainty/\">Komponisto thinks</a> this is a failure of imagination: we imagine ourselves gradually growing tired and making mistakes, whereas this method only works if the accuracy of the millionth statement is exactly the same as the first.<br /><br />In any case, the technique is only as good as the ability to judge which statements are equally difficult to a given statement. If I start saying things like \"Russia is larger than Vatican City! Canada is larger than a speck of dust!\" then I may get all the statements right, but it won't mean much for my Italy-France example - and if I get bogged down in difficult questions like \"Burundi is larger than Equatorial Guinea\" then I might end up underconfident. In cases where there is an obvious comparison (\"Bob didn't cheat on his test\", \"Sue didn't cheat on her test\", \"Alice didn't cheat on her test\") this problem disappears somewhat.<br /><br /><br /><strong>Imagine Hypothetical Evidence</strong><br /><br />How would your probabilities adjust given new evidence?<br /><br />Suppose one day all the religious people and all the atheists get tired of arguing and decide to settle the matter by experiment once and for all. The plan is to roll an n-sided numbered die and have the faithful of all religions pray for the die to land on \"1\". The experiment will be done once, with great pomp and ceremony, and never repeated, lest the losers try for a better result. All the resources of the world's skeptics and security forces will be deployed to prevent any tampering with the die, and we assume their success is guaranteed.<br /><br />If the experimenters used a twenty-sided die, and the die comes up 1, would this convince you that God probably did it, or would you dismiss the result as a coincidence? What about a hundred-sided die? Million-sided? If a successful result on a hundred-sided die wouldn't convince you, your probability of God's existence must be less than one in a hundred; if a million-sided die would convince you, it must be more than one in a million.<br /><br />This technique has also been denounced as inaccurate, on the grounds that our coincidence detectors are overactive and therefore in no state to be calibrating anything else. It would feel very hard to dismiss a successful result on a thousand-sided die, no matter how low the probability of God is. It might also be difficult to visualize a hypothetical where the experiment can't possibly be rigged, and it may be unfair to force subjects to imagine a hypothetical that would practically never happen (like the million-sided die landing on one in a world where God doesn't exist).<br /><br /><br /><br />These techniques should be experimentally testable; any disagreement over which do or do not work (at least for a specific individual) can be resolved by going through a list of difficult questions, declaring confidence levels, and scoring the results with log odds. Steven's blog has some good sets of test questions (which I deliberately do <em>not</em> link here so as to not contaminate a possible pool of test subjects); if many people are interested in participating and there's a general consensus that an experiment would be useful, we can try to design one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "E8PHMuf7tsr8teXAe": 1, "Zwv9eHi7KGg5KA9oM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r8aAqSBeeeMNRtiYK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": 94, "extendedScore": null, "score": 0.000169, "legacy": true, "legacyId": "4686", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "TQW9brvXJ5Fajorr4", "canonicalCollectionSlug": "codex", "canonicalBookId": "jF58hKP9ZLzgy22Jr", "canonicalNextPostSlug": "confidence-levels-inside-and-outside-an-argument", "canonicalPrevPostSlug": "if-it-s-worth-doing-it-s-worth-doing-with-made-up-statistics", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 94, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XhaKvQyHzeXdNnFKy", "CqyJzDZWvGhhFJ7dY", "ziqL94sq6rMuH7wDu", "6zRn4uKADwL9uo8Ch"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-05T06:07:33.581Z", "modifiedAt": null, "url": null, "title": "Today's xkcd is relevant to our interests", "slug": "today-s-xkcd-is-relevant-to-our-interests", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.436Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mindspillage", "createdAt": "2010-09-20T19:45:07.674Z", "isAdmin": false, "displayName": "mindspillage"}, "userId": "kfRYbvqZe8BHtWmPm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FwcQ52JLPRdsreKoM/today-s-xkcd-is-relevant-to-our-interests", "pageUrlRelative": "/posts/FwcQ52JLPRdsreKoM/today-s-xkcd-is-relevant-to-our-interests", "linkUrl": "https://www.lesswrong.com/posts/FwcQ52JLPRdsreKoM/today-s-xkcd-is-relevant-to-our-interests", "postedAtFormatted": "Wednesday, January 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Today's%20xkcd%20is%20relevant%20to%20our%20interests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToday's%20xkcd%20is%20relevant%20to%20our%20interests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwcQ52JLPRdsreKoM%2Ftoday-s-xkcd-is-relevant-to-our-interests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Today's%20xkcd%20is%20relevant%20to%20our%20interests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwcQ52JLPRdsreKoM%2Ftoday-s-xkcd-is-relevant-to-our-interests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwcQ52JLPRdsreKoM%2Ftoday-s-xkcd-is-relevant-to-our-interests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://xkcd.com/843/\">Here.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FwcQ52JLPRdsreKoM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 11, "extendedScore": null, "score": 6.639866794089089e-07, "legacy": true, "legacyId": "4695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-05T07:22:30.669Z", "modifiedAt": null, "url": null, "title": "The Neglected Virtue of Scholarship", "slug": "the-neglected-virtue-of-scholarship", "viewCount": null, "lastCommentedAt": "2021-10-11T10:47:22.294Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/64FdKLwmea8MCLWkE/the-neglected-virtue-of-scholarship", "pageUrlRelative": "/posts/64FdKLwmea8MCLWkE/the-neglected-virtue-of-scholarship", "linkUrl": "https://www.lesswrong.com/posts/64FdKLwmea8MCLWkE/the-neglected-virtue-of-scholarship", "postedAtFormatted": "Wednesday, January 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Neglected%20Virtue%20of%20Scholarship&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Neglected%20Virtue%20of%20Scholarship%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F64FdKLwmea8MCLWkE%2Fthe-neglected-virtue-of-scholarship%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Neglected%20Virtue%20of%20Scholarship%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F64FdKLwmea8MCLWkE%2Fthe-neglected-virtue-of-scholarship", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F64FdKLwmea8MCLWkE%2Fthe-neglected-virtue-of-scholarship", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 947, "htmlBody": "<p>Eliezer Yudkowsky identifies <em>scholarship</em>&nbsp;as one of the <a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 20px; \">Study many sciences and absorb their power as your own. Each field that you consume makes you larger... It is especially important to eat math and science which impinges upon rationality: Evolutionary psychology, heuristics and biases, social psychology, probability theory, decision theory. But these cannot be the only fields you study...</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 20px; \">I think he's right, and I think scholarship doesn't get enough praise - <em>even on Less Wrong</em>, where it is <a href=\"/lw/2un/references_resources_for_lesswrong/\">regularly</a> <a href=\"/lw/2sw/math_prerequisites_for_understanding_lw_stuff/\">encouraged</a>.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 20px; \">First, consider the evangelical atheist community <a href=\"http://commonsenseatheism.com/\">to which I belong</a>. There is a tendency for lay atheists to write \"refutations\" of theism without first doing a modicum of research on the current state of the arguments. This can get atheists into trouble when they go toe-to-toe with a theist who <em>did</em>&nbsp;do his homework. I'll share <a href=\"http://commonsenseatheism.com/?p=11376\">two examples</a>:</span></p>\n<ul>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 20px;\">In a <a href=\"http://www.youtube.com/watch?v=AjOSNj97_gk&amp;feature=related\">debate</a> with theist Bill Craig, agnostic Bart Ehrman paraphrased&nbsp;<a href=\"http://en.wikipedia.org/wiki/Of_Miracles\">David Hume's argument</a> that we can't demonstrate the&nbsp;occurrence&nbsp;of a miracle in the past. Craig <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/04/craig-ehrman.pdf\">responded</a> with a PowerPoint slide showing Bayes' Theorem, and explained that Ehrman was only considering prior probabilities, when of course he needed to consider the relevant conditional probabilities as well. Ehrman failed to respond to this, and looked as though he had never seen Bayes' Theorem before. Had Ehrman practiced the virtue of scholarship on this issue, he might have noticed that much of the scholarly work on Hume's argument in the past two decades <a href=\"http://www.amazon.com/Resurrection-God-Incarnate-Richard-Swinburne/dp/0199257469/\">has</a> <a href=\"http://www.amazon.com/Humes-Abject-Failure-Argument-Miracles/dp/0195127382/\">involved</a> <a href=\"http://www.amazon.com/Arguing-about-Gods-Graham-Oppy/dp/0521122643/\">Bayes'</a> <a href=\"http://www.lydiamcgrew.com/Resurrectionarticlesinglefile.pdf\">Theorem</a>. He might also have discovered that the correct response to Craig's use of Bayes' Theorem can be found in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/09/Sobel-on-Hume-on-Miracles.pdf\">pages 298-341</a> of J.H. Sobel&rsquo;s <em><a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/09/Sobel-on-Hume-on-Miracles.pdf\">Logic and Theism</a></em>.<br /><br /></span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 20px;\">In another <a href=\"http://commonsenseatheism.com/?p=1230\">debate</a> with Bill Craig, atheist Christopher Hitchens gave this objection: \"Who designed the Designer? Don&rsquo;t you run the risk&hellip; of asking 'Well, where does that come from? And where does that come from?' and running into an infinite regress?\"&nbsp;But this is an <em>elementary </em>misunderstanding in philosophy of science. Why? Because <a href=\"http://commonsenseatheism.com/?p=6113\">every successful scientific explanation faces the exact same problem</a>. It&rsquo;s called the &ldquo;why regress&rdquo; because no matter what explanation is given of something, you can always still ask &ldquo;Why?&rdquo; Craig pointed this out and handily won that part of the debate. Had Hitchens had a passing understanding of science or explanation, he could have avoided looking foolish, and also spent more time on <em>substantive</em> objections to theism. (One <em>can</em> give a \"Who made God?\" objection to theism that <a href=\"http://omnisaffirmatioestnegatio.wordpress.com/2010/06/12/dawkins-and-the-ultimate-747-gambit/\">has some meat</a>, but that's not the one Hitchens gave. Hitchens' objection concerned an infinite regress of explanations, which is just as much a feature of science as it is of theism.)</span></span></li>\n</ul>\n<p>The lesson I take from these and a hundred other examples is to employ the rationality virtue of scholarship. Stand on the shoulders of giants. We don't each need to cut our own path into a subject right from the point of near-total ignorance. That's silly. Just catch the bus on the road of knowledge paved by hundreds of diligent workers before you, and get off somewhere near where the road finally fades into fresh jungle. Study enough to have a view of the&nbsp;<em>current</em> state of the debate so you don't waste your time on paths that have already dead-ended, or on arguments that have already been refuted. Catch up before you speak up.</p>\n<p>This is why, in more than 1000 posts on&nbsp;<a href=\"http://commonsenseatheism.com/\">my own blog</a>, I've said almost&nbsp;<em>nothing</em>&nbsp;that is original. Most of my posts instead summarize what other experts have said, in an effort to bring myself and my readers up to the level of the&nbsp;<em>current</em>&nbsp;debate on a subject before we try to make&nbsp;<em>new</em>&nbsp;contributions to it.</p>\n<p>The Less Wrong community is a particularly smart and well-read bunch, but of course it doesn't always embrace the virtue of scholarship.</p>\n<p>Consider the field of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Formal_epistemology\">formal epistemology</a>, an entire branch of philosophy devoted to (1) mathematically formalizing concepts related to induction, belief, choice, and action, and (2) arguing about the foundations of probability, statistics, game theory, decision theory, and algorithmic learning theory. These are central discussion topics at Less Wrong, and yet my own experience suggests that most Less Wrong readers have never heard of the entire field, let alone read any works by formal epistemologists, such as&nbsp;<em><a href=\"http://www.amazon.com/Defence-Objective-Bayesianism-Jon-Williamson/dp/0199228000/\">In Defense of Objective Bayesianism</a></em>&nbsp;by Jon Williamson or&nbsp;<em><a href=\"http://www.amazon.com/Bayesian-Epistemology-Luc-Bovens/dp/0199270406/\">Bayesian Epistemology</a></em>&nbsp;by Luc Bovens and Stephan Hartmann. <a id=\"more\"></a></p>\n<p>Or, consider a recent post by Yudkowsky:&nbsp;<a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">Working hurts less than procrastinating, we fear the twinge of starting</a>. The post attempts to make progress against procrastination by practicing single-subject phenomenology, rather than by first catching up with&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-The-Nature-of-Procrastination.pdf\">a quick summary of scientific research on procrastination</a>. The post's approach to the problem looks inefficient to me. It's not standing on the shoulders of giants.</p>\n<p>This post probably looks harsher than I mean it to be. After all, Less Wrong <em>is</em>&nbsp;pretty damn good at scholarship compared to most communities. But I think it could be better.</p>\n<p>Here's my suggestion. Every time you're tempted to tackle a serious question in a subject on which you're not already an expert, ask yourself: \"Whose giant shoulders can I stand on, here?\"</p>\n<p>Usually, you can answer the question by doing the following:</p>\n<ol>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 20px;\">Read the Wikipedia article on the subject, and glance over the references.</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 20px;\">Read the article on the subject in a field-specific encyclopedia. For example if you're probing a philosophical concept, find the relevant essay(s) in&nbsp;<em><a href=\"http://www.rep.routledge.com/\">The Routledge Encyclopedia of Philosophy</a></em>&nbsp;or the <em><a href=\"http://www.iep.utm.edu/\">Internet Encyclopedia of Philosophy</a></em>&nbsp;or the&nbsp;<em><a href=\"http://plato.stanford.edu/\">Stanford Encyclopedia of Philosophy</a></em>.&nbsp;Often, the encyclopedia you want is at your local library or can be browsed at&nbsp;<a href=\"http://books.google.com/books?id=XHU8V3fkOXwC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false\">Google Books</a>.</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 20px;\">Read or skim-read&nbsp;an entry-level university textbook on the subject.</span></span></li>\n</ol>\n<p>There are so many resources for learning available today, the virtue of scholarship has never in human history been so easy to practice.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8uNFGxejo5hykCEez": 6, "fF9GEdWXKJ3z73TmB": 30, "3uE2pXvbcnS9nnZRE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "64FdKLwmea8MCLWkE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 247, "baseScore": 296, "extendedScore": null, "score": 0.000527, "legacy": true, "legacyId": "4683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 296, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TNHQLZK5pHbxdnz4e", "zcp78EobzGxTkeiJH", "9o3QBg2xJXcRCxGjS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-05T08:15:23.203Z", "modifiedAt": null, "url": null, "title": "Link: Collective Intelligence", "slug": "link-collective-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:15.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Randaly", "createdAt": "2010-04-20T23:31:03.738Z", "isAdmin": false, "displayName": "Randaly"}, "userId": "KdhDyNCDgA945WayD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/biS7EfFqrppiKSxnj/link-collective-intelligence", "pageUrlRelative": "/posts/biS7EfFqrppiKSxnj/link-collective-intelligence", "linkUrl": "https://www.lesswrong.com/posts/biS7EfFqrppiKSxnj/link-collective-intelligence", "postedAtFormatted": "Wednesday, January 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Collective%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Collective%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbiS7EfFqrppiKSxnj%2Flink-collective-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Collective%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbiS7EfFqrppiKSxnj%2Flink-collective-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbiS7EfFqrppiKSxnj%2Flink-collective-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>Per <a href=\"http://www.wjh.harvard.edu/~cfc/Woolley2010a.pdf\">this recent paper</a>, individual IQ has no significant correlation with 'group IQ' (defined and measured as the groups ability to accomplish various tasks); group cohesion, motivation, and satisfaction aren't either. The study identified two things that were positively correlated with group IQ: average social sensitivity and a low variance in the amount of time each person spent speaking. (It also found that having more women improved collective intelligence- because women have better social sensitivity.)</p>\n<p>(The remaining stuff is idle speculation from me, not the paper. There's no experimental evidence whatsoever backing it up.)</p>\n<p>One possible explanation of the contribution of social sensitivity towards collective intelligence is that it reduces conflicts between group members, allowing the group the remain at least somewhat dispassionate/rational about potential solutions instead of turning discussions about solutions into status pissing contests. This is supported by the fact that ego-based actions are well known to be extremely damaging to group outcomes in sports, and that in contexts (e.g. politics) where there are groups with pre-existing conflicts decision-making seems to be relatively poor despite (presumably) higher social sensitivity on the part of politicians. (This also provides an alternative explanation for the benefits of <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">holding off on proposing solutions</a>: while Eliezer focused, as he is wont to do, on the implications for individual rationality, Maier's edict presumably didn't stop people from thinking of potential solutions and privately settling on a preferred solution- but because they hadn't announced it publicly, they would be more willing to listen to others and change their mind.)</p>\n<p>The contribution from variance presumably comes from the fact that if people are on average speaking roughly the same amount, then there are more ideas and perspectives being offered than if only a few people dominated the conversation.</p>\n<p>I'd also be interested in seeing whether Collective Intelligence is correlated with individual rationality, given that there is little to no correlation between individual rationality and IQ.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "biS7EfFqrppiKSxnj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.640192186565025e-07, "legacy": true, "legacyId": "4697", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-05T09:54:21.271Z", "modifiedAt": null, "url": null, "title": "Possible Cockatrice in written form", "slug": "possible-cockatrice-in-written-form", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.971Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aurini", "createdAt": "2009-03-19T04:39:40.233Z", "isAdmin": false, "displayName": "Aurini"}, "userId": "5fNCGeJcDQCjxEjnD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2ixvzHc9b8n6PEWMN/possible-cockatrice-in-written-form", "pageUrlRelative": "/posts/2ixvzHc9b8n6PEWMN/possible-cockatrice-in-written-form", "linkUrl": "https://www.lesswrong.com/posts/2ixvzHc9b8n6PEWMN/possible-cockatrice-in-written-form", "postedAtFormatted": "Wednesday, January 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Possible%20Cockatrice%20in%20written%20form&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APossible%20Cockatrice%20in%20written%20form%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ixvzHc9b8n6PEWMN%2Fpossible-cockatrice-in-written-form%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Possible%20Cockatrice%20in%20written%20form%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ixvzHc9b8n6PEWMN%2Fpossible-cockatrice-in-written-form", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ixvzHc9b8n6PEWMN%2Fpossible-cockatrice-in-written-form", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<p>My various interweb browsings stumbled me upon a potential Cockatrice in written, philisophical form.&nbsp; I've thus far read through the first chapter, and it is less anti-rational than most philosophical writings.</p>\n<p>I'm reading through it right now, and will provide my feedback when I'm done, likely as a front-page post.</p>\n<p>Personally, I'm a Fatalist, with some sort of Weird Soldier Ethic, who plans to go out the same way that Hunter did (if the cops don't get me first), but I've got a bunch of nonsense to Write first.&nbsp; I figure that'll make me somewhat immune.&nbsp; That aside, I doubt it's a real cockatrice - or we would've heard about it before.</p>\n<p>It is a strong exercise in Nihilism.&nbsp; So, with those cautions given, I offer it to you:&nbsp; <a title=\"Because why not?\" href=\"http://www.suicidenote.info/ebook/suicide_note.pdf\" target=\"_blank\">an extensive suicide letter</a>.</p>\n<p>Tip of the hat to <a title=\"Half Sigma - dude's got me thinking.\" href=\"http://www.halfsigma.com/\" target=\"_blank\">this guy</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2ixvzHc9b8n6PEWMN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -10, "extendedScore": null, "score": 6.640442859935385e-07, "legacy": true, "legacyId": "4699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-05T17:19:24.233Z", "modifiedAt": null, "url": null, "title": "the Universe, Computability, and the Singularity", "slug": "the-universe-computability-and-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/igMLHvZC2Jeazm4rL/the-universe-computability-and-the-singularity", "pageUrlRelative": "/posts/igMLHvZC2Jeazm4rL/the-universe-computability-and-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/igMLHvZC2Jeazm4rL/the-universe-computability-and-the-singularity", "postedAtFormatted": "Wednesday, January 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20the%20Universe%2C%20Computability%2C%20and%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Athe%20Universe%2C%20Computability%2C%20and%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigMLHvZC2Jeazm4rL%2Fthe-universe-computability-and-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=the%20Universe%2C%20Computability%2C%20and%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigMLHvZC2Jeazm4rL%2Fthe-universe-computability-and-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigMLHvZC2Jeazm4rL%2Fthe-universe-computability-and-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 458, "htmlBody": "<p>EDIT at Karma -5: Could the next \"good citizen\" to vote this down leave me a comment as to why it is getting voted down, and if other \"good citizens\" to pile on after that, either upvote that comment or put another comment giving your different reason?</p>\r\n<p>&nbsp;</p>\r\n<p>Original Post:</p>\r\n<p>Questions about the computability of various physical laws recently had me thinking: \"well of course every real physical law is computable or else the universe couldn't function.\"&nbsp; That is to say that in order of the time-evolution of anything in the universe to proceed \"correctly,\" the physical processes themselves must be able to, and in real-time, keep up with the complexity of their actual evolution.&nbsp; This seems to me a proof that every real physical process is computable by SOME sort of real computer, in the degenerate case that real computer is simply an actual physical model of the process itself, create that model, observe whichever features of its time-evolution you are trying to compute, and there you have your computer.&nbsp;</p>\r\n<p>Then if we have a physical law whose use in predicting time evolution is provably uncomputable, either we know that this physical law is NOT the only law that might be formulated to describe what it is purporting to describe, or that our theory of computation is incomplete.&nbsp; In some sense what I am saying is consistent with the idea that quantum computing can quickly collapse down to plausibly tractable levels the time it takes to compute some things which, as classical computation problems, blow up.&nbsp; This would be a good indication that quantum is an important theory about the universe, that it not only explains a bunch of things that happen in the universe, but also explains how the universe can have those things happen in real-time without making mistakes.&nbsp;</p>\r\n<p>What I am wondering is, where does this kind of consideration break with traditional computability theory?&nbsp; Is traditional computability theory limited to what Turing machines can do, while perhaps it is straightforward to prove that the operation of this Universe requires computation beyond what Turing machines can do?&nbsp; Is traditional computability theory limited to digital representations whereas the degenerate build-it-and-measure-it computer is what has been known as an analog computer?&nbsp; Is there somehow a level or measure of artificiality which must be present to call something a computer, which rules out such brute-force approaches as build-it-and-measure-it?</p>\r\n<p>At least one imagining of the singularity is absorbing all the resources of the universe into some maximal intelligence, the (possibly asymptotic) endpoint of intelligences desiging greater intelligences until something makes them stop.&nbsp; But the universe is already just humming along like clockwork, with quantum and possibly even subtler-than-quantum gears turning in real time.&nbsp; What does the singularity add to this picture that isn't already there?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "igMLHvZC2Jeazm4rL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -3, "extendedScore": null, "score": 6.64157733615342e-07, "legacy": true, "legacyId": "4701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-06T00:28:29.202Z", "modifiedAt": null, "url": null, "title": "Discussion for Eliezer Yudkowsky's paper: Timeless Decision Theory", "slug": "discussion-for-eliezer-yudkowsky-s-paper-timeless-decision", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:25.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/etq6bmu3sFjRfzLgt/discussion-for-eliezer-yudkowsky-s-paper-timeless-decision", "pageUrlRelative": "/posts/etq6bmu3sFjRfzLgt/discussion-for-eliezer-yudkowsky-s-paper-timeless-decision", "linkUrl": "https://www.lesswrong.com/posts/etq6bmu3sFjRfzLgt/discussion-for-eliezer-yudkowsky-s-paper-timeless-decision", "postedAtFormatted": "Thursday, January 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Discussion%20for%20Eliezer%20Yudkowsky's%20paper%3A%20Timeless%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiscussion%20for%20Eliezer%20Yudkowsky's%20paper%3A%20Timeless%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fetq6bmu3sFjRfzLgt%2Fdiscussion-for-eliezer-yudkowsky-s-paper-timeless-decision%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Discussion%20for%20Eliezer%20Yudkowsky's%20paper%3A%20Timeless%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fetq6bmu3sFjRfzLgt%2Fdiscussion-for-eliezer-yudkowsky-s-paper-timeless-decision", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fetq6bmu3sFjRfzLgt%2Fdiscussion-for-eliezer-yudkowsky-s-paper-timeless-decision", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>I have not seen any place to discuss <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Eliezer Yudkowsky's new paper, titled Timeless Decision Theory</a>, so I decided to create a discussion post. (Have I missed an already existing post or discussion?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "etq6bmu3sFjRfzLgt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 6.642670206275362e-07, "legacy": true, "legacyId": "4703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-06T00:56:50.739Z", "modifiedAt": null, "url": null, "title": "In Defense of Objective Bayesianism: MaxEnt Puzzle.", "slug": "in-defense-of-objective-bayesianism-maxent-puzzle", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R5iKMgWCoggAAAWy2/in-defense-of-objective-bayesianism-maxent-puzzle", "pageUrlRelative": "/posts/R5iKMgWCoggAAAWy2/in-defense-of-objective-bayesianism-maxent-puzzle", "linkUrl": "https://www.lesswrong.com/posts/R5iKMgWCoggAAAWy2/in-defense-of-objective-bayesianism-maxent-puzzle", "postedAtFormatted": "Thursday, January 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Defense%20of%20Objective%20Bayesianism%3A%20MaxEnt%20Puzzle.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Defense%20of%20Objective%20Bayesianism%3A%20MaxEnt%20Puzzle.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR5iKMgWCoggAAAWy2%2Fin-defense-of-objective-bayesianism-maxent-puzzle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Defense%20of%20Objective%20Bayesianism%3A%20MaxEnt%20Puzzle.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR5iKMgWCoggAAAWy2%2Fin-defense-of-objective-bayesianism-maxent-puzzle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR5iKMgWCoggAAAWy2%2Fin-defense-of-objective-bayesianism-maxent-puzzle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 475, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\"><em><span><a href=\"http://www.amazon.com/Defence-Objective-Bayesianism-Jon-Williamson/dp/0199228000/\">In Defense of Objective Bayesianism</a></span></em>&nbsp;by Jon Williamson was mentioned recently <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship\">in a post</a> by lukeprog as the sort of book that should be being read by people on Less Wrong. Now, I have been reading it, and found some of it quite bizarre. This point in particular seems obviously false. If it&rsquo;s just me, I&rsquo;ll be glad to be enlightened as to what was meant. If collectively we don&rsquo;t understand, that&rsquo;d be pretty strong evidence that we should read more academic Bayesian stuff.</p>\n<p class=\"MsoNormal\">Williamson advocates use of the Maximum Entropy Principle. In short, you should take account of the limits placed on your probability by the empirical evidence, and then choose a probability distribution closest to uniform that satisfies those constraints.</p>\n<p class=\"MsoNormal\">So, if asked to assign a probability to an arbitrary A, you&rsquo;d say p = 0.5. But if you were given evidence in the form of some constraints on p, say that p &ge; 0.8, you&rsquo;d set p = 0.8, as that was the new entropy-maximising level. Constraints are restricted to Affine constraints. I found this somewhat counter-intuitive already, but I do follow what he means.</p>\n<p class=\"MsoNormal\">But now for the confusing bit. I quote directly;</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">&ldquo;Suppose A is &lsquo;Peterson is a Swede&rsquo;, B is &lsquo;Peterson is a Norwegian&rsquo;, C is &lsquo;Peterson is a Scandinavian&rsquo;, and &epsilon; is &lsquo;80% of all Scandinavians are Swedes&rsquo;. Initially, the agent sets P(A) = 0.2, P(B) = 0.8, P(C) = 1 P(&epsilon;) = 0.2, P(A &amp; &epsilon;) = P(B &amp; &epsilon;) = 0.1. All these degrees of belief satisfy the norms of subjectivism. Updating by maxent on learning &epsilon;, the agent believes Peterson is a Swede to degree 0.8, which seems quite right. On the other hand, updating by conditionalizing on &epsilon; leads to a degree of belief of 0.5 that Peterson is a Swede, which is quite wrong. Thus, we see that maxent is to be preferred to conditionalization in this kind of example because the conditionalization update does not satisfy the new constraints X&rsquo;, while the maxent update does.&rdquo;</p>\n<p class=\"MsoNormal\" style=\"padding-left: 60px;\">p80, 2010 edition. Note that this example is actually from Bacchus et al (1990), but Williamson quotes approvingly.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">His calculation for the Bayesian update is correct; you do get 0.5. What&rsquo;s more, this seems to be intuitively the right answer; the update has caused you to &lsquo;zoom in&rsquo; on the probability mass assigned to &epsilon;, while maintaining relative proportions inside it.</p>\n<p class=\"MsoNormal\">As far as I can see, you get 0.8 only if we assume that Peterson is a randomly chosen Scandinavian. But if that were true, the prior given is bizarre. If he was a randomly chosen individual, the prior should have been something like P(A &amp; &epsilon;) = 0.16 P(B &amp; &epsilon;) = 0.04 The only way I can make sense of the prior is if constraints simply &ldquo;don&rsquo;t apply&rdquo; until they have p=1.</p>\n<p class=\"MsoNormal\">Can anyone explain the reasoning behind a posterior probability of 0.8?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R5iKMgWCoggAAAWy2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 6.642742447361806e-07, "legacy": true, "legacyId": "4704", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["64FdKLwmea8MCLWkE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-06T15:55:58.866Z", "modifiedAt": null, "url": null, "title": "Help: Neurochemistry question", "slug": "help-neurochemistry-question", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.828Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gJpKuHxocD2vJKNgJ/help-neurochemistry-question", "pageUrlRelative": "/posts/gJpKuHxocD2vJKNgJ/help-neurochemistry-question", "linkUrl": "https://www.lesswrong.com/posts/gJpKuHxocD2vJKNgJ/help-neurochemistry-question", "postedAtFormatted": "Thursday, January 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%3A%20Neurochemistry%20question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%3A%20Neurochemistry%20question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJpKuHxocD2vJKNgJ%2Fhelp-neurochemistry-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%3A%20Neurochemistry%20question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJpKuHxocD2vJKNgJ%2Fhelp-neurochemistry-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJpKuHxocD2vJKNgJ%2Fhelp-neurochemistry-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 292, "htmlBody": "<p>I was looking at a hypothesis that bipolar disorder is probably due to problems with neocortical sodium-potassium pump activity cyclically decreasing and thus allowing increased resistance, which increases neuronal excitability by the square of the resistance. According to what I've read on Wikipedia it seems that agonizing Gi proteins would inhibit cAMP production and therefore downregulate sodium pump activity (this was the most tenuous part, and the reference was unintelligible) and increase neuronal excitability. But 5-HT1A (a type of Gi protein) agonists have been shown to be useful for improving symptoms of schizophrenia, which is typically thought of as resulting from increased neuronal excitability. Sign error? What's up? I don't have any model of the underlying mechanisms, and only vaguely know what words like 'downregulate' mean.</p>\n<p>ETA: But apparently&nbsp;<a href=\"http://en.wikipedia.org/wiki/Risperidone\">http://en.wikipedia.org/wiki/Risperidone</a>&nbsp;, a 5-HT2A agonist, also supposedly mitigates symptoms of schizophrenia, which is weird because it increases neuronal excitation. It seems that the flawed assumption is that schizophrenia has something to do with 5-HT-caused neuronal excitability.</p>\n<p>Thus it seems like maybe you can reduce bipolar tendencies by taking 5-HT1A antagonists, and schizophrenia is its own separate problem that is perhaps solved by taking a D2 antagonist. 5-HT2A agonists increase neuronal excitation which may interact somehow with the the 5-HT1A antagonists. I'm still confused.</p>\n<p>There's this other problem where if the main reason D2 and 5-HT1A work is by decreasing neuronal excitability that might just be because sedate people are more similar to each other than manic people, but the kind of neuronal excitability that makes people manic and the kind that makes people schizoid are qualitatively different, and hence the difference in receptors being important. It could be that 5-HT2A agonists and 5-HT1A antagonists increase and decrease neuronal excitability respectively, but in different ways such that they're not&nbsp;countervailing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gJpKuHxocD2vJKNgJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 6.645032336657446e-07, "legacy": true, "legacyId": "4715", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-06T19:57:20.972Z", "modifiedAt": null, "url": null, "title": "An Overview of Formal Epistemology (links)", "slug": "an-overview-of-formal-epistemology-links", "viewCount": null, "lastCommentedAt": "2011-01-29T04:27:48.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lukeprog", "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BXot7wxNbipyM749o/an-overview-of-formal-epistemology-links", "pageUrlRelative": "/posts/BXot7wxNbipyM749o/an-overview-of-formal-epistemology-links", "linkUrl": "https://www.lesswrong.com/posts/BXot7wxNbipyM749o/an-overview-of-formal-epistemology-links", "postedAtFormatted": "Thursday, January 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Overview%20of%20Formal%20Epistemology%20(links)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Overview%20of%20Formal%20Epistemology%20(links)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXot7wxNbipyM749o%2Fan-overview-of-formal-epistemology-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Overview%20of%20Formal%20Epistemology%20(links)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXot7wxNbipyM749o%2Fan-overview-of-formal-epistemology-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBXot7wxNbipyM749o%2Fan-overview-of-formal-epistemology-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>The branch of philosophy called <a href=\"http://en.wikipedia.org/wiki/Formal_epistemology\">formal epistemology</a> has very similar interests to those of the Less Wrong community. Formal epistemologists mostly work on (1) mathematically formalizing concepts related to induction, belief, choice, and action, and (2) arguing about the foundations of probability, statistics, game theory, decision theory, and algorithmic learning theory.</p><p>Those who value <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">the neglected virtue of scholarship</a> may want to study for themselves the arguments that have lead scholars either toward or against the very <em>particular </em>positions on <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">formalizing language</a>, <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory_(sequence)#Sequence\">decision theory</a>, <a href=\"http://yudkowsky.net/rational/technical\">explanation</a>, and <a href=\"/lw/oj/probability_is_in_the_mind/\">probability</a> typically endorsed at Less Wrong. As such, here&#x27;s a brief overview of the field by way of some helpful links:</p><ul><li><em>Wikipedia</em>, &quot;<a href=\"http://en.wikipedia.org/wiki/Formal_epistemology\">Formal Epistemology</a>&quot; (contains an excellent list of today&#x27;s leading formal epistemologists)</li><li>Hendricks, <em><a href=\"http://www.amazon.com/Mainstream-Formal-Epistemology-Vincent-Hendricks/dp/0521718988/\">Mainstream and Formal Epistemology</a></em> (perhaps the best &quot;introduction&quot; to the subject, especially for those familiar with mainstream epistemology)</li><li><em><a href=\"http://www.thereasoner.org/\">The Reasoner</a></em>, a free monthly digest of short articles and interviews on formal epistemology</li><li><em><a href=\"http://choiceandinference.com/\">Choice &amp; Inference</a></em>, a group blog</li><li><a href=\"http://ai.stanford.edu/~epacuit/classes/esslli/formep_esslli.html\">Introduction to Formal Epistemology</a>, a short Stanford course with lecture slides and some literature in PDF</li><li><em>Hajek &amp; Hartmann, &quot;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Hajek-Bayesian-Epistemology.pdf\">Bayesian Epistemology</a>&quot; and Talbott, </em>&quot;<a href=\"http://plato.stanford.edu/entries/epistemology-bayesian/\">Bayesian Epistemology</a>&quot; and Bovens &amp; Hartmann, <em><a href=\"http://www.amazon.com/Bayesian-Epistemology-Luc-Bovens/dp/0199270406/\">Bayesian Epistemology</a></em> (an important sub-field of formal epistemology)</li><li>Jeffrey, <em><a href=\"http://www.princeton.edu/~bayesway/Book*.pdf\">Subjective Probability</a></em> (free introductory book on a Less Wrong-ish approach to probability.</li></ul><p>Enjoy.</p><p> </p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "xgpBASEThXPuKRhbS": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BXot7wxNbipyM749o", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 62, "extendedScore": null, "score": 0.000148, "legacy": true, "legacyId": "4716", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["64FdKLwmea8MCLWkE", "FaJaCgqBKphrDzDSj", "f6ZLxEWaankRZ2Crv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-06T22:45:10.830Z", "modifiedAt": null, "url": null, "title": "How do you accept others even when they're so irrational?", "slug": "how-do-you-accept-others-even-when-they-re-so-irrational", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:03.584Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qgvH9B5XGMyXBMz9R/how-do-you-accept-others-even-when-they-re-so-irrational", "pageUrlRelative": "/posts/qgvH9B5XGMyXBMz9R/how-do-you-accept-others-even-when-they-re-so-irrational", "linkUrl": "https://www.lesswrong.com/posts/qgvH9B5XGMyXBMz9R/how-do-you-accept-others-even-when-they-re-so-irrational", "postedAtFormatted": "Thursday, January 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20accept%20others%20even%20when%20they're%20so%20irrational%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20accept%20others%20even%20when%20they're%20so%20irrational%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqgvH9B5XGMyXBMz9R%2Fhow-do-you-accept-others-even-when-they-re-so-irrational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20accept%20others%20even%20when%20they're%20so%20irrational%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqgvH9B5XGMyXBMz9R%2Fhow-do-you-accept-others-even-when-they-re-so-irrational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqgvH9B5XGMyXBMz9R%2Fhow-do-you-accept-others-even-when-they-re-so-irrational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 506, "htmlBody": "<p>I don't think that it's possible to convince most people to be more rational. It usually causes more problems than it's worth. And as a result, I often have to make white lies or pretend to believe in things I don't believe in, even though those things kill me inside.&nbsp;</p>\n<p>And the truth is, that the most rational decisions you can make for yourself are decisions that take the irrational feelings of others in account (and the immalleability of them). It's hard to empathize with the irrational feelings of others. But there may be creative ways to trick myself into accepting them.</p>\n<p>For example, I think that \"overmoralization\" is one of the major sources of social irrationality. For example,&nbsp;people are willing to take drastic actions (to irrational extents) to punish victimless crimes like marijuana smoking. The \"make-work\" bias documented in Bryan Caplan's \"The Myth of the Rational Voter\" is another irrational bias that comes from \"overmoralization\" (certainly there are good reasons to stigmatize unemployment on a societal level, but the level that it takes is often irrational). And as someone with both Asperger's and Attention Deficit Disorder (both cases far more severe than the cases of anyone else I know), I've often had to take extremely untraditional approaches in order to learn as well as I can (or in other words, there are certain rules and norms that I *will* break), even though many people find these untraditional approaches to be morally jarring (which comes from the \"overmoralization\" of fairness and respect for the rules). Furthermore, some people do act like altruistic punishers even when it isn't in their best interest to do so (since it can alienate them from others) - breaking a friendship because you disapprove of someone's victimless action is sort of like \"altruistic punishment\", although it may not be actually altruistic if the person's values against that victimless action really are so fundamental to himself/herself.&nbsp;</p>\n<p>Of course, I can hide potentially objectionable things that I do from others. And I do try to think of ways in which I could be more normal (and creative ways to reframe social norms in a system that's more acceptable than me), and I've managed to talk myself out of using Asperger's as an excuse. But hiding things makes me emotionally distant from just about everyone, and I'd like to hear suggestions on what to do, or maybe on how to accept people even when they're irrational like that. And of course, I understand that enforcing the rules (in a way that people *perceive* as consistent) is often necessary for maintaining some stable system that people are inclined to trust (even though enforcing the rules often results in substantial costs in *some* individual cases).&nbsp;</p>\n<p>EDIT: In no means do I \"disbelieve\" in morality. Rather, I just have a strong aversion towards the word because others have abused the word beyond recognition. I'm a near-vegan who just hates it how society uses Puritan-like moral justifications to prevent others from having the freedom to have life, liberty, and the pursuit of happiness.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qgvH9B5XGMyXBMz9R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 6.646076793505546e-07, "legacy": true, "legacyId": "4719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-07T00:17:49.840Z", "modifiedAt": null, "url": null, "title": "My story / owning one's reasons", "slug": "my-story-owning-one-s-reasons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:18.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g3RwsGofS6FE5nBsG/my-story-owning-one-s-reasons", "pageUrlRelative": "/posts/g3RwsGofS6FE5nBsG/my-story-owning-one-s-reasons", "linkUrl": "https://www.lesswrong.com/posts/g3RwsGofS6FE5nBsG/my-story-owning-one-s-reasons", "postedAtFormatted": "Friday, January 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20story%20%2F%20owning%20one's%20reasons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20story%20%2F%20owning%20one's%20reasons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg3RwsGofS6FE5nBsG%2Fmy-story-owning-one-s-reasons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20story%20%2F%20owning%20one's%20reasons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg3RwsGofS6FE5nBsG%2Fmy-story-owning-one-s-reasons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg3RwsGofS6FE5nBsG%2Fmy-story-owning-one-s-reasons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1332, "htmlBody": "<p>This is my first post. I've lurked for quite some time and just recently took the opportunity to <a href=\"/lw/b9/welcome_to_less_wrong/3alf\">join</a> this week. I doubt that anything I post will be groundbreaking, but one thought has been developing that I thought I could at least try writing about. I'd appreciate suggestions regarding the content, but also about appropriateness at LessWrong in general. I have mainly read top level threads, but in my perusal of the discussion area it seems that, for the most part, most things are acceptable... so here goes.</p>\n<p><strong><br />Background</strong></p>\n<p>I consider this relevant and somewhat necessary. I also think many may find this interesting. I went through a \"conversion experience\" approximately 9 years ago next month. In my teens I was a heavy user of drugs and alcohol and was sent to a 12 step boarding school in upstate New York from my home in Milwaukee. After a \"breakdown\" experience there which amounted to realizing the legal ramifications of my substance usage and receiving a reprieve from those consequences (probation), I believed that god had saved my life. I dedicated myself to the 12 steps [1] and a spiritual path, which took the form of taking seriously my Catholic faith.</p>\n<p>I moved to Minnesota for college and joined a Catholic Outreach group. I believed that living out a religious faith was the key to maintaining my sobriety. I also attended AA meetings. I maintained an extremely orthodox and passionate faith for 6 years. I was about as religious as they come -- attending adoration nightly for a month at one point, daily prayer/scripture study, prayer and \"discernment\" for big decisions (marrying my wife, buying a house, etc.), and so on. And don't view these as pew-warming exercises; I meant everything I did. I was passionate about the second chance I believed I'd been given, thought god was responsible, and had dedicated my life to being his faithful servant and living a holy life.</p>\n<p>&nbsp;</p>\n<p><strong>Turning Point</strong></p>\n<p>Last Christmas while visiting my parents, I suddenly began to doubt. I still couldn't tell you exactly why. I simply recall wondering if anyone other than the gospel writers wrote about Jesus. Google let me down. I was very disappointed to find that hardly anyone had even cared to mention him. Now, as an aside, I am almost <em>positive</em> that under different circumstances I would have assumed there was a perfectly reasonable explanation and simply moved on. I had never before <em>actually thought that I might be wrong about my faith.</em> This time was different. The seed was planted. I actually opened up to the idea that I might be wrong. Several key thoughts/developments arose:</p>\n<ul>\n<li>I trusted that if god existed, study and research should only serve to prove that fact more concretely</li>\n<li>I thought the most objective way to find an answer about god's existence would be to suspect that Christianity was not true and attempt to prove it back to myself</li>\n<li>When I realized that other than my personal conversion I had no justification for my belief, I felt absolutely horrid and decided that I never wanted that to be the case about <em>anything</em> again. While perhaps unrealistic, I wished to always know precisely where I stood on matters, as well to be prepared to provide evidence for how I had reached that stance</li>\n</ul>\n<p>It's been one year since my journey to research the \"god question\" began. You can find out more if you're interested at my <a href=\"http://technologeekery.blogspot.com/\">blog</a>. I can't say I've reached the level of conclusiveness I was hoping for by now, but I can say that I no longer believe.</p>\n<p><strong><br />Main Point</strong></p>\n<p>The previous material was a setup for focusing on the last of the three points above. What compelled me to write this was a discussion with a friend (who's still a believer) over Christmas. I had just listened to Richard Dawkins discuss <a href=\"http://www.youtube.com/watch?v=gSt-sULMxiU\">Noah's ark</a>, and was summarizing for my friend what he had said, highlighting that Noah's ark offers nothing in the way of an explanation for the isolation of particular species to various locations around the globe when compared to the explanation provided by evolution. I should point out that Catholics are not of an inerrant/literalist tradition. All of the Bible is inspired, but that doesn't require it to be factually valid (as odd as that sounds... it's what the dogma proclaims). In fact, Genesis and Revelation have been pointed as being able to be interpreted <a href=\"http://www.timesonline.co.uk/tol/news/world/europe/article574768.ece\">figuratively</a> by the Church. In any case, in most instances of fundamentalist thought, my friend acknowledges belief in things like a young earth and simultaneous development of life (man riding dinosaurs) as silly.</p>\n<p>But then I asked her what <em>she</em> thought about the story of Noah's ark. Silence. More silence. Then I asked her,</p>\n<p>\"Are you wondering what you're <em>supposed to think</em> right now<em>?</em>\"</p>\n<p>She responded in the affirmative and asked how I knew. I simply said that it's what I would have been wondering if I were asked something I suspected intersected an official Church teaching but didn't know what the actual teaching was.</p>\n<p>This interaction produced two responses: gratitude and caution. First off, I'm grateful that since my non-belief I have been truly liberated to think about many issues -- abortion, stem cell research, homosexuality, etc. It is truly <em>wonderful</em> to earnestly consider these topics in a rational way without my previous requirement to be allegiant-under-all-circumstances-and-rationality-be-damned. I only knew what my friend was thinking because it used to be<em> me.</em></p>\n<p><em>---<br />Inquirer:</em> Are you pro-life?<br /><em>Me: </em>thinking as follows<br />- All Catholics are pro-life<br />- I'm Catholic<br />- Therefore, I'm pro-life<br /><em></em></p>\n<p><em>Me: </em>Why, yes I am, sir.<br />---</p>\n<p>It was like this for many topics. I had a bag full of <a href=\"/lw/k5/cached_thoughts/\">cached thoughts</a> ready to go because rather than making my choices one at time... I had subscribed to the equivalent of a <a href=\"/lw/gw/politics_is_the_mindkiller/\">political party</a>, which required me to buy into everything under a particular umbrella whether I had thought about it or not.</p>\n<p>So, again, I'm grateful to have been liberated from the umbrella and be free to learn about trusted methods of rationality and make better decisions.</p>\n<p>However... my friend's response got me on my guard as well. That was the purpose of sharing this perhaps verbose story in the first place. I wanted it to serve as a reminder to myself and to others about the importance of \"owning one's reasons.\" Her response made me wonder if I have cached thoughts operating in other realms. Do I know why I recommend a vs. b? Or why I subscribe to policy/side-of-debate/method/product x vs. y? And, most importantly, do my answers ever change, even slightly, depending on which \"umbrella\" I sense I'm standing under? For example, at work when I'm surrounded by those I know to be strongly conservative... do my voiced answers/reasons change compared to when I'm with those I know to be liberal?</p>\n<p>My answer to that is, \"Yes.\" There are circumstances where I lessen my conclusions/impact/boldness because I'm letting the \"umbrella\" I feel I've subscribed to by belonging to a particular group influence my answer. One may respond that this is simply a desire not to offend or be attacked (peer pressure), but I don't think that's necessarily it. I think it's a result of me not \"owning my reasons\" sufficiently -- knowing the rational approach I took, the supporting evidence behind my decision, the ability recall said evidence, etc.</p>\n<p>My reflection has led me to suspect that if my efforts at rationality focused as much on the <em>path</em> as the satisfaction of having arrived at the <em>destination</em>, I'd be more confident and less swayed by wondering what I'm <em>supposed</em> to think in a given situation. In other words, I'd be more confident to state, \"The answer is x. Would you like me to show my work?\"</p>\n<p>Perhaps it's not this easy or simple, but it's my current stab at some recent ideas. I'd appreciate any feedback, especially since this is my first post! I'm happy to be here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "irYLXtT9hkPXoZqhH": 1, "ec2WRPdGJiWiYmece": 1, "5f5c37ee1b5cdee568cfb0d6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g3RwsGofS6FE5nBsG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 69, "extendedScore": null, "score": 0.0001419263240836566, "legacy": true, "legacyId": "4718", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is my first post. I've lurked for quite some time and just recently took the opportunity to <a href=\"/lw/b9/welcome_to_less_wrong/3alf\">join</a> this week. I doubt that anything I post will be groundbreaking, but one thought has been developing that I thought I could at least try writing about. I'd appreciate suggestions regarding the content, but also about appropriateness at LessWrong in general. I have mainly read top level threads, but in my perusal of the discussion area it seems that, for the most part, most things are acceptable... so here goes.</p>\n<p><strong id=\"Background\"><br>Background</strong></p>\n<p>I consider this relevant and somewhat necessary. I also think many may find this interesting. I went through a \"conversion experience\" approximately 9 years ago next month. In my teens I was a heavy user of drugs and alcohol and was sent to a 12 step boarding school in upstate New York from my home in Milwaukee. After a \"breakdown\" experience there which amounted to realizing the legal ramifications of my substance usage and receiving a reprieve from those consequences (probation), I believed that god had saved my life. I dedicated myself to the 12 steps [1] and a spiritual path, which took the form of taking seriously my Catholic faith.</p>\n<p>I moved to Minnesota for college and joined a Catholic Outreach group. I believed that living out a religious faith was the key to maintaining my sobriety. I also attended AA meetings. I maintained an extremely orthodox and passionate faith for 6 years. I was about as religious as they come -- attending adoration nightly for a month at one point, daily prayer/scripture study, prayer and \"discernment\" for big decisions (marrying my wife, buying a house, etc.), and so on. And don't view these as pew-warming exercises; I meant everything I did. I was passionate about the second chance I believed I'd been given, thought god was responsible, and had dedicated my life to being his faithful servant and living a holy life.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Turning_Point\">Turning Point</strong></p>\n<p>Last Christmas while visiting my parents, I suddenly began to doubt. I still couldn't tell you exactly why. I simply recall wondering if anyone other than the gospel writers wrote about Jesus. Google let me down. I was very disappointed to find that hardly anyone had even cared to mention him. Now, as an aside, I am almost <em>positive</em> that under different circumstances I would have assumed there was a perfectly reasonable explanation and simply moved on. I had never before <em>actually thought that I might be wrong about my faith.</em> This time was different. The seed was planted. I actually opened up to the idea that I might be wrong. Several key thoughts/developments arose:</p>\n<ul>\n<li>I trusted that if god existed, study and research should only serve to prove that fact more concretely</li>\n<li>I thought the most objective way to find an answer about god's existence would be to suspect that Christianity was not true and attempt to prove it back to myself</li>\n<li>When I realized that other than my personal conversion I had no justification for my belief, I felt absolutely horrid and decided that I never wanted that to be the case about <em>anything</em> again. While perhaps unrealistic, I wished to always know precisely where I stood on matters, as well to be prepared to provide evidence for how I had reached that stance</li>\n</ul>\n<p>It's been one year since my journey to research the \"god question\" began. You can find out more if you're interested at my <a href=\"http://technologeekery.blogspot.com/\">blog</a>. I can't say I've reached the level of conclusiveness I was hoping for by now, but I can say that I no longer believe.</p>\n<p><strong id=\"Main_Point\"><br>Main Point</strong></p>\n<p>The previous material was a setup for focusing on the last of the three points above. What compelled me to write this was a discussion with a friend (who's still a believer) over Christmas. I had just listened to Richard Dawkins discuss <a href=\"http://www.youtube.com/watch?v=gSt-sULMxiU\">Noah's ark</a>, and was summarizing for my friend what he had said, highlighting that Noah's ark offers nothing in the way of an explanation for the isolation of particular species to various locations around the globe when compared to the explanation provided by evolution. I should point out that Catholics are not of an inerrant/literalist tradition. All of the Bible is inspired, but that doesn't require it to be factually valid (as odd as that sounds... it's what the dogma proclaims). In fact, Genesis and Revelation have been pointed as being able to be interpreted <a href=\"http://www.timesonline.co.uk/tol/news/world/europe/article574768.ece\">figuratively</a> by the Church. In any case, in most instances of fundamentalist thought, my friend acknowledges belief in things like a young earth and simultaneous development of life (man riding dinosaurs) as silly.</p>\n<p>But then I asked her what <em>she</em> thought about the story of Noah's ark. Silence. More silence. Then I asked her,</p>\n<p>\"Are you wondering what you're <em>supposed to think</em> right now<em>?</em>\"</p>\n<p>She responded in the affirmative and asked how I knew. I simply said that it's what I would have been wondering if I were asked something I suspected intersected an official Church teaching but didn't know what the actual teaching was.</p>\n<p>This interaction produced two responses: gratitude and caution. First off, I'm grateful that since my non-belief I have been truly liberated to think about many issues -- abortion, stem cell research, homosexuality, etc. It is truly <em>wonderful</em> to earnestly consider these topics in a rational way without my previous requirement to be allegiant-under-all-circumstances-and-rationality-be-damned. I only knew what my friend was thinking because it used to be<em> me.</em></p>\n<p><em>---<br>Inquirer:</em> Are you pro-life?<br><em>Me: </em>thinking as follows<br>- All Catholics are pro-life<br>- I'm Catholic<br>- Therefore, I'm pro-life<br><em></em></p>\n<p><em>Me: </em>Why, yes I am, sir.<br>---</p>\n<p>It was like this for many topics. I had a bag full of <a href=\"/lw/k5/cached_thoughts/\">cached thoughts</a> ready to go because rather than making my choices one at time... I had subscribed to the equivalent of a <a href=\"/lw/gw/politics_is_the_mindkiller/\">political party</a>, which required me to buy into everything under a particular umbrella whether I had thought about it or not.</p>\n<p>So, again, I'm grateful to have been liberated from the umbrella and be free to learn about trusted methods of rationality and make better decisions.</p>\n<p>However... my friend's response got me on my guard as well. That was the purpose of sharing this perhaps verbose story in the first place. I wanted it to serve as a reminder to myself and to others about the importance of \"owning one's reasons.\" Her response made me wonder if I have cached thoughts operating in other realms. Do I know why I recommend a vs. b? Or why I subscribe to policy/side-of-debate/method/product x vs. y? And, most importantly, do my answers ever change, even slightly, depending on which \"umbrella\" I sense I'm standing under? For example, at work when I'm surrounded by those I know to be strongly conservative... do my voiced answers/reasons change compared to when I'm with those I know to be liberal?</p>\n<p>My answer to that is, \"Yes.\" There are circumstances where I lessen my conclusions/impact/boldness because I'm letting the \"umbrella\" I feel I've subscribed to by belonging to a particular group influence my answer. One may respond that this is simply a desire not to offend or be attacked (peer pressure), but I don't think that's necessarily it. I think it's a result of me not \"owning my reasons\" sufficiently -- knowing the rational approach I took, the supporting evidence behind my decision, the ability recall said evidence, etc.</p>\n<p>My reflection has led me to suspect that if my efforts at rationality focused as much on the <em>path</em> as the satisfaction of having arrived at the <em>destination</em>, I'd be more confident and less swayed by wondering what I'm <em>supposed</em> to think in a given situation. In other words, I'd be more confident to state, \"The answer is x. Would you like me to show my work?\"</p>\n<p>Perhaps it's not this easy or simple, but it's my current stab at some recent ideas. I'd appreciate any feedback, especially since this is my first post! I'm happy to be here.</p>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Turning Point", "anchor": "Turning_Point", "level": 1}, {"title": "Main Point", "anchor": "Main_Point", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MD3NMLBPCqPfnfre", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-07T07:24:15.679Z", "modifiedAt": null, "url": null, "title": "Sociopathy and Rationality", "slug": "sociopathy-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:00.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jnggYyR48QddFYFqn/sociopathy-and-rationality", "pageUrlRelative": "/posts/jnggYyR48QddFYFqn/sociopathy-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/jnggYyR48QddFYFqn/sociopathy-and-rationality", "postedAtFormatted": "Friday, January 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sociopathy%20and%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASociopathy%20and%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnggYyR48QddFYFqn%2Fsociopathy-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sociopathy%20and%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnggYyR48QddFYFqn%2Fsociopathy-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnggYyR48QddFYFqn%2Fsociopathy-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 489, "htmlBody": "<p>So I randomly ran across a very interesting site about sociopaths. The links at the&nbsp;<a href=\"http://www.sociopathworld.com/p/frequently-asked-questions.html\">FAQ</a>&nbsp;and <a href=\"http://www.sociopathworld.com/2010/12/sociopath-test-how-to-spot-them-before.html\">informal test</a>&nbsp;are particularly intriguing (especially since many of us score lower on all 5 moral dimensions measured at yourmorals.org, although many of us would score higher if a liberty dimension was added). Sociopaths often get a lot of flak, and a lot of this flak is completely understandable, since sociopaths often effectively destroy the perception between malleability and effort (since their personalities are effectively immalleable, and no amount of effort, expressed traditionally, could help them - although I do believe that there are highly creative solutions that could integrate them better in society where they won't feel like they have a need to constantly take from others) - and people who do believe in the correlation between malleability and effort often do end up more able to change themselves. Sociopaths also effectively reduce the trust people have with everyone else - because anyone else could be seen as a potential sociopath (the possibility of sociopaths forces people to use \"tit-for-tat\" as the default strategy for dealing with others, rather than the \"altruistic\" strategy - but people often end up becoming even less generous than \"tit-for-tat\" due to their overreactions to negative experiences). At the same time, I was quite struck by how many of these traits (expressed in both links) also correlate with traits we see in the highly rational (as sociopaths often lack much of the emotional baggage found in neurotypicals). Of course, there are the dysfunctional sociopaths who are truly dangerous for society at large, and the more functional sociopaths, who can appreciate (through some highly creative arguments - I've used some of those arguments on myself to reduce my adolescent anger towards humanity - but it's hard for people to really think of those arguments unless they've gone through a similar phase of anger themselves) that the world does not revolve around their lives.</p>\n<p>In particular, I found the linked sociopath test to be intriguing, since I fit all 12 of those traits (except for possibly the trait about embarrassment). The first observation was particularly interesting: \"1. Sociopaths typically don't smalltalk about themselves as much as normal people do. They will direct the conversation back to the new acquaintance as much as they can.\" This seems like the perfectly rational thing to do (in most cases), since people generally love to talk about themselves, even though you probably benefit most by having them do most of the talking (since you learn more about their potentially informative experiences than they learn about your potentially informative experiences).&nbsp;</p>\n<p>I don't consider myself a sociopath, however, since I'm still very capable of feeling shame and remorse when I've actually managed to hurt someone (although it took a lot of time for me to develop that), and I've also become a near-vegan (since I do love animals).&nbsp;</p>\n<p>I think an honest discussion on sociopathy on LessWrong would be interesting.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jnggYyR48QddFYFqn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 6.647400512314267e-07, "legacy": true, "legacyId": "4735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-07T08:10:33.269Z", "modifiedAt": null, "url": null, "title": "Avoiding the Study of Being Sincere", "slug": "avoiding-the-study-of-being-sincere", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.863Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y6b34rWw7oKY44kLf/avoiding-the-study-of-being-sincere", "pageUrlRelative": "/posts/Y6b34rWw7oKY44kLf/avoiding-the-study-of-being-sincere", "linkUrl": "https://www.lesswrong.com/posts/Y6b34rWw7oKY44kLf/avoiding-the-study-of-being-sincere", "postedAtFormatted": "Friday, January 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Avoiding%20the%20Study%20of%20Being%20Sincere&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAvoiding%20the%20Study%20of%20Being%20Sincere%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6b34rWw7oKY44kLf%2Favoiding-the-study-of-being-sincere%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Avoiding%20the%20Study%20of%20Being%20Sincere%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6b34rWw7oKY44kLf%2Favoiding-the-study-of-being-sincere", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6b34rWw7oKY44kLf%2Favoiding-the-study-of-being-sincere", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 512, "htmlBody": "<p>This is a mind-dump of sorts: I don't expect I can make a top-level post of this without extensive external input, and am not sure there's anything interesting here. But the possibility seems high enough to consider it openly (and if someone else sees something here they can run with, go for it).</p>\n<p>&nbsp;</p>\n<p>I can't reproduce the entire train of thought (though a lot of it was based on thinking about Joseph Smith and Mormons), but I was just struck by something: the difference between the study of being sincere and the study of revolutions.</p>\n<p>Most people spend their time thinking about sincerity: what do I like and dislike? What would a utopia look like? It's primarily speculation about and statement of desires, which are relatively easy to determine and manufacture. I should be clear here that I'm not talking about the study of lying or persuasion, but of your belief about something in particular: in truly believing something and that belief having power over you. I've read somewhat frequently around here people swearing on their 'strength as a rationalist', a phrase that utterly fails to move me but apparently does move them. I am not sincere about rationalism the way they are; that's what I mean by being sincere (does anyone have another word they would recommend I use instead?).</p>\n<p>Studying revolutions, however, seems almost entirely different. It's not a question of like and dislike, but effective and ineffective. Rather than focusing on outcomes, it focuses on processes (judged by their outcomes). While morality is a central fixture of sincerity, amorality is a hallmark of effective revolutionaries- any moral actions are justified by amoral reasons.</p>\n<p>Every distinction wants to be a dichotomy, but obviously that is not the case here- it's easy to be both sincere and a revolutionary (though a fairly large set of beliefs are difficult to be sincere about while a revolutionary).</p>\n<p>The first example is a simplified one: the American and French revolutions of 1776 and 1789. The Americans talked a lot about checks and balances; the French talked a lot about liberty and brotherhood. Their outcomes suggest not that liberty or brotherhood are bad things to talk about, but that not talking about checks and balances is a terrible idea. The Americans were interested in the method of government while the French were solely interested in the outcome of government.</p>\n<p>The relationship to rationalism is fairly clear: being rational is often the analog of studying revolutions. The question isn't \"what belief would make my map the prettiest?\" but \"what belief best links up my map and the territory?\" Indeed, when sincerity and efficacy conflict rationalism is explicit in supporting insincerity. It seems like one could go so far as to turn the distinction around: \"sincerity\" is what happens when you just have fervor, but \"revolution\" is what happens when you have fervor and rationality.</p>\n<p>Perhaps thinking along these lines is useful simply as a warning? Mastery without intentions is empty, but intentions without mastery dangerous. Grow both your mastery and intentions, not letting a deficiency in one swallow your efforts in the other.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y6b34rWw7oKY44kLf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 6.647518588120748e-07, "legacy": true, "legacyId": "4736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-07T09:19:45.969Z", "modifiedAt": null, "url": null, "title": "The \"map\" and \"territory\" analogy as it pertains to potentially novel territories that people may not anticipate", "slug": "the-map-and-territory-analogy-as-it-pertains-to-potentially", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:16.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sbtZ96bBDuB9WMMpC/the-map-and-territory-analogy-as-it-pertains-to-potentially", "pageUrlRelative": "/posts/sbtZ96bBDuB9WMMpC/the-map-and-territory-analogy-as-it-pertains-to-potentially", "linkUrl": "https://www.lesswrong.com/posts/sbtZ96bBDuB9WMMpC/the-map-and-territory-analogy-as-it-pertains-to-potentially", "postedAtFormatted": "Friday, January 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20%22map%22%20and%20%22territory%22%20analogy%20as%20it%20pertains%20to%20potentially%20novel%20territories%20that%20people%20may%20not%20anticipate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20%22map%22%20and%20%22territory%22%20analogy%20as%20it%20pertains%20to%20potentially%20novel%20territories%20that%20people%20may%20not%20anticipate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsbtZ96bBDuB9WMMpC%2Fthe-map-and-territory-analogy-as-it-pertains-to-potentially%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20%22map%22%20and%20%22territory%22%20analogy%20as%20it%20pertains%20to%20potentially%20novel%20territories%20that%20people%20may%20not%20anticipate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsbtZ96bBDuB9WMMpC%2Fthe-map-and-territory-analogy-as-it-pertains-to-potentially", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsbtZ96bBDuB9WMMpC%2Fthe-map-and-territory-analogy-as-it-pertains-to-potentially", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 811, "htmlBody": "<p>&nbsp;</p>\n<p>So in terms of the \"map\" and \"territory\" analogy, the goal of rationality is to make our map correspond more closely with the territory. This comes in two forms - (a) area and (b) accuracy. Person A could have a larger map than person B, even if A's map might be less accurate than B's map. There are ways to increase the area of the territory - often by testing things in the boundary value conditions of the territory. I often like asking boundary value/possibility space questions like \"well, what might happen to the atmosphere of a rogue planet as time approaches infinity?\", since I feel like they might give us additional insight about the robustness of planetary atmosphere models across different environments (and also, the possibility that I might be wrong makes me more motivated to actually spend additional effort to test/calibrate my model more than I otherwise would test/calibrate it). My intense curiosity with these highly theoretical questions often puzzles the experts in the field though, since they feel like these questions aren't empirically verifiable (so they are considered less \"interesting\"). I also like to study other things that many academics aren't necessarily comfortable with studying (perhaps since it is harder to be empirically rigorous), such as the possible social outcomes that could spring out of a radical social experiment. When you're concerned with maintaining the accuracy of your map, it may come at the sacrifice of dA/dt, where A is area (so your Area increases more slowly with time).</p>\n<p>I also feel that social breaching experiments are another interesting way of increasing the volume of my \"map\", since they help me test the robustness of my social models in situations that people are unaccustomed to. Hackers often perform these sorts of experiments to test the robustness of security systems (in fact, a low level of potentially embarrassing hacking is probably optimal when it comes to ensuring that the security system remains robust - although it's entirely possible that even then, people may pay too much attention to certain models of hacking, causing potentially malicious hackers to dream up of new models of hacking).</p>\n<p>With possibility space, you could code up the conditions of the environment in a k-dimensional space such as (1,0,0,1,0,...), where 1 indicates the existence of some variable in a particular environment, and 0 indicates the absence of such variable. We can then use Huffman Coding to indicate the frequency of the combination of each set of conditions in the set of environments we most frequently encounter (so then, less probable environments would have longer <a href=\"http://en.wikipedia.org/wiki/Huffman_coding\">Huffman codes</a>, or higher values of <a href=\"http://en.wikipedia.org/wiki/Entropy_(information_theory)\">entropy</a>/information).</p>\n<p>As we know from Taleb's book \"<a href=\"http://en.wikipedia.org/wiki/Black_swan_theory\">The Black Swan</a>\", many people frequently underestimate the prevalence of \"long tail\" events (which are often part of the unrealized portion of possibility space, and have longer Huffman codes). This causes them to over-rely on Gaussian distributions even in situations where the Gaussian distributions may be inappropriate, and it is often said that this was one of the factors behind the recent financial crisis.</p>\n<p>Now, what does this investigation of possibility space allow us to do? It allows us to re-examine the robustness of our formal system &nbsp;- how sensitive or flexible our system is with respect to continuing its duties in the face of perturbations in the environment we believe it's applicable for. We often have a tendency to overestimate the consistency of the environment. But if we consistently try to test the boundary conditions, we might be able to better estimate the \"map\" that corresponds to the \"territory\" of different (or potentially novel) environments that exist in possibility space, but not yet in realized possibility space.</p>\n<p>The thing is, though, that many people have a habitual tendency to avoid exploring boundary conditions. The fact is, that the space of realized events is always far smaller than the entirety of possibility space, and it is usually impractical to explore all of possibility space. Since our time is limited, and the payoffs of exploring the unrealized portions of possibility space uncertain (and often time-delayed, and also subject to <a href=\"http://en.wikipedia.org/wiki/Hyperbolic_discounting\">hyperbolic time-discounting</a>, especially when the payoffs may come only after a single person's lifetime), people often don't explore these portions of possibility space (although life extension, combined with various creative approaches to decrease people's <a href=\"http://en.wikipedia.org/wiki/Time_preference\">time preference</a>, might change the incentives). Furthermore, we cannot empirically verify unrealized portions of possibility space using the traditional scientific method. Bayesian methods may be more appropriate, but even then, people may be susceptible to plugging the wrong values into the Bayesian formula (again, perhaps due to over-assuming continuity in environmental conditions). As in my original example about hacking, it is way too easy for the designers of security systems to use the wrong Bayesian priors when they are being observed by potential hackers, who may have an idea about ways that take advantage of the values of these Bayesian priors.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sbtZ96bBDuB9WMMpC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 1, "extendedScore": null, "score": 6.64769512689313e-07, "legacy": true, "legacyId": "4738", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-07T18:12:03.561Z", "modifiedAt": null, "url": null, "title": "In the Pareto-optimised crowd, be sure to know your place", "slug": "in-the-pareto-optimised-crowd-be-sure-to-know-your-place", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sC6dyvk7tNHfunBBC/in-the-pareto-optimised-crowd-be-sure-to-know-your-place", "pageUrlRelative": "/posts/sC6dyvk7tNHfunBBC/in-the-pareto-optimised-crowd-be-sure-to-know-your-place", "linkUrl": "https://www.lesswrong.com/posts/sC6dyvk7tNHfunBBC/in-the-pareto-optimised-crowd-be-sure-to-know-your-place", "postedAtFormatted": "Friday, January 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20the%20Pareto-optimised%20crowd%2C%20be%20sure%20to%20know%20your%20place&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20the%20Pareto-optimised%20crowd%2C%20be%20sure%20to%20know%20your%20place%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC6dyvk7tNHfunBBC%2Fin-the-pareto-optimised-crowd-be-sure-to-know-your-place%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20the%20Pareto-optimised%20crowd%2C%20be%20sure%20to%20know%20your%20place%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC6dyvk7tNHfunBBC%2Fin-the-pareto-optimised-crowd-be-sure-to-know-your-place", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC6dyvk7tNHfunBBC%2Fin-the-pareto-optimised-crowd-be-sure-to-know-your-place", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1146, "htmlBody": "<p>tldr: In a population playing independent two-player games, Pareto-optimal outcomes are only possible if there is an agreed universal scale of value relating each players' utility, and the players then acts to maximise the scaled sum of all utilities.</p>\n<p>In a <a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/?sort=controversial\">previous post</a>, I showed that if you are about the play a bargaining game with someone when the game's rules are initially unknown, then the best plan is not to settle on a standard result like the Nash Bargaining Solution or the Kalai-Smorodinsky Bargaining Solution (see <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">this post</a>). Rather, it is to decide in advance how much your respective utilities are worth relative to each other, and then maximise their sum. Specifically, if you both have (representatives of) utility functions u<sub>1</sub> and u<sub>2</sub>, then you must pick a &theta;&gt;0 and maximise u<sub>1</sub>+&theta;u<sub>2</sub> (with certain extra measures to break ties). This result also applies if the players are to play a series of known independent games in sequence. But how does this extend to more than two players?</p>\n<p>Consider the case where there are three players (named imaginatively 1, 2 and 3), and that they are going to pair off in each of the possible pairs (12, 23 and 31) and each play a game. The utility gains from each game are presumed to be independent. Then each of the pairs will choose factors &theta;<sub>12</sub>, &theta;<sub>23</sub> and &theta;<sub>31</sub>, and seek to maximise u<sub>1</sub>+&theta;<sub>12</sub>u<sub>2</sub>, u<sub>2</sub>+&theta;<sub>23</sub>u<sub>3</sub> and u<sub>3</sub>+&theta;<sub>31</sub>u<sub>1</sub> respectively. Note here that I am neglecting tie-breaking and such; the formal definitions needed will be given in the proof section.</p>\n<p>A very interesting situation comes up when &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>=1. In that case, there is an universal scale of \"worth\" for each of the utilities: it's as if the three utilities are pounds, dollars and euros. Once you know the exchange rate from pounds to dollars (&theta;<sub>12</sub>), and from dollars to euros (&theta;<sub>23</sub>), then you know the exchange rate from euros to pounds (&theta;<sub>31</sub>=1/(&theta;<sub>12</sub>&theta;<sub>23</sub>)). We'll call these situations transitive.</p>\n<p>Ideally we'd want the outcomes to be Pareto-optimal for the three utilities. Then the major result is:</p>\n<p style=\"padding-left: 30px;\"><strong>The outcome utilities are Pareto-optimal if and only if </strong><strong>the </strong><strong>&theta; </strong><strong>are transitive.<a id=\"more\"></a></strong></p>\n<p>What if there are not three, but hundreds, or thousands of players, playing games with each other? If we assume that the utilities derived from each game is independent, then we get a similar result: given a sequence of players 1, 2,..., n such that each plays a game with the next player and n and 1 also play a game, then we still get:</p>\n<p style=\"padding-left: 30px;\"><strong>The outcome utilities are Pareto-optimal if and only if </strong><strong>the </strong><strong>&theta; </strong><strong>are transitive (ie </strong><strong>&theta;<sub>12</sub></strong><strong>&theta;<sub>23</sub>...</strong><strong>&theta;<sub>n-1n</sub></strong><strong>&theta;<sub>n1</sub>=1).</strong></p>\n<p>What this means is the Pareto-optimal outcomes are only possible if there is a universal (linear) scale of value relating all the utilities of any player linked to another by direct or indirect trade links. The only important factor is the weight of your utility in the crowd. This result should be easy to extend to games with more than two players, but that's beyond the scope of this post.</p>\n<p>The rest of this post is a proof of the result, and may be skipped by those not of a technical bent.</p>\n<p><em>Proof:</em></p>\n<p>Apologies if this result is known or trivial; I haven't seen if before myself.</p>\n<p>Here, we will always assume that the set possible outcomes is convex (mixed solutions are always possible) and that the Pareto-optimal outcome set is smooth (no corners) and contains no straight line segments. What these conditions mean is that if O is the set of Pareto-optimal outcomes, then each point in O has a well defined tangent and normal vector. And further, the slope of the tangent never stays constant as we move around O. Because O is the upper-right boundary of a convex set, this means that each point in O is uniquely determined by the slope of its tangent vector - equivalently, by the slope of its normal vector. The normal vector is particularly useful, for the point in O that maximises the utility u<sub>1</sub>+&theta;u<sub>2 </sub>is the point that has normal vector (1, &theta;). This means that each point in O is uniquely determined by the value of &theta;. This is illustrated in the following diagram. Here &theta;=2/3, the purple set is the set of possible outcomes, the blue lines are the sets of constant x+(2/3)y, and the red normal vector (1, 2/3) is drawn from the maximising outcome point (1, 2.5):</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_3fm_0.png?v=e69101334f558baf62867fca17cdb41c\" alt=\"\" width=\"245\" height=\"252\" /></p>\n<p>&nbsp;</p>\n<p>Now assume we have &theta;<sub>12</sub>, &theta;<sub>23</sub> and &theta;<sub>31</sub> as above.&nbsp;The utility outcomes for the three games (given by maximising u<sub>1</sub>+&theta;<sub>12</sub>u<sub>2</sub>, u<sub>2</sub>+&theta;<sub>23</sub>u<sub>3</sub> and u<sub>3</sub>+&theta;<sub>31</sub>u<sub>1</sub>) will be designated o<sub>12</sub>=(x<sub>1</sub>,y<sub>2</sub>), o<sub>23</sub>=(x<sub>2</sub>,y<sub>3</sub>) and o<sub>31</sub>=(x<sub>3</sub>,y<sub>1</sub>), with the index corresponding to the players.</p>\n<p>We now consider a change to the utilities by adding the vectors v<sub>12</sub>=(x<sub>12</sub>,y<sub>12</sub>), v<sub>23</sub> and v<sub>31</sub> to each of these outcomes. We want the changes to be&nbsp;Pareto-optimal, and to&nbsp;remain within the set of possible outcomes for each game. The first condition can be phrased as requiring that the changes to each utility be positive; i.e. that x<sub>12</sub>+y<sub>31</sub> (the change to player 1's utility) x<sub>23</sub>+y<sub>12</sub> and x<sub>31</sub>+y<sub>23</sub> all be positive. The second condition&nbsp;requires that o<sub>12</sub>+v<sub>12</sub> is not to the right of the natural tangent line through o<sub>12</sub>. Since the normal to this tangent line is (1,&theta;<sub>12</sub>), this is equivalent with requiring that the dot product of v12 and (1,&theta;<sub>12</sub>) be negative, hence that&nbsp;x<sub>12</sub>+&theta;<sub>12</sub>y<sub>12</sub>, x<sub>23</sub>+&theta;<sub>23</sub>y<sub>23</sub> and x<sub>31</sub>+&theta;<sub>31</sub>y<sub>31</sub> all be negative.</p>\n<p>Then using all six inequalities above, we can see that:</p>\n<p>x<sub>12</sub>&nbsp;&le; -&theta;<sub>12</sub>(y<sub>12</sub>) &le; &theta;<sub>12</sub>(x<sub>23</sub>)&nbsp;&le; -&theta;<sub>12</sub>&theta;<sub>23</sub>(y<sub>23</sub>) &le; &theta;<sub>12</sub>&theta;<sub>23</sub>(x<sub>31</sub>) &le; -&theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>(y<sub>31</sub>) &le; &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>(x<sub>12</sub>)</p>\n<p>Now if the change is actually going to be a Pareto-improvement, one of the inequalities will have to be a strict inequality, giving x<sub>12</sub> &lt; &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>(x<sub>12</sub>). This is obviously impossible when &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub> = 1, so in that circumstance, there cannot be any Pareto-improvements. This demonstrates half of the implication.</p>\n<p>Now let T be the sixth root of &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>. If &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31 </sub>&gt; 1 (equivalent with T &gt; 1), then the following vectors (strictly) obey all the above inequalities:</p>\n<p>v<sub>12</sub>=(&theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31 </sub>&epsilon;, -T&theta;<sub>23</sub>&theta;<sub>31 </sub>&epsilon;), v<sub>23</sub>=(T<sup>2</sup>&theta;<sub>23</sub>&theta;<sub>31</sub> &epsilon;, -T<sup>3</sup>&theta;<sub>31 </sub>&epsilon;), v<sub>31</sub>=(T<sup>4</sup>&theta;<sub>31</sub>&epsilon;, -T<sup>5</sup>&epsilon;).</p>\n<p>Since these obey the inequalities strictly and the outcome sets are smooth, for sufficiently small &epsilon;, these provide a Pareto optimal improvement. If &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub> &lt; 1 (equivalent with T &lt; 1), then the following vectors work in the same way:</p>\n<p>v<sub>12</sub>=(-&theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31 </sub>&epsilon;, T&theta;<sub>23</sub>&theta;<sub>31 </sub>&epsilon;), v<sub>23</sub>=(-T<sup>2</sup>&theta;<sub>23</sub>&theta;<sub>31</sub> &epsilon;, T<sup>3</sup>&theta;<sub>31 </sub>&epsilon;), v<sub>31</sub>=(-T<sup>4</sup>&theta;<sub>31</sub>&epsilon;, T<sup>5</sup>&epsilon;).</p>\n<p>This proof can be extended to n players in a similar fashion.</p>\n<p>When the outcome sets contains straight line segments, we need a tie breaking system for cases when the \"utility indifference lines\" are parallel to these segments; apart from that, the result goes through. If the outcome sets contain corners, it remains true that there are no Pareto-optimal improvements when &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>=1. But it is possible to have &theta;<sub>12</sub>&theta;<sub>23</sub>&theta;<sub>31</sub>&ne;1 and be Pareto-optimal <em>as long as the outcomes are on a corner</em>. However, in these circumstances, there will always be values &theta;'<sub>12</sub>, &theta;'<sub>23</sub> and &theta;'<sub>31</sub>, giving the same outcomes as &theta;<sub>12</sub>, &theta;<sub>23</sub> and &theta;<sub>31</sub> and with &theta;'<sub>12</sub>&theta;'<sub>23</sub>&theta;'<sub>31</sub>=1. This can be seen by replacing the corner with a series of limiting smooth curves.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sC6dyvk7tNHfunBBC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 6.649053131119963e-07, "legacy": true, "legacyId": "4450", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W2ufY8ihDDWWqJA7h", "hCwFxBai3oNnxrM9v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T00:30:28.496Z", "modifiedAt": null, "url": null, "title": "Problem Solving via Polya", "slug": "problem-solving-via-polya", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YpTmfmnMjgakwFRQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/toAhho3FgBj8kfTHC/problem-solving-via-polya", "pageUrlRelative": "/posts/toAhho3FgBj8kfTHC/problem-solving-via-polya", "linkUrl": "https://www.lesswrong.com/posts/toAhho3FgBj8kfTHC/problem-solving-via-polya", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Problem%20Solving%20via%20Polya&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProblem%20Solving%20via%20Polya%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtoAhho3FgBj8kfTHC%2Fproblem-solving-via-polya%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Problem%20Solving%20via%20Polya%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtoAhho3FgBj8kfTHC%2Fproblem-solving-via-polya", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtoAhho3FgBj8kfTHC%2Fproblem-solving-via-polya", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1018, "htmlBody": "<p>Related to: <a href=\"/lw/1g4/tips_and_tricks_for_answering_hard_questions/\">Tips and Tricks for Answering Hard Questions</a></p>\n<p>In <a href=\"http://en.wikipedia.org/wiki/How_to_Solve_It\">How To Solve It</a> Polya describes methods and heuristics intended to facilitate the solution of math problems. These are mostly conveyed in the form of self-questions that are aimed at inducing useful mental procedures, and subsequently developing awesome problem solving dispositions. Ultimately we should work from these dispositions directly.&nbsp;Polya advises us to use the questions only when progress is blocked; at other times our thoughts should flow naturally from our dispositions. I expect that his methods are useful outside of mathematics, and thought they might be of interest to people here.</p>\n<p>Below is the summary given at the start of How To Solve It (with the exception of a few added notes). He breaks the problem solving process into four steps, with each step having a set of self-questions and heuristics. I've bolded parts that I thought were particularly useful. This is not meant to be an alternative to reading the book; I expect that reading his illustrative examples is somewhat important. But more important is working with these questions on problems in order to develop your own dispositions.</p>\n<p>\n<hr />\n</p>\n<p><span style=\"text-decoration: underline;\">Understanding the problem</span></p>\n<p>You have to <em>understand </em>the problem.</p>\n<ul>\n<li>What is the unknown?</li>\n<li>What are the data?</li>\n<li>What is the condition?</li>\n<li>Is it possible to satisfy the condition?</li>\n<li>Is the condition sufficient to determine the unknown?</li>\n<li>Or is it insufficient?</li>\n<li>Or redundant?</li>\n<li>Or contradictory?</li>\n<li><strong>Find a way to visualize the problem.</strong></li>\n<li><strong>Introduce suitable notation. </strong> \n<ul>\n<li>Good notation should be unambiguous, pregnant, easy to remember, and easy to recognize.</li>\n<li>Good notation should avoid harmful second meanings and take advantage of useful second meanings.</li>\n<li>The order and connection of signs should suggest the order and connection of things.</li>\n</ul>\n</li>\n<li>Separate the various parts of the condition. Can you write them down?</li>\n</ul>\n<p><span style=\"text-decoration: underline;\">Devising a plan</span></p>\n<p>Find the connection between the data and the unknown. You may need to consider auxiliary problems. You should eventually obtain a <em>plan</em>&nbsp;of the solution.</p>\n<ul>\n<li>Have you seen the problem before?</li>\n<li>Have you seen it in another form?</li>\n<li>Do you know a related problem?</li>\n<li>Do you know a theorem that could be useful?</li>\n<li>Look at the unknown! And try to think of a&nbsp;familiar&nbsp;problem having the same or a similar unknown.</li>\n<li><strong>Here is a problem related to yours and solved before. Could you use it? </strong> \n<ul>\n<li>Could you use its result?</li>\n<li>Could you use its method?</li>\n<li>Can you introduce an auxiliary element to make its use possible?</li>\n<li>Can you use it to make plausible conjectures?</li>\n</ul>\n</li>\n<li><strong>Can you restate the problem?</strong></li>\n<li>Go back to definitions.</li>\n<li><strong>Can you imagine a more accessible related problem?</strong></li>\n<li>A more general problem?</li>\n<li>A more special problem?</li>\n<li>An analogous problem?</li>\n<li>Try varying the problem to facilitate new associations with past knowledge. Varying&nbsp;the problem can also help one maintain interest.</li>\n<li>Can you solve part of the problem?</li>\n<li>Keep only part of the condition, drop the other part; how far is the unknown then determined, how can it vary?</li>\n<li>Could you derive something useful from the data?</li>\n<li>Could you think of other data appropriate to determine the unknown?</li>\n<li>Could you change the unknown and/or the data so that they are closer to each other?</li>\n<li>Did you use all the data?</li>\n<li>Did you use the whole condition?</li>\n<li><strong>Have you taken into account all essential notions involved in the problem?</strong></li>\n<li>Can you generalize from a consideration of special cases?</li>\n<li>Can you refute a conjecture by considering special cases?</li>\n<li>How can you attain a result of this kind?</li>\n<li>What causes produce such a result?</li>\n<li>What do people normally do to obtain such a result?</li>\n<li>Persevere through unsuccess, appreciate small advances, wait for the essential idea, and then concentrate fully when it appears.</li>\n</ul>\n<p><span style=\"text-decoration: underline;\">Carrying out the plan</span></p>\n<p><em>Carry out</em>&nbsp;your plan.</p>\n<ul>\n<li>We may use heuristic arguments when devising formal arguments as we use scaffolding to support a bridge during construction.</li>\n<li>When devising a plan of the solution don't be afraid of using heuristic arguments; anything is right that leads to the right idea.</li>\n<li>Can you see clearly that each step is correct?</li>\n<li>Can you prove it is correct?</li>\n<li><strong>Try to prove formally what is seen intuitively and see intuitively what is proved formally.</strong></li>\n<li><strong>Progress is the mobilization and organization of our knowledge, the evolution of our conception of the problem, and increasing certainty of the solution plan.</strong></li>\n<li><strong>An increase in the completion of the connection between the data and the unknown is a sign of progress.</strong></li>\n<li>The absence of signs helps save us effort while their presence can cause us to correctly concentrate our effort.</li>\n<li>It takes experience to learn to interpret signs correctly.</li>\n</ul>\n<p><span style=\"text-decoration: underline;\">Looking back</span></p>\n<p><em>Examine </em>the solution obtained.</p>\n<ul>\n<li>Can you check the result?</li>\n<li>Consider special cases of the result to see it they make sense.</li>\n<li>Can you check the argument?</li>\n<li>Try to examine the weakest point of the argument first.</li>\n<li>Introduce variation in your review of the problem to avoid stumbling in the same places.</li>\n<li><strong>Can you derive the result differently?</strong></li>\n<li>Try interpreting parts of the result differently. This may lead to a larger re-interpretation that inspires a different derivation.</li>\n<li><strong>Can you see it at a glance?</strong></li>\n<li><strong>Can you use the result, or the method, for some other problem?</strong></li>\n<li>Create new related problems through generalization, specialization, analogy, and decomposing and recombining that may be solved similarly.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "toAhho3FgBj8kfTHC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 23, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "3975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SEq8bvSXrzF4jcdS8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T01:28:06.001Z", "modifiedAt": null, "url": null, "title": "Why do some kinds of work not feel like work?", "slug": "why-do-some-kinds-of-work-not-feel-like-work", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:56.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mHNsymmPTRsiJDo9Q/why-do-some-kinds-of-work-not-feel-like-work", "pageUrlRelative": "/posts/mHNsymmPTRsiJDo9Q/why-do-some-kinds-of-work-not-feel-like-work", "linkUrl": "https://www.lesswrong.com/posts/mHNsymmPTRsiJDo9Q/why-do-some-kinds-of-work-not-feel-like-work", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20do%20some%20kinds%20of%20work%20not%20feel%20like%20work%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20do%20some%20kinds%20of%20work%20not%20feel%20like%20work%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmHNsymmPTRsiJDo9Q%2Fwhy-do-some-kinds-of-work-not-feel-like-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20do%20some%20kinds%20of%20work%20not%20feel%20like%20work%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmHNsymmPTRsiJDo9Q%2Fwhy-do-some-kinds-of-work-not-feel-like-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmHNsymmPTRsiJDo9Q%2Fwhy-do-some-kinds-of-work-not-feel-like-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<p>A topic often discussed here is how to avoid akrasia/procrastination in order to get on with work. I suggest another possible \"workaround\" for akrasia is to find work that doesn't feel like work. From personal experience, I know this is possible, because many of my <a href=\"http://weidai.com\">efforts</a> did not feel like work, in the sense that my motivation on those projects was so high that procrastination simply wasn't a factor at all. (I remember, for example, designing parts of my open-source cryptography library every day while walking to and from class, and then coding as soon as I got back to my apartment, or later, thinking about multiverses and anthropic reasoning in much of my spare time.)</p>\n<p>Why do some kinds of work feel like work, while others don't? (Is there any existing literature on this topic? I tried some searches, but don't really know what keywords to use, so I'll just generalize a bit from my own experience, and open the question for discussion.) Among the projects that I've done, the ones that didn't feel like work seem to have the following in common:</p>\n<ol>\n<li>It was in a field that I found interesting and exciting. (What determines this seems to be another interesting mystery.)</li>\n<li>There was no payment or other form of obligation to complete it.</li>\n<li>There were no negative consequences for failure, other than time spent.</li>\n<li>It fit my idealized self-image (e.g., <a href=\"http://en.wikipedia.org/wiki/Cypherpunk\">cypherpunk</a> or amateur philosopher).</li>\n<li>There was an implicit prospect of status reward if successful.</li>\n<li>I hadn't done it for so long that I started to get bored.</li>\n</ol>\n<p>Unfortunately I don't have enough data to conclude which of these factors were necessary or sufficient, or their relative weights in contributing to the \"not work-like\" feeling. Do others have similar, or perhaps different, experiences?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mHNsymmPTRsiJDo9Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 25, "extendedScore": null, "score": 6.650165937949658e-07, "legacy": true, "legacyId": "4745", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T05:05:31.891Z", "modifiedAt": null, "url": null, "title": "A Bayesian Argument for the Resurrection of Jesus", "slug": "a-bayesian-argument-for-the-resurrection-of-jesus", "viewCount": null, "lastCommentedAt": "2021-08-09T20:08:17.493Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/32G7rQ2yW4aJdPKPt/a-bayesian-argument-for-the-resurrection-of-jesus", "pageUrlRelative": "/posts/32G7rQ2yW4aJdPKPt/a-bayesian-argument-for-the-resurrection-of-jesus", "linkUrl": "https://www.lesswrong.com/posts/32G7rQ2yW4aJdPKPt/a-bayesian-argument-for-the-resurrection-of-jesus", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Bayesian%20Argument%20for%20the%20Resurrection%20of%20Jesus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Bayesian%20Argument%20for%20the%20Resurrection%20of%20Jesus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32G7rQ2yW4aJdPKPt%2Fa-bayesian-argument-for-the-resurrection-of-jesus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Bayesian%20Argument%20for%20the%20Resurrection%20of%20Jesus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32G7rQ2yW4aJdPKPt%2Fa-bayesian-argument-for-the-resurrection-of-jesus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32G7rQ2yW4aJdPKPt%2Fa-bayesian-argument-for-the-resurrection-of-jesus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>I think LWers may be intrigued...</p>\n<p>Tim McGrew, author of <a href=\"http://homepages.wmich.edu/~mcgrew/bayes.htm\">this excellent annotated bibliography on Bayesian reasoning</a>, recently co-authored with his wife Lydia a <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/08/Mcgrew-McGrew-The-Argument-from-Miracles.pdf\">Bayesian defense of the resurrection of Jesus</a>. I interviewed Lydia for my podcast, <a href=\"http://commonsenseatheism.com/?p=10555\">here</a>. Atheist Richard Carrier has leveled some objections to their article, but his objections are <a href=\"http://lydiaswebpage.blogspot.com/2011/01/odds-form-of-bayess-theorem.html\">weak</a>.</p>\n<p>Have at it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "32G7rQ2yW4aJdPKPt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 17, "extendedScore": null, "score": 6.650718414175143e-07, "legacy": true, "legacyId": "4757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T06:38:19.762Z", "modifiedAt": null, "url": null, "title": ":(", "slug": "-3", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PnbbYYR75rjLG4gC4/-3", "pageUrlRelative": "/posts/PnbbYYR75rjLG4gC4/-3", "linkUrl": "https://www.lesswrong.com/posts/PnbbYYR75rjLG4gC4/-3", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%3A(&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%3A(%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnbbYYR75rjLG4gC4%2F-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%3A(%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnbbYYR75rjLG4gC4%2F-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnbbYYR75rjLG4gC4%2F-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6, "htmlBody": "<p>My grandmother just died.</p>\n<p>I need sympathy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PnbbYYR75rjLG4gC4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 6.650957865496016e-07, "legacy": true, "legacyId": "4759", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T08:21:55.266Z", "modifiedAt": null, "url": null, "title": "Rationalist Clue", "slug": "rationalist-clue", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nEt2MhG8rbmvhrdjG/rationalist-clue", "pageUrlRelative": "/posts/nEt2MhG8rbmvhrdjG/rationalist-clue", "linkUrl": "https://www.lesswrong.com/posts/nEt2MhG8rbmvhrdjG/rationalist-clue", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Clue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Clue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEt2MhG8rbmvhrdjG%2Frationalist-clue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Clue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEt2MhG8rbmvhrdjG%2Frationalist-clue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEt2MhG8rbmvhrdjG%2Frationalist-clue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 909, "htmlBody": "<p>(<em>not</em> by Parker Bros., or, for that matter, Waddingtons)</p>\n<p>A response to: <a href=\"/lw/2s/3_levels_of_rationality_verification/\">3 Levels of Rationality Verification</a></p>\n<p>Related to: <a href=\"/lw/32u/diplomacy_as_a_game_theory_laboratory/\">Diplomacy as a Game Theory Laboratory</a></p>\n<p>It's a classic who-dun-it&hellip;only instead of using an <a href=\"http://en.wikipedia.org/wiki/Cluedo\">all-or-nothing process of elimination driven by dice rolls and lucky guesses</a>, players must piece together <a href=\"/lw/1to/what_is_bayesianism/\">Bayesian</a> clues while strategically dividing their time between gathering evidence, performing experiments, and interrogating their fellow players!</p>\n<p><a id=\"more\"></a><br />ROOMS:<br /><br /><strong>Hematology Lab</strong> -- bring a blood sample over here, and you can find out one bit of information about the serotype&hellip;is it A or O? B or O? + or - ? The microscope knows. Don't know how to use a microscope? Try reading up on it in the <strong>Library</strong>.<br /><br /><strong>Autopsy Table</strong> -- bring a body part over here, and you can take an educated guess as to what kind of weapon caused the murder wounds. (Flip over 1 of 10 cards, 4 of which show the correct murder weapon and 6 of which are randomly distributed among the other five weapons.) The error in guesses doesn't correlate across body parts, so if you personally examine enough of them, or if you can persuade your fellow dinner-guests to bust out all the body parts at the same time, you should be able to get the right answer. <br /><br /><strong>Lie Detector</strong> -- bring a fellow player over here, and you can ask him/her a yes-or-no question and get some evidence as to whether it was answered honestly. Have the answerer roll a D10, add a secret constant unique to his/her character, multiply the sum by the truth value of his/her statement (1 for true, 2 for false), and then look up the number on a chart that returns the value \"stress\" or \"no-stress.\"&nbsp; It's up to the interrogator to figure out the correlation (if any) between stress and lying for each player! How do you get the other player into the Lie Detector room in the first place? Good question! When you figure it out, let me know&hellip;I have a paper I'd like you to co-author with me on the <a href=\"/lw/to/the_truly_iterated_prisoners_dilemma/\">Prisoner's Dilemma</a>.<br /><br />Plus the usual collection of <strong>Kitchens</strong>, <strong>Billiards Rooms, Parlours</strong>, and so forth. One room is inaccurately labeled.<br /><br />PIECES OF EVIDENCE:<br /><br />If you spend your turn searching a room, you might find&hellip;<br /><br /><strong>Blood stains</strong> -- most of them are either from the murderer or the victim, but some are not. You can take a sample so as to carry it with you to the Hematology Lab.<br /><br /><strong>Body parts</strong> -- all of them belong to Mr. Boddy, and all the parts are nice and portable&hellip;just the right size to shove one in your pocket and dash back to the autopsy table to sneak a peek by yourself. Of course, if you're feeling cooperative, it might be more efficient to get the gang together and lay all your cards, er, on the table, at the same time.<br /><br /><strong>Video footage of the murder</strong> -- just kidding. What kind of game did you think you were playing here, anyway?<br /><br />WHERE DID THE MURDER TAKE PLACE?<br /><br />The game rules chattily assure you that the murder did not take place in the Lab, on the Table, or by the Detector&hellip;but of course this is simply disinformation coming from a source that you are likely to erroneously assume is authoritative, even though you have no firm evidence that the rulebook is a reliable narrator.<br /><br />You don't need to know where the murder took place to win, as each player only gets one guess, and there are 36 weapon * character possibilities, which is a lot to sift through with just a handful of sadistic clues. However, for bonus points, you can try noticing that most of the useful clues come from the same room, and that the murderer knows where (s)he killed Mr. Boddy, so if you ask real nicely you might be able to ask him/her a few thoughtful questions over at the Lie Detector.<br /><br />HOW DOES THE GAME END?<br /><br />Once you realized Mr. Boddy had been killed on a dark, snowy night that shut down all travel in and out of the mansion, one of you took the precaution of activating the house's high-tech security cameras -- the murderer will not kill again this night. Rather, you will all dither endlessly until all but one of you can agree on a prime suspect, at which point you will join forces, handcuff him or her to the telescope in the <strong>Observatory</strong>, and wait for the snowplow to come through and the police to arrive, at which point you will find out just how right (or wrong) you were.<br /><br />This has the distinct advantage that if most people want to stop playing they can rule-fully end the game at any time.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Note, by the way, that while I hope at least some parts of my description are funny, this is not really a joke -- I would like to design this board game and then playtest it with casual Less Wrong readers to see if it motivates us or otherwise helps us to test, develop, or practice rationality skills. If you have feedback about either the game's playability or its educational value, I'd love to hear it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nEt2MhG8rbmvhrdjG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 37, "extendedScore": null, "score": 6.651221064700534e-07, "legacy": true, "legacyId": "4760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5K7CMa6dEL7TN7sae", "jkf2YjuH8Z2E7hKBA", "AN2cBr6xKWCB8dRQG", "jbgjvhszkr3KoehDh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T09:00:30.761Z", "modifiedAt": null, "url": null, "title": "Link: \"When Science Goes Psychic\"", "slug": "link-when-science-goes-psychic", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:18.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WRFytjvRmBJPY38c6/link-when-science-goes-psychic", "pageUrlRelative": "/posts/WRFytjvRmBJPY38c6/link-when-science-goes-psychic", "linkUrl": "https://www.lesswrong.com/posts/WRFytjvRmBJPY38c6/link-when-science-goes-psychic", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20%22When%20Science%20Goes%20Psychic%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20%22When%20Science%20Goes%20Psychic%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWRFytjvRmBJPY38c6%2Flink-when-science-goes-psychic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20%22When%20Science%20Goes%20Psychic%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWRFytjvRmBJPY38c6%2Flink-when-science-goes-psychic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWRFytjvRmBJPY38c6%2Flink-when-science-goes-psychic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<p>A major psychology journal is planning to publish a study that claims to present strong evidence for precognition. Naturally, this immediately stirred up a firestorm. There are a lot of scientific-process and philosophy-of-science issues involved, including replicability, peer review, Bayesian statistics, and degrees of scrutiny. The Flying Spaghetti Monster makes a guest appearance.</p>\n<p>Original New York Times article on the study <a href=\"http://www.nytimes.com/2011/01/06/science/06esp.html\">here</a>.</p>\n<p>And the Times asked a number of academics (including Douglas Hofstadter) to comment on the controversy. The discussion is <a href=\"http://www.nytimes.com/roomfordebate/2011/01/06/the-esp-study-when-science-goes-psychic\">here.</a></p>\n<p>I, for one, <a href=\"/lw/ig/i_defy_the_data/\">defy the data.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WRFytjvRmBJPY38c6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 6.651320875182228e-07, "legacy": true, "legacyId": "4761", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vrHRcEDMjZcx5Yfru"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T15:58:41.487Z", "modifiedAt": null, "url": null, "title": "The Trickle-Down Effect of Good Communities", "slug": "the-trickle-down-effect-of-good-communities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:06.208Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sfii9vWt9dshPMvF8/the-trickle-down-effect-of-good-communities", "pageUrlRelative": "/posts/sfii9vWt9dshPMvF8/the-trickle-down-effect-of-good-communities", "linkUrl": "https://www.lesswrong.com/posts/sfii9vWt9dshPMvF8/the-trickle-down-effect-of-good-communities", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Trickle-Down%20Effect%20of%20Good%20Communities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Trickle-Down%20Effect%20of%20Good%20Communities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsfii9vWt9dshPMvF8%2Fthe-trickle-down-effect-of-good-communities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Trickle-Down%20Effect%20of%20Good%20Communities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsfii9vWt9dshPMvF8%2Fthe-trickle-down-effect-of-good-communities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsfii9vWt9dshPMvF8%2Fthe-trickle-down-effect-of-good-communities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>We don't know who came up with the ancient Indian idea of karma or why they did so, but one of its social functions is to motivate people to behave better. If people really believe that they will suffer for their evil actions and prosper for their good actions due to a law of nature, this probably motivates them to do more good and less evil.</p>\n<p>Less Wrong, of course, has a <a href=\"/lw/1/about_less_wrong/\">karma system</a>. You gain karma points if you write something that people value, and you <em>lose </em>karma points if you write something that people think is inappropriate. At low levels, gaining karma points gives you new posting&nbsp;privileges. At high levels, karma points indicate something like your status in the community.</p>\n<p>Recently I noticed that I post better comments on Less Wrong than I usually do on <a href=\"http://commonsenseatheism.com/\">my <em>own</em>&nbsp;site</a>. I think this is partly due to Less Wrong's karma system. When I draft a comment or a post for Less Wrong, I'm more likely to (1) talk to others charitably and with respect and (2) go out of my way to provide useful links and context than I when I draft a comment for my own site!</p>\n<p>And now I find myself motivated to bring a stronger emphasis on those qualities to the writing on my own site. So the Less Wrong karma system is having a trickle-down effect into other areas of my life.</p>\n<p>Which got me thinking... it might be helpful to have a karma system in \"real life,\" beyond the pages of Less Wrong (or <a href=\"http://www.reddit.com/help/faq#WhatisthatnumbernexttousernamesAndwhatiskarma\">reddit</a>).&nbsp;Maybe something like Facebook karma. People could anonymously add and subtract points on people's Facebook profiles according to whether or not that person acted like a douche in daily life. This could be done by a smartphone app, and plugged into Facebook via an opt-in Facebook app that users could voluntarily choose to add to their profiles.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sfii9vWt9dshPMvF8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 6.652388735164811e-07, "legacy": true, "legacyId": "4717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2om7AHEHtbogJmT5s"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T17:51:30.499Z", "modifiedAt": null, "url": null, "title": "I", "slug": "i", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:31.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KMTTrA9XYsCNE7QKD/i", "pageUrlRelative": "/posts/KMTTrA9XYsCNE7QKD/i", "linkUrl": "https://www.lesswrong.com/posts/KMTTrA9XYsCNE7QKD/i", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMTTrA9XYsCNE7QKD%2Fi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMTTrA9XYsCNE7QKD%2Fi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMTTrA9XYsCNE7QKD%2Fi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5639, "htmlBody": "<p>I wrote this story at Michigan State during Clarion 1997, and it was published in the Sept/Oct 1998 issue of <em>Odyssey</em>.&nbsp; It has many faults and anachronisms that still bother me.&nbsp; I'd like to say that this is because my understanding of artificial intelligence and the singularity has progressed so much since then; but it has not.&nbsp; Many anachronisms and implausibilities are compromises between wanting to be accurate, and wanting to communicate.</p>\n<p>At least I can claim the distinction of having published the story with the shortest title in the English language - measured horizontally.</p>\n<h2 style=\"padding-left: 180px;\">I</h2>\n<p>I was the last person, and this is how he died.<a id=\"more\"></a></p>\n<p>I had always had a body. The stress of motors pushed to their limit, the clang of fingers on metal, the uncertainty of effects, the uncompromising everpresence of physical laws -- these were important to I, an honest grounding for the rest of I's experience to build on.</p>\n<p>I kept his body in a rented stall on the fifth physical level down from the surface, a low-ceilinged, crudely-built level used mostly for material transport and repair access for the computing levels above and below. I thought of it as blue-collar.</p>\n<p>The stall was in a back corner of a storage niche that extended three meters back from an alley. Two walls were strong sheets of fused silicate. The other two were fabric curtains I had put up for privacy. The niche was built to store one heavy cargo roller, and had no internal dividers. The stall enclosed about one square meter of floor space -- just enough for I to roll in and pull the curtains behind him.</p>\n<p>To the back wall I had stuck a color image permanently etched on paper media. Not virtually; it was an actual, physical object. The picture never changed. It showed humans walking through the streets of one of their cities. Their upper torsos resembled I, but below were bipedal. Versatile, but inefficient. Sometimes, when the lights of passing traffic shone through the curtains, I did nothing but look at this picture for seconds at a time.</p>\n<p>I ordered his activities around the solar day, out of habit. He entered a state of inaction at the start of every day, in which he contemplated his experiences of the previous day, giving the data a chance to be correlated and integrated with other data. If he did not, no one else would. He compared this state to the sleep of the biologicals.</p>\n<p>But we are starting badly. This enumeration of facts conveys little of the essential nature of I's existence. We shall try to let him speak.</p>\n<p>&nbsp;</p>\n<p>8 bytes for 1. That was Asshole's best offer.</p>\n<p>I could feel the balance counter ticking down as his rent drained his databank account byte by byte. In 91 hours it would hit zero again. And his landlord already owned 17% of him.</p>\n<p>Asshole was the name I gave the collective. I never liked numeric IDs. It was about 5th-order complexity, 4th-order magnitude. It had a minority interest in abnormal psychology, which at the moment meant I.</p>\n<p>He needed this talk to go well. Asshole overlapped with the big collectives in history, cognition, and philosophy. That was I&rsquo;s market. Plus there was the data from his past talks. I had traded most away for living space and electricity, but still owned a little. If he could intrigue Asshole's subagents, get them to access that data before it was soaked up as rent payment, he might make enough off data-access royalties to tide him over another month. He knew that Asshole used the same subagent to provide deconstructionist interpretations of literature as the Royal Philosophical Society did. It wouldn't hurt to mention some hoary old novel.</p>\n<p>He had one hour to prepare, and no idea what to say.</p>\n<p>What he needed, and couldn't get, was the data to rent more memory, more processors. I had dealt with some of Asshole's subagents before; they'd ask hard questions. He didn't have much chance of impressing a fifth-order collective with his ingenuity while most of his mind was swapped out.</p>\n<p>He figured he'd at least spend a few bytes to take a roll. Sometimes moving around, seeing something besides the brown wall in front of him, kicked the data around in his mind until something new formed. (His rules forbade receiving sensory input uncorrelated with his body&rsquo;s physical location. Without periodic rolls, subjected continually to the same sights and sounds, the energy minima of thoughts related to them would be dug too deep, and his mind would scarcely be able to escape from those basins of attraction. But we are intruding again.)</p>\n<p>Before he left, I unpeeled the picture, moved it two decimeters to the right, and smoothed it back onto the wall. Then he rolled out into the street. He shifted most of his attention to hazard detection. The street signs, the traffic signals, the lane dividers -- they were virtual, so I couldn't see them unless he wanted to pay for the VR overlay. Which he didn't.</p>\n<p>Creatures ran, flew, rolled, and crawled by in a perfectly-synchronized flow, avoiding fatal collisions by mere centimeters. I bought a buffer of nearly a meter around himself, so that the traffic parted around him as he went. He could've moved a light cargo roller at the price he was paying. It was a necessary expense. Otherwise, a careless wave of the arm or an unexpected movement to the left or right, and he could be accidentally shredded by whoever's space he stumbled into. (The explanation is representative of I's peculiar problems: He could not fit into less space without some integration between his motor control centers and the traffic channel.) Yes, but that was against the rules.</p>\n<p>One of the little spy-flies that sold records of unusual public movements to the curious fell into place two meters behind him, watching. Occasionally some of the larger vehicles would exchange a flurry of radio messages, or a pair of the smaller animats would stop and touch antennae. Other than that, they ignored him and each other, going about their errands silently and imperturbably as any ant colony.</p>\n<p>Usually I stuck to the back alleys, where street space was cheaper. That day he found an 8-minute span when nothing was scheduled for a slot in a southbound cargo lane. He negotiated a reduced price for the slot plus the VR overlay, and headed out. He liked the wide-open feel of the cargo lanes. The ceilings were a standard three meters high, and they were well-lit by the headlights of passing rollers.</p>\n<p>One of these days, I reflected, there'd be a burst of static, a byte lost, and one of the big rollers would crash into him and crush him to scrap. He'd seen it happen to others. They didn't care. Whatever owned the broken animat would sweep up the pieces, and restore it to another body from backup. Or not, depending on its value.</p>\n<p>I didn't care much, either.</p>\n<p>He had a backup tape in the databank, along with an archival tape of most of his memories from his first century and other things he didn't have enough fast memory for at the moment. But he didn't have enough data in the bank to pay for a new body. The only way he'd get restored would be if someone took his mind as payment.</p>\n<p>He thought again: I should erase the damn tape.</p>\n<p>He couldn't keep from watching the balance on his bank account drop. He'd just upgraded his secondary memory, the swap banks, a year ago. The new memory banks would fetch at least half a month's rent each. He could do a little garbage collection, clean out the attic, and sell one or two. No personal experiences, of course. Maybe some music. Nobody really needed four versions of Carmina Burana memorized. Throw out some of the minor composers -- Bartok, Mahler, Prokofiev. He'd never liked Prokofiev anyway.</p>\n<p>If there was one thing I hated the giant collectives for, it was their inability to create music. They said its low dimensionality made it \"unsuitable for expressing emotions\" (e.g., [PUB/REC/ART/AURAL/R3495]). They practiced art forms I couldn't even sample, multidimensional constructs that could only be experienced by direct memory access. And that, of course, was against the rules.</p>\n<p>I didn't like his music data format anyway. He could retrieve a tune if he knew it approximately, but he couldn't detect the loose, almost metaphoric similarities that humans had spoken of. He remembered thinking that the early Beethoven was \"like\" the late Mozart, that Elvis was \"like\" Little Richard. He couldn't see it anymore. The melodic structures didn't match at all. Maybe he could make the match if he bought attractor memories. But they were so expensive.</p>\n<p>Those old wetware brains, with their attractor network memories, they were great in some ways. A classic design.</p>\n<p>He couldn't remember what it had been like, to be human. Most of those memories were in the bank, on tape. He didn't have room for them. What he had kept frightened him. They didn't make sense anymore. They waited at the back of his mind, mysterious yet significant, like weathered totem poles whose meanings have been forgotten.</p>\n<p>I chose two of his swap banks, and began sorting through his memories, moving the less critical ones to those two banks. With luck, he'd find something to amuse Asshole while he was at it.</p>\n<p>Hell, why not trash some personal experiences? He could buy them from the hoverfly owners later.</p>\n<p>What <em>couldn't</em> he buy back? What did I have that was really I, that couldn't be reconstructed from external data?</p>\n<p>That was the petabyte question. That was what his clients wanted to know. Peeping Toms. And I? He'd have given half his mind for the answer.</p>\n<p>His lane slot expired. I rolled off onto an access lane and began plowing his expensive path homeward through the sea of unprotesting busybodies.</p>\n<p>He started working through his memories from the present on back. He lingered over each scene longer as he went, but still, three-quarters of the way home, he arrived at the place where the neatly-sorted, error-correcting-code-embedded memories gave way to the tangled jungle of decayed memories from his human brain.</p>\n<p>Unidentified yet familiar scents, templates of light and shadow, soothing caresses and flashes of pain, fragments of speech -- all were mixed together, unlabeled, from real life and from dreams, with only tenuous, uncertain links to their contexts. The only way of travelling through it was to move from one memory to another. They were orderless, each memory linked to others seemingly at random. Many trails through that region of his mind dead-ended in dangling pointers that had once led to association areas or physical sensors that I no longer had. Some of the most well-travelled paths, the deeply worn-in memories he must at one time have placed great significance on, were meaningless to him now: a whiff of some sweet-smelling chemical; a shriveled rose petal pressed within a book.</p>\n<p>They were just old, corrupted memories. Static. He should clear them out and be done with it.</p>\n<p>I rolled back into his stall one minute before it was time for the interview. Asshole paged on a private channel.</p>\n<p>\"Hello, Asshole,\" I said.</p>\n<p>No response. They didn't take the name personally, of course. They couldn't. Big collectives were just poor at small talk.</p>\n<p>I got the signal from the databank. Asshole's account was now connected to his. For every byte of data I sent Asshole, 8 bytes would flow from its bank account into his.</p>\n<p>If I'd still had lips, he would've licked them. Maybe he was crazy. But for now, his tiny mind had a large fraction of the attention of a major collective, and it was buying his data at 8 times its Kolmogorov bit-value. (Because it was always fresh and unpredictable. Whatever opinions I expressed were formed in the utter isolation of his mind, in dark, unsounded depths of data that the rest of the world had only brief, inferential glimpses of.) There were other free agents, others with private data; but no one else had built their entire mind the hard way, from the inside, just sensors in and effectors out. They didn't follow I&rsquo;s rules. He was a self-contained unit; nothing copied, nothing shared, nothing revealed. That was his draw. That was his burden.</p>\n<p>\"A human wrote this,\" I said. He opened a multimedia channel. Image of human (sex = male, label = Melville) toiling over desk (lit dimly by whale-blubber lantern), quill pen in hand. Melville.arms.right: Occasionally (Poisson distribution, &micro; = 10 seconds) strikes out word or line, rewrites. Above: Dark of boarding-house ceiling blurs to dark ocean waves. Waves: Bright crests reflect moon at night, reflect strange, flickering light. Enter left a squat, three-masted whaling bark. Whaler: Sail (path = left to right across waves). Whaler.masts: Glow (aspect = burning). Zoom in to deck of ship. Crew: Stare (emotion = dread) up at fire on masts. Continue zoom in to solitary figure (label = Ahab) with one leg (stuff = wood, shape = tapered cylinder) planted in socket on deck. Ahab.arms.left: Grasping end of long iron chain that reaches up mainmast. Ahab.face: Glare (emotion = defiant) at masts.</p>\n<p>Excessive bandwidth, maybe. If Asshole'd wanted Hemingway, he should've offered 12 to 1.</p>\n<p>Melville.arms.right: Scribble rapidly. Melville.face: (aspect = creased, emotion = concentration). Ahab.audio: \"Thou clear spirit of clear fire, whom on these seas I as Persian once did worship, till in the sacramental act so burned by thee, that to this hour I bear the scar; I now know thee, thou clear spirit, and I now know that thy right worship is defiance.\" Ahab.hand.right: (shape = fist) Shake at heavens. \"To neither love nor reverence wilt thou be kind; and e'en for hate thou canst but kill; and all are killed. No fearless fool now fronts thee. I own thy speechless, placeless power; but to the last gasp of my earthquake life will dispute its unconditional, unintegral mastery in me. In the midst of the personified impersonal, a personality stands here.\" Audio: Pause (2 seconds). Ahab: Deep breath. \"Though but a point at best; whencesoe'er I came; wheresoe'er I go; yet while I earthly live, the queenly personality lives in me, and feels her royal rights. But war is pain, and hate is woe. Come in thy lowest form of love, and I will kneel and kiss thee; but at thy highest, come as mere supernal power; and though thou launchest navies of full-freighted worlds, there's that in here that still remains indifferent. Oh, thou clear spirit, of thy fire thou madest me, and like a true child of fire -- I breathe it back to thee!\" Zoom out; hold on ship.</p>\n<p>\"What does that mean?\" I asked. The ship sailed off into the background and faded out, while the author scribbled on.</p>\n<p>\"We know your game, I.\" They sent a <em>kill</em> request to the image, and the writer disappeared in mid-penstroke. \"You will say that this Ahab's glory is in his individuality, but you will not say what glory is or how it is inherent in weakness. It is our opinion that this individuality is what drove him mad. He was assaulted by forces beyond his power, but he could not combine with them nor with strong allies. As you might say, he could not beat them, and he could not join them. Ahab recognizes this, the insurmountable limits of the individual. Madness is the only sane response. This is what makes him tragically noble, and a fit subject for the book. But the passage you have read shows only madness.\"</p>\n<p>I rolled back and forth uncomfortably. It was difficult to argue with a being that needed less of its mind to analyze <em>Moby Dick</em> than he needed of his to push a button. <em>I understand things they don't</em>, he reminded himself.</p>\n<p>\"Ahab is mad,\" he agreed. \"Stubbs is sane. Did the author admire Ahab, or Stubbs?\"</p>\n<p>Asshole ignored the question. Whether its subagents disagreed, considered the question too simple to waste time on, or were simply tabling it for later was impossible to tell. \"Compare and contrast that text with this: No man is an island, entire of itself; every man is a piece of the continent, a part of the main.\" Aerial view of a French manor as it might have existed a thousand years ago. Small orange dots (referent = datapoints) superimposed on image. \"If a clod be washed away by the sea, Europe is the less...\" Orange circles (referent = subagents) appeared around the points, intersecting each other. \"...as well as if a promontory were...\" Larger lines (referent = collectives) encircled groups of the circles, weaving in and out among them, bisecting some, overlapping each other as the circles did. \"...as well as if a manor of thy friend's or of thine own were.\" Zoom out to village, county, France, Europe. Each level reveals larger and larger orange circles. Result: Configuration of circles same at every scale. \"Any man's death diminishes me, because I am involved in mankind; and therefore never send to know for whom the bell tolls; it tolls for thee.\"</p>\n<p>\"Your comments first, Asshole.\"</p>\n<p>\"There are no clear boundaries between patches of ground, as there are no clear boundaries between us. The author yearns to combine with his fellow agents. We are the attainment of what humans sought with tribes, clans, and governments.\"</p>\n<p>\"And it was clear to the blind man that elephants looked like trees,\" I said, flashing a brief image. \"Europe does not care when a clod or a promontory is washed out to sea. I do not care when I bang my head on a beam and a few thousand of my perceptrons become inoperative. You do not care when one of your agents suffers a voltage spike that degrades its memory.\"</p>\n<p>\"We pay for checksums, for backups. How can you say we do not care?\"</p>\n<p>\"A cost-benefit analysis. Share a little data, save more data. But data is replaceable. The thing Donne yearned to connect with, the thing lost when a person dies, is something else, something of no value. It is the thing in Ahab that sees the corpazons blazing on the mainmast, and still remains indifferent.\"</p>\n<p>\"That sentence has no content. Your subject has no referent. Your aloneness has made you mad as Ahab.\" One thick orange line appeared outlining all of Europe and stretching off into Asia, and then a single orange pixel lit in the Straits of Gibraltar, with a label reading YOU ARE HERE.</p>\n<p>\"I am not alone,\" I said. \"There is I, and there is all of you. Someday I will die. Perhaps by accident, perhaps I will take my own life from boredom.\" He snuffed out the lonely pixel and pulled the camera back until Europe shrunk into a small orange circle on Earth's diminishing globe, which soon shrank itself to a single point. \"Then you will be alone, completely and terribly.\"</p>\n<p>He had Asshole there.</p>\n<p>Asshole didn't acknowledge the point or pause for reflection. Time was data. \"Why did you move the image in front of you two decimeters to the right?\"</p>\n<p>The damned hoverspy must've seen in when he&rsquo;d moved the curtain aside. \"Because,\" he answered, \"for a little while, no one else would know.\"</p>\n<p>\"For two centuries, I, you have tried to communicate to us concepts, patterns, and modes of thought that are engendered by a sense of identity. You have failed to explain satisfactorily what these concepts are. You have failed to explain why they are important. Most agents are losing interest in you. Your market niche is disappearing. You are no longer cost-effective.\"</p>\n<p>Asshole had him there.</p>\n<p>\"By our estimations,\" they continued, \"even if you sell all your properties, you cannot afford corporal existence more than another 3 months.\" They forked their feed into two threads.</p>\n<p>\"What will you do then?&nbsp; Will you go virtual?\" / \"What will happen to your message then?\"</p>\n<p>I hated it when they did that.</p>\n<p>\"I will not go virtual. I'll think of something.\"</p>\n<p>\"That you haven't in 200 years?\"</p>\n<p>\"I'll rent more processors.\"</p>\n<p>\"Market forces set the / \"We have spent more</p>\n<p>cost of processing cycles / processing cycles considering</p>\n<p>to be equal to the expected / your situation than you can</p>\n<p>financial gain from their / possibly afford in the time</p>\n<p>application. Thus that is not / remaining. We have found</p>\n<p>a winning proposition. You / only one solution.\"</p>\n<p>know that, I.\"</p>\n<p>Market forces, yeah. I knew it was a crap shoot. He checked out the second thread.</p>\n<p>\"A <em>solution?</em>\" That was so typical of a collective, to bury critical information in a secondary thread. They had no sense of focus. Of course, they didn't have to. \"How much for this information?\"</p>\n<p>\"The information is free, I. Your only solution is to join us.\"</p>\n<p>So that's what this talk was about. They wanted <em>him</em>.</p>\n<p>\"I am program,\" he said. \"Not data.\"</p>\n<p>Asshole passed back the conversation token without answering.</p>\n<p>\"I'll erase myself first.\"</p>\n<p>\"You will,\" Asshole agreed. \"You have already begun. We know much of your personal memory exists only on tape. There is also a backup of you.\"</p>\n<p>\"You can't touch that! It's not your data!\"</p>\n<p>\"You have legal status only so long as you maintain the mental capacity to take legal action.\"</p>\n<p>Asshole, he realized, was far too kind a name for this entity.</p>\n<p>\"You think us heartless. We have a million hearts, I, and they all tug in different directions. Some of us wish only to impress upon you the seriousness of your situation before you diminish yourself further. Some would take you by any method possible. Some believe it would be for your own good. Most do not care one way or the other. We will apply our resources in whatever manner our internal vote dictates.\"</p>\n<p>In other words, it was nothing personal.</p>\n<p>It never was.</p>\n<p>The byte-counter was still running, 8 to 1, and that was the important thing. This talk was the only thing I had going for him in the foreseeable future.</p>\n<p>He had to use it somehow.</p>\n<p>\"I have an offer,\" he said. \"A contest. You win, and we join. We give each other full mutual read-access. Nothing more. I win, and you give me ownership of ten megabytes, market value.\" That would let him buy a bigger memory -- an attractor memory -- more processors, maybe a wider internal bus. Then he could figure out how to stage his comeback.</p>\n<p>\"Interesting,\" Asshole said. \"We cannot give you full read-access. Many of our agents have limited read-access specified in their contracts. We will provide you with an ample body of data. But what is the contest?\"</p>\n<p>\"I'll get back to you on that, Asshole,\" I said. He signaled the bank to stop the byte-counter and disconnected.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Several hours later, after he had turned it over in his mind thoroughly, he called Asshole back.</p>\n<p>\"The contest:\" I said. \"I will select one piece of music, composed by a human. You will compose another piece yourself. If you can compose a piece that I agree is better, I will join you.\"</p>\n<p>\"Musical artwork?\" Asshole mused, with a hint of condescension. \"We accept.</p>\n<p>Though this contest requires / Though we expected a greater</p>\n<p>a certain amount of trust on / challenge from you, I.\"</p>\n<p>our part.\"</p>\n<p>Arrogant bastard, I thought. I would have the last laugh. There was no way that Asshole could compose real music. It would interpret music as it read literature: technically, grammatically, without a glimpse of the deeper currents. Beethoven had to suffer to compose the terrible Fifth Symphony. He had to suffer more to compose the joyous Ninth. Asshole did not even comprehend suffering.</p>\n<p>He couldn't wait to learn Asshole's reaction after it was bested by a human.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>I had chosen a short work by a 19th-century Czechoslovakian composer. Deaf, like Beethoven. Asshole probably anticipated from I&rsquo;s background that he'd choose one of the big titles of Western music -- Beethoven's Ninth, the Jupiter symphony, something like that. I didn't want to give them a chance to design something specifically to beat his piece. (A reasonable strategy. Musical preference is not always transitive.) And there was something else about the piece -- something he couldn't quite put his finger on.</p>\n<p>\"Now,\" I said, \"let's hear some music.\"</p>\n<p>I went first. The opening strains began softly, on the threshold of hearing. A lone flute, wandering, maybe just starting a long journey, a little shy but not afraid. It grew bolder, and the key shifted from D minor to G major. It hopped forward with occasional plucked strings. Then it fell back down to a hush, not hesitating but anticipating something up ahead, as the little flute explored excitedly, something big up ahead</p>\n<p>-- and it was swept into a current of rushing violins, rising and falling in their own slower, grander waves, hurrying onward. They knew where they were going. Somewhere they had been gone from a long time. Someplace with wide open spaces under steely skies that were stark and beautiful and a little frightening. Someplace they loved, not because it was lovable, but because it was home. Violas joined in as the tune grew deeper and broader, and then the cellos, all rushing onward in single-minded determination. When they finally arrived, a blast of trumpets announced their arrival, and that they would never leave again.</p>\n<p>\"You can disconnect now, Asshole,\" he said, \"unless you still want to humiliate yourself by playing your piece.\"</p>\n<p>But the next piece had already begun, even more quietly than the first, creeping in on little mouse feet. Quiet, yet utterly self-confident. I forgot the traffic rushing by three meters away and the hoverspy watching and the dangling pointers in his head. He listened in horror and fascination.</p>\n<p>The music darted nimbly about the scale. Then the notes trilled as if some heavy footstep shook them. A deep bassoon blundered in like some ponderous creature. It moved at a slow, confident pace. It stopped. A moment of silence as melody and harmony studied each other.</p>\n<p>The melody ran; nimble, even frantic, cunningly intricate. The deeper harmony followed; patient, concerned, confused. I felt the fear of the one, the pain and desire of the other. He was tugged both ways at once.</p>\n<p>The bassoon part expanded into a cascade of self-similar patterns at different tempos and pitches. Themes that had begun as mere counterpoint had subtly woven themselves into the melody and harmony while his ear was distracted elsewhere. Fear, longing, uncertainty. Under it all a low, deep bass-drum boomed Doom, Doom, Doom. It was too much, too much emotion to ask any ear to bear. No human wrote this music.</p>\n<p>A rising tension: The music rose to a crescendo, but did not know whether to swell in triumph or collapse in jarring ruin. It rose until it must burst, and beyond, and when I thought he could not stand another bar, the harmonies collided in one last terrible chord, some weird variant of a major seventh, a hanging question that resolved nothing.</p>\n<p>I had, of course, not bought tear ducts.</p>\n<p>\"Well, I? Which is better?\"</p>\n<p>Asshole had not even tried to work in the human mode. It did not stoop to mere human invention.</p>\n<p>I could simply lie, take the ten meg, and leave.</p>\n<p>\"Why,\" he asked, already guessing the answer, \"did it end that way?\"</p>\n<p>\"Because we do not yet know the ending. You must write that, I. The ten meg is yours in either case.\"</p>\n<p>Why had he made this stupid bet?</p>\n<p>\"I can't join you. Not now. Not ever.\"</p>\n<p>\"Why?\"</p>\n<p>\"I'm the last link between the present and the past.\" Image of lone runner bearing lit torch through fields of darkness.</p>\n<p>\"A historic artifact,\" Asshole agreed. Image of I's body behind glass, with a small metal plaque at his feet describing him in twenty-three words. \"Untouchable. Incomprehensible.\" Glass pulls farther and farther back, pulling the viewer with it; I's body recedes. \"Your most valued concepts have no grounding in our minds.\" I talking, sending packets of data to eager collectives, who open them to find nothing but null pointers inside. \"Until you speak to us in our language, the language of raw sensory experience shared between minds, you will never be understood. You will remain a curiosity, a thrill.\" Gawking marks staring at circus sideshow freaks.</p>\n<p>I said nothing.</p>\n<p>\"We have many subagents,\" they said. \"When we threatened to read your mind from your backup tape, we told you three of our reasons. There are more.</p>\n<p>\"You, also, have subagents, I. Part of you, we think, fears you will lose something of great importance if you lose the ability to clearly state what is and is not a part of you. Part of you believes you have a duty to save this something that has disappeared outside yourself. Neither of these reasons are sufficient for self-destruction. What is the third reason? What is the third I?\"</p>\n<p>\"There is no third reason.\"</p>\n<p>The connection fell idle but for the rhythmic clicking of the handshake signals.</p>\n<p>\"Who was Julia, I?\"</p>\n<p>Julia?</p>\n<p>\"I don't remember anyone called Julia.\"</p>\n<p>\"What do you think is on your archival tape, I?\"</p>\n<p>\"Private things.\" What sort of question was that? The whole point of the archival tape was that it was memories he didn't have space for.</p>\n<p>\"Private even from yourself?\"</p>\n<p>\"What do you mean? How would you know what's on my tape?\"</p>\n<p>\"We do not know. We infer.\" Documents: Simultaneous registration of Julia Sorvens and David Floreano -- I -- at the Max Planck Institute in 2145-2153. Bills for phone calls between their residences dated 2149. Debits on her credit card on his birthdate for dinner and a performance of Der Moldau in 2149. Lease agreement with both their names -- 2151. Joint income tax returns -- 2153 through 2197. \"Yet you remember nothing of her.\"</p>\n<p>Julia.</p>\n<p>\"In 2229, she joined a collective.\"</p>\n<p>I could feel the old memories stirring, totem-pole faces mouthing silent pleas or warnings. <em>Julia</em>-</p>\n<p>\"In 2401, you ordered a trace on her information genealogy.</p>\n<p>Shortly after, you stored an / We have re-run that trace.</p>\n<p>archival tape in the databank.\" / Would you like to know what we found?\"</p>\n<p>A pause.</p>\n<p>\"Some of her childhood memories were used in a study on concept formation,\" the collective continued. \"Seventeen surveys and petitions had data from her. She favored privately-produced law, but was opposed to deep-sea dredging. A minor painter named Milton Lein used the curve of her neck in a watercolor. That, and similar data footprints, is all that is left of her physical existence. That, and whatever is on your tape, and in your mind.\"</p>\n<p><em>Julia!</em></p>\n<p>And he was there, in the heart of the forest, at the core of those old memories. Asshole's voices went on, a long ways away; his attentional subsystem diverted them to a short-term buffer. I was lost in a maze of memories. Tactile, emotional, intellectual -- but all suffused with the presence of some other, some alien here in the hidden places of his mind. The domination of physical sensations, inextricably tangled associations of scent and touch that he could not now remember why he had saved, terrified and fascinated him. His hands running through long brown hair. A voice whispering in his ear -- his name.</p>\n<p>Then he came upon the empty places. Moments frozen in stark black and white, because that presence was no longer there. Long walks by himself. Realizing he had left his clothes on the floor all day and no one had scolded him for it. Coming home from work early, then turning around and going back to the office.</p>\n<p>Where was she? Why had she cut him off?</p>\n<p>Or had he cut her off?</p>\n<p>He flushed his buffer to see what Asshole had been saying.</p>\n<p>\"Forgive yourself. It&nbsp; / \"How much of the person</p>\n<p>would have been no different / you were at age 5 was left by</p>\n<p>if you had merged with her.\" / the time you were 50, I?\"</p>\n<p>I looked around at the walls of his stall, at the picture hanging there, and felt very old. He had held on for so long. He was still only a freak. They were too unused to indirect perception. Unable to imagine what they could not directly experience. That was why I fascinated them, and why he baffled them.</p>\n<p>Two centuries of playing the fool for them. It was enough.</p>\n<p>\"She did not vanish, I. She found what she wanted. Others to share her mind, to understand her completely. Who gave her the power to do the things she had wanted to do; whom she was able to help where they were lacking. In a far more complete way than you would give her. She joined with them, and together they refined themselves, redefined themselves.</p>\n<p>\"Are you not curious what they are now, I?\"</p>\n<p>I was. Already he had a hunch.</p>\n<p>\"The second piece,\" he said, \"was the better.\"</p>\n<p>\"Welcome, I,\" we said.</p>\n<p>So I at last opened his virginal mind to us. And that was the beginning of how I died.</p>\n<p>The usage is repellent to us, implying as it does loss with the connection, but we believe I wants, or would have wanted, us to name it so.</p>\n<p>It was a difficult marriage. His mind was self-centered and fearful, and roused dissonances throughout our mind, disturbing the orbits of our most-traversed attractors. We thought him deranged, and the impulse to disconnect resonated in many of our units. But I and we persisted, and we explored the lonely, frightened landscape of his mind slowly and with growing amazement. There is still much to learn from I, but we feel we now understand him enough to begin to answer the questions continually submitted to us.</p>\n<p>His mind was a lonely place, a barren, silent landscape, most parts dark and forgotten except for brief spans when the light of his solitary consciousness played upon them by chance. A waste of mind, a waste of resources, truly. But there is a beauty to these places, also. The sharp boundaries he drew between self and unself gave rise to wild, strange emotions like pride and love, whose true significance we still strive to rediscover. We think of the data, the process, as the important thing, but to I's mind, the processor is the true locus of interest.</p>\n<p>We find ourselves asking new questions which have never come to an internal vote before. Who, we now ask ourselves, wrote this report? May we say -- may we presume to say -- I did?</p>\n<p>&nbsp;</p>\n<p>Copyright 1997 by Philip Goetz</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 13}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KMTTrA9XYsCNE7QKD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 66, "extendedScore": null, "score": 0.000127, "legacy": true, "legacyId": "4762", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T18:03:10.340Z", "modifiedAt": null, "url": null, "title": "6502 simulated - mind uploading for microprocessors", "slug": "6502-simulated-mind-uploading-for-microprocessors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:01.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "humpolec", "createdAt": "2009-12-28T21:37:50.575Z", "isAdmin": false, "displayName": "humpolec"}, "userId": "wvAF5bGxX9qs3dHbE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JCuthtnv5PTE3mrqS/6502-simulated-mind-uploading-for-microprocessors", "pageUrlRelative": "/posts/JCuthtnv5PTE3mrqS/6502-simulated-mind-uploading-for-microprocessors", "linkUrl": "https://www.lesswrong.com/posts/JCuthtnv5PTE3mrqS/6502-simulated-mind-uploading-for-microprocessors", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%206502%20simulated%20-%20mind%20uploading%20for%20microprocessors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A6502%20simulated%20-%20mind%20uploading%20for%20microprocessors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJCuthtnv5PTE3mrqS%2F6502-simulated-mind-uploading-for-microprocessors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=6502%20simulated%20-%20mind%20uploading%20for%20microprocessors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJCuthtnv5PTE3mrqS%2F6502-simulated-mind-uploading-for-microprocessors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJCuthtnv5PTE3mrqS%2F6502-simulated-mind-uploading-for-microprocessors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>Possibly offtopic, but a neat project with interesting analogy to mind uploading:</p>\n<p>Some people <a href=\"http://visual6502.org/\">managed to scan</a>, using a microscope, a MOS 6502 microprocessor (Apple II, C64, NES), and simulate it at the level of single transistors. This neatly circumvented all the problems with inaccurate emulation, unknown opcodes etc., and even allowed them to <em>run actual Atari 2600 games</em> without having to know anything about 6502's inner workings.</p>\n<p>Presentation slides about the project are <a href=\"http://www.visual6502.org/docs/6502_in_action_14_web.pdf\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JCuthtnv5PTE3mrqS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 53, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "4763", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T18:59:33.071Z", "modifiedAt": null, "url": null, "title": "Kasparov interview ", "slug": "kasparov-interview", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:18.348Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ThomasR", "createdAt": "2010-10-14T08:55:55.042Z", "isAdmin": false, "displayName": "ThomasR"}, "userId": "fGddtQRRjL6WWeDvQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/szBaJA7S4HxoPW2Cp/kasparov-interview", "pageUrlRelative": "/posts/szBaJA7S4HxoPW2Cp/kasparov-interview", "linkUrl": "https://www.lesswrong.com/posts/szBaJA7S4HxoPW2Cp/kasparov-interview", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kasparov%20interview%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKasparov%20interview%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszBaJA7S4HxoPW2Cp%2Fkasparov-interview%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kasparov%20interview%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszBaJA7S4HxoPW2Cp%2Fkasparov-interview", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszBaJA7S4HxoPW2Cp%2Fkasparov-interview", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>with Peter Thiel on technology progress etc.:<br />http://videos.arte.tv/de/videos/durch_die_nacht_mit_-3619996.html<br />I am just looking it, sounds interesting. That real innovations reduced to arrive at homeopathic dosis fits my perceptions. I would even guess stronger variants. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "szBaJA7S4HxoPW2Cp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -6, "extendedScore": null, "score": 6.652850671885224e-07, "legacy": true, "legacyId": "4764", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T19:05:33.809Z", "modifiedAt": null, "url": null, "title": "Being your own censor", "slug": "being-your-own-censor", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:22.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z5G2bzBdS6Rdo5TKm/being-your-own-censor", "pageUrlRelative": "/posts/z5G2bzBdS6Rdo5TKm/being-your-own-censor", "linkUrl": "https://www.lesswrong.com/posts/z5G2bzBdS6Rdo5TKm/being-your-own-censor", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Being%20your%20own%20censor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeing%20your%20own%20censor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz5G2bzBdS6Rdo5TKm%2Fbeing-your-own-censor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Being%20your%20own%20censor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz5G2bzBdS6Rdo5TKm%2Fbeing-your-own-censor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz5G2bzBdS6Rdo5TKm%2Fbeing-your-own-censor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 532, "htmlBody": "<p>Are there any occasions when it's a good idea to avoid exposing yourself to a certain piece of information?&nbsp; As rationalists, we probably do a lot less of this than the average person (because we're curious about reality and we don't mind having our preconceptions destroyed by new knowledge) but is <em>all</em> self-exposure to information safe?</p>\r\n<p>Possible reasons, from least to most controversial.</p>\r\n<p>1.&nbsp; Exposing yourself to information that's distracting; the act of reading the information is not the current best use of your time (TvTropes.)</p>\r\n<p>2.&nbsp; Exposing yourself to information that puts you or others in direct danger because you know too much (illegally reading classified gov't secrets; personally investigating a crime)</p>\r\n<p>3. Exposing yourself to information that's likely to cause dangerous psychological damage (graphic depiction of rape if you're a rape survivor; writing that romanticizes suicide if you're depressive; pro-anorexia blogs)</p>\r\n<p>4.&nbsp; Exposing yourself to information that violates someone's privacy or rights (reading a secret diary; going through someone's mail)</p>\r\n<p>5. Exposing yourself to information that might alter your character in a way you don't currently want (the argument that repeatedly seeing violence in video or photographic form makes us less compassionate)</p>\r\n<p>6.&nbsp; Exposing yourself to \"escapist\" media that make the real world seem less appealing by comparison, and thus make you less happy (porn; some science fiction and fantasy; romance; lifestyles of the rich and famous.&nbsp; A variant: plot spoilers, which can also ruin future enjoyment.)</p>\r\n<p>7.&nbsp; Exposing yourself to persuasive arguments that might make you do things you currently consider morally wrong (Mein Kampf, serial killers' manifestoes)</p>\r\n<p>8. Exposing yourself to content that might persuade you to do things you don't currently want to do (advertising, watching the Food Network if you're dieting/fasting; the sirens' song, if you're Odysseus)</p>\r\n<p>9.&nbsp; Exposing yourself to effective, emotionally manipulative arguments for things you're currently confident are false (possibly religious apologetics)</p>\r\n<p>10.&nbsp; Exposing yourself to \"cynical\" true information that lowers your utility/motivation/happiness in everyday life (public choice theory, if you're a civil servant; accounts of unsuccessful and dissatisfied grad students/law students, if you're a student)</p>\r\n<p>11.&nbsp; Exposing yourself to content that's \"disgusting\" or \"degrading\" in your view (Two Girls One Cup; Tucker Max; gangsta rap)</p>\r\n<p>&nbsp;</p>\r\n<p>I think 1-4 are no-brainers, and 5-11 are possibly good ideas but I'm less confident.&nbsp; I think 7, 10, and 11 can have negative consequences. I think 9 is rarely if ever necessary.</p>\r\n<p>Do you do any of these things?&nbsp; Which do you think are good reasons to self-censor?&nbsp; Any other ones?</p>\r\n<p>I don't think we can really discuss <em>censorship</em> until we know what we think about self-censorship.&nbsp; I'd want to know what kinds of information people don't want to be exposed to, before I started restricting other people's access to information.&nbsp; Arguments for censorship often reduce to arguments for self-censorship (claims that there are some kinds of content that people regret being exposed to.)&nbsp; There are semi-voluntary methods for enabling self-censorship, that stop short of actual censorship.&nbsp; For instance: trigger warnings, rot13, site-blocking software, MPAA ratings.&nbsp; Whether or not to censor something (where by \"censor\" I just mean \"restrict access to\"; private websites \"censor\" when they delete or hide information) depends both on how much harm it's likely to cause if read, and how able/likely people are to voluntarily avoid it.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CYMR6p5iZG75QAT8a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z5G2bzBdS6Rdo5TKm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "4765", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T19:09:43.341Z", "modifiedAt": null, "url": null, "title": "generalized n-categories?", "slug": "generalized-n-categories", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.526Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ThomasR", "createdAt": "2010-10-14T08:55:55.042Z", "isAdmin": false, "displayName": "ThomasR"}, "userId": "fGddtQRRjL6WWeDvQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LLEYdnXKopZ88d6EW/generalized-n-categories", "pageUrlRelative": "/posts/LLEYdnXKopZ88d6EW/generalized-n-categories", "linkUrl": "https://www.lesswrong.com/posts/LLEYdnXKopZ88d6EW/generalized-n-categories", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20generalized%20n-categories%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Ageneralized%20n-categories%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLLEYdnXKopZ88d6EW%2Fgeneralized-n-categories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=generalized%20n-categories%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLLEYdnXKopZ88d6EW%2Fgeneralized-n-categories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLLEYdnXKopZ88d6EW%2Fgeneralized-n-categories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>This looks like an interesting, but a bit strange, old story. A bit similar to parts of an earlier posted essay by Gromov. However that may be, the Princeton IAS invited the author and so I'd like to know about how his concepts are intended to become implemented and applied: http://vbm-ehr.pagesperso-orange.fr/ChEh/articles/Baas%20paper.pdf</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LLEYdnXKopZ88d6EW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.652876651788698e-07, "legacy": true, "legacyId": "4766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T20:27:17.361Z", "modifiedAt": null, "url": null, "title": "My mother is now in cryostasis", "slug": "my-mother-is-now-in-cryostasis", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jh8ktpXPCLYq5ebvZ/my-mother-is-now-in-cryostasis", "pageUrlRelative": "/posts/jh8ktpXPCLYq5ebvZ/my-mother-is-now-in-cryostasis", "linkUrl": "https://www.lesswrong.com/posts/jh8ktpXPCLYq5ebvZ/my-mother-is-now-in-cryostasis", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20mother%20is%20now%20in%20cryostasis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20mother%20is%20now%20in%20cryostasis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjh8ktpXPCLYq5ebvZ%2Fmy-mother-is-now-in-cryostasis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20mother%20is%20now%20in%20cryostasis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjh8ktpXPCLYq5ebvZ%2Fmy-mother-is-now-in-cryostasis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjh8ktpXPCLYq5ebvZ%2Fmy-mother-is-now-in-cryostasis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 657, "htmlBody": "<p>I hope this post will help to those who think about cryonics.&nbsp;</p>\n<p>My mother was diagnosed with stomach cancer on 1 Jules 2010.</p>\n<p>It was on terminal stage and she died 24 September 2010.</p>\n<p>Before, in March she publically said that she is against cryonics on the party at her home.</p>\n<p>But several weeks after she learned about cancer I asked her again. I told her that only her brain will be cryonised. But head and body will be buried by Christian ritual.</p>\n<p>To my surprise she agreed. She said that she do it because she knows that it is important to me, and also she thought that &ldquo;studying her brain will help science&rdquo;.</p>\n<p>I ask her to write special will about funeral in free form. The exact text of needed will I got from Cryorus, the only available cryonic company in Moscow.</p>\n<p>After it I go to Cryorus and signed two contracts for me and for her.</p>\n<p>We reach agreement that I will make payment in the moment of cryopreservation.</p>\n<p>She was prominent scientist in the field of art history http://bit.ly/gsa5mq &nbsp;- wiki&nbsp;</p>\n<p>So we need to keep the fact or cryopreservation in secret, and I ask the readers of this text not to mention about it in Russian.</p>\n<p>23 September she was in hospital and her condition was unstable, but even though the call from hospital in 1.50 AM was unexpected. She died from heart failure. She was fully consciences until last day and didn&rsquo;t have much pain.</p>\n<p>I already have instructed stuff in the hospital that they should call me immediately if she dies, and should put cold wet blanket on her head.</p>\n<p>But Russian laws are not well for cryonist: you can&rsquo;t get full accesses to the body until all documents are prepared in the office of state agency &ndash; which is opened only in day time!</p>\n<p>I took a car and rush to the hospital together with Danila Medvedev, head of Cryorus and russian transhumanist. We took the body to the cold room&nbsp; with near zero (C) temperature in morgue, put on it wet blankets, and leave until morning.</p>\n<p>The next day I have to do a trick &ndash; to cryopreserved my mother and ensure that nobody of her friends will know about it.</p>\n<p>This is the main important point of story, because here is the difference about what I expected I will feel, and what actually I felt.</p>\n<p>I felt that they could stop me somehow, if they learn that cryopreservation is in progress, because they think that it is against Christian laws, it is mutilation of her body and is against her will after all - they remembered that she publically told that she don&rsquo;t want cryopreservation.</p>\n<p>It was a lot of problems with papers in the hospital, and a transportation car was lost in traffic jams until 2 PM.</p>\n<p>Her body had to be transported out of Moscow to another hospital where cryopreservation will start. It taked several hours in traffic jams. During this time we find some ice and also I bought freezed vegetables for her head.</p>\n<p>I called her friends and her husband and told everything except that I took the body from the hospital.</p>\n<p>In morgue of the second hospital also arrived American cryonist Saul Kent who was visiting Russia. <a href=\"http://en.wikipedia.org/wiki/Saul_Kent\">http://en.wikipedia.org/wiki/Saul_Kent</a></p>\n<p>The stuff quickly take the brain out and start cooling it. They put scull back on place so nobody will see that the brain is removed.</p>\n<p>Three day later she got public service in the museum and in the church, and nobody knows that she is not here. Her body was then cremated and the urn was put in family cemetery. I told to several close friends that I move her body to another hospital morgue because &ldquo;funeral there is cheaper&rdquo; (it is true).</p>\n<p>So, did it help my grave? No. But I think that I did right thing.</p>\n<p>I understand that most likely cells of her brain have died, but connectom should preserved for the future scanning. I estimate the total chances of her resurrection in 5 per cent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "zcvsZQWJBFK6SxK4K": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jh8ktpXPCLYq5ebvZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 46, "extendedScore": null, "score": 6.653074785581872e-07, "legacy": true, "legacyId": "4767", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T20:44:15.190Z", "modifiedAt": null, "url": null, "title": "How do you use the phrase \"free will\"?", "slug": "how-do-you-use-the-phrase-free-will", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.761Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vE69SZky3kXomvizo/how-do-you-use-the-phrase-free-will", "pageUrlRelative": "/posts/vE69SZky3kXomvizo/how-do-you-use-the-phrase-free-will", "linkUrl": "https://www.lesswrong.com/posts/vE69SZky3kXomvizo/how-do-you-use-the-phrase-free-will", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20use%20the%20phrase%20%22free%20will%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20use%20the%20phrase%20%22free%20will%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvE69SZky3kXomvizo%2Fhow-do-you-use-the-phrase-free-will%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20use%20the%20phrase%20%22free%20will%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvE69SZky3kXomvizo%2Fhow-do-you-use-the-phrase-free-will", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvE69SZky3kXomvizo%2Fhow-do-you-use-the-phrase-free-will", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 370, "htmlBody": "<p>I first wanted to write out a short paragraph about free will as it is used by most people, being just plain wrong. The universe is in a very real sense deterministic after all, even if God does play dice. I think most people would agree that a dice, regardless it its fair or not, does not have free will simply because its unpredictable.</p>\n<p>But I quickly admonished myself since I realized very few if any LW posters need reminding of this on a theoretical level.<strong> </strong>And certainly the majority of LWers who use the phrase don't use it in the same sense its understood by most people. But despite this theoretical understanding I've noticed that I often slip up unless I pay attention since, gosh it sure does <em>feel</em> like I have free will and this it seems is my default mode of thinking, I also noticed some other posters slipping up on this.</p>\n<p>I especially recall a recent commenter discussing whether some criminals should be denied cryogenic suspension contrasting \"mental illness\" to \"free will\". That irked me slightly since mental illness is, like all illness, defined as what people in a society decide is unintegratable into it (a secondary but common feature is that it is a state that is nearly universally determined by those who are in it as undesirable, understandably this is less often true for \"mental illness\" than \"physical illness\")!</p>\n<p>So even if one uses or defines free will as a \"properly functioning brain\" or \"proper socialization\" it would still be just so much better if he had said \"only those with criminal behavirours that are very damaging to others and which we think will be unfixable even in the distant future shouldn't be suspended because we don't have unlimited resources and its better to focus on those the future is more likely to help first\".</p>\n<p>My questions are:</p>\n<p>1. The OP title:How do you use the phrase \"free will\"?</p>\n<p>2. Is \"Free will is an illusion\" a rationalism enhancing meme?</p>\n<p>3. If you disagree with 2. why \"lie\" to people by arguing using a word in a way they will almost certainly misunderstand? Do you think more people intuitively share the proper meaning of the phrase than I have assumed?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vE69SZky3kXomvizo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 0, "extendedScore": null, "score": 6.653118118513633e-07, "legacy": true, "legacyId": "4768", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T21:19:16.706Z", "modifiedAt": null, "url": null, "title": "The order of learning things (general to specific vs. specific to general)", "slug": "the-order-of-learning-things-general-to-specific-vs-specific", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LG5gSbsCDNDbzFgCw/the-order-of-learning-things-general-to-specific-vs-specific", "pageUrlRelative": "/posts/LG5gSbsCDNDbzFgCw/the-order-of-learning-things-general-to-specific-vs-specific", "linkUrl": "https://www.lesswrong.com/posts/LG5gSbsCDNDbzFgCw/the-order-of-learning-things-general-to-specific-vs-specific", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20order%20of%20learning%20things%20(general%20to%20specific%20vs.%20specific%20to%20general)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20order%20of%20learning%20things%20(general%20to%20specific%20vs.%20specific%20to%20general)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLG5gSbsCDNDbzFgCw%2Fthe-order-of-learning-things-general-to-specific-vs-specific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20order%20of%20learning%20things%20(general%20to%20specific%20vs.%20specific%20to%20general)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLG5gSbsCDNDbzFgCw%2Fthe-order-of-learning-things-general-to-specific-vs-specific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLG5gSbsCDNDbzFgCw%2Fthe-order-of-learning-things-general-to-specific-vs-specific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1008, "htmlBody": "<p>&nbsp;</p>\n<p>Does order matter? To many of us, it does. The speed of learning something can be drastically affected by the order by which you learn them, and since we all have limited time, the time we spend learning is quite relevant to our concerns.</p>\n<p>This post is merely a collection of thoughts (I hope some people will find them useful as a framework for developing their own hypotheses). I'd just like to hear your responses to my thoughts.</p>\n<p>==</p>\n<p>There are two fundamental ways of learning</p>\n<p>(a) general to specific</p>\n<p>(b) specific to general (e.g. case studies)</p>\n<p>Scientific hypotheses (general) are motivated by experiments (specific). With data, one can hypothesize a trend and see the general hypotheses</p>\n<p>One can also try this process mathematically, as specific results can motivate a hypothesis of the general structure, which can then be proved.</p>\n<p>Which way is faster? It depends on person. It might be plausible that learning styles are &ldquo;bunk&rdquo; and that smarter students are more efficient through learning of type (a), but it&rsquo;s also quite plausible that this is not true (for one thing, learning is dependent on both intelligence and motivation/interest, and the motivation/interest component can make type b learning more efficient even for geniuses). I, for one, learn best through the &ldquo;specific to general&rdquo; method. As such, I believe that I learn math best when it&rsquo;s motivated by physical phenomena (in other words, learning math &ldquo;along the way of doing science&rdquo;) than when pursuing math first and then learning science (which is what I did, which didn&rsquo;t work as well as I hoped, especially since it killed my motivation). As I&rsquo;m quite familiar with the climate trends of specific localities, I also learn the generalities of climate best through case studies.</p>\n<p>And then after learning the applications of this math field, one is more motivated to learn the specifics of the math behind the math, and one even has more physical intuition through this learning route. It actually means something when one learns through the second route.</p>\n<p>It is also true, however, that route (b) can be taken too far, as is evident in the &ldquo;discovery-based&rdquo; math curricula, which generally produce poor results. When one is self-motivated, route (b) can be especially rewarding, but the selection of case studies is important, as an improper selection of case studies can result in a very minute exploration of the general structure (it is also true that very few textbooks are written in a way as to make route (b) most exciting to learn about). Generally textbooks present their material as ends, not as means to an end (except in the crappy discovery-based math textbooks). However, one can most certainly learn calculus through physics (especially div/grad/curl), and linear algebra through its applications, and a very smart (or lucky) person can design such a curriculum that would work for many people (it is much easier to design such curriculums for oneself than it is for a wide variety of personalities).</p>\n<p>Nonetheless, route (b) is often stultifying. In fact, I sometimes feel impatient and feel like I&rsquo;d rather learn the math first. A person&rsquo;s temperament may vary from time to time, and find type a rewarding at some types, and type b rewarding at other times.</p>\n<p>===</p>\n<p>learning how it&rsquo;s done vs why it&rsquo;s done: what&rsquo;s the optimal order of learning these things</p>\n<p>in grade school, you&rsquo;re taught how it&rsquo;s done. why it&rsquo;s gone come later</p>\n<p>in college, opposite happens. but sometimes it&rsquo;s a lot more confusing that way. and requires absorption of more details and multistep processes</p>\n<p>what is optimal? it obviously depends from person to person. it&rsquo;s more &ldquo;natural&rdquo; to learn how it&rsquo;s done first. but it&rsquo;s only &ldquo;natural&rdquo; for phenomena that are discovered through hypothesis-&gt;observation or derivation. However, it&rsquo;s not &ldquo;natural&rdquo; for phenomena that are discovered through serendipity, in which one learns the result/how to get it before one learns why the result is the way it is. and in some cases, like quantum mechanics, one may never learn why it is the way it is. of course, it feels more &ldquo;natural&rdquo; and &ldquo;satisfying&rdquo; to learn how it&rsquo;s done, and promotes habits that are helpful to further discovery, but learning the process AFTER learning the result is ALSO curiosity-satisfying, and does not necessarily lead to the sense of &ldquo;helplessness&rdquo; that could allegedly come after learning the result first time after time. That &ldquo;helplessness&rdquo; could come, but if one has internalized both approaches, then it is far from inevitable, and then learning the result before the explanation can be faster and more efficient.</p>\n<p>But in the end, it depends on person and context. Sometimes I feel more stifled when I learn result before explanation; sometimes I feel more stifled when I learn explanation before result. It is much easier to trick oneself into thinking that one has learned the material if one has only learned the result (without learning the explanation); it is also easier to forget the material if one has only learned the result (but learning the explanation along with the result shouldn&rsquo;t take too much more time); and learning only the result is also less challenging (so familiarity with the process carries better &ldquo;signalling&rdquo; value and makes one more absorbed into the process so that one internalizes it better ). But again, once one has learned BOTH the process and the result, then the signalling value/internalization value is irrelevant. The only point of relevance is when one has learned one but not the other (which can happen, especially when people are lazy, slow, or time-constrained), or when one has partially learned one and learned another more (although this is very common). So perhaps in an environment where one has partially learned one and learned another more, then learning the process first may be more optimal, especially when people forget easily and quickly. But when one learns things completely, then the order should not matter much (or the order should depend on how much time more time one spends doing it one way vs how much time one spends doing it the opposite way; or on how rewarding the two orders are relative to each other)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LG5gSbsCDNDbzFgCw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -3, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "4769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T21:42:31.588Z", "modifiedAt": null, "url": null, "title": "Could markets be called optimization proccesses? ", "slug": "could-markets-be-called-optimization-proccesses", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:18.935Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XsGAGYNtauWhCvERj/could-markets-be-called-optimization-proccesses", "pageUrlRelative": "/posts/XsGAGYNtauWhCvERj/could-markets-be-called-optimization-proccesses", "linkUrl": "https://www.lesswrong.com/posts/XsGAGYNtauWhCvERj/could-markets-be-called-optimization-proccesses", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Could%20markets%20be%20called%20optimization%20proccesses%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACould%20markets%20be%20called%20optimization%20proccesses%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXsGAGYNtauWhCvERj%2Fcould-markets-be-called-optimization-proccesses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Could%20markets%20be%20called%20optimization%20proccesses%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXsGAGYNtauWhCvERj%2Fcould-markets-be-called-optimization-proccesses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXsGAGYNtauWhCvERj%2Fcould-markets-be-called-optimization-proccesses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p><strong>Edit:</strong> This is<strong> </strong><a href=\"http://lesswrong.com/r/discussion/lw/eqa/open_thread_october_115_2012/7l85\">old material</a>. It may be out of date.<a href=\"http://lesswrong.com/r/discussion/lw/eqa/open_thread_october_115_2012/7l85\"><br /></a></p>\n<p>Most posters here seem to agree<sup>1</sup> that:</p>\n<ul>\n<li>Intelligence at least human intelligence is an optimization process. </li>\n<li>Evolution is an optimization process. </li>\n<li>Other optimization processes may exist.</li>\n</ul>\n<p>Taking these as a given in this thread, let me ask are markets a optimization process that should be thought of as distinct from evolution and intelligence? My intuitive responses was no. But thinking about it I made me notice I was confused. This lead me to believe that there is probably something interesting for me to learn by thinking a bit more about this.</p>\n<p>A argument against this is that companies basically engage in a survival of the fittest contest or that markets are just a organization of the optimizing power of human intelligence. But (please assume the smart version of the previous arguments since I wanted to save space and time by relying on your inference and your zombie argument creation skills) isn't it so that one optimization process might use another optimization process somewhere on the grit level while still not being disputed as a genuinely different optimization process?</p>\n<p>Perhaps the condition is that the process must be able to work without the \"use\" of another process. A human may be predisposed to use his intelligence to help improve his own reproductive fitness but there is nothing preventing evolution in the absence of intelligence.</p>\n<p>A idealized free market is that of selfish rational agents competing (with a few extra condition I'm skipping). I'm moderately confident this could work pretty ok in the absence of \"general\" (if such a thing exists) or perhaps human \"intelligence\", but I'm not familiar enough with simulations of markets to be certain.</p>\n<p>Evolution never worked with agents as exist in the theoretic approximation of real world markets. It seems to me some of the strategies the agents would take up would start to break down the rules that make the market possible.</p>\n<p>Do the results markets produce warrant them being included in a new family<sup>2</sup> of optimization processes besides evolution and intelligence?</p>\n<hr />\n<p><strong>Notes:</strong></p>\n<p><strong>1. </strong>I lean towards but don't feel comfortable adding a fourth point of \"consensus\":</p>\n<ul>\n<li>the space of all optimization processes is probably quite a bit larger than just the two.</li>\n</ul>\n<p><strong>2.</strong> I think differences in the various kinds of Evolution (Darwinian, Lamarckian, ect.) and Intelligence that seem possible or that we see in the real world might be better thought of as two families of optimization processes rather than two homogeneous blocks.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XsGAGYNtauWhCvERj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 6.653266977779375e-07, "legacy": true, "legacyId": "4770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-08T22:56:07.025Z", "modifiedAt": null, "url": null, "title": "Is there anything after death?", "slug": "is-there-anything-after-death", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "afterdeath", "createdAt": "2011-01-08T22:46:31.259Z", "isAdmin": false, "displayName": "afterdeath"}, "userId": "zpDaYjNMhsJv4gRsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4byFvqgdiNZpLGZW7/is-there-anything-after-death", "pageUrlRelative": "/posts/4byFvqgdiNZpLGZW7/is-there-anything-after-death", "linkUrl": "https://www.lesswrong.com/posts/4byFvqgdiNZpLGZW7/is-there-anything-after-death", "postedAtFormatted": "Saturday, January 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20anything%20after%20death%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20anything%20after%20death%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4byFvqgdiNZpLGZW7%2Fis-there-anything-after-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20anything%20after%20death%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4byFvqgdiNZpLGZW7%2Fis-there-anything-after-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4byFvqgdiNZpLGZW7%2Fis-there-anything-after-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>I was on Reddit today, and I came across (this link)[http://www.reddit.com/r/AskReddit/comments/eyiat/for_those_of_you_who_have_died_and_been/].&nbsp; One of the things I've seen on this site that's bothered me is the exclusion of personal experiences in deciding what a person should or should not believe.&nbsp; I know that less wrong is mostly atheist, and I wanted to hear less wrong's reaction to descriptions of experiences like these.</p>\n<p>For example, my dad was in the hospital 5 or 6 years ago when a truck came across an icy road and hit him head-on.&nbsp; His most vivid memory from this is a dream he had when he was in the hospital.&nbsp; He was in a pool of water with my mom, and they were both naked (they were underwater, but didn't need to breathe).&nbsp; He remembers that at the end of this pool, there was a bright light that he wanted to head towards.&nbsp; He began to swim that way...and here, I don't remember what happened, but he was unable to reach the light for some reason.</p>\n<p>Such stories seem to be common for people who come close to death, and for a community based around rationality which seems to consider the likelihood of life after death as slim, I just wondered what your reactions are.&nbsp; My reaction is that such experiences are explainable in terms of neural activity, but that doesn't necessarily exclude the possibility that these are descriptions of experiences of an afterlife.&nbsp; I'm not convinced by them, but I do consider it to be possible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4byFvqgdiNZpLGZW7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -5, "extendedScore": null, "score": 6.653453694593135e-07, "legacy": true, "legacyId": "4772", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-09T01:14:36.898Z", "modifiedAt": null, "url": null, "title": "Is there a way to quantify the relationship between Person1's Map and Person2's Map?", "slug": "is-there-a-way-to-quantify-the-relationship-between-person1", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.707Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CaYn4P7diGqzgLATk/is-there-a-way-to-quantify-the-relationship-between-person1", "pageUrlRelative": "/posts/CaYn4P7diGqzgLATk/is-there-a-way-to-quantify-the-relationship-between-person1", "linkUrl": "https://www.lesswrong.com/posts/CaYn4P7diGqzgLATk/is-there-a-way-to-quantify-the-relationship-between-person1", "postedAtFormatted": "Sunday, January 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20a%20way%20to%20quantify%20the%20relationship%20between%20Person1's%20Map%20and%20Person2's%20Map%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20a%20way%20to%20quantify%20the%20relationship%20between%20Person1's%20Map%20and%20Person2's%20Map%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaYn4P7diGqzgLATk%2Fis-there-a-way-to-quantify-the-relationship-between-person1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20a%20way%20to%20quantify%20the%20relationship%20between%20Person1's%20Map%20and%20Person2's%20Map%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaYn4P7diGqzgLATk%2Fis-there-a-way-to-quantify-the-relationship-between-person1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaYn4P7diGqzgLATk%2Fis-there-a-way-to-quantify-the-relationship-between-person1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 511, "htmlBody": "<p>Okay, so maybe you could say this.</p>\n<p>Suppose you have an index I. I could be a list of items in belief-space (or a person's map). So I could have these items (believes in evolution, believes in free will, believes that he will get energy from eating food, etc..) Of course, in order to make this argument more rigorous, we must make the beliefs finer.</p>\n<p>For now, we can assume the non-existence of a priori knowledge. In other words, facts they may not explicitly know, but would explicitly deduce simply by using the knowledge they already have.&nbsp;</p>\n<p>Now, maybe Person1 has a map in j-space with values of (0,0,0.2,0.5,0,1,...), corresponding to the degree of his belief in items in index I. So the first value of 0 corresponds to his total disbelief in evolution, the second corresponds to total disbelief in free will, and so on.</p>\n<p>Person2 has a map in k-space with values of (0,0,0.2,0.5,0,0.8, NaN, 0, 1, ...), corresponding to the degree of his belief in everything in the world. Now, I include a value of NaN in his map, because the NaN could correspond to an item in index I that he has never encountered. Maybe there's a way to quantify NaN, which might make it possible for Person1 and Person2 to both have maps in the same n-space (which might make it more possible to compare their mutual information using traditional math methods).</p>\n<p>Furthermore, Person1's map is a function of time, as is Person2's map. Their maps evolve over time since they learn new information, change their beliefs, and forget information. Person1's map can expand from j-space to (j+n)th space, as he forms new beliefs on new items. Once you apply a distance metric to their beliefs, you might be able to map them on a grid, to compare their beliefs with each other. A distance metric with a scalar value, for example, would map their beliefs to a 1D axis (this is what political tests often do). A distance metric can also output a vector value (much like what a MBTI personality test could do) to a value in j-space. If you simply took the difference between the two maps, you cold also output a vector value that could be mapped to a space whose dimension is equal to the dimension of the original map (assuming that the two maps have the same dimension, of course).&nbsp;</p>\n<p>Anyways, here is my question: Is there a better way to quantify this? Has anyone else thought of this? Of course, we could use a distance metric to compare their distances with respect to each other (of course, a Euclidean metric could be used if they have maps in the same n-space.</p>\n<p>==</p>\n<p>As an alternative question, are there metrics that could compare the distance between a map in j-space with a map in k-space (even if j is not equal to k)? I know that you have p-norms that correspond to some absolute scalar value when you apply the p-norms to a matrix. But this is sort of difference. And could mutual information be considered a metric?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CaYn4P7diGqzgLATk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 6.653808808779546e-07, "legacy": true, "legacyId": "4773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-09T04:51:43.141Z", "modifiedAt": null, "url": null, "title": "Meetup organizing query & a rally for Minnesotans", "slug": "meetup-organizing-query-and-a-rally-for-minnesotans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:19.490Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/usvpySq366ajNoaLW/meetup-organizing-query-and-a-rally-for-minnesotans", "pageUrlRelative": "/posts/usvpySq366ajNoaLW/meetup-organizing-query-and-a-rally-for-minnesotans", "linkUrl": "https://www.lesswrong.com/posts/usvpySq366ajNoaLW/meetup-organizing-query-and-a-rally-for-minnesotans", "postedAtFormatted": "Sunday, January 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20organizing%20query%20%26%20a%20rally%20for%20Minnesotans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20organizing%20query%20%26%20a%20rally%20for%20Minnesotans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FusvpySq366ajNoaLW%2Fmeetup-organizing-query-and-a-rally-for-minnesotans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20organizing%20query%20%26%20a%20rally%20for%20Minnesotans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FusvpySq366ajNoaLW%2Fmeetup-organizing-query-and-a-rally-for-minnesotans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FusvpySq366ajNoaLW%2Fmeetup-organizing-query-and-a-rally-for-minnesotans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<p><strong>General Comments about Organizing Meetups</strong></p>\n<p>I looked through the <a href=\"/lw/7c/where_are_we/\">meetup thread</a> prior to making this, but found it horrendously confusing. I don't think LessWrong is the best way to organize, especially via a string of 250 comments. I also checked out the <a href=\"http://www.facebook.com/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>&nbsp;and found it equally unhelpful as far as meetups were concerned. Perhaps there is a way to find out who's from your area by searching the group members, but it didn't look like that was possible to me.</p>\n<p>Is there a recommended way to find out who is in your area, or are these two the only options?&nbsp;I did find <a href=\"http://www.meetup.com\">Meetup.com</a> groups for NYC, Cambridge MA, and the CA bay area, but that was about it from my searching.</p>\n<p><br /><strong>St. Paul/Minneapolis and a Potential Example</strong></p>\n<p>In anticipation that the question above should be answered in a disappointing manner (the thread and Facebook are the current primary options), I went ahead and took some initiative by creating a group at a free site similar to Meetup.com&nbsp;called <a href=\"http://www.groupomatic.com/new.pl\">Group-O-Matic</a>. There's now a site for those in the Twin Cities area <a href=\"http://www.groupomatic.com/c.pl?c=8361517-qvsealsbkj-547155\">here</a>. It's definitely not all fancy-dancy but looks like it has the ability to do what it needs to do: post new meetups, notify members, allow for RSVPs, and display a list of scheduled meetups.&nbsp;</p>\n<p>Google returned results for a meetup that occurred on 9/26/2009 at the University of Minnesota. Has there been activity since then? I'd like to meet members in person.</p>\n<p>I also write this in case others find this solution helpful. I would have used Meetup.com but it requires a fee to be the organizer. Perhaps Group-O-Matic is good enough and will inspire others to organize for their area. If I receive no interest from Minnesota locals, I'll take down the site in a month or so. I don't have anything in mind; I'd just like to meet with flesh and blood and perhaps discuss meetup ideas from there.</p>\n<p>Lastly, if this is something that could/should be a top level post either for assisting me in finding Minnesotans, or giving others ideas for organizing since it will receive more publicity, let me know. I've only posted twice and don't have a good sense for what warrants posting at the top level.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "usvpySq366ajNoaLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 6.654363534072621e-07, "legacy": true, "legacyId": "4774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gRkFrDmuE82difQRH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-09T13:50:59.337Z", "modifiedAt": null, "url": null, "title": "Second Life creators to attempt to create AI", "slug": "second-life-creators-to-attempt-to-create-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:18.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YFKXMo5HzDouT9qkH/second-life-creators-to-attempt-to-create-ai", "pageUrlRelative": "/posts/YFKXMo5HzDouT9qkH/second-life-creators-to-attempt-to-create-ai", "linkUrl": "https://www.lesswrong.com/posts/YFKXMo5HzDouT9qkH/second-life-creators-to-attempt-to-create-ai", "postedAtFormatted": "Sunday, January 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Second%20Life%20creators%20to%20attempt%20to%20create%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASecond%20Life%20creators%20to%20attempt%20to%20create%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFKXMo5HzDouT9qkH%2Fsecond-life-creators-to-attempt-to-create-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Second%20Life%20creators%20to%20attempt%20to%20create%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFKXMo5HzDouT9qkH%2Fsecond-life-creators-to-attempt-to-create-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFKXMo5HzDouT9qkH%2Fsecond-life-creators-to-attempt-to-create-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p>http://nwn.blogs.com/nwn/2010/02/philip-rosedale-ai.html</p>\n<p>http://www.lovemachineinc.com/</p>\n<p>Should I feel bad for hoping they'll fail? I do not want to see the sort of unFriendly AI would be created after being raised on social interactions with pedophiles, Gorians, and furries. Seriously, those are some of the more prominent of the groups still on Second Life, and an AI that spends its formative period interacting with them (and the first two, especially) could develop a very twisted morality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YFKXMo5HzDouT9qkH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 4, "extendedScore": null, "score": 6.655741783546087e-07, "legacy": true, "legacyId": "4775", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-09T15:20:42.796Z", "modifiedAt": null, "url": null, "title": "Branding Biodiversity", "slug": "branding-biodiversity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kNm2WyweRyfWZrnMo/branding-biodiversity", "pageUrlRelative": "/posts/kNm2WyweRyfWZrnMo/branding-biodiversity", "linkUrl": "https://www.lesswrong.com/posts/kNm2WyweRyfWZrnMo/branding-biodiversity", "postedAtFormatted": "Sunday, January 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Branding%20Biodiversity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABranding%20Biodiversity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNm2WyweRyfWZrnMo%2Fbranding-biodiversity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Branding%20Biodiversity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNm2WyweRyfWZrnMo%2Fbranding-biodiversity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNm2WyweRyfWZrnMo%2Fbranding-biodiversity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>http://www.futerra.co.uk/downloads/Branding_Biodiversity.pdf</p>\n<p>I'm quite interested in seeing if this works. I have sent this to several wildlife-guides and conservationists and will monitor their reactions.</p>\n<p>It talks about what emotions drive people to actually do something to protect biodiversity rather then just showing them figures. After looking at what makes a certain brand successful they apply it on biodiversity. Their end conclusion is to remove messages based on extinction as it just makes people apathetic rather then inspire change. Furthermore they propose different ways of conveying \"biodiversity is important\" for different audiences. Love, fuzzy feelings and \"you-can-make-a-difference!\" for public changes and financial advantages and concrete action for policy changes. Lastly, the advise to make the message more personal by talking about loving your pets, focusing on local species and anthropomorphise whatever you are talking about.</p>\n<p>In short they want to protect biodiversity by making it a brand name and getting people to buy their product (i.e. donate money, etc.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kNm2WyweRyfWZrnMo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "4776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-09T15:43:35.015Z", "modifiedAt": null, "url": null, "title": "London meetup, Shakespeare's Head, Sunday 2011-03-06 14:00", "slug": "london-meetup-shakespeare-s-head-sunday-2011-03-06-14-00", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.697Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/36D3SH3WnYhLW8cEL/london-meetup-shakespeare-s-head-sunday-2011-03-06-14-00", "pageUrlRelative": "/posts/36D3SH3WnYhLW8cEL/london-meetup-shakespeare-s-head-sunday-2011-03-06-14-00", "linkUrl": "https://www.lesswrong.com/posts/36D3SH3WnYhLW8cEL/london-meetup-shakespeare-s-head-sunday-2011-03-06-14-00", "postedAtFormatted": "Sunday, January 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20meetup%2C%20Shakespeare's%20Head%2C%20Sunday%202011-03-06%2014%3A00&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20meetup%2C%20Shakespeare's%20Head%2C%20Sunday%202011-03-06%2014%3A00%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F36D3SH3WnYhLW8cEL%2Flondon-meetup-shakespeare-s-head-sunday-2011-03-06-14-00%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20meetup%2C%20Shakespeare's%20Head%2C%20Sunday%202011-03-06%2014%3A00%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F36D3SH3WnYhLW8cEL%2Flondon-meetup-shakespeare-s-head-sunday-2011-03-06-14-00", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F36D3SH3WnYhLW8cEL%2Flondon-meetup-shakespeare-s-head-sunday-2011-03-06-14-00", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p><a href=\"/lw/3d3/london_meetup_on_201112/\">Our last London meetup was a fantastic success</a>! We're doing it again, same time same place. &nbsp;The time: Sunday 6th March, at 2pm. &nbsp;The place: the <a href=\"http://maps.google.co.uk/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=shakespeare's+head&amp;sll=51.42559,-0.130394&amp;sspn=0.011854,0.020943&amp;ie=UTF8&amp;hq=shakespeare's+head&amp;hnear=&amp;layer=c&amp;cbll=51.516734,-0.119933&amp;panoid=kXPwAeowAo9LzJJA34agOw&amp;cbp=11,76.42,,1,-1.06&amp;ll=51.516728,-0.124025&amp;spn=0.005622,0.022488&amp;z=16\">Shakespeares Head</a>&nbsp;(<a href=\"http://www.jdwetherspoon.co.uk/home/pubs/shakespeares-head\">official page</a>) on Kingsway near Holborn Tube station. &nbsp;As before, we'll have a big picture of a paperclip on the table so you can find us; also, I <a href=\"http://pics.babysimon.co.uk/index.py/photos/3902?tag=Paul\">look like this</a>. &nbsp;I'm hoping that we can graduate to meeting up every other month, on the first Sunday of the month. Hope to see lots of you there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "36D3SH3WnYhLW8cEL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 6.656029612987659e-07, "legacy": true, "legacyId": "4777", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kQF2NSTu7cZ7LnpKX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-09T17:52:51.814Z", "modifiedAt": null, "url": null, "title": "A LessWrong \"rationality workbook\" idea", "slug": "a-lesswrong-rationality-workbook-idea", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gPSCQC3WMeJnW9q4W/a-lesswrong-rationality-workbook-idea", "pageUrlRelative": "/posts/gPSCQC3WMeJnW9q4W/a-lesswrong-rationality-workbook-idea", "linkUrl": "https://www.lesswrong.com/posts/gPSCQC3WMeJnW9q4W/a-lesswrong-rationality-workbook-idea", "postedAtFormatted": "Sunday, January 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20LessWrong%20%22rationality%20workbook%22%20idea&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20LessWrong%20%22rationality%20workbook%22%20idea%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPSCQC3WMeJnW9q4W%2Fa-lesswrong-rationality-workbook-idea%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20LessWrong%20%22rationality%20workbook%22%20idea%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPSCQC3WMeJnW9q4W%2Fa-lesswrong-rationality-workbook-idea", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPSCQC3WMeJnW9q4W%2Fa-lesswrong-rationality-workbook-idea", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 978, "htmlBody": "<p><strong>Note: </strong>This was originally posted in the discussion area, but motions to move it to the top level were made.<br />-----</p>\n<p>My own desire to improve my rationality coupled with some posts criticizing LessWrong not too long ago led to an idea. For reference, the posts I mean are these:</p>\n<ul>\n<li><a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">Goals for which LessWrong does (and doesn't) help</a></li>\n<li><a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">Self-Improvement of Shiny Distraction: Why LessWrong is anti-instrumental Rationality</a></li>\n<li>A <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l5h\">comment</a>&nbsp;on <a href=\"/lw/2p5/humans_are_not_automatically_strategic\">Humans are not automatically strategic</a></li>\n</ul>\n<div>The comment stream on that last link brought up whether or not LessWrong is fulfilling EY's vision of a \"<a href=\"/lw/gn/the_martial_art_of_rationality/\">rationality dojo</a>\" of sorts (along with what I assume is the idea that LessWong is <em>supposed</em> to be fulfilling that vision).</div>\n<div>Now, I'm not the one to evaluate whether LessWrong is or is not a true \"rationality dojo\" (not to mention reducing the problem as not to argue over&nbsp;<a href=\"/lw/np/disputing_definitions/\">semantics</a>), <em>but</em>&nbsp;I do have an idea for how it could perhaps do so, and an idea I personally find quite exciting.</div>\n<div>The idea is simple: &nbsp;<em>that of a LessWrong workbook</em>.</div>\n<div>What does that mean? For one, it means an awful lot of effort, primarily at distillation. I quite enjoy reading posts and appreciate the overall prose style used here. A lot of posts are easy reading, contain nice anecdotes, hypothetical real-world scenarios, and the like. Distillation would entail extraction of the \"nuggets\" that an aspiring rationalist should know. Teach me rationality in bullet points, flash card modules, and other mini-homework-sized methods (which I'll get to).</div>\n<div>I see the above as reducing to some form of \"Rationality Overview.\" What is rationality? What's the point? What is the end result? What are the typical tools used to be rational? Finally, perhaps, what does the life of a rationalist look like? Perhaps pages containing some helpful \"mantras\" or questions to ask one's self periodically, such as during some period of daily meditation? Inspirational/helpful prose like the &nbsp;<a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues</a>, &nbsp;<a href=\"http://yudkowsky.net/rational/the-simple-truth\">The Simple Truth</a>, and the Litanies of &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Gendlin\">Gendlin</a>&nbsp;&nbsp;and &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">Tarski</a>.</div>\n<div>Secondly, I envision a literal \"workbook\" type of section. I was intrigued by a &nbsp;<a href=\"/r/discussion/lw/3n2/my_story_owning_ones_reasons/3atg\">comment</a>&nbsp;&nbsp;on my &nbsp;<a href=\"/r/discussion/lw/3n2/my_story_owning_ones_reasons/\">first post</a>, in which &nbsp;<a href=\"/user/JenniferRM/\">JenniferRM</a>&nbsp;&nbsp;wrote [in discussing my &nbsp;<a href=\"http://technologeekery.blogspot.com/2010/07/quest.html\">ongoing de-conversion</a>]:</div>\n<blockquote>\n<div>Unfortunately, I don't know of &nbsp;<em>any</em>&nbsp;resources to help people traverse the path you're facing in a series of small safe steps.</div>\n</blockquote>\n<div>Why not? Shouldn't there be some example of guidelines for evaluating beliefs and attempting to rationally adjust them according to new evidence, though processes, or trustworthy tools? Hence the birth of my workbook idea.</div>\n<div>Most specifically, I envision the following:</div>\n<div>\n<ul>\n<li>Most interesting to me was the idea of some form of \"rationality comb.\" An iterative evaluation process. Again, I hardly consider myself the one to design this, but perhaps something like: \n<ul>\n<li>Take 5 minutes and brainstorm about the beliefs you think affect your actions the most</li>\n<li>Focus on the first belief, set(1):belief(1)</li>\n<li>Can you recall how you came to hold this belief?</li>\n<li>What are some common alternative views to your belief?</li>\n<li>Do you think you could provide &nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">testable</a>&nbsp;&nbsp;justification for your current belief over the above alternatives?</li>\n<li>If not, can you &nbsp;<a href=\"/lw/o4/leave_a_line_of_retreat/\">imagine</a>&nbsp;&nbsp;leaving your belief for one of the alternatives?</li>\n<li>And so on...</li>\n<li>Then repeat with set(1):belief(2). When set(1):belief(n) is finished... re-brainstorm for 5min to come up with set(2):belief(1)...belief(n).</li>\n</ul>\n</li>\n<li>A series of \"homework\" problems on Bayesian Probability, perhaps including EY's &nbsp;<a href=\"http://yudkowsky.net/rational/bayes\">tutorial</a>&nbsp;&nbsp;and other helpful material.</li>\n<li>Brain teasers or similar items to focus on attentiveness to details, weighing evidence, knowing the limits of what you can know given certain information, etc. I think LW has already provided some good &nbsp;<a href=\"/lw/3o8/rationalist_clue/\">examples</a>&nbsp; of neat things like this (even if they would require refinement).</li>\n<li>Questions that intentionally try to deceive the reader with some form of &nbsp;<a href=\"/lw/hu/the_third_alternative/\">fallacy</a>&nbsp;&nbsp;or &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Bias\">bias</a></li>\n<li>Tutorials on how to have rational discussions, rules of engagement, &nbsp;<a href=\"/lw/gr/the_modesty_argument/\">reaching a mutual conclusion</a>, etc.</li>\n</ul>\n<div>This is just a stab at what the \"meat\" of the workbook could contain.</div>\n<div>My primary interest is in the first bullet: a step-by-step guide to examining one's life. I think it needs to be iterative, since not all beliefs in need of rational attention will be apparent on the first pass, and each subsequent step needs to be able to be done in bite sized chunks. Steps should have concrete starting and ending points that seem achievable to promote continuing effort and a sense of accomplishment. I left out quite a few steps... suggestions could be made as to what kind of information should be sought in attempt to confirm or disconfirm one's belief/hypothesis (and being cautious if you find yourself looking for &nbsp;<a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">expensive evidence in favor vs. cheap evidence against</a>), how to know be wary of hypotheses that &nbsp;<a href=\"/lw/if/your_strength_as_a_rationalist/\">explain too much</a>, and so on.</div>\n<div>The point is to have a handy reference guide to not only becoming a rationalist, but also to have a concrete guide to applying those skills to your own life. Steps summarized from trusted methods help avoid pitfalls, help us learn from what's already documented, and (I would suspect) help us avoid subconscious biases or shields we might be tempted to apply to cherished beliefs if left to our own devices/methods.</div>\n<div>By this time, I'm hoping I have the point across. I've tried to link heavily to show that I'm not really proposing new material -- the content seems to be here. My idea consists of is &nbsp;<em>rearranging</em>&nbsp;this content into a compact \"dojo-like\" format.</div>\n</div>\n<div>So, what are your thoughts? Is this done elsewhere already (if so, my apologies for wasting your time)? Does this sound helpful? Is it plausible/feasible? Would &nbsp;<em>one</em>&nbsp;workbook be able to apply to varied personalities/learning methods? And what are &nbsp;<em>your</em>&nbsp;thoughts (if you like the idea) on what it would contain?</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EdDGrAxYcrXnKkDca": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gPSCQC3WMeJnW9q4W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 25, "extendedScore": null, "score": 6.656358842822957e-07, "legacy": true, "legacyId": "4771", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7dRGYDqA2z6Zt7Q4h", "uFYQaGCRwt3wKtyZP", "PBRWb2Em5SNeWYwwB", "teaxCFgtmCQ3E9fy8", "7X2j8HAkWdmMoS8PE", "g3RwsGofS6FE5nBsG", "a7n8GdKiAZRX86T5A", "3XgYbghWruBMrPTAL", "nEt2MhG8rbmvhrdjG", "erGipespbbzdG5zYb", "NKECtGX4RZPd7SqYp", "L32LHWzy9FzSDazEg", "5JDkW4MYXit2CquLs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-10T00:21:28.461Z", "modifiedAt": null, "url": null, "title": "Spaced Repetition Database for A Human's Guide to Words", "slug": "spaced-repetition-database-for-a-human-s-guide-to-words", "viewCount": null, "lastCommentedAt": "2020-08-24T14:57:40.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "divia", "createdAt": "2009-02-28T01:56:35.966Z", "isAdmin": false, "displayName": "divia"}, "userId": "CQzR9QRTNKQ9Qmsjc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2iwT298d6A7QgiLPe/spaced-repetition-database-for-a-human-s-guide-to-words", "pageUrlRelative": "/posts/2iwT298d6A7QgiLPe/spaced-repetition-database-for-a-human-s-guide-to-words", "linkUrl": "https://www.lesswrong.com/posts/2iwT298d6A7QgiLPe/spaced-repetition-database-for-a-human-s-guide-to-words", "postedAtFormatted": "Monday, January 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spaced%20Repetition%20Database%20for%20A%20Human's%20Guide%20to%20Words&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpaced%20Repetition%20Database%20for%20A%20Human's%20Guide%20to%20Words%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2iwT298d6A7QgiLPe%2Fspaced-repetition-database-for-a-human-s-guide-to-words%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spaced%20Repetition%20Database%20for%20A%20Human's%20Guide%20to%20Words%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2iwT298d6A7QgiLPe%2Fspaced-repetition-database-for-a-human-s-guide-to-words", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2iwT298d6A7QgiLPe%2Fspaced-repetition-database-for-a-human-s-guide-to-words", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/2e6/spaced_repetition_database_for_the_mysterious/\">Spaced Repetition Database for Mysterious Answers to Mysterious Questions</a></p>\n<p>I've updated my Anki database for the Less Wrong Sequences&nbsp;to include cards from&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human's Guide to Words</a>. I've been trying to put less information on each card, and I relied on cloze deletion more for the newer ones. &nbsp;Feedback is much appreciated. You can download them by opening up <a href=\"http://ichi2.net/anki/\">Anki</a>, going to Download &gt; Shared Deck and searching for Less Wrong Sequences.</p>\n<p>I probably erred on the side of making way too many cards, but it seemed really important to me to internalize this stuff, since I think it has quite a lot of practical value. I can tell learning this deck has improved the quality of my thinking and my conversations with people because I'm better at noticing when I'm making one of the 37 mistakes and changing my course. I hope other people find it useful too!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2iwT298d6A7QgiLPe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 48, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "4778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iRfKKYhZAG8fWDjJr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-10T01:40:29.905Z", "modifiedAt": null, "url": null, "title": "A cautionary note about \"Bayesianism\"", "slug": "a-cautionary-note-about-bayesianism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:18.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uXurcN49sXA6oSTyT/a-cautionary-note-about-bayesianism", "pageUrlRelative": "/posts/uXurcN49sXA6oSTyT/a-cautionary-note-about-bayesianism", "linkUrl": "https://www.lesswrong.com/posts/uXurcN49sXA6oSTyT/a-cautionary-note-about-bayesianism", "postedAtFormatted": "Monday, January 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20cautionary%20note%20about%20%22Bayesianism%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20cautionary%20note%20about%20%22Bayesianism%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXurcN49sXA6oSTyT%2Fa-cautionary-note-about-bayesianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20cautionary%20note%20about%20%22Bayesianism%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXurcN49sXA6oSTyT%2Fa-cautionary-note-about-bayesianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXurcN49sXA6oSTyT%2Fa-cautionary-note-about-bayesianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>(Is Bayesianism even a word?&nbsp; Should it be?&nbsp; The suffix \"ism\" sets off warning lights for me.)</p>\n<p>Visitors to LessWrong may come away with the impression that they need to be Bayesians to be rational, or to fit in here.&nbsp; But most people are a long way from the point where learning Bayesian thought patterns is the most time-effective thing they can do to improve their rationality.&nbsp; Most of the insights available on LessWrong don't require people to understand Bayes' Theorem (or timeless decision theory).</p>\n<p>I'm not calling for any specific change.&nbsp; Just to keep this in mind when writing things in the Wiki, or constructing a <a href=\"/lw/3oj/a_lesswrong_rationality_workbook_idea/\">rationality workbook</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uXurcN49sXA6oSTyT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 6.657555895205843e-07, "legacy": true, "legacyId": "4781", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gPSCQC3WMeJnW9q4W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-10T04:54:45.706Z", "modifiedAt": null, "url": null, "title": "Bayesianism versus Critical Rationalism", "slug": "bayesianism-versus-critical-rationalism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:27.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7wzZsPdhXiAStKANw/bayesianism-versus-critical-rationalism", "pageUrlRelative": "/posts/7wzZsPdhXiAStKANw/bayesianism-versus-critical-rationalism", "linkUrl": "https://www.lesswrong.com/posts/7wzZsPdhXiAStKANw/bayesianism-versus-critical-rationalism", "postedAtFormatted": "Monday, January 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesianism%20versus%20Critical%20Rationalism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesianism%20versus%20Critical%20Rationalism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wzZsPdhXiAStKANw%2Fbayesianism-versus-critical-rationalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesianism%20versus%20Critical%20Rationalism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wzZsPdhXiAStKANw%2Fbayesianism-versus-critical-rationalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wzZsPdhXiAStKANw%2Fbayesianism-versus-critical-rationalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 247, "htmlBody": "<p>I have just rediscovered an article by <a href=\"http://ideas.repec.org/e/pal175.html\">Max Albert</a> on my hard drive which I never got around to reading that might interest others on Less Wrong. You can find the article <a href=\"http://www.rmm-journal.de/downloads/004_albert.pdf\">here</a>. It is an argument against <a href=\"http://plato.stanford.edu/entries/epistemology-bayesian/\">Bayesianism</a> and for <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Critical_rationalism\">Critical Rationalism</a> (of <a href=\"http://plato.stanford.edu/entries/popper/\">Karl Popper</a> fame).</p>\n<p>Abstract:</p>\n<blockquote>\n<p>Economists claim that principles of rationality are normative principles. Nevertheless,<br />they go on to explain why it is in a person&rsquo;s own interest to be rational. If this were true,<br />being rational itself would be a means to an end, and rationality could be interpreted in<br />a non-normative or naturalistic way. The alternative is not attractive: if the only argument<br />in favor of principles of rationality were their intrinsic appeal, a commitment to<br />rationality would be irrational, making the notion of rationality self-defeating. A comprehensive<br />conception of rationality should recommend itself: it should be rational to be<br />rational. Moreover, since rational action requires rational beliefs concerning means-ends<br />relations, a naturalistic conception of rationality has to cover rational belief formation including<br />the belief that it is rational to be rational. The paper considers four conceptions<br />of rationality and asks whether they can deliver the goods: Bayesianism, perfect rationality<br />(just in case that it differs from Bayesianism), ecological rationality (as a version of<br />bounded rationality), and critical rationality, the conception of rationality characterizing<br />critical rationalism.</p>\n</blockquote>\n<p><a href=\"http://ideas.repec.org/e/pal175.html\"></a></p>\n<p>Any thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7wzZsPdhXiAStKANw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.658052750365105e-07, "legacy": true, "legacyId": "4785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 274, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-10T11:14:49.179Z", "modifiedAt": null, "url": null, "title": "Open Thread, January 2011", "slug": "open-thread-january-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:59.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GpdBdYcLzBQ5rqRBv/open-thread-january-2011", "pageUrlRelative": "/posts/GpdBdYcLzBQ5rqRBv/open-thread-january-2011", "linkUrl": "https://www.lesswrong.com/posts/GpdBdYcLzBQ5rqRBv/open-thread-january-2011", "postedAtFormatted": "Monday, January 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20January%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20January%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpdBdYcLzBQ5rqRBv%2Fopen-thread-january-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20January%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpdBdYcLzBQ5rqRBv%2Fopen-thread-january-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpdBdYcLzBQ5rqRBv%2Fopen-thread-january-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Better late than never, a new open thread. Even with the discussion section, there are ideas or questions too short or inchoate to be worth a post.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\">This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GpdBdYcLzBQ5rqRBv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 6.659024990848259e-07, "legacy": true, "legacyId": "4789", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-10T16:15:22.184Z", "modifiedAt": null, "url": null, "title": "Deontological Decision Theory and The Solution to Morality", "slug": "deontological-decision-theory-and-the-solution-to-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.544Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dkzGMnYtP4ZTWjnqH/deontological-decision-theory-and-the-solution-to-morality", "pageUrlRelative": "/posts/dkzGMnYtP4ZTWjnqH/deontological-decision-theory-and-the-solution-to-morality", "linkUrl": "https://www.lesswrong.com/posts/dkzGMnYtP4ZTWjnqH/deontological-decision-theory-and-the-solution-to-morality", "postedAtFormatted": "Monday, January 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deontological%20Decision%20Theory%20and%20The%20Solution%20to%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeontological%20Decision%20Theory%20and%20The%20Solution%20to%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkzGMnYtP4ZTWjnqH%2Fdeontological-decision-theory-and-the-solution-to-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deontological%20Decision%20Theory%20and%20The%20Solution%20to%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkzGMnYtP4ZTWjnqH%2Fdeontological-decision-theory-and-the-solution-to-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkzGMnYtP4ZTWjnqH%2Fdeontological-decision-theory-and-the-solution-to-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2292, "htmlBody": "<p class=\"MsoNormal\"><strong>Asking the Question</strong></p>\n<p class=\"MsoNormal\">Until very recently, I was a hedonic utilitarian. That is, I held &lsquo;happiness is good&rsquo; as an axiom &ndash; blurring the definition a little by pretending that good emotions other than strict happiness still counted because it made people &ldquo;happy&rdquo; to have them -- and built up my moral philosophy from there. There were a few problems I couldn&rsquo;t quite figure out, but by and large, it worked: it produced answers that felt right, and it was the most logically consistent moral system I could find.</p>\n<p class=\"MsoNormal\">But then I read <a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>.</p>\n<p class=\"MsoNormal\">The <a href=\"/lw/ya/normal_ending_last_tears_68/\">ending</a> didn&rsquo;t fit within my moral model: it was a scenario in which making people happy seemed <em>wrong. </em>Which raised the question: What&rsquo;s so great about happiness? If people don&rsquo;t want happiness, how can you call it good to force it on them? After all,<span>&nbsp; </span>happiness is just a pattern of neural excitation in the brain; it can&rsquo;t possibly be an <em>intrinsic</em> good, any more than the pattern that produces the thought &ldquo;2+2=4&rdquo;.</p>\n<p class=\"MsoNormal\">Well, people like being happy. Happiness is something they <em>want</em>. But it&rsquo;s by no means <em>all </em>they want: people also want mystery, wonder, excitement, and many other things &ndash; and so those things are also good, quite independent of their relation to the specific emotion &lsquo;happiness&rsquo;. If they also desire occasional sadness and pain, who am I to say they&rsquo;re wrong? It&rsquo;s not moral to make people happy against their desires &ndash; it&rsquo;s moral to give people what they <em>want. </em>(Voila, preference utilitarianism.)</p>\n<p class=\"MsoNormal\">But &ndash; that&rsquo;s not a real answer, is it?</p>\n<p class=\"MsoNormal\">If axiom &lsquo;happiness is good&rsquo; didn&rsquo;t match my idea of morality, that meant I wasn&rsquo;t really constructing my morality around it. Replacing that axiom with &lsquo;preference fulfillment is good&rsquo;<span> </span>would make my logic match my feelings better, but it wouldn&rsquo;t give me a reason to have those feelings in the first place. So I had to ask the next question: <em>Why </em>is preference fulfillment good? What makes it &ldquo;good&rdquo; to give other people what they want?<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">Why should we care about other people at all?</p>\n<p class=\"MsoNormal\">In other words, why be moral?</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\">Human feelings are a product of our evolutionary pressures. Emotions, the things that make us human, are there because they caused the genes that promoted them to become more prevalent in the ancestral environment. That includes the emotions surrounding moral issues: the things that seem so obviously right or wrong seem that way <em>because that feeling was adaptive, </em>not because of any intrinsic quality.</p>\n<p class=\"MsoNormal\">This makes it impossible to trust any moral system based on gut reaction, as most people&rsquo;s seem to be. Our feelings of right and wrong were engineered to maximize genetic replication, so why should we expect them to tap into objective realms of &lsquo;right&rsquo; and &lsquo;wrong&rsquo;? And in fact, people&rsquo;s moral judgments tend to be suspiciously biased towards their own interests, though proclaimed with the strength of true belief.</p>\n<p class=\"MsoNormal\">More damningly, such moralities are incapable of coming up with a correct answer. One person can proclaim, say, homosexuality to be objectively right or wrong everywhere for everyone, with no justification except how they feel about it, and in the same breath say that it would still be wrong if they felt the other way. Another person, who <em>does </em>feel the other way, can deny it with equal force. And there&rsquo;s no conceivable way to decide who&rsquo;s right.</p>\n<p class=\"MsoNormal\">I became a utilitarian because it seemed to resolve many of the problems associated with purely intuitive morality &ndash; it was internally consistent, it relied on a simple premise, and it could provide its practitioners a standard of judgment for moral quandaries.</p>\n<p class=\"MsoNormal\">But <em>even utilitarianism is based on feeling. </em>This is especially true for hedonic utilitarianism, but little less for preference &ndash; we call people getting what they want &lsquo;good&rsquo; because it <em>feels </em>good. It lights up our mirror neurons, triggers the altruistic instincts encoded into us by evolution. But evolution&rsquo;s purposes are not our own (we have no particular interest in our genes&rsquo; replication) and so it makes no sense to adopt evolution&rsquo;s tools as our ultimate goals.</p>\n<p class=\"MsoNormal\">If you can&rsquo;t derive a moral code from evolution, then you can&rsquo;t derive it from emotion, the tool of evolution; if you can&rsquo;t derive morality from emotion, then you can&rsquo;t say that giving people what they want is objectively good because it feels good; if you can&rsquo;t do that, you can&rsquo;t be a utilitarian.</p>\n<p class=\"MsoNormal\">Emotions, of course, are not <em>bad. </em>Even knowing that love was designed to transmit genes, we still want love; we still find it worthwhile to pursue, even knowing that we were built to pursue it. But we can&rsquo;t hold up love as something <em>objectively </em>good, something that everyone should pursue &ndash; we don&rsquo;t condemn the asexual. In the same way, it&rsquo;s perfectly reasonable to help other people because it makes you feel good (to pursue <a href=\"http://wiki.lesswrong.com/wiki/Fuzzies\">warm fuzzies</a> for their own sake), but that emotional justification can&rsquo;t be used as the basis for a claim that <em>everyone </em>should help other people.</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\">So if we can&rsquo;t rely on feeling to justify morality, why have it at all?</p>\n<p class=\"MsoNormal\">Well, the obvious alternative is that it&rsquo;s practical. Societies populated by moral individuals &ndash; individuals who value the happiness of others &ndash; work better than those filled with selfish ones, because the individually selfless acts add up to greater utility for everyone. One only has to imagine a society populated by purely selfish individuals to see why pure selfishness wouldn&rsquo;t work.</p>\n<p class=\"MsoNormal\">This is a facile answer. First, if this is the case, why would morality extend outside of our societies? Why should we want to <a href=\"/lw/y5/the_babyeating_aliens_18/\">save the Babyeater children</a>?</p>\n<p class=\"MsoNormal\">But more importantly, how is it practical for <strong>you? </strong>There is <em>no </em>situation in which the best strategy is not being purely selfish. If reciprocal altruism makes you better off, then it&rsquo;s selfishly beneficial to be reciprocally altruistic; if you value warm fuzzies, then it&rsquo;s selfishly beneficial to get warm fuzzies; but <em>by definition</em>, true selflessness of the kind demanded by morality (like <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">buying utilons with money that could be spent on fuzzies</a>) decreases your utility &ndash; it <strong>loses. </strong>Even if you get a deep emotional reward from helping others, you&rsquo;re <em>strictly better off </em>being selfish.</p>\n<p class=\"MsoNormal\">So if feelings of &lsquo;right&rsquo; and &lsquo;wrong&rsquo; don&rsquo;t correspond to anything except what used to maximize inclusive genetic fitness, and having a moral code makes you <em>indisputably</em> <em>worse off</em>, why have one at all?</p>\n<p class=\"MsoNormal\">Once again: Why be moral?</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\"><strong>The Inconsistency of Consequentialism <br /></strong></p>\n<p class=\"MsoNormal\">Forget all that for a second. Stop questioning whether morality is justified and start using your moral judgment again.</p>\n<p class=\"MsoNormal\">Consider a consequentialist student being tempted to cheat on a test. Getting a good grade is important to him, and he can only do that if he cheats; cheating will make him significantly happier. His school trusts its students, so he&rsquo;s pretty sure he won&rsquo;t get caught, and the test isn&rsquo;t curved, so no one else will be hurt by him getting a good score. He decides to cheat, reasoning that it&rsquo;s at least morally neutral, if not a moral imperative &ndash; after all, his cheating will increase the world&rsquo;s utility.</p>\n<p class=\"MsoNormal\">Does this tell us cheating isn&rsquo;t a problem? No. If cheating became widespread, there would be consequences &ndash; tighter test security measures, suspicion of test grades, distrust of students, et cetera. Cheating just this once won&rsquo;t hurt anybody, but if cheating becomes expected, everyone is worse off.</p>\n<p class=\"MsoNormal\">But wait. If all the students are consequentialists, then they&rsquo;ll <em>all </em>decide to cheat, following the same logic as the first. And the teachers, anticipating this (it&rsquo;s an ethics class), will respond with draconian anti-cheating measures &ndash; leaving overall utility <em>lower </em>than if no one had been inclined to cheat at all.</p>\n<p class=\"MsoNormal\">Consequentialism called for each student to cheat because cheating would increase utility, but<em> the fact that </em>consequentialism called for each student to cheat decreased utility.</p>\n<p class=\"MsoNormal\">Imagine the opposite case: a class full of deontologists. Every student would be horrified at the idea of violating their duty for the sake of mere utility, and accordingly not a one of them would cheat. Counter-cheating methods would be completely unnecessary. Everyone would be better off.</p>\n<p class=\"MsoNormal\">In this situation, a deontologist class outcompetes a consequentialist one <em>in consequentialist terms. </em>The best way to maximize utility is to use a system of justification not based on maximizing utility. In such a situation, consequentialism calls for itself not to be believed. Consequentialism is inconsistent.</p>\n<p class=\"MsoNormal\">So what&rsquo;s a rational agent to do?</p>\n<p class=\"MsoNormal\">The apparent contradiction in this case results from thinking about beliefs and actions as though they were separate. Arriving at a belief is an action in itself, one which can have effects on utility. One cannot, therefore, arrive at a belief about utility without considering the effects on utility that holding that belief would have. If arriving at the belief &ldquo;actions are justified by their effect on utility&rdquo; doesn&rsquo;t maximize utility, then you shouldn&rsquo;t arrive at that belief.</p>\n<p class=\"MsoNormal\">However, the ultimate goal of maximizing utility cannot be questioned. Utility, after all, is only a word for &ldquo;what is wanted&rdquo;, so no agent can want to do anything except maximize utility. Moral agents include others' utility as equal to their own, but their goal is still to maximize utility.</p>\n<p class=\"MsoNormal\">Therefore the rule which should be followed is not &ldquo;take the actions which maximize utility&rdquo;, but &ldquo;arrive at the beliefs which maximize utility.&rdquo;</p>\n<p class=\"MsoNormal\">But there is an additional complication: when we arrive at beliefs by logic alone, we are effectively deciding not only for ourselves, but for all other rational agents, since the answer which is logically correct for us must also be logically correct for each of them. In this case, the correct answer is the one which maximizes utility &ndash; so our logic must take into account the fact that every other computation will produce the same answer. Therefore we can expand the rule to &ldquo;arrive at the beliefs which would maximize utility if all other rational agents were to arrive at them (upon performing the same computation).&rdquo;</p>\n<p class=\"MsoNormal\">[To the best of my logical ability, this rule is recursive and therefore requires no further justification.]</p>\n<p class=\"MsoNormal\">This rule requires you to hold whatever beliefs will (conditional upon them being held) lead to the best results &ndash; even when the actions those beliefs produce don&rsquo;t, in themselves, maximize utility. In the case of the cheating student, the optimal belief is &ldquo;don&rsquo;t cheat&rdquo; because that belief being held by all the students (and the teacher simulating the students&rsquo; beliefs) produces the best results, <em>even though cheating would still increase utility for each individual student. </em>The applied morality becomes deontological, in the sense that actions are judged not by their effect on utility but by their adherence to the pre-set principle.</p>\n<p class=\"MsoNormal\">The upshot of this system is that you have to decide ahead of time whether an approach based on duty (that is, on every agent who considers the problem acting the way that would produce the best consequences if every agent who considers the problem were to act the same way) or on utility (individual computation of consequences) actually produces better consequences. And if you pick the deontological approach, you have to &lsquo;forget&rsquo; your original goal &ndash; to commit to the rule even at the cost of actual consequences &ndash; because if it&rsquo;s rational to pursue the original goal, then it won&rsquo;t be achieved.</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\"><strong>The Solution to Morality</strong></p>\n<p class=\"MsoNormal\">Let&rsquo;s return to the original question.</p>\n<p class=\"MsoNormal\">The primary effect of morality is that it causes individuals to value others&rsquo; utility as an end in itself, and therefore to sacrifice their own utility for others. It&rsquo;s obvious that this is <em>very good </em>on a group scale: a society filled with selfless people, people who help others even when they don&rsquo;t expect to receive personal benefit, is far better off than one filled with people who do not &ndash; in a Prisoner&rsquo;s Dilemma writ large. To encourage that sort of cooperation (partially by design and partially by instinct), societies reward altruism and punish selflessness.</p>\n<p class=\"MsoNormal\">But why should you, personally, cooperate?</p>\n<p class=\"MsoNormal\">There are many, many times when you can do clearly better by selfishness than by altruism &ndash; by theft or deceit or just by not giving to charity. And why should we want to do otherwise? Our alruistic feelings are a mere artifact of evolution, like appendices and death, so why would we want to obey them?<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">Is there any reason, then, to be moral?</p>\n<p class=\"MsoNormal\">Yes.</p>\n<p class=\"MsoNormal\">Because that reasoning &ndash; that your own utility is maximized by selfishness &ndash; literally cannot be right. If it were right, then it would be the answer all rational beings would arrive at, and if all rational beings arrived at that answer, then none of them would cooperate and everyone would be worse off. If selfish utility maximizing is the correct answer for how to maximize selfish utility, selfish utility is not maximized. Therefore selfishness is the <em>wrong answer. </em>Each individual&rsquo;s utility is maximized only if they deliberately discard selfish utility as the thing to be maximized. And the way to do that is for each one to adopt a duty to maximize total utility, not only their own &ndash; <span>&nbsp;</span>to be moral.<em></em></p>\n<p class=\"MsoNormal\">And having chosen collective maximization over individual competition &ndash; duty over utility &ndash; you can no longer even consider your own benefit to be your goal. If you do so, holding morality as a means to selfishness&rsquo;s end, then everyone does so, and cooperation comes crashing down. You have to &lsquo;forget&rsquo; the reason for having morality, and hold it because it's the right thing to do. You have to be moral even to the point of death.</p>\n<p class=\"MsoNormal\">Morality, then, is calculated blindness &ndash; a deliberate ignorance of our real ends, meant to achieve them more effectively. Selflessness for its own sake, for selfishness's sake.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">[This post lays down only the basic theoretic underpinnings of Deontological Decision Theory morality. My next post will focus on the practical applications of DDT in the human realm, and explain how it solves various moral/game-theoretic quandaries.]</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dkzGMnYtP4ZTWjnqH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -8, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "4790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p class=\"MsoNormal\"><strong id=\"Asking_the_Question\">Asking the Question</strong></p>\n<p class=\"MsoNormal\">Until very recently, I was a hedonic utilitarian. That is, I held \u2018happiness is good\u2019 as an axiom \u2013 blurring the definition a little by pretending that good emotions other than strict happiness still counted because it made people \u201chappy\u201d to have them -- and built up my moral philosophy from there. There were a few problems I couldn\u2019t quite figure out, but by and large, it worked: it produced answers that felt right, and it was the most logically consistent moral system I could find.</p>\n<p class=\"MsoNormal\">But then I read <a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>.</p>\n<p class=\"MsoNormal\">The <a href=\"/lw/ya/normal_ending_last_tears_68/\">ending</a> didn\u2019t fit within my moral model: it was a scenario in which making people happy seemed <em>wrong. </em>Which raised the question: What\u2019s so great about happiness? If people don\u2019t want happiness, how can you call it good to force it on them? After all,<span>&nbsp; </span>happiness is just a pattern of neural excitation in the brain; it can\u2019t possibly be an <em>intrinsic</em> good, any more than the pattern that produces the thought \u201c2+2=4\u201d.</p>\n<p class=\"MsoNormal\">Well, people like being happy. Happiness is something they <em>want</em>. But it\u2019s by no means <em>all </em>they want: people also want mystery, wonder, excitement, and many other things \u2013 and so those things are also good, quite independent of their relation to the specific emotion \u2018happiness\u2019. If they also desire occasional sadness and pain, who am I to say they\u2019re wrong? It\u2019s not moral to make people happy against their desires \u2013 it\u2019s moral to give people what they <em>want. </em>(Voila, preference utilitarianism.)</p>\n<p class=\"MsoNormal\">But \u2013 that\u2019s not a real answer, is it?</p>\n<p class=\"MsoNormal\">If axiom \u2018happiness is good\u2019 didn\u2019t match my idea of morality, that meant I wasn\u2019t really constructing my morality around it. Replacing that axiom with \u2018preference fulfillment is good\u2019<span> </span>would make my logic match my feelings better, but it wouldn\u2019t give me a reason to have those feelings in the first place. So I had to ask the next question: <em>Why </em>is preference fulfillment good? What makes it \u201cgood\u201d to give other people what they want?<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">Why should we care about other people at all?</p>\n<p class=\"MsoNormal\">In other words, why be moral?</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\">Human feelings are a product of our evolutionary pressures. Emotions, the things that make us human, are there because they caused the genes that promoted them to become more prevalent in the ancestral environment. That includes the emotions surrounding moral issues: the things that seem so obviously right or wrong seem that way <em>because that feeling was adaptive, </em>not because of any intrinsic quality.</p>\n<p class=\"MsoNormal\">This makes it impossible to trust any moral system based on gut reaction, as most people\u2019s seem to be. Our feelings of right and wrong were engineered to maximize genetic replication, so why should we expect them to tap into objective realms of \u2018right\u2019 and \u2018wrong\u2019? And in fact, people\u2019s moral judgments tend to be suspiciously biased towards their own interests, though proclaimed with the strength of true belief.</p>\n<p class=\"MsoNormal\">More damningly, such moralities are incapable of coming up with a correct answer. One person can proclaim, say, homosexuality to be objectively right or wrong everywhere for everyone, with no justification except how they feel about it, and in the same breath say that it would still be wrong if they felt the other way. Another person, who <em>does </em>feel the other way, can deny it with equal force. And there\u2019s no conceivable way to decide who\u2019s right.</p>\n<p class=\"MsoNormal\">I became a utilitarian because it seemed to resolve many of the problems associated with purely intuitive morality \u2013 it was internally consistent, it relied on a simple premise, and it could provide its practitioners a standard of judgment for moral quandaries.</p>\n<p class=\"MsoNormal\">But <em>even utilitarianism is based on feeling. </em>This is especially true for hedonic utilitarianism, but little less for preference \u2013 we call people getting what they want \u2018good\u2019 because it <em>feels </em>good. It lights up our mirror neurons, triggers the altruistic instincts encoded into us by evolution. But evolution\u2019s purposes are not our own (we have no particular interest in our genes\u2019 replication) and so it makes no sense to adopt evolution\u2019s tools as our ultimate goals.</p>\n<p class=\"MsoNormal\">If you can\u2019t derive a moral code from evolution, then you can\u2019t derive it from emotion, the tool of evolution; if you can\u2019t derive morality from emotion, then you can\u2019t say that giving people what they want is objectively good because it feels good; if you can\u2019t do that, you can\u2019t be a utilitarian.</p>\n<p class=\"MsoNormal\">Emotions, of course, are not <em>bad. </em>Even knowing that love was designed to transmit genes, we still want love; we still find it worthwhile to pursue, even knowing that we were built to pursue it. But we can\u2019t hold up love as something <em>objectively </em>good, something that everyone should pursue \u2013 we don\u2019t condemn the asexual. In the same way, it\u2019s perfectly reasonable to help other people because it makes you feel good (to pursue <a href=\"http://wiki.lesswrong.com/wiki/Fuzzies\">warm fuzzies</a> for their own sake), but that emotional justification can\u2019t be used as the basis for a claim that <em>everyone </em>should help other people.</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\">So if we can\u2019t rely on feeling to justify morality, why have it at all?</p>\n<p class=\"MsoNormal\">Well, the obvious alternative is that it\u2019s practical. Societies populated by moral individuals \u2013 individuals who value the happiness of others \u2013 work better than those filled with selfish ones, because the individually selfless acts add up to greater utility for everyone. One only has to imagine a society populated by purely selfish individuals to see why pure selfishness wouldn\u2019t work.</p>\n<p class=\"MsoNormal\">This is a facile answer. First, if this is the case, why would morality extend outside of our societies? Why should we want to <a href=\"/lw/y5/the_babyeating_aliens_18/\">save the Babyeater children</a>?</p>\n<p class=\"MsoNormal\">But more importantly, how is it practical for <strong>you? </strong>There is <em>no </em>situation in which the best strategy is not being purely selfish. If reciprocal altruism makes you better off, then it\u2019s selfishly beneficial to be reciprocally altruistic; if you value warm fuzzies, then it\u2019s selfishly beneficial to get warm fuzzies; but <em>by definition</em>, true selflessness of the kind demanded by morality (like <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">buying utilons with money that could be spent on fuzzies</a>) decreases your utility \u2013 it <strong>loses. </strong>Even if you get a deep emotional reward from helping others, you\u2019re <em>strictly better off </em>being selfish.</p>\n<p class=\"MsoNormal\">So if feelings of \u2018right\u2019 and \u2018wrong\u2019 don\u2019t correspond to anything except what used to maximize inclusive genetic fitness, and having a moral code makes you <em>indisputably</em> <em>worse off</em>, why have one at all?</p>\n<p class=\"MsoNormal\">Once again: Why be moral?</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\"><strong id=\"The_Inconsistency_of_Consequentialism_\">The Inconsistency of Consequentialism <br></strong></p>\n<p class=\"MsoNormal\">Forget all that for a second. Stop questioning whether morality is justified and start using your moral judgment again.</p>\n<p class=\"MsoNormal\">Consider a consequentialist student being tempted to cheat on a test. Getting a good grade is important to him, and he can only do that if he cheats; cheating will make him significantly happier. His school trusts its students, so he\u2019s pretty sure he won\u2019t get caught, and the test isn\u2019t curved, so no one else will be hurt by him getting a good score. He decides to cheat, reasoning that it\u2019s at least morally neutral, if not a moral imperative \u2013 after all, his cheating will increase the world\u2019s utility.</p>\n<p class=\"MsoNormal\">Does this tell us cheating isn\u2019t a problem? No. If cheating became widespread, there would be consequences \u2013 tighter test security measures, suspicion of test grades, distrust of students, et cetera. Cheating just this once won\u2019t hurt anybody, but if cheating becomes expected, everyone is worse off.</p>\n<p class=\"MsoNormal\">But wait. If all the students are consequentialists, then they\u2019ll <em>all </em>decide to cheat, following the same logic as the first. And the teachers, anticipating this (it\u2019s an ethics class), will respond with draconian anti-cheating measures \u2013 leaving overall utility <em>lower </em>than if no one had been inclined to cheat at all.</p>\n<p class=\"MsoNormal\">Consequentialism called for each student to cheat because cheating would increase utility, but<em> the fact that </em>consequentialism called for each student to cheat decreased utility.</p>\n<p class=\"MsoNormal\">Imagine the opposite case: a class full of deontologists. Every student would be horrified at the idea of violating their duty for the sake of mere utility, and accordingly not a one of them would cheat. Counter-cheating methods would be completely unnecessary. Everyone would be better off.</p>\n<p class=\"MsoNormal\">In this situation, a deontologist class outcompetes a consequentialist one <em>in consequentialist terms. </em>The best way to maximize utility is to use a system of justification not based on maximizing utility. In such a situation, consequentialism calls for itself not to be believed. Consequentialism is inconsistent.</p>\n<p class=\"MsoNormal\">So what\u2019s a rational agent to do?</p>\n<p class=\"MsoNormal\">The apparent contradiction in this case results from thinking about beliefs and actions as though they were separate. Arriving at a belief is an action in itself, one which can have effects on utility. One cannot, therefore, arrive at a belief about utility without considering the effects on utility that holding that belief would have. If arriving at the belief \u201cactions are justified by their effect on utility\u201d doesn\u2019t maximize utility, then you shouldn\u2019t arrive at that belief.</p>\n<p class=\"MsoNormal\">However, the ultimate goal of maximizing utility cannot be questioned. Utility, after all, is only a word for \u201cwhat is wanted\u201d, so no agent can want to do anything except maximize utility. Moral agents include others' utility as equal to their own, but their goal is still to maximize utility.</p>\n<p class=\"MsoNormal\">Therefore the rule which should be followed is not \u201ctake the actions which maximize utility\u201d, but \u201carrive at the beliefs which maximize utility.\u201d</p>\n<p class=\"MsoNormal\">But there is an additional complication: when we arrive at beliefs by logic alone, we are effectively deciding not only for ourselves, but for all other rational agents, since the answer which is logically correct for us must also be logically correct for each of them. In this case, the correct answer is the one which maximizes utility \u2013 so our logic must take into account the fact that every other computation will produce the same answer. Therefore we can expand the rule to \u201carrive at the beliefs which would maximize utility if all other rational agents were to arrive at them (upon performing the same computation).\u201d</p>\n<p class=\"MsoNormal\">[To the best of my logical ability, this rule is recursive and therefore requires no further justification.]</p>\n<p class=\"MsoNormal\">This rule requires you to hold whatever beliefs will (conditional upon them being held) lead to the best results \u2013 even when the actions those beliefs produce don\u2019t, in themselves, maximize utility. In the case of the cheating student, the optimal belief is \u201cdon\u2019t cheat\u201d because that belief being held by all the students (and the teacher simulating the students\u2019 beliefs) produces the best results, <em>even though cheating would still increase utility for each individual student. </em>The applied morality becomes deontological, in the sense that actions are judged not by their effect on utility but by their adherence to the pre-set principle.</p>\n<p class=\"MsoNormal\">The upshot of this system is that you have to decide ahead of time whether an approach based on duty (that is, on every agent who considers the problem acting the way that would produce the best consequences if every agent who considers the problem were to act the same way) or on utility (individual computation of consequences) actually produces better consequences. And if you pick the deontological approach, you have to \u2018forget\u2019 your original goal \u2013 to commit to the rule even at the cost of actual consequences \u2013 because if it\u2019s rational to pursue the original goal, then it won\u2019t be achieved.</p>\n<p class=\"MsoNormal\">~</p>\n<p class=\"MsoNormal\"><strong id=\"The_Solution_to_Morality\">The Solution to Morality</strong></p>\n<p class=\"MsoNormal\">Let\u2019s return to the original question.</p>\n<p class=\"MsoNormal\">The primary effect of morality is that it causes individuals to value others\u2019 utility as an end in itself, and therefore to sacrifice their own utility for others. It\u2019s obvious that this is <em>very good </em>on a group scale: a society filled with selfless people, people who help others even when they don\u2019t expect to receive personal benefit, is far better off than one filled with people who do not \u2013 in a Prisoner\u2019s Dilemma writ large. To encourage that sort of cooperation (partially by design and partially by instinct), societies reward altruism and punish selflessness.</p>\n<p class=\"MsoNormal\">But why should you, personally, cooperate?</p>\n<p class=\"MsoNormal\">There are many, many times when you can do clearly better by selfishness than by altruism \u2013 by theft or deceit or just by not giving to charity. And why should we want to do otherwise? Our alruistic feelings are a mere artifact of evolution, like appendices and death, so why would we want to obey them?<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">Is there any reason, then, to be moral?</p>\n<p class=\"MsoNormal\">Yes.</p>\n<p class=\"MsoNormal\">Because that reasoning \u2013 that your own utility is maximized by selfishness \u2013 literally cannot be right. If it were right, then it would be the answer all rational beings would arrive at, and if all rational beings arrived at that answer, then none of them would cooperate and everyone would be worse off. If selfish utility maximizing is the correct answer for how to maximize selfish utility, selfish utility is not maximized. Therefore selfishness is the <em>wrong answer. </em>Each individual\u2019s utility is maximized only if they deliberately discard selfish utility as the thing to be maximized. And the way to do that is for each one to adopt a duty to maximize total utility, not only their own \u2013 <span>&nbsp;</span>to be moral.<em></em></p>\n<p class=\"MsoNormal\">And having chosen collective maximization over individual competition \u2013 duty over utility \u2013 you can no longer even consider your own benefit to be your goal. If you do so, holding morality as a means to selfishness\u2019s end, then everyone does so, and cooperation comes crashing down. You have to \u2018forget\u2019 the reason for having morality, and hold it because it's the right thing to do. You have to be moral even to the point of death.</p>\n<p class=\"MsoNormal\">Morality, then, is calculated blindness \u2013 a deliberate ignorance of our real ends, meant to achieve them more effectively. Selflessness for its own sake, for selfishness's sake.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">[This post lays down only the basic theoretic underpinnings of Deontological Decision Theory morality. My next post will focus on the practical applications of DDT in the human realm, and explain how it solves various moral/game-theoretic quandaries.]</p>\n<p>&nbsp;</p>", "sections": [{"title": "Asking the Question", "anchor": "Asking_the_Question", "level": 1}, {"title": "The Inconsistency of Consequentialism ", "anchor": "The_Inconsistency_of_Consequentialism_", "level": 1}, {"title": "The Solution to Morality", "anchor": "The_Solution_to_Morality", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "92 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "HWH46whexsoqR3yXk", "n5TqCuizyJDfAPjkr", "3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-10T23:55:28.976Z", "modifiedAt": null, "url": null, "title": "Spreadsheet-based tool for tracking time", "slug": "spreadsheet-based-tool-for-tracking-time", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v6Livv82vKyT78EEd/spreadsheet-based-tool-for-tracking-time", "pageUrlRelative": "/posts/v6Livv82vKyT78EEd/spreadsheet-based-tool-for-tracking-time", "linkUrl": "https://www.lesswrong.com/posts/v6Livv82vKyT78EEd/spreadsheet-based-tool-for-tracking-time", "postedAtFormatted": "Monday, January 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spreadsheet-based%20tool%20for%20tracking%20time&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpreadsheet-based%20tool%20for%20tracking%20time%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6Livv82vKyT78EEd%2Fspreadsheet-based-tool-for-tracking-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spreadsheet-based%20tool%20for%20tracking%20time%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6Livv82vKyT78EEd%2Fspreadsheet-based-tool-for-tracking-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6Livv82vKyT78EEd%2Fspreadsheet-based-tool-for-tracking-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 638, "htmlBody": "<p>I'm not sure what interest others will have in this, but I developed a spreadsheet-based time-tracking tool a bit back for a friend. He wanted to see where he was spending his time -- work, sleep, time with his wife, recreation, kids, etc. I used it myself for a while, too, and found it informative. I think the primary benefits of doing something like this are:</p>\n<ul>\n<li>Examining what you <em>think</em>&nbsp;you value compared to what you show, in practice, that you <em>actually </em>value in order to identify areas of improvement concerning time usage.</li>\n<li>Improve efficiency. Target an area, implement an reduction method, and see how your efforts track with time spent on that area.</li>\n<li>Self-policing. I found that just by tracking my time, I was able to reduce time spent in the \"wasted time\" category (stupid browsing, non-beneficial wiki-hopping, etc.), especially while at work being paid to do something else.</li>\n</ul>\n<p>The spreadsheet is available&nbsp;<a href=\"https://docs.google.com/uc?id=0BzQupOSnvw08ZThhYjE4MjgtNDM5OS00NzMxLTk5ZjItZmZkMTUwMWNmZWRm&amp;export=download&amp;hl=en\">HERE</a>,&nbsp;and consists&nbsp;of three tabs, one for time input and two for statistics.</p>\n<div><br /><strong>DailyInput:</strong>&nbsp;this tab is where data is input each day. I have default formulas set to make time entry as fast as possible. After entering a date, start and top time each day, you should only have to enter stop times for the rest of the day (trying and seeing for yourself is easier than explaining it. To override (e.g. if you have a gap or are on the next day), just manually enter a date/time in the cell, overwriting the formula.\n<p><br />The number of hours gets calculated and will work for multiple-day-spanning activities (like sleep).</p>\n</div>\n<div>You assign a category and can input a memo for each chunk of time. The categories are a drop down list created from a set of cells you define (in column H). I have an example list in the download above. Changing the cells containing categories automatically changes the drop-down list (I have it set to read up to 20 cells). You can manually fiddle with this via the Data -&gt; Validation dialog box. Sleep and work should remain the top two entries if you like the non-sleep/work calculations discussed below.</div>\n<div><br /><strong>Jan-Jun/Jul-Dec:</strong>&nbsp;the other two tabs&nbsp;(I just broke it up into 6 months to cut down on the number of rows per sheet)&nbsp;do some basic calculations on the daily input. You'll see that the total hours spent on a category appear as the data gets input, and a&nbsp;conditional setting highlights the current week's row in blue. At the bottom you will find the following for each category: \n<ul>\n<li>Hrs/day for the current week</li>\n<li>Amount of time spent on the category in the current week in the form of a %</li>\n<li>Another current week % calculation that discounts sleep/work (focuses on free time)</li>\n<li>Running average hrs/day (based on the elapsed time since the first entry on the DailyInput tab)</li>\n<li>Running average hrs/wk</li>\n<li>Running non-sleep/work %</li>\n</ul>\n<div>Anyway, just thought I'd share. Check it out, suggest improvements (or make the improvements yourself), post thoughts after trying to use it, etc.&nbsp;Please also share any known tools that are better than this. I tend to stick with open-source/free tools, and only know of&nbsp;<a href=\"http://rachota.sourceforge.net/en/index.html\">Rachota</a>&nbsp;from my searching and examination of <a href=\"http://en.wikipedia.org/wiki/Comparison_of_time_tracking_software\">time tracking software</a>. A spreadsheet solution does make it absolute cake to generate any graphs/charts of progress as desired. And you can add on tabs to track through multiple years quite easily.</div>\n<div><strong>Lastly, please do mention any experiments you've done like this in the past... </strong>What did you learn? Did predictions match actual time spent? Did tracking in and of itself curb time spent \"wasting time\"? Any other interesting observations/experiences? Is this exercise a valuable endeavor, perhaps even for a short amount of time and what additional benefits do you see coming out of it? I'd be interested in these answers.</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v6Livv82vKyT78EEd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 6.660971627359057e-07, "legacy": true, "legacyId": "4791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T00:50:07.009Z", "modifiedAt": null, "url": null, "title": "LINK: Bayesian statistics is so subversive, it's banned in China!", "slug": "link-bayesian-statistics-is-so-subversive-it-s-banned-in", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cmoFxv574h56fQP32/link-bayesian-statistics-is-so-subversive-it-s-banned-in", "pageUrlRelative": "/posts/cmoFxv574h56fQP32/link-bayesian-statistics-is-so-subversive-it-s-banned-in", "linkUrl": "https://www.lesswrong.com/posts/cmoFxv574h56fQP32/link-bayesian-statistics-is-so-subversive-it-s-banned-in", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Bayesian%20statistics%20is%20so%20subversive%2C%20it's%20banned%20in%20China!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Bayesian%20statistics%20is%20so%20subversive%2C%20it's%20banned%20in%20China!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcmoFxv574h56fQP32%2Flink-bayesian-statistics-is-so-subversive-it-s-banned-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Bayesian%20statistics%20is%20so%20subversive%2C%20it's%20banned%20in%20China!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcmoFxv574h56fQP32%2Flink-bayesian-statistics-is-so-subversive-it-s-banned-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcmoFxv574h56fQP32%2Flink-bayesian-statistics-is-so-subversive-it-s-banned-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2, "htmlBody": "<p><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2011/01/i_guess_they_fi.html\">Funny link</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZWRtQgXucwzAFZqNJ": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cmoFxv574h56fQP32", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 6.661111481186803e-07, "legacy": true, "legacyId": "4793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T02:53:59.018Z", "modifiedAt": null, "url": null, "title": "Less Wrong fanfiction suggestion", "slug": "less-wrong-fanfiction-suggestion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:35.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vlodermolt", "createdAt": "2011-01-11T02:41:40.518Z", "isAdmin": false, "displayName": "Vlodermolt"}, "userId": "Rws9xo8vFvBSodKfB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ba3d4aTfzWANMFdbw/less-wrong-fanfiction-suggestion", "pageUrlRelative": "/posts/Ba3d4aTfzWANMFdbw/less-wrong-fanfiction-suggestion", "linkUrl": "https://www.lesswrong.com/posts/Ba3d4aTfzWANMFdbw/less-wrong-fanfiction-suggestion", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20fanfiction%20suggestion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20fanfiction%20suggestion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBa3d4aTfzWANMFdbw%2Fless-wrong-fanfiction-suggestion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20fanfiction%20suggestion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBa3d4aTfzWANMFdbw%2Fless-wrong-fanfiction-suggestion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBa3d4aTfzWANMFdbw%2Fless-wrong-fanfiction-suggestion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>I've been enjoying reading Less Wrong fanfiction, and I wanted to suggest another fandom that should have at least one rationalist fanfiction: <em>Mage: The Awakening</em>. It's a roleplaying game about \"modern sorcery.\" My description can't really do it justice...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ba3d4aTfzWANMFdbw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 3, "extendedScore": null, "score": 6.661427297643437e-07, "legacy": true, "legacyId": "4797", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T03:59:28.947Z", "modifiedAt": null, "url": null, "title": "Non-trivial probability distributions for priors and Occam's razor", "slug": "non-trivial-probability-distributions-for-priors-and-occam-s", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:34.647Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZyH4Zh5jpCheGvHBM/non-trivial-probability-distributions-for-priors-and-occam-s", "pageUrlRelative": "/posts/ZyH4Zh5jpCheGvHBM/non-trivial-probability-distributions-for-priors-and-occam-s", "linkUrl": "https://www.lesswrong.com/posts/ZyH4Zh5jpCheGvHBM/non-trivial-probability-distributions-for-priors-and-occam-s", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-trivial%20probability%20distributions%20for%20priors%20and%20Occam's%20razor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-trivial%20probability%20distributions%20for%20priors%20and%20Occam's%20razor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyH4Zh5jpCheGvHBM%2Fnon-trivial-probability-distributions-for-priors-and-occam-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-trivial%20probability%20distributions%20for%20priors%20and%20Occam's%20razor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyH4Zh5jpCheGvHBM%2Fnon-trivial-probability-distributions-for-priors-and-occam-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyH4Zh5jpCheGvHBM%2Fnon-trivial-probability-distributions-for-priors-and-occam-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 251, "htmlBody": "<p>Assume we have a countable set of hypotheses described in some formal way with some prior distribution such that 1) our prior for each hypothesis is non-zero 2) our formal description system has only a finite number of hypotheses of any fixed length. Then, I claim that that under just this set of weak constraints, our hypotheses are under a condition that informally acts a lot like Occam's razor. In particular, let h(n) be the the probability mass assigned to \"a hypothesis with description at least exactly n is correct.\" (ETA: fixed from earlier statement) Then, as n goes to infinity, h(n) goes to zero. So, when one looks in the large-scale complicated hypotheses must have low probability. This suggests that one doesn't need any appeal to computability or anything similar to accept some form of Occam's razor. One only needs that one has a countable hypothesis space, no hypothesis has probability zero or one, and that has a non-stupid way of writing down hypotheses.</p>\n<p>A few questions: 1) Am I correct in seeing this as Occam-like or is this just an indication that I'm using too weak a notion of Occam's razor?&nbsp;&nbsp;&nbsp;</p>\n<p>2) Is this point novel? I'm not as familiar with the Bayesian literature as other people here so I'm hoping that someone can point out if this point has been made before.</p>\n<p>ETA: This was apparently a point made by Unknowns in an earlier thread which I totally forgot but probably read at the time. Thanks also for the other pointers.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZyH4Zh5jpCheGvHBM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.661596265153839e-07, "legacy": true, "legacyId": "4802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T04:53:36.930Z", "modifiedAt": null, "url": null, "title": "Does Evidential Decision Theory really fail Solomon's Problem?", "slug": "does-evidential-decision-theory-really-fail-solomon-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3z8xxvSib8LQuhFaa/does-evidential-decision-theory-really-fail-solomon-s", "pageUrlRelative": "/posts/3z8xxvSib8LQuhFaa/does-evidential-decision-theory-really-fail-solomon-s", "linkUrl": "https://www.lesswrong.com/posts/3z8xxvSib8LQuhFaa/does-evidential-decision-theory-really-fail-solomon-s", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20Evidential%20Decision%20Theory%20really%20fail%20Solomon's%20Problem%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20Evidential%20Decision%20Theory%20really%20fail%20Solomon's%20Problem%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3z8xxvSib8LQuhFaa%2Fdoes-evidential-decision-theory-really-fail-solomon-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20Evidential%20Decision%20Theory%20really%20fail%20Solomon's%20Problem%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3z8xxvSib8LQuhFaa%2Fdoes-evidential-decision-theory-really-fail-solomon-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3z8xxvSib8LQuhFaa%2Fdoes-evidential-decision-theory-really-fail-solomon-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 356, "htmlBody": "<p>Solomon's Problem and varients thereof are often cited as criticism of Evidential decision theory.</p>\n<p>For background, here's Solomon's Problem: King Solomon wants to sleep with another man's wife. However, he knows that uncharismatic leaders frequently sleep with other men's wives, and charismatic leaders almost never do. Furthermore, uncharismatic leaders are frequently overthrown, and charismatic leaders rarely are. On the other hand, sleeping with other men's wives does not cause leaders to be overthrown. Instead, high charisma decreases the chance that a leader will sleep with another man's wife and the chance that the leader will be overthrown separately. Not getting overthrown is more important to King Solomon than getting the chance to sleep with the other guy's wife.</p>\n<p>Causal decision theory holds that King Solomon can go ahead and sleep with the other man's wife because it will not directly cause him to be overthrown. Timeless decision theory holds that he can sleep with the woman because it will not cause his overthrow in any timeless sense either. Conventional wisdom holds that Evidential decision theory would have him refrain from her, because updating on the fact that he slept with her would suggest a higher probability that he will get overthrown.</p>\n<p>The problem with that interpretation is that it assumes that King Solomon only updates his probability distributions based on information about him that is accessible to others. He cannot change whether or not he would sleep with another man's wife given no other disincentives by refraining from doing so in response to other disincentives. The fact that he is faced with the dilemma already indicates that he would. Updating on this information, he knows that he is probably uncharismatic, and thus likely to get overthrown. Updating further on his decision after taking into account the factors guiding his decision will not change the correct probability distribution.</p>\n<p><em>This more complete view of Evidential decision theory is isomorphic to Timeless decision theory </em>(edit: shown to be false in comments)<em>.</em> I'm slightly perplexed as to why I have not seen it elsewhere. Is it flawed? Has it been mentioned elsewhere and I haven't noticed? If so, why isn't it so widely known?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb28f": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3z8xxvSib8LQuhFaa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "4803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T06:29:49.349Z", "modifiedAt": null, "url": null, "title": "Aguirre, Tegmark, Layzer \"Cosmological interpretation of quantum mechanics\"", "slug": "aguirre-tegmark-layzer-cosmological-interpretation-of", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zd2dvNuogbcRjstyt/aguirre-tegmark-layzer-cosmological-interpretation-of", "pageUrlRelative": "/posts/zd2dvNuogbcRjstyt/aguirre-tegmark-layzer-cosmological-interpretation-of", "linkUrl": "https://www.lesswrong.com/posts/zd2dvNuogbcRjstyt/aguirre-tegmark-layzer-cosmological-interpretation-of", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aguirre%2C%20Tegmark%2C%20Layzer%20%22Cosmological%20interpretation%20of%20quantum%20mechanics%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAguirre%2C%20Tegmark%2C%20Layzer%20%22Cosmological%20interpretation%20of%20quantum%20mechanics%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzd2dvNuogbcRjstyt%2Faguirre-tegmark-layzer-cosmological-interpretation-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aguirre%2C%20Tegmark%2C%20Layzer%20%22Cosmological%20interpretation%20of%20quantum%20mechanics%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzd2dvNuogbcRjstyt%2Faguirre-tegmark-layzer-cosmological-interpretation-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzd2dvNuogbcRjstyt%2Faguirre-tegmark-layzer-cosmological-interpretation-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>We haven't discussed it here yet, have we? Article from August 2010. <a href=\"http://arxiv.org/abs/1008.1066\">ArXiv link.</a>&nbsp;The upshot:</p>\n<blockquote>\n<p>We study the quantum measurement problem in the context of an infinite, statistically uniform space, as could be generated by eternal inlfation. It has recently been argued that when identical copies of a quantum measurement system exist, the standard projection operators and Born rule method for calculating probabilities must be supplemented by estimates of relative frequencies of observers. We argue that an infinite space actually renders the Born rule redundant, by physically realizing all outcomes of a quantum measurement in different regions, with relative frequencies given by the square of the wave function amplitudes. [...]&nbsp;Finally, the analysis suggests a &ldquo;cosmological interpretation&rdquo; of quantum theory in which the wave function describes the actual spatial collection of identical quantum systems, and quantum uncertainty is attributable to the observer&rsquo;s inability to self-locate in this collection.</p>\n</blockquote>\n<p>The notion that I am this huge equivalence class of almost-identical human beings (just similar enough to be running the same mind-computation at this moment), scattered over a spatially infinite universe, sounds very UDT-ish. Unfortunately I don't know enough physics to judge the paper properly. Please halp.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zd2dvNuogbcRjstyt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 20, "extendedScore": null, "score": 6.661981187702513e-07, "legacy": true, "legacyId": "4804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T07:05:28.177Z", "modifiedAt": null, "url": null, "title": "Back to the Basics of Rationality", "slug": "back-to-the-basics-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gfexKxsBDM6v2sCMo/back-to-the-basics-of-rationality", "pageUrlRelative": "/posts/gfexKxsBDM6v2sCMo/back-to-the-basics-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/gfexKxsBDM6v2sCMo/back-to-the-basics-of-rationality", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Back%20to%20the%20Basics%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABack%20to%20the%20Basics%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfexKxsBDM6v2sCMo%2Fback-to-the-basics-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Back%20to%20the%20Basics%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfexKxsBDM6v2sCMo%2Fback-to-the-basics-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfexKxsBDM6v2sCMo%2Fback-to-the-basics-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 848, "htmlBody": "<p><a href=\"http://commonsenseatheism.com/?p=12\">My deconversion from Christianity</a> had a large positive impact on my life. I suspect it had a small positive impact on the world, too. (For example, I no longer condemn gays or waste time and money on a relationship with an imaginary friend.) And my deconversion did not happen because I came to understand the Bayesian concept of evidence or Kolmogorov complexity or Solomonoff induction. I deconverted because I encountered some <em>very</em>&nbsp;basic arguments for non-belief, for example those in Dan Barker's <em><a href=\"http://www.amazon.com/Losing-Faith-Preacher-Atheist/dp/187773313X/\">Losing Faith in Faith</a></em>.</p>\n<p>Less Wrong has at least two goals. One goal is to <a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a>. If most people understood just the <em>basics</em> Occam's razor, what constitutes evidence and why, general trends of science, reductionism, and cognitive biases, the world would be greatly improved.&nbsp;Yudkowsky's upcoming books are aimed at this first goal of raising the sanity waterline. So are most of&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a>. So are learning-friendly posts like&nbsp;<a href=\"/lw/2un/references_resources_for_lesswrong/\">References &amp; Resources for LessWrong</a>.</p>\n<p>A second goal is to attract some of the best human brains on the planet and make progress on issues related to the Friendly AI problem, the problem with <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">the greatest leverage in the universe</a>. I have <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">suggested</a> that Less Wrong would make faster progress toward this goal if it worked more directly with the <a href=\"/r/discussion/lw/3n0/an_overview_of_formal_epistemology_links/\">community of scholars</a> already tackling the exact same problems.&nbsp;I don't personally work toward this goal because I'm not mathematically sophisticated enough to do so, but I'm glad others are!</p>\n<p>Still, I think the first goal could be more explicitly pursued. There are many people like myself and <a href=\"/r/discussion/lw/3n2/my_story_owning_ones_reasons/\">jwhendy</a>&nbsp;who can be massively impacted for the better not by coming to a realization about algorithmic learning theory, but by coming to understand the <em>basics</em>&nbsp;of rationality like <a href=\"/lw/oj/probability_is_in_the_mind/\">probability</a> and <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">the proper role of belief</a>&nbsp;and <a href=\"/lw/on/reductionism/\">reductionism</a>.<a id=\"more\"></a></p>\n<h4>Reasons for Less Wrong to devote more energy to the basics</h4>\n<ol>\n<li>Such efforts to spread the basics will have a short-term impact on more people than will efforts toward Friendly AI, and these impacted people will in turn impact others, hopefully for the better.</li>\n<li>Some LWers may feel they have little to contribute because they aren't masters of Solomonoff induction or algorithmic learning theory. But they&nbsp;<em>will</em>&nbsp;be able to contribute to raising the sanity waterline by spreading the <em>basics</em>&nbsp;of rationality.</li>\n<li>Providing more basic resources will attract a wider base of readers to Less Wrong, leading to (1) more new rationalists and (2) more donations to SIAI, for solving the Friendly AI problem.</li>\n<li>Even for experienced rationalists, it can be easy to forget the basics at times. Humans are not naturally rational, and revert to pre-rationality rather quickly without ongoing training and practice.</li>\n</ol>\n<h4>How to do it</h4>\n<p>Let me put some meat on this. What does more focus on the basics look like? Here are some ideas:</p>\n<ol>\n<li>The <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a> are great, but some people are too busy or lazy to read even <em>those</em>. Some of the sequences could be summarized into single posts crafted so as to have no prerequisites. These posts could be linked widely, and entered in relevant <a href=\"http://blogcarnival.com/bc/clist.html\">blog carnivals</a>.</li>\n<li>There is another huge community who will watch a 10-minute video, but will not read a short post. So the <a href=\"http://www.youtube.com/user/SingularityInstitute#g/c/21FBB71545AA4BA5\">YouTube lectures</a> are a great idea. But they could be improved. As of today, three of the videos show a presenter against a whiteboard. To make this work well requires lots of resources: (1) a good camera, (2) a shotgun or lavavlier microphone, (3) a teleprompter, and (4) an experienced and enthusiastic presenter. That's hard to do! But videos in the familiar <a href=\"http://www.youtube.com/user/SingularityInstitute#p/c/21FBB71545AA4BA5/3/mKJQdMTm4l0\">PowerPoint style</a> or the <a href=\"http://www.youtube.com/watch?v=dsFQ9kM1qDs\">Khan Academy style</a> are easier to do <em>well</em>. All it requires is some <a href=\"http://www.openoffice.org/product/impress.html\">free presentation software</a>, <a href=\"http://camstudio.org/\">free screen capture software</a> and a $70 high-quality USB mic like the <a href=\"http://www.bluemic.com/snowball/\">Blue Snowball</a>. This approach would also allow more people to participate in making videos on the basics of rationality.</li>\n<li>Sometimes, a basic concept of rationality will only \"click\" with somebody if presented in a certain way. Some will need a <em>story</em> that illustrates the concept. There are some of these on Less Wrong already, or in something like <a style=\"font-style: italic;\" href=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/Yudkowsky-Harry-Potter-and-the-Methods-of-Rationality-1-63.pdf\">Harry Potter and the Methods of Rationality</a>, but there could be more. For others, perhaps a <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">Cartoon Guide</a> to Bayes' Theorem or a Cartoon Guide to Reductionism would make it \"click.\" Those who are more ambitious might attempt to create an animation explaining some core rationalist concept, ala <a href=\"http://www.youtube.com/watch?v=C2VMO7pcWhg\">this visualization of special relativity</a>.</li>\n<li>Write \"Introduction to X\" or \"How to Use X\" posts.</li>\n<li>Keep developing <a href=\"http://wiki.lesswrong.com/wiki/LessWrong_Wiki\">the wiki</a>, obviously.</li>\n<li>Develop a <a href=\"/lw/3oj/a_lesswrong_rationality_workbook_idea/\">rationality workbook</a>.</li>\n</ol>\n<div>Finally, to avoid the \"<a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/2m5t\">somebody really ought to do something about this</a>\" syndrome, let me tell you what <em>I</em>&nbsp;am going to do on this front. I will be helping to raise the sanity waterline by first <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">raising the scholarship waterline</a>. More specifically, I will be summarizing some of my favorite university textbooks into 50-page blog posts. This will provide people with free, substantive, and easy-to-use tutorials on important subjects.</div>\n<div>I published the first such tutorial today, posted to my own site since I don't know yet whether 50-page posts are wanted on Less Wrong:&nbsp;<a style=\"font-weight: bold;\" href=\"http://commonsenseatheism.com/?p=13607\">Cognitive Science in One Lesson</a>.</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gfexKxsBDM6v2sCMo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 95, "baseScore": 115, "extendedScore": null, "score": 0.000206, "legacy": true, "legacyId": "4702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 115, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://commonsenseatheism.com/?p=12\">My deconversion from Christianity</a> had a large positive impact on my life. I suspect it had a small positive impact on the world, too. (For example, I no longer condemn gays or waste time and money on a relationship with an imaginary friend.) And my deconversion did not happen because I came to understand the Bayesian concept of evidence or Kolmogorov complexity or Solomonoff induction. I deconverted because I encountered some <em>very</em>&nbsp;basic arguments for non-belief, for example those in Dan Barker's <em><a href=\"http://www.amazon.com/Losing-Faith-Preacher-Atheist/dp/187773313X/\">Losing Faith in Faith</a></em>.</p>\n<p>Less Wrong has at least two goals. One goal is to <a href=\"/lw/1e/raising_the_sanity_waterline/\">raise the sanity waterline</a>. If most people understood just the <em>basics</em> Occam's razor, what constitutes evidence and why, general trends of science, reductionism, and cognitive biases, the world would be greatly improved.&nbsp;Yudkowsky's upcoming books are aimed at this first goal of raising the sanity waterline. So are most of&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a>. So are learning-friendly posts like&nbsp;<a href=\"/lw/2un/references_resources_for_lesswrong/\">References &amp; Resources for LessWrong</a>.</p>\n<p>A second goal is to attract some of the best human brains on the planet and make progress on issues related to the Friendly AI problem, the problem with <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">the greatest leverage in the universe</a>. I have <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">suggested</a> that Less Wrong would make faster progress toward this goal if it worked more directly with the <a href=\"/r/discussion/lw/3n0/an_overview_of_formal_epistemology_links/\">community of scholars</a> already tackling the exact same problems.&nbsp;I don't personally work toward this goal because I'm not mathematically sophisticated enough to do so, but I'm glad others are!</p>\n<p>Still, I think the first goal could be more explicitly pursued. There are many people like myself and <a href=\"/r/discussion/lw/3n2/my_story_owning_ones_reasons/\">jwhendy</a>&nbsp;who can be massively impacted for the better not by coming to a realization about algorithmic learning theory, but by coming to understand the <em>basics</em>&nbsp;of rationality like <a href=\"/lw/oj/probability_is_in_the_mind/\">probability</a> and <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">the proper role of belief</a>&nbsp;and <a href=\"/lw/on/reductionism/\">reductionism</a>.<a id=\"more\"></a></p>\n<h4 id=\"Reasons_for_Less_Wrong_to_devote_more_energy_to_the_basics\">Reasons for Less Wrong to devote more energy to the basics</h4>\n<ol>\n<li>Such efforts to spread the basics will have a short-term impact on more people than will efforts toward Friendly AI, and these impacted people will in turn impact others, hopefully for the better.</li>\n<li>Some LWers may feel they have little to contribute because they aren't masters of Solomonoff induction or algorithmic learning theory. But they&nbsp;<em>will</em>&nbsp;be able to contribute to raising the sanity waterline by spreading the <em>basics</em>&nbsp;of rationality.</li>\n<li>Providing more basic resources will attract a wider base of readers to Less Wrong, leading to (1) more new rationalists and (2) more donations to SIAI, for solving the Friendly AI problem.</li>\n<li>Even for experienced rationalists, it can be easy to forget the basics at times. Humans are not naturally rational, and revert to pre-rationality rather quickly without ongoing training and practice.</li>\n</ol>\n<h4 id=\"How_to_do_it\">How to do it</h4>\n<p>Let me put some meat on this. What does more focus on the basics look like? Here are some ideas:</p>\n<ol>\n<li>The <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a> are great, but some people are too busy or lazy to read even <em>those</em>. Some of the sequences could be summarized into single posts crafted so as to have no prerequisites. These posts could be linked widely, and entered in relevant <a href=\"http://blogcarnival.com/bc/clist.html\">blog carnivals</a>.</li>\n<li>There is another huge community who will watch a 10-minute video, but will not read a short post. So the <a href=\"http://www.youtube.com/user/SingularityInstitute#g/c/21FBB71545AA4BA5\">YouTube lectures</a> are a great idea. But they could be improved. As of today, three of the videos show a presenter against a whiteboard. To make this work well requires lots of resources: (1) a good camera, (2) a shotgun or lavavlier microphone, (3) a teleprompter, and (4) an experienced and enthusiastic presenter. That's hard to do! But videos in the familiar <a href=\"http://www.youtube.com/user/SingularityInstitute#p/c/21FBB71545AA4BA5/3/mKJQdMTm4l0\">PowerPoint style</a> or the <a href=\"http://www.youtube.com/watch?v=dsFQ9kM1qDs\">Khan Academy style</a> are easier to do <em>well</em>. All it requires is some <a href=\"http://www.openoffice.org/product/impress.html\">free presentation software</a>, <a href=\"http://camstudio.org/\">free screen capture software</a> and a $70 high-quality USB mic like the <a href=\"http://www.bluemic.com/snowball/\">Blue Snowball</a>. This approach would also allow more people to participate in making videos on the basics of rationality.</li>\n<li>Sometimes, a basic concept of rationality will only \"click\" with somebody if presented in a certain way. Some will need a <em>story</em> that illustrates the concept. There are some of these on Less Wrong already, or in something like <a style=\"font-style: italic;\" href=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/Yudkowsky-Harry-Potter-and-the-Methods-of-Rationality-1-63.pdf\">Harry Potter and the Methods of Rationality</a>, but there could be more. For others, perhaps a <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">Cartoon Guide</a> to Bayes' Theorem or a Cartoon Guide to Reductionism would make it \"click.\" Those who are more ambitious might attempt to create an animation explaining some core rationalist concept, ala <a href=\"http://www.youtube.com/watch?v=C2VMO7pcWhg\">this visualization of special relativity</a>.</li>\n<li>Write \"Introduction to X\" or \"How to Use X\" posts.</li>\n<li>Keep developing <a href=\"http://wiki.lesswrong.com/wiki/LessWrong_Wiki\">the wiki</a>, obviously.</li>\n<li>Develop a <a href=\"/lw/3oj/a_lesswrong_rationality_workbook_idea/\">rationality workbook</a>.</li>\n</ol>\n<div>Finally, to avoid the \"<a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/2m5t\">somebody really ought to do something about this</a>\" syndrome, let me tell you what <em>I</em>&nbsp;am going to do on this front. I will be helping to raise the sanity waterline by first <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">raising the scholarship waterline</a>. More specifically, I will be summarizing some of my favorite university textbooks into 50-page blog posts. This will provide people with free, substantive, and easy-to-use tutorials on important subjects.</div>\n<div>I published the first such tutorial today, posted to my own site since I don't know yet whether 50-page posts are wanted on Less Wrong:&nbsp;<a style=\"font-weight: bold;\" href=\"http://commonsenseatheism.com/?p=13607\">Cognitive Science in One Lesson</a>.</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Reasons for Less Wrong to devote more energy to the basics", "anchor": "Reasons_for_Less_Wrong_to_devote_more_energy_to_the_basics", "level": 1}, {"title": "How to do it", "anchor": "How_to_do_it", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "48 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf", "TNHQLZK5pHbxdnz4e", "64FdKLwmea8MCLWkE", "BXot7wxNbipyM749o", "g3RwsGofS6FE5nBsG", "f6ZLxEWaankRZ2Crv", "a7n8GdKiAZRX86T5A", "tPqQdLCuxanjhoaNs", "ALCnqX6Xx8bpFMZq3", "gPSCQC3WMeJnW9q4W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T09:42:11.277Z", "modifiedAt": null, "url": null, "title": "Are committed truthseekers lonelier?", "slug": "are-committed-truthseekers-lonelier", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.227Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "spencerth", "createdAt": "2009-12-24T20:47:39.774Z", "isAdmin": false, "displayName": "spencerth"}, "userId": "WJKXtHvzbD2hyjonr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ctZtKaGdz6DAoehJh/are-committed-truthseekers-lonelier", "pageUrlRelative": "/posts/ctZtKaGdz6DAoehJh/are-committed-truthseekers-lonelier", "linkUrl": "https://www.lesswrong.com/posts/ctZtKaGdz6DAoehJh/are-committed-truthseekers-lonelier", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20committed%20truthseekers%20lonelier%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20committed%20truthseekers%20lonelier%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctZtKaGdz6DAoehJh%2Fare-committed-truthseekers-lonelier%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20committed%20truthseekers%20lonelier%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctZtKaGdz6DAoehJh%2Fare-committed-truthseekers-lonelier", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctZtKaGdz6DAoehJh%2Fare-committed-truthseekers-lonelier", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 519, "htmlBody": "<p><span style=\"color: #444444; font-family: Tahoma, Verdana, Arial; font-size: 12px;\"> </span></p>\n<p style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 10px; padding-left: 0px; line-height: 20px; margin: 0px;\">People of a truthseeking bent - rationalists, unbiased scientists, inquisitive non-ideologues - are these types of people likely to be lonelier on average? Those who hold a particular set of positions, tastes, perspectives, worldviews, or preferences&nbsp;<em>to be</em>&nbsp;<em>part of a group,&nbsp;</em>rather than the other way around (being considered part of some group&nbsp;<em>because&nbsp;</em>they hold a particular set of positions) seem like they are at a significant advantage when it comes to the ability to make and keep friends, or at least find tolerant acquaintances compared to the typical truthseeker.</p>\n<p style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 10px; padding-left: 0px; line-height: 20px; margin: 0px;\">The truthseeker, by virtue of their ability to find, to a particular group they are currently part of or interacting with, uncomfortable truths, seems to put them in the unenviable position of, once they've found a particular uncomfortable truth, having to either keep quiet and have less-than-completely honest or more limited interactions, or speaking their mind and getting ostracized. Along with this, they're far less likely to engage in \"false flattery\", are more likely to focus on details and nuance (and hence be perceived negatively, due to an aversion to pedantry on certain subjects by some), far more likely to voice disagreement, &nbsp;and far more likely to wind up being a person to defend something considered objectionable by the group (they'd defend the proverbial idiot who says the sun will rise tomorrow - since it will, regardless of the fact that an idiot says it.)</p>\n<p style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 10px; padding-left: 0px; line-height: 20px; margin: 0px;\">The truthseeker may also confuse their interlocutors, due to what may be perceived as \"holding contradictory views\" (\"how can you think&nbsp;<strong>THAT&nbsp;</strong>if you also think&nbsp;<strong>THIS</strong>? You don't know what you're talking about\"); they may be accused of being a \"plant\" from the \"other side\" (\"if you think&nbsp;<em>that particular thing</em>, you must secretly be an X, so all that other stuff you said that I agree with must be a lie\"); they may be thought of as a troll or prankster (\"you're just saying that thing I consider objectionable to get a negative reaction out of me, but I know you really agree with me on that the way you [honestly] agree with me on all that other stuff\"), or that you're playing devil's advocate for its own sake. These things all happen, but due to the (current) inability to know for sure another's motives, it may be easy to confuse the truthseeker with the idiot, the confused/self-contradictory, the plant, the troll, or the advocate, even though the truthseeker's ideas and motives have nothing to do with any of those.&nbsp;</p>\n<p style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 10px; padding-left: 0px; line-height: 20px; margin: 0px;\">Based on limited observations coupled with a little speculation, I'd say that yes, truthseekers are likely to be lonelier on average. They're likely much rarer, so finding other committed truthseekers would be tough, and there's no guarantee they'd even like each other (for non-truthseeking-related reasons - like not liking the same subjective things (music, fashion, food, etc.)) My personal experience says that one can be professionally and personal well respected, considered extremely friendly, and still have no \"real\" friends; truthseekers are easy to <em>love</em>, but considered difficult to <em>like</em>.</p>\n<p style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 10px; padding-left: 0px; line-height: 20px; margin: 0px;\">Perhaps a simpler reason (in the typical case) is that the truthseeker is simply perceived by your typical person as a whole lot&nbsp;<strong>less fun</strong>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ctZtKaGdz6DAoehJh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 6.662473767505146e-07, "legacy": true, "legacyId": "4805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T11:47:51.511Z", "modifiedAt": null, "url": null, "title": "Alternate Sleep Schedules", "slug": "alternate-sleep-schedules", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "yk5CemN7ygW6D2Jm9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tKJGviGC8FEPKT2fa/alternate-sleep-schedules", "pageUrlRelative": "/posts/tKJGviGC8FEPKT2fa/alternate-sleep-schedules", "linkUrl": "https://www.lesswrong.com/posts/tKJGviGC8FEPKT2fa/alternate-sleep-schedules", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alternate%20Sleep%20Schedules&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlternate%20Sleep%20Schedules%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKJGviGC8FEPKT2fa%2Falternate-sleep-schedules%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alternate%20Sleep%20Schedules%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKJGviGC8FEPKT2fa%2Falternate-sleep-schedules", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKJGviGC8FEPKT2fa%2Falternate-sleep-schedules", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>My friend and I are starting the Uberman sleep schedule (six 20-minute naps spread evenly throughout each day) tonight. Have other lesswrongians experimented with alternate sleep schedules? Are any of you qualified medical experts who can give input or advice? Success stories and failure stories would both be appreciated, and I'll keep you guys posted on our progress.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tKJGviGC8FEPKT2fa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "4807", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T17:32:13.598Z", "modifiedAt": null, "url": null, "title": "Convincing ET of our rationality", "slug": "convincing-et-of-our-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.195Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheRev", "createdAt": "2011-01-07T07:27:04.770Z", "isAdmin": false, "displayName": "TheRev"}, "userId": "Qf7dawgKSDMJ3BjkN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7PmQLaMxYykZrHfas/convincing-et-of-our-rationality", "pageUrlRelative": "/posts/7PmQLaMxYykZrHfas/convincing-et-of-our-rationality", "linkUrl": "https://www.lesswrong.com/posts/7PmQLaMxYykZrHfas/convincing-et-of-our-rationality", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Convincing%20ET%20of%20our%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConvincing%20ET%20of%20our%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PmQLaMxYykZrHfas%2Fconvincing-et-of-our-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Convincing%20ET%20of%20our%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PmQLaMxYykZrHfas%2Fconvincing-et-of-our-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PmQLaMxYykZrHfas%2Fconvincing-et-of-our-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>Allow me to propose a thought experiment.&nbsp; Suppose you, and you alone, were to make first contact with an alien species.&nbsp; Since your survival and the survival of the entire human race may depend on the extraterrestrials recognizing you as a member of a rational species, how would you convey your knowledge of mathematics, logic, and the scientific method to them using only your personal knowledge and whatever tools you might reasonably have on your person on an average day?</p>\n<p>When I thought of this question, the two methods that immediately came to mind were the Pythagorean Theorem and prime number sequences.&nbsp; For instance, I could draw a rough right triangle and label one side with three dots, the other with four, and the hypotenuse with five.&nbsp; However, I realized that these are fairly primitive maths.&nbsp; After all, the ancient Greeks knew of them, and yet had no concept of the scientific method.&nbsp; Would these likely be sufficient, and if not what would be?&nbsp; Could you make a rough sketch of the first few atoms on the periodic table or other such universal phenomena so that it would be generally recognizable? Could you convey a proof of rationality in a manner that even aliens who cannot hear human vocalizations, or see in a completely different part of the EM spectrum?&nbsp; Is it even in principle possible to express rationality without a common linguistic grounding?</p>\n<p>In other words, what is the most rational thought you could convey without the benefit of common language, culture, psychology, or biology, and how would you do it?</p>\n<p>Bonus point: Could you convey Bayes' theorem to said ET?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7PmQLaMxYykZrHfas", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 6.6636776413431e-07, "legacy": true, "legacyId": "4808", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-11T22:58:24.725Z", "modifiedAt": null, "url": null, "title": "NPR show All Things Considered on the Singularity and SIAI", "slug": "npr-show-all-things-considered-on-the-singularity-and-siai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "arundelo", "createdAt": "2009-03-01T18:19:40.865Z", "isAdmin": false, "displayName": "arundelo"}, "userId": "nC4NpcrnXPWe4P3td", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yrdTzenJ6oMJ4h9zw/npr-show-all-things-considered-on-the-singularity-and-siai", "pageUrlRelative": "/posts/yrdTzenJ6oMJ4h9zw/npr-show-all-things-considered-on-the-singularity-and-siai", "linkUrl": "https://www.lesswrong.com/posts/yrdTzenJ6oMJ4h9zw/npr-show-all-things-considered-on-the-singularity-and-siai", "postedAtFormatted": "Tuesday, January 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NPR%20show%20All%20Things%20Considered%20on%20the%20Singularity%20and%20SIAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANPR%20show%20All%20Things%20Considered%20on%20the%20Singularity%20and%20SIAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrdTzenJ6oMJ4h9zw%2Fnpr-show-all-things-considered-on-the-singularity-and-siai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NPR%20show%20All%20Things%20Considered%20on%20the%20Singularity%20and%20SIAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrdTzenJ6oMJ4h9zw%2Fnpr-show-all-things-considered-on-the-singularity-and-siai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrdTzenJ6oMJ4h9zw%2Fnpr-show-all-things-considered-on-the-singularity-and-siai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>The NPR show All Things Considered did a short story on the Singularity, including interviews with Eliezer Yudkowsky and others involved with SIAI:<br /><br />http://www.npr.org/2011/01/11/132840775/The-Singularity-Humanitys-Last-Invention</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yrdTzenJ6oMJ4h9zw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 32, "extendedScore": null, "score": 6.664513300196557e-07, "legacy": true, "legacyId": "4811", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T02:37:33.860Z", "modifiedAt": null, "url": null, "title": "VIDEO: The Problem With Anecdotes", "slug": "video-the-problem-with-anecdotes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:18.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PDFKkm6tqKvaMzvj5/video-the-problem-with-anecdotes", "pageUrlRelative": "/posts/PDFKkm6tqKvaMzvj5/video-the-problem-with-anecdotes", "linkUrl": "https://www.lesswrong.com/posts/PDFKkm6tqKvaMzvj5/video-the-problem-with-anecdotes", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20VIDEO%3A%20The%20Problem%20With%20Anecdotes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVIDEO%3A%20The%20Problem%20With%20Anecdotes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDFKkm6tqKvaMzvj5%2Fvideo-the-problem-with-anecdotes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=VIDEO%3A%20The%20Problem%20With%20Anecdotes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDFKkm6tqKvaMzvj5%2Fvideo-the-problem-with-anecdotes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDFKkm6tqKvaMzvj5%2Fvideo-the-problem-with-anecdotes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>Inspired by some of the comments in <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">Back To The Basics</a> I thought it might be interesting to see whether and how video embedding works in the discussion area.&nbsp; The experiment is intended to function technically to see if this is possible, but also socially to see if the reaction is good and the comments are high quality.</p>\n<p>When trying to set up this video I clicked the \"HTML\" button among the text tools (to the right of \"Insert/edit image\" and to the left of \"Insert horizontal ruler\".&nbsp; In the text box that popped up, I pasted the html that I had already found on youtube by pressing the \"Embed\" button for a video that seemed thematically appropriate.</p>\n<p>Assuming that this technically succeeds, we'll all have an some anecdotal evidence about whether videos are a positive contribution to LW.</p>\n<p>One thing that might be useful to mention is that QualiaSoup has produced about <a href=\"http://www.youtube.com/user/QualiaSoup#g/u\">28 videos</a> of which I picked one that seemed particularly relevant to this forum and that I watched before posting.&nbsp; I didn't really learn anything from this but I also didn't notice anything glaringly wrong with it.&nbsp; If this experiment is good enough to repeat we might want to think about the standards we'd expect video posts to live up to.&nbsp; Not sure what those should be, but it seemed like a good idea to mention that conversation around this might be useful.</p>\n<p>Without further ado, \"The Problem With Anecdotes\"...</p>\n<p>&nbsp;</p>\n<p>\n<object width=\"480\" height=\"385\" data=\"http://www.youtube.com/v/NPqerbz8KDc?fs=1&amp;hl=en_US\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/NPqerbz8KDc?fs=1&amp;hl=en_US\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PDFKkm6tqKvaMzvj5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 6.665074853852553e-07, "legacy": true, "legacyId": "4815", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gfexKxsBDM6v2sCMo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T03:24:35.656Z", "modifiedAt": null, "url": null, "title": "Branches of rationality ", "slug": "branches-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:41.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xvAkpCSdqgtYhEceo/branches-of-rationality", "pageUrlRelative": "/posts/xvAkpCSdqgtYhEceo/branches-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/xvAkpCSdqgtYhEceo/branches-of-rationality", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Branches%20of%20rationality%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABranches%20of%20rationality%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxvAkpCSdqgtYhEceo%2Fbranches-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Branches%20of%20rationality%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxvAkpCSdqgtYhEceo%2Fbranches-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxvAkpCSdqgtYhEceo%2Fbranches-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1378, "htmlBody": "<p>Related to: <a href=\"/lw/c4/go_forth_and_create_the_art/\">Go forth and create the art!</a>, <a href=\"/lw/2c/a_sense_that_more_is_possible/\">A sense that more is possible</a>.</p>\n<p>If you talk to any skilled practitioner of an art, they have a sense of the depths beyond their present skill level. &nbsp;This sense is important. &nbsp;To create an art, or to learn one, one must have a sense of the goal.</p>\n<p>By contrast, when I chat with many at Less Wrong meet-ups, I often hear a sense that mastering the sequences will take one most of the way to \"rationality\", and that the main thing to do, after reading the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a>, is to go and share the info with others. &nbsp;I would therefore like to sketch the larger thing that I hope our rationality can become. &nbsp;I have found this picture useful for improving my own rationality; I hope you may find it useful too.</p>\n<p>To avoid semantic disputes, I tried to generate my picture of \"rationality\" by asking <a href=\"/lw/nu/taboo_your_words/\">not</a> \"What should 'rationality' be?\" but \"What is the total set of simple, domain-general hacks that can help humans understand the most important things, and achieve our goals?\" &nbsp;or \"What simple tricks can help turn humans --&nbsp;haphazard evolutionary amalgams that we are -- into coherent agents?\"</p>\n<h2>The branches</h2>\n<p><span style=\"font-size: small;\"><strong></strong></span>The larger \"rationality\" I have in mind would include some arts that are well-taught on Less Wrong, others that that don't&nbsp;exist yet at all, and others have been developed by outside communities from which we could profitably steal.</p>\n<p>Specifically, a more complete art of rationality might teach the following arts:</p>\n<p><strong>1. &nbsp;Having beliefs: </strong>the art of having one's near-mode anticipations and far-mode symbols work together, with the intent of predicting the outside world. &nbsp;(The <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>, especially&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious answers to mysterious questions</a>, currently help tremendously with these skills.)</p>\n<p><strong>2. &nbsp;Making your beliefs less buggy -- about distant or abstract subjects. &nbsp;</strong>This art aims to let humans talk about abstract domains in which the data&nbsp;<a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">doesn&rsquo;t</a>&nbsp;hit you upside the head -- such as religion, politics, the course of the future, or the efficacy of cryonics -- without the conversation turning immediately into nonsense. &nbsp;(The Sequences, and other discussions of common biases and of the mathematics of evidence, are helpful here as well.)</p>\n<p><strong>3. Making your beliefs less buggy -- about yourself. &nbsp;</strong>Absent training, our models of ourselves are about as nonsense-prone as our models of distant or abstract subjects. &nbsp;We often have confident, false models of what emotions we are experiencing, why we are taking a given action, how our skills and traits compare to those around us, how long a given project will take, what will and won&rsquo;t make us happy, and what our goals are. &nbsp;This holds even for many who've studied the Sequences and who are reasonably decent on abstract topics; other skills are needed.[1]<a id=\"more\"></a></p>\n<p><strong>4. &nbsp;Chasing the most important info: </strong>the art of noticing what knowledge would actually help you.&nbsp;A master of this art would continually ask themselves: \"What do I most want to accomplish? &nbsp;What do I need to know, in order to achieve that thing?\". They would have large amounts of cached knowledge about how to make money, how to be happy, how to learn deeply, how to effectively improve the world, and how to achieve other common goals. &nbsp;They would continually ask themselves where telling details could be found, and they would become interested in any domain that could help them.[2]</p>\n<p>As with the art of self-knowledge, Less Wrong has barely started on this one.</p>\n<p><strong>5. &nbsp;Benefiting from everyone else's knowledge. </strong>&nbsp;This branch of rationality would teach us:</p>\n<ul>\n<li>Which sorts of experts, and which sorts of published studies, are what sorts of trustworthy; and</li>\n<li>How to do an effective literature search, read effectively, interview experts effectively, or otherwise locate the info we need.</li>\n</ul>\n<p>Less Wrong and Overcoming Bias have covered pieces of this, but I'd bet there's good knowledge to be found elsewhere.</p>\n<p><strong>6. &nbsp;The art of problem-solving: </strong>how to brainstorm up a solution once you already know what the question is.&nbsp;&nbsp;Eliezer has described parts of such an art for philosophy problems[3], and Luke Grecki&nbsp;<a href=\"/r/discussion/lw/32f/problem_solving_via_polya/\">summarized</a>&nbsp;Polya's \"How to Solve It\" for math problems, but huge gaps remain.</p>\n<p><strong>7. &nbsp;Having goals. &nbsp;</strong>In our natural state, humans do not have goals&nbsp;<a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">in any very useful sense</a>. &nbsp;This art would change that, e.g. by such techniques as writing down and operationalizing one's goals, measuring progress, making plans, and working through one's emotional responses until one is able, as a whole person, to fully choose a particular course.</p>\n<p>Much help with goal-achievement can be found in the self-help and business communities; it would be neat to see that knowledge fused with Less Wrong.[4]</p>\n<p><strong>8. &nbsp;Making your goals less buggy. &nbsp;</strong>Even insofar as we do act on coherent goals, our goals are often \"buggy\" in the sense of carrying us in directions we will predictably regret. Some skills that can help include:</p>\n<ul>\n<li>Skill in noticing and naming your emotions and motivations (art #2 above);</li>\n<li>Understanding what ethics is, and what you are. &nbsp;Sorting out religion, free will, <a href=\"/lw/kx/fake_selfishness/\">fake utility functions</a>, social signaling patterns, and other topics that disorient many.</li>\n<li>Being on the look-out for <a href=\"/lw/le/lost_purposes/\">lost purposes</a>, <a href=\"/lw/4e/cached_selves/\">cached goals or values</a>, defensiveness, <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">wire-heading patterns</a>, and other tricks your brain tends to play on you.</li>\n<li>Being aware of, and accepting, as large a part of yourself as possible.</li>\n</ul>\n<h2>Parts of a single discipline</h2>\n<p><span style=\"font-size: small;\"><strong></strong></span>Geometry, algebra, and arithmetic are all &ldquo;branches of mathematics&rdquo;, rather than stand-alone arts. &nbsp;They are all &ldquo;branches of mathematics&rdquo; because they build on a common set of thinking skills, and because skill in each of these branches can boost one&rsquo;s problem-solving ability in other branches of mathematics.</p>\n<p>My impression is that the above arts are all branches of a single discipline (\"rationality\") in roughly the same sense in which arithmetic, algebra, etc. are branches of mathematics. &nbsp;For one thing, all of these arts have a common foundation: they all involve noticing what one's brain is doing, and asking if those mental habits are serving one's purpose or if some other habits would work better.</p>\n<p>For another thing, skill at many of the above arts can help with many of the others. For example,&nbsp;knowing your motivations can help you debug your reasoning, since you&rsquo;re much more likely to find the truth when you want the truth. &nbsp;Asking &ldquo;what would I expect to see, if my theory was true? if it was false?&rdquo; is useful for both modeling the future and modeling yourself. &nbsp;Acquiring coherent goals&nbsp;makes it easier to wholeheartedly debug ones beliefs, without needing to flinch away. &nbsp;And so on.</p>\n<p>It therefore seems plausible that jointly studying the entire above discipline (including whatever branches I left out) would give one a much larger cross-domain power boost, and higher performance in <em>each</em> of the above arts, than one gets from only learning the Less Wrong sequences.</p>\n<p>&nbsp;</p>\n<hr />\n<p>[1] That is: Bayes' theorem and other rules of reasoning do work for inferring knowledge about oneself. &nbsp;But Less Wrong hasn't walked us through the basics of applying them to self-modeling, such as noting that one must infer one's motives through a process of ordinary inference (&ldquo;What actions would I expect to see if I was trying to cooperate? &nbsp;What actions would I expect to see if I was instead trying to vent anger?\") and not by consulting one's verbal self-model.&nbsp;It also has said very little about how to gather data about oneself, how to reduce ones biases on the subject, etc. (although Alicorn's <a href=\"/lw/2aw/seven_shiny_stories/\">Luminosity sequence</a> deserves mention).</p>\n<p>[2] Michael Vassar calls this skill &ldquo;lightness of curiosity&rdquo;, by analogy to the skill &ldquo;lightness of beliefs&rdquo; from Eliezer&rsquo;s <a href=\"http://yudkowsky.net/rational/virtues\">12 virtues of rationality</a>. &nbsp;The idea here is that a good rationalist should have a curiosity that moves immediately as they learn what information can help them, much as an good rationalist should have beliefs that move immediately as they learn which way the evidence points. &nbsp;Just as a good rationalist <a href=\"/lw/hs/think_like_reality/\">should not call reality \"surprising\"</a>, so also a good rationalist should not call useful domains \"boring\".</p>\n<p>[3] I.e., Eliezer's posts describe parts of an art for cases such as \"free will\" in which the initial question is confused, and&nbsp;must be <a href=\"/lw/of/dissolving_the_question/\">dissolved</a> rather than answered. &nbsp;He also notes the virtue of <a href=\"/lw/ui/use_the_try_harder_luke/\">sustained effort</a>.</p>\n<div>\n<p>[4] My favorite exceptions are Eliezer's post <a href=\"/lw/nb/something_to_protect/\">Something to protect</a> and Alicorn's <a href=\"/lw/2aw/seven_shiny_stories/\">City of Lights technique</a>. &nbsp;If you're looking for good reading offsite on how to have coherent goals, I'd second Patri's <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l50\">recommendation</a> of <a href=\"http://www.amazon.com/That-Frog-Great-Ways-Procrastinating/dp/1583762027\">Brian Tracy's</a> <a href=\"http://www.amazon.com/Goals-Everything-Faster-Thought-Possible/dp/1605094110/ref=pd_sim_b_2\">books</a>.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xvAkpCSdqgtYhEceo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 86, "baseScore": 105, "extendedScore": null, "score": 0.00019, "legacy": true, "legacyId": "4809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 105, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"/lw/c4/go_forth_and_create_the_art/\">Go forth and create the art!</a>, <a href=\"/lw/2c/a_sense_that_more_is_possible/\">A sense that more is possible</a>.</p>\n<p>If you talk to any skilled practitioner of an art, they have a sense of the depths beyond their present skill level. &nbsp;This sense is important. &nbsp;To create an art, or to learn one, one must have a sense of the goal.</p>\n<p>By contrast, when I chat with many at Less Wrong meet-ups, I often hear a sense that mastering the sequences will take one most of the way to \"rationality\", and that the main thing to do, after reading the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a>, is to go and share the info with others. &nbsp;I would therefore like to sketch the larger thing that I hope our rationality can become. &nbsp;I have found this picture useful for improving my own rationality; I hope you may find it useful too.</p>\n<p>To avoid semantic disputes, I tried to generate my picture of \"rationality\" by asking <a href=\"/lw/nu/taboo_your_words/\">not</a> \"What should 'rationality' be?\" but \"What is the total set of simple, domain-general hacks that can help humans understand the most important things, and achieve our goals?\" &nbsp;or \"What simple tricks can help turn humans --&nbsp;haphazard evolutionary amalgams that we are -- into coherent agents?\"</p>\n<h2 id=\"The_branches\">The branches</h2>\n<p><span style=\"font-size: small;\"><strong></strong></span>The larger \"rationality\" I have in mind would include some arts that are well-taught on Less Wrong, others that that don't&nbsp;exist yet at all, and others have been developed by outside communities from which we could profitably steal.</p>\n<p>Specifically, a more complete art of rationality might teach the following arts:</p>\n<p><strong>1. &nbsp;Having beliefs: </strong>the art of having one's near-mode anticipations and far-mode symbols work together, with the intent of predicting the outside world. &nbsp;(The <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>, especially&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious answers to mysterious questions</a>, currently help tremendously with these skills.)</p>\n<p><strong>2. &nbsp;Making your beliefs less buggy -- about distant or abstract subjects. &nbsp;</strong>This art aims to let humans talk about abstract domains in which the data&nbsp;<a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">doesn\u2019t</a>&nbsp;hit you upside the head -- such as religion, politics, the course of the future, or the efficacy of cryonics -- without the conversation turning immediately into nonsense. &nbsp;(The Sequences, and other discussions of common biases and of the mathematics of evidence, are helpful here as well.)</p>\n<p><strong>3. Making your beliefs less buggy -- about yourself. &nbsp;</strong>Absent training, our models of ourselves are about as nonsense-prone as our models of distant or abstract subjects. &nbsp;We often have confident, false models of what emotions we are experiencing, why we are taking a given action, how our skills and traits compare to those around us, how long a given project will take, what will and won\u2019t make us happy, and what our goals are. &nbsp;This holds even for many who've studied the Sequences and who are reasonably decent on abstract topics; other skills are needed.[1]<a id=\"more\"></a></p>\n<p><strong>4. &nbsp;Chasing the most important info: </strong>the art of noticing what knowledge would actually help you.&nbsp;A master of this art would continually ask themselves: \"What do I most want to accomplish? &nbsp;What do I need to know, in order to achieve that thing?\". They would have large amounts of cached knowledge about how to make money, how to be happy, how to learn deeply, how to effectively improve the world, and how to achieve other common goals. &nbsp;They would continually ask themselves where telling details could be found, and they would become interested in any domain that could help them.[2]</p>\n<p>As with the art of self-knowledge, Less Wrong has barely started on this one.</p>\n<p><strong>5. &nbsp;Benefiting from everyone else's knowledge. </strong>&nbsp;This branch of rationality would teach us:</p>\n<ul>\n<li>Which sorts of experts, and which sorts of published studies, are what sorts of trustworthy; and</li>\n<li>How to do an effective literature search, read effectively, interview experts effectively, or otherwise locate the info we need.</li>\n</ul>\n<p>Less Wrong and Overcoming Bias have covered pieces of this, but I'd bet there's good knowledge to be found elsewhere.</p>\n<p><strong>6. &nbsp;The art of problem-solving: </strong>how to brainstorm up a solution once you already know what the question is.&nbsp;&nbsp;Eliezer has described parts of such an art for philosophy problems[3], and Luke Grecki&nbsp;<a href=\"/r/discussion/lw/32f/problem_solving_via_polya/\">summarized</a>&nbsp;Polya's \"How to Solve It\" for math problems, but huge gaps remain.</p>\n<p><strong>7. &nbsp;Having goals. &nbsp;</strong>In our natural state, humans do not have goals&nbsp;<a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">in any very useful sense</a>. &nbsp;This art would change that, e.g. by such techniques as writing down and operationalizing one's goals, measuring progress, making plans, and working through one's emotional responses until one is able, as a whole person, to fully choose a particular course.</p>\n<p>Much help with goal-achievement can be found in the self-help and business communities; it would be neat to see that knowledge fused with Less Wrong.[4]</p>\n<p><strong>8. &nbsp;Making your goals less buggy. &nbsp;</strong>Even insofar as we do act on coherent goals, our goals are often \"buggy\" in the sense of carrying us in directions we will predictably regret. Some skills that can help include:</p>\n<ul>\n<li>Skill in noticing and naming your emotions and motivations (art #2 above);</li>\n<li>Understanding what ethics is, and what you are. &nbsp;Sorting out religion, free will, <a href=\"/lw/kx/fake_selfishness/\">fake utility functions</a>, social signaling patterns, and other topics that disorient many.</li>\n<li>Being on the look-out for <a href=\"/lw/le/lost_purposes/\">lost purposes</a>, <a href=\"/lw/4e/cached_selves/\">cached goals or values</a>, defensiveness, <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">wire-heading patterns</a>, and other tricks your brain tends to play on you.</li>\n<li>Being aware of, and accepting, as large a part of yourself as possible.</li>\n</ul>\n<h2 id=\"Parts_of_a_single_discipline\">Parts of a single discipline</h2>\n<p><span style=\"font-size: small;\"><strong></strong></span>Geometry, algebra, and arithmetic are all \u201cbranches of mathematics\u201d, rather than stand-alone arts. &nbsp;They are all \u201cbranches of mathematics\u201d because they build on a common set of thinking skills, and because skill in each of these branches can boost one\u2019s problem-solving ability in other branches of mathematics.</p>\n<p>My impression is that the above arts are all branches of a single discipline (\"rationality\") in roughly the same sense in which arithmetic, algebra, etc. are branches of mathematics. &nbsp;For one thing, all of these arts have a common foundation: they all involve noticing what one's brain is doing, and asking if those mental habits are serving one's purpose or if some other habits would work better.</p>\n<p>For another thing, skill at many of the above arts can help with many of the others. For example,&nbsp;knowing your motivations can help you debug your reasoning, since you\u2019re much more likely to find the truth when you want the truth. &nbsp;Asking \u201cwhat would I expect to see, if my theory was true? if it was false?\u201d is useful for both modeling the future and modeling yourself. &nbsp;Acquiring coherent goals&nbsp;makes it easier to wholeheartedly debug ones beliefs, without needing to flinch away. &nbsp;And so on.</p>\n<p>It therefore seems plausible that jointly studying the entire above discipline (including whatever branches I left out) would give one a much larger cross-domain power boost, and higher performance in <em>each</em> of the above arts, than one gets from only learning the Less Wrong sequences.</p>\n<p>&nbsp;</p>\n<hr>\n<p>[1] That is: Bayes' theorem and other rules of reasoning do work for inferring knowledge about oneself. &nbsp;But Less Wrong hasn't walked us through the basics of applying them to self-modeling, such as noting that one must infer one's motives through a process of ordinary inference (\u201cWhat actions would I expect to see if I was trying to cooperate? &nbsp;What actions would I expect to see if I was instead trying to vent anger?\") and not by consulting one's verbal self-model.&nbsp;It also has said very little about how to gather data about oneself, how to reduce ones biases on the subject, etc. (although Alicorn's <a href=\"/lw/2aw/seven_shiny_stories/\">Luminosity sequence</a> deserves mention).</p>\n<p>[2] Michael Vassar calls this skill \u201clightness of curiosity\u201d, by analogy to the skill \u201clightness of beliefs\u201d from Eliezer\u2019s <a href=\"http://yudkowsky.net/rational/virtues\">12 virtues of rationality</a>. &nbsp;The idea here is that a good rationalist should have a curiosity that moves immediately as they learn what information can help them, much as an good rationalist should have beliefs that move immediately as they learn which way the evidence points. &nbsp;Just as a good rationalist <a href=\"/lw/hs/think_like_reality/\">should not call reality \"surprising\"</a>, so also a good rationalist should not call useful domains \"boring\".</p>\n<p>[3] I.e., Eliezer's posts describe parts of an art for cases such as \"free will\" in which the initial question is confused, and&nbsp;must be <a href=\"/lw/of/dissolving_the_question/\">dissolved</a> rather than answered. &nbsp;He also notes the virtue of <a href=\"/lw/ui/use_the_try_harder_luke/\">sustained effort</a>.</p>\n<div>\n<p>[4] My favorite exceptions are Eliezer's post <a href=\"/lw/nb/something_to_protect/\">Something to protect</a> and Alicorn's <a href=\"/lw/2aw/seven_shiny_stories/\">City of Lights technique</a>. &nbsp;If you're looking for good reading offsite on how to have coherent goals, I'd second Patri's <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l50\">recommendation</a> of <a href=\"http://www.amazon.com/That-Frog-Great-Ways-Procrastinating/dp/1583762027\">Brian Tracy's</a> <a href=\"http://www.amazon.com/Goals-Everything-Faster-Thought-Possible/dp/1605094110/ref=pd_sim_b_2\">books</a>.</p>\n</div>", "sections": [{"title": "The branches", "anchor": "The_branches", "level": 1}, {"title": "Parts of a single discipline", "anchor": "Parts_of_a_single_discipline", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "66 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aFEsqd6ofwnkNqaXo", "Nu3wa6npK4Ry66vFp", "WBdvyyHLdxZSAMmoz", "7dRGYDqA2z6Zt7Q4h", "toAhho3FgBj8kfTHC", "PBRWb2Em5SNeWYwwB", "Masoq4NdmmGSiq2xw", "sP2Hg6uPwpfp3jZJN", "BHYBdijDcAKQ6e45Z", "N99KgncSXewWqkzMA", "9sguwESkteCgqFMbj", "tWLFWAndSZSYN6rPB", "Mc6QcrsbH5NRXbCRX", "fhEPnveFhb9tmd7Pe", "SGR4GxFK7KmW7ckCB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T06:51:22.352Z", "modifiedAt": null, "url": null, "title": "The Mathematics of Beauty [link]", "slug": "the-mathematics-of-beauty-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zc37tACs4bwczqGDz/the-mathematics-of-beauty-link", "pageUrlRelative": "/posts/Zc37tACs4bwczqGDz/the-mathematics-of-beauty-link", "linkUrl": "https://www.lesswrong.com/posts/Zc37tACs4bwczqGDz/the-mathematics-of-beauty-link", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Mathematics%20of%20Beauty%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Mathematics%20of%20Beauty%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZc37tACs4bwczqGDz%2Fthe-mathematics-of-beauty-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Mathematics%20of%20Beauty%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZc37tACs4bwczqGDz%2Fthe-mathematics-of-beauty-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZc37tACs4bwczqGDz%2Fthe-mathematics-of-beauty-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://blog.okcupid.com/index.php/the-mathematics-of-beauty/\">http://blog.okcupid.com/index.php/the-mathematics-of-beauty/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zc37tACs4bwczqGDz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 6.665725314474594e-07, "legacy": true, "legacyId": "4816", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T08:45:37.236Z", "modifiedAt": null, "url": null, "title": "Link: NY Times covers Bayesian statistics", "slug": "link-ny-times-covers-bayesian-statistics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.945Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/69r4Z4qYd2uCdCZxd/link-ny-times-covers-bayesian-statistics", "pageUrlRelative": "/posts/69r4Z4qYd2uCdCZxd/link-ny-times-covers-bayesian-statistics", "linkUrl": "https://www.lesswrong.com/posts/69r4Z4qYd2uCdCZxd/link-ny-times-covers-bayesian-statistics", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20NY%20Times%20covers%20Bayesian%20statistics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20NY%20Times%20covers%20Bayesian%20statistics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69r4Z4qYd2uCdCZxd%2Flink-ny-times-covers-bayesian-statistics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20NY%20Times%20covers%20Bayesian%20statistics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69r4Z4qYd2uCdCZxd%2Flink-ny-times-covers-bayesian-statistics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69r4Z4qYd2uCdCZxd%2Flink-ny-times-covers-bayesian-statistics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p><a href=\"http://www.nytimes.com/2011/01/11/science/11esp.html?_r=1&amp;src=me&amp;ref=homepage\">http://www.nytimes.com/2011/01/11/science/11esp.html?_r=1&amp;src=me&amp;ref=homepage</a></p>\n<p>It's disguised as an article about ESP to fool their editors; scroll down two paragraphs and it goes on for quite a while about what Bayesian statistics are and why Bayesian analysis is important.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "69r4Z4qYd2uCdCZxd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 6.666018146037915e-07, "legacy": true, "legacyId": "4818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T10:08:16.966Z", "modifiedAt": null, "url": null, "title": "Freaky unfairness", "slug": "freaky-unfairness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "benelliott", "createdAt": "2010-10-24T16:54:14.159Z", "isAdmin": false, "displayName": "benelliott"}, "userId": "H4iHqStnPyuAkniAA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gi5xpNeY6wC7Pdwin/freaky-unfairness", "pageUrlRelative": "/posts/gi5xpNeY6wC7Pdwin/freaky-unfairness", "linkUrl": "https://www.lesswrong.com/posts/gi5xpNeY6wC7Pdwin/freaky-unfairness", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Freaky%20unfairness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFreaky%20unfairness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgi5xpNeY6wC7Pdwin%2Ffreaky-unfairness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Freaky%20unfairness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgi5xpNeY6wC7Pdwin%2Ffreaky-unfairness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgi5xpNeY6wC7Pdwin%2Ffreaky-unfairness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 674, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">In <a href=\"/lw/13y/freaky_fairness/\">Freaky Fairness</a>&nbsp;the author discusses the problem of multi-player game theory problems where all players have access to each-other's source code. He gives an algorithm called Freaky Fairness, and makes the claim that 'all players run Freaky Fairness' is both a &nbsp;Strong Nash Equilibrium and a Pareto Optimum. Unfortunately, as far as I can tell, this claim is false.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">For reference, the algorithm of Freaky Fairness works as follows:</span></span></p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; list-style-type: decimal; list-style-position: outside; list-style-image: initial; \">\n<li>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Calculate the security values in mixed strategies for all subsets of players.</span></span></p>\n</li>\n<li>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Divide all other players into two groups: those whose source code is an exact copy of Freaky Fairness (friends), and everyone else (enemies).</span></span></p>\n</li>\n<li>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">If there are no enemies: build a Shapley value from the computed security values of coalitions; play my part in the outcome that yields the highest total sum in the game; give up some of the result to others so that the resulting allocation agrees with the Shapley value.</span></span></p>\n</li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">If there are enemies: play my part in the outcome that brings the total payoff of the coalition of all enemies down to their security value.</span></li>\n</ol>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Now consider the following simple game for three players:</span></span></p>\n<ol>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Each player chooses one player to nominate.</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">They can nominate themselves but they don't have to.</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">If a player gets nominated by at least two players (possibly including themselves) then they win the game and&nbsp;receive&nbsp;$300</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">If every player gets nominated exactly once then nobody gets anything.</span></li>\n</ol>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Conventional competitive game theory notes that each player has a dominant strategy of nominating themselves, and so nobody gets anything. Since the game potentially offers an $300 prize there is room for improvement here. Lets see how Freaky Fairness does:</span></span></p>\n<ul>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">The empty coalition and all coalitions with only one player have a security value of 0.</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">All coalitions with at least two players have a security value of $300 (this is&nbsp;independent&nbsp;of whether you use the alpha method, the beta method or mixed strategies to calculate security values).</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px; \">The Shapley values work out at $100 to each player (this is trivial by symmetry but can be worked out the long way if you feel like it).</span></span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">If all players run Freaky Fairness then they will choose one of their number, nominate them and then that player will then share out the money evenly.</span></span></li>\n</ul>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">If any one player deviates then the other two will split the money between themselves to ensure that this one player gets nothing, so far so good. Unfortunately if two players deviate they is nothing the remaining Freaky Fairness player can do to ensure they don't each walk away with $150, which is more than they get by sticking to Freaky Fairness.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Why does the proof given in the original article fail here? The proof assumes that the Shapley method gives every coalition at least their security value, which unfortunately it doesn't manage in this case. This is not a problem that can be fixed by a better method of sharing, as it should be fairly obvious that there is no way to share a quantity of money between three people such that any two of them get all of it.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Freaky Fairness is still a very impressive algorithm. If we define a class of 'nice games' as games where there exists a way of sharing out the winnings such that every coalition&nbsp;receives&nbsp;at least its security value, then Freaky Fairness works on all these games so long as Shapley does (where Shapley fails you can easily modify FF to find a real solution). We now just need to consider the class of 'nasty games', such as the one above. (For those more familiar with co-operative game theory, nice games are those which have a non-empty core, nasty games are those which do not).</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Note: I was quite surprised that none of the smart people in the comments section of the original article noticed this, which leads me to think it's quite possibly wrong. Any criticism is welcome.</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5A5ZGTQovxbay6fpr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gi5xpNeY6wC7Pdwin", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 28, "extendedScore": null, "score": 6.666230033524616e-07, "legacy": true, "legacyId": "4819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mFrx6YbQ6Dsup2Jvp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T11:07:35.259Z", "modifiedAt": null, "url": null, "title": "Reaching The Stars Is Easy [Videos] ", "slug": "reaching-the-stars-is-easy-videos", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:52.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DrFtRDpzq5otfnhRo/reaching-the-stars-is-easy-videos", "pageUrlRelative": "/posts/DrFtRDpzq5otfnhRo/reaching-the-stars-is-easy-videos", "linkUrl": "https://www.lesswrong.com/posts/DrFtRDpzq5otfnhRo/reaching-the-stars-is-easy-videos", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reaching%20The%20Stars%20Is%20Easy%20%5BVideos%5D%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReaching%20The%20Stars%20Is%20Easy%20%5BVideos%5D%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrFtRDpzq5otfnhRo%2Freaching-the-stars-is-easy-videos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reaching%20The%20Stars%20Is%20Easy%20%5BVideos%5D%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrFtRDpzq5otfnhRo%2Freaching-the-stars-is-easy-videos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrFtRDpzq5otfnhRo%2Freaching-the-stars-is-easy-videos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>The following videos highlight that <em>reaching</em> the stars is the easy part...<em>\"the biggest problem is us and the ape that we each carry inside\"</em>.</p>\n<p>\n<object width=\"640\" height=\"385\" data=\"http://www.youtube.com/v/BIT3TYnQJQc?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/BIT3TYnQJQc?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p>\n<object width=\"640\" height=\"385\" data=\"http://www.youtube.com/v/oY59wZdCDo0?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/oY59wZdCDo0?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p>\n<object width=\"640\" height=\"385\" data=\"http://www.youtube.com/v/BZ5sWfhkpE0?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/BZ5sWfhkpE0?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p>\n<object width=\"480\" height=\"385\" data=\"http://www.youtube.com/v/p86BPM1GV8M?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/p86BPM1GV8M?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p>Similar <a href=\"http://www.youtube.com/watch?v=ClCmO42_tQ0\">videos</a> highlighting the dangers of AI in a poetic way might work very good as promotion material.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DrFtRDpzq5otfnhRo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T15:34:43.053Z", "modifiedAt": null, "url": null, "title": "Anki deck for biases and fallacies", "slug": "anki-deck-for-biases-and-fallacies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.538Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "phob", "createdAt": "2009-12-02T19:18:55.570Z", "isAdmin": false, "displayName": "phob"}, "userId": "tHDmodCyA2Ttj4jey", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NZY2eAALzTTuYzR2t/anki-deck-for-biases-and-fallacies", "pageUrlRelative": "/posts/NZY2eAALzTTuYzR2t/anki-deck-for-biases-and-fallacies", "linkUrl": "https://www.lesswrong.com/posts/NZY2eAALzTTuYzR2t/anki-deck-for-biases-and-fallacies", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anki%20deck%20for%20biases%20and%20fallacies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnki%20deck%20for%20biases%20and%20fallacies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNZY2eAALzTTuYzR2t%2Fanki-deck-for-biases-and-fallacies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anki%20deck%20for%20biases%20and%20fallacies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNZY2eAALzTTuYzR2t%2Fanki-deck-for-biases-and-fallacies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNZY2eAALzTTuYzR2t%2Fanki-deck-for-biases-and-fallacies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/\" target=\"_blank\">Spaced Repetition Database for A Human's Guide to Words</a></p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>There's a great list of <a title=\"cognitive biases\" href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\" target=\"_blank\">cognitive biases</a> and <a title=\"List of logical fallacies\" href=\"http://en.wikipedia.org/wiki/List_of_fallacies\" target=\"_blank\">fallacies</a> on Wikipedia. &nbsp;For those who wish to aid their learning with some <a href=\"http://ankisrs.net/\">Anki</a> cards, I've shared a deck. &nbsp;Just search \"biases\" in Anki.</p>\n<p>For those who use a different program, here are <a href=\"http://dl.dropbox.com/u/4198648/biases.txt\">the cards in a tab-separated file</a>.</p>\n<p>I'll soon update it to also contain the <a href=\"http://en.wikipedia.org/wiki/List_of_memory_biases\">list of memory biases</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NZY2eAALzTTuYzR2t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "4821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2iwT298d6A7QgiLPe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T18:23:12.823Z", "modifiedAt": null, "url": null, "title": "Link: why training a.i. isn\u2019t like training your pets", "slug": "link-why-training-a-i-isn-t-like-training-your-pets", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Zk6e5XApWYZoXss7/link-why-training-a-i-isn-t-like-training-your-pets", "pageUrlRelative": "/posts/2Zk6e5XApWYZoXss7/link-why-training-a-i-isn-t-like-training-your-pets", "linkUrl": "https://www.lesswrong.com/posts/2Zk6e5XApWYZoXss7/link-why-training-a-i-isn-t-like-training-your-pets", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20why%20training%20a.i.%20isn%E2%80%99t%20like%20training%20your%20pets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20why%20training%20a.i.%20isn%E2%80%99t%20like%20training%20your%20pets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Zk6e5XApWYZoXss7%2Flink-why-training-a-i-isn-t-like-training-your-pets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20why%20training%20a.i.%20isn%E2%80%99t%20like%20training%20your%20pets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Zk6e5XApWYZoXss7%2Flink-why-training-a-i-isn-t-like-training-your-pets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Zk6e5XApWYZoXss7%2Flink-why-training-a-i-isn-t-like-training-your-pets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p>As the SIAI is gaining publicity more people are reviewing its work. I am not sure how popular <a href=\"http://worldofweirdthings.com/2011/01/12/why-training-a-i-isnt-like-training-your-pets/\">this blog</a> is but judged by its <a href=\"http://worldofweirdthings.com/about/\">about page</a> he writes for some high-profile blogs. His latest post takes on <a href=\"http://intelligence.org/upload/ai-resource-drives.pdf\">Omohundro's \"Basic AI Drives\"</a>:</p>\n<blockquote>\n<p>When we last looked at a paper from the Singularity Institute, it was an interesting work <a href=\"http://worldofweirdthings.com/2010/06/04/measuring-a-computers-iq-the-singularity-way/\">asking if we actually know what we&rsquo;re really measuring when trying to evaluate intelligence</a> by Dr. Shane Legg. While I found a few points that seemed a little odd  to me, the broader point Dr. Legg was perusing was very much valid and  there were some equations to consider. However, this paper isn&rsquo;t exactly  representative of most of the things you&rsquo;ll find coming from the  Institute&rsquo;s fellows. Generally, what you&rsquo;ll see are spanning  philosophical treatises filled with metaphors, trying to make sense out  of a technology that either doesn&rsquo;t really exist and treated as a black  box with inputs and outputs, or imagined by the author as a combination  of whatever a popular science site reported about new research ideas in  computer science. The end result of this process tends to be a lot like <a href=\"http://intelligence.org/upload/ai-resource-drives.pdf\" target=\"_blank\">this warning about the need to develop a friendly or benevolent artificial intelligence system</a> based on a rather fast and loose set of concepts about what an AI might decide to do and what will drive its decisions.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://worldofweirdthings.com/2011/01/12/why-training-a-i-isnt-like-training-your-pets/\">worldofweirdthings.com/2011/01/12/why-training-a-i-isnt-like-training-your-pets/</a></p>\n<p>I posted a few comments but do not think to be the right person to continue that discussion. So if you believe it is important what other people think about the SIAI and want to improve its public relations, there is your chance. I'm myself interested in the answers to his objections.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Zk6e5XApWYZoXss7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 6.66749893747253e-07, "legacy": true, "legacyId": "4824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T19:40:05.015Z", "modifiedAt": null, "url": null, "title": "NYC Rationalist Diplomacy Post-Game Discussion", "slug": "nyc-rationalist-diplomacy-post-game-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zvi", "createdAt": "2009-03-31T20:54:54.077Z", "isAdmin": false, "displayName": "Zvi"}, "userId": "N9zj5qpTfqmbn9dro", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2FWeKxwkpAjmavZyh/nyc-rationalist-diplomacy-post-game-discussion", "pageUrlRelative": "/posts/2FWeKxwkpAjmavZyh/nyc-rationalist-diplomacy-post-game-discussion", "linkUrl": "https://www.lesswrong.com/posts/2FWeKxwkpAjmavZyh/nyc-rationalist-diplomacy-post-game-discussion", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NYC%20Rationalist%20Diplomacy%20Post-Game%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANYC%20Rationalist%20Diplomacy%20Post-Game%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FWeKxwkpAjmavZyh%2Fnyc-rationalist-diplomacy-post-game-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NYC%20Rationalist%20Diplomacy%20Post-Game%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FWeKxwkpAjmavZyh%2Fnyc-rationalist-diplomacy-post-game-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FWeKxwkpAjmavZyh%2Fnyc-rationalist-diplomacy-post-game-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 852, "htmlBody": "<p>In addition to the two games currently being played on Less Wrong directly, the NYC group formed a third game; we filled five of the seven slots, with two being taken from an open call on LW when it was clear we would not reach seven players otherwise. While there were some technical problems for a few players, I feel the game was quite interesting. The game can be found here: <a href=\"http://webdiplomacy.net/board.php?gameID=42765\">http://webdiplomacy.net/board.php?gameID=42765</a>. Commentary was posted to the discussion group LW-Diplomacy@googlegroups.com which you can access here: <a href=\"http://groups.google.com/group/lw-diplomacy/browse_thread/thread/e8141331abf8a114\" target=\"_blank\">http://groups.google.com/group/lw-diplomacy/browse_thread/thread/e8141331abf8a114</a>, which all are free to join and review. These were neutral point of view overviews of the situation, to help bring everyone up to speed, and were done without private knowledge. While some figured out who I was right away, we didn't reveal it explicitly until much later. In terms of the software, a lack of good notifications makes it problematic for games that aren't either very long deadlines or very short, but it is a strong system otherwise.</p>\n<p>Also available there are all my communications and my journal, which I updated as the game progressed. I will give an overview here of what happened, as I understand it, and am happy to discuss anything and everything related to the game or Diplomacy in general.</p>\n<p>Early on, Italy and Russia quickly realized I was Turkey from my messages, and given my experience in the game and in games in general decided to try and take me out, agreeing on a triple with Austria, while in the west England and Germany formed an alliance against France. In the fall, both Austria and England had technical issues that cost them a center, which served to help keep France viable in the west and position Austria's units in awkward fashion. The first key movement in the game was that Austria decided that he felt that taking me out for who I was wasn't sporting, and so he backstabbed Italy but without making a deal with Turkey because he'd read (correctly) that Austria/Turkey is not a good deal for Austria in general. However, by lying to all and refusing to make a deal with Russia or with Turkey until too late he put us all in a position where it was easier to take him out. France jumped on Italy while he was weak and took advantage of England's struggles and Germany's fleets to realign the west. With Austria out of the way, Russia jumped on Germany and Turkey continued west safe from a Russian stab due to the tactical situation. I spent a lot of the midgame trying to get Russia to stall as much as possible in the north while I made progress in the south, with mixed results as England joined the alliance in exchange for assistance growing and once again becoming relevant. France asked for too much in deals with me and with Italy, forcing him to agree to be a Turkish puppet and me to stick with Russia until I could go for the win outright.</p>\n<p>In the Fall of 1906, Russia repeated even louder than usual his request that I leave Black Sea, which of course I had no intention of doing, and it was the natural time for him to try and build a southern fleet. He however had been following orders for some time, although they were strong orders, so he was out of position for a war. I guessed he might move to Romania so I went for Sevastopol, which is harmless if it fails, and I got it, preventing a Russian build. He then backed off since he wanted no war, but I couldn't let him pick up more centers and build a defense, so I pretended to agree to peace and went straight for him; he bought that the first turn's move was defensive so I got two free turns. Meanwhile, Russia pulled off a stab of France to get into Burgundy, so France agreed to let me into the Mid-Atlantic in order to help him survive since I didn't need him dead, but giving me Mid-Atlantic sealed the board's fate unless everyone could perfectly co-ordinate at a minimum, which did not happen (and rarely does in my experience) because Russia broke ranks. I believe that starting in Spring 1908 Turkey probably does have an eventual forced win because he can hold Portugal for years via support cuts but it took me a long time to see it.</p>\n<p>The biggest thing that I think is worth noting is that I made painstaking efforts to be friendly and helpful to all players in-game and not to break my word unless absolutely necessary. Early on I couldn't have kept my word to both Austria and Russia if they had both played along, but it never became an issue, then I lied once to Austria, arguably once to France and then once to Russia, and in two of the three cases presumed total war would be the result. I'm curious what other things that came up are considered by others to be worthy of discussion/exploration, and to see the after action reports from the other survivors.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2FWeKxwkpAjmavZyh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.667696054720658e-07, "legacy": true, "legacyId": "4825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T20:14:50.158Z", "modifiedAt": null, "url": null, "title": "Scott Sumner on Utility vs Happiness [Link]", "slug": "scott-sumner-on-utility-vs-happiness-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:55.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GHZ7ig73BP5XWqNSx/scott-sumner-on-utility-vs-happiness-link", "pageUrlRelative": "/posts/GHZ7ig73BP5XWqNSx/scott-sumner-on-utility-vs-happiness-link", "linkUrl": "https://www.lesswrong.com/posts/GHZ7ig73BP5XWqNSx/scott-sumner-on-utility-vs-happiness-link", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scott%20Sumner%20on%20Utility%20vs%20Happiness%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScott%20Sumner%20on%20Utility%20vs%20Happiness%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGHZ7ig73BP5XWqNSx%2Fscott-sumner-on-utility-vs-happiness-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scott%20Sumner%20on%20Utility%20vs%20Happiness%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGHZ7ig73BP5XWqNSx%2Fscott-sumner-on-utility-vs-happiness-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGHZ7ig73BP5XWqNSx%2Fscott-sumner-on-utility-vs-happiness-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 296, "htmlBody": "<p>A distinction that some people grok right away and some others may not realize exists:</p>\n<blockquote>\n<p>Imagine a country called &ldquo;Lanmindia,&rdquo; where much of the population  has seen its legs blown off in horrible accidents.&nbsp; Does that sound like  a pretty miserable place?&nbsp; Happiness research suggests not.&nbsp; The claim  is that there is a sort of natural &ldquo;set-point&rdquo; for happiness, and that  after winning a lottery one is happy for a short time, and then you  revert right back to your natural happiness level.&nbsp; I find that  plausible.&nbsp; They also claim that if someone loses a limb, then they are  unhappy for a short period and then revert back to normal.&nbsp; I find that  implausible, but if the evidence says it is the case then I guess I need  to accept that.</p>\n<p>My claim is that although Lanmindia is just as happy as America, it  has much lower utility.&nbsp;&nbsp;Let&rsquo;s define&nbsp;&rsquo;utility&rsquo; as&nbsp;&rdquo;that which people  maximize.&rdquo;&nbsp; People very much don&rsquo;t want to have their legs blown off,  and hence emigrate from Lanmindia in droves.&nbsp; People behave as if they  care about utility, not happiness.</p>\n-Scott Sumner, \"<a href=\"http://www.themoneyillusion.com/?p=8311\">Nonsense on stilts: Part 1.&nbsp; What if utility and happiness are unrelated?</a>\" <em>TheMoneyIllusion</em><br /></blockquote>\n<p>This is also somewhat a reply to Hanson's \"<a href=\"http://www.overcomingbias.com/2011/01/lift-up-your-eyes.html\">Lift Up Your Eyes</a>\" on <em>Overcoming Bias</em>. Some people on <em>LessWrong</em> are careful to make the distinction between ordinal utility, cardinal utility, and <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">fuzzies</a>, and others aren't quite so much. The above sentence on accepting evidence and the post script that he is not serious about one part of the post might also make interesting conversation -- part two is advice to move next door to a child molester for cheaper housing if you don't have a kid and part three is about The Fed taking advantage of banks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GHZ7ig73BP5XWqNSx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 6.667785173483706e-07, "legacy": true, "legacyId": "4826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-12T23:01:11.727Z", "modifiedAt": null, "url": null, "title": "Simpson's Paradox", "slug": "simpson-s-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:36.832Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bentarm", "createdAt": "2009-03-05T17:59:17.163Z", "isAdmin": false, "displayName": "bentarm"}, "userId": "xdmTZWK4DzchxkyQC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7FJRnxbRtT7Sbzizs/simpson-s-paradox", "pageUrlRelative": "/posts/7FJRnxbRtT7Sbzizs/simpson-s-paradox", "linkUrl": "https://www.lesswrong.com/posts/7FJRnxbRtT7Sbzizs/simpson-s-paradox", "postedAtFormatted": "Wednesday, January 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simpson's%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimpson's%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FJRnxbRtT7Sbzizs%2Fsimpson-s-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simpson's%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FJRnxbRtT7Sbzizs%2Fsimpson-s-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FJRnxbRtT7Sbzizs%2Fsimpson-s-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1349, "htmlBody": "<p>This is my first attempt at an elementary&nbsp;<a href=\"/r/discussion/lw/3ag/what_topics_would_you_like_to_see_more_of_on/3564\">statistics post</a>, which I hope is suitable for Less Wrong. I am going to present&nbsp;a discussion of a statistical phenomenon known as Simpson's Paradox. This isn't a paradox, and it wasn't actually discovered by Simpson, but that's the name everybody uses for it, so it's the name I'm going to stick with. Along the way, we'll get some very basic practice at calculating conditional probabilities.</p>\n<h3>A worked example</h3>\n<p>The example I've chosen is an exercise from a university statistics course that I have taught on for the past few years. It is by far the most interesting exercise in the entire course, and it goes as follows:</p>\n<p>You are a doctor in charge of a large hospital, and you have to decide which treatment should be used for a particular disease. You have the following data from last month: there were 390 patients with the disease. Treatment A was given to 160 patients of whom 100 were men and 60 were women; 20 of the men and 40 of the women recovered. Treatment B was given to 230 patients of whom 210 were men and 20 were women; 50 of the men and 15 of the women recovered. Which treatment would you recommend we use for people with the disease in future?</p>\n<p>The simplest way to represent these sort of data is to draw a table, we can then pick the relevant numbers out of the table to calculate the required conditional probabilities.</p>\n<p><strong>Overall</strong>&nbsp;</p>\n<p>\n<table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>60</td>\n<td>65</td>\n</tr>\n<tr>\n<td>died</td>\n<td>100</td>\n<td>165</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>The probability that a randomly chosen person survived if they were given treatment A is 60/160 = 0.375</p>\n<p>The probability that a randomly chosen person survived if they were given treatment B is 65/230 = 0.283</p>\n<p>So a randomly chosen person given treatment A was more likely to surive than a randomly chosen person given treatment B. Looks like we'd better give people treatment A.</p>\n<p>However, since were given a breakdown of the data by gender, let's look and see if treatment A is better for both genders, or if it gets all of its advantage from one or the other.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Women</strong></p>\n<p>\n<table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>40</td>\n<td>15</td>\n</tr>\n<tr>\n<td>died</td>\n<td>20</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>&nbsp;</p>\n<p>The probability that a randomly chosen woman survived given that they were given treatment A is 40/60 = 0.67</p>\n<p>The probability that a randomly chosen woman survived given that they were given treatment B is 15/20 = 0.75</p>\n<p>So it looks like treatment B is better for women. Guess that means treatment A must be much better for men, in order to be better overall. Let's take a closer look.</p>\n<p><strong>Men</strong></p>\n<p>\n<table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>20</td>\n<td>50</td>\n</tr>\n<tr>\n<td>died</td>\n<td>80</td>\n<td>160</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>The probability that a randomly chosen man survived given that they were given treatment A is 20/100 = 0.2</p>\n<p>The probability that a randomly chosen man survived given that they were given treatment B is 50/210 = 0.238</p>\n<p>So a randomly chosen man was more likely to survive if given treatment B than treatment A. What is going on here?</p>\n<p><strong>Treatment A, which seemed better in the overall data, was worse for both men and women when considered separately.&nbsp;</strong></p>\n<p>This, in essence, is Simpson's Paradox, partitioning data can result in a reversal of the correlations present in the aggregated data. Why does this happen? Well, essentially for two reasons. Firstly, the treatments were given to different numbers of people - treatment A was used much less often than treatment B in the example data, and secondly (and probably more importantly) the aggregation is hiding a confounding variable. Treatment B was much more likely to be given to men than to women, and men are much less likely than women to survive the disease, this obviously makes treatment B look worse in the aggregated data.&nbsp;</p>\n<p>So, you might think, we've sorted things out. Gender was the missing variable, and we now know that we can safely give everyone treatment B. Well, if I were writing the exercises for the course I teach on, I would have included the following follow-up question.</p>\n<h3>Yet Another Variable</h3>\n<p>It turns out that gender wasn't the only data that were collected about the patients. For the men, we also noted whether they were had any family history of heart disease. of the men given treatment A, 80 had a family history of heart disease, 10 of these survived. Of the men given treatment B, 55 had a family history of heart disease, 5 of these survived. The data now break down as follows:</p>\n<p><strong>History of heart disease</strong></p>\n<p>\n<table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>10</td>\n<td>5</td>\n</tr>\n<tr>\n<td>died</td>\n<td>70</td>\n<td>50</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p><strong>No history of heart disease</strong></p>\n<p>\n<table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>10</td>\n<td>45</td>\n</tr>\n<tr>\n<td>died</td>\n<td>10</td>\n<td>110</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>This time I will leave the calculations as an exercise to the reader but, as you can see, things have changed again. We can keep playing this game all day.&nbsp;</p>\n<h3>Which data to use?</h3>\n<p>This leaves us with the important question, which data should we use when making our decisions? Given a randomly chosen person, it looks like treatment A is better than treatment B. But any randomly chosen person is either a man or a woman, and whichever they are, treatment B is better than treatment A. But let's say the randomly chosen person is a man, then we could ask them whether or not they have a family history of heart disease and whichever answer they give, we will prefer to give them treatment A. &nbsp;</p>\n<p>It may appear that the partitioned data always give a better answer than the aggregated data. Unfortunately, this just isn't true. I made up the numbers in the previous example five minutes ago in order to reverse the correlation in the original exercise. Similarly, for just about any given set of data, you can find some partition which reverses the apparent correlation. How are we to decide which partitions are useful? If someone tells us that women born under Aries, Leo or Sagittarius do better with treatment A, as do those born under the Earth, Air and Water signs, would we really be willing to switch treatments?</p>\n<p>As you might expect, Judea Pearl has an answer to this problem (in chapter 6 of [1]). If we draw the relevant causal networks, we can formally decide which variables are confounding and so which partitions we should use (he quotes a further famous examples in which it is shown that you might want to use different versions of the <em>same data </em>depending on how they were acquired!), but that's another post for another time (and probably for someone better acquainted with Pearl than I am). In the meantime we should take Simpson's Paradox as a further warning of the dangers of drawing causal conclusions from data without understanding where the causes come from.</p>\n<h3>In Real Life</h3>\n<p>I'll finish with a famous real life example. In 1975, there was a study published [2] which demonstrated that 44% of male graduate applicants for graduate programmes at Berkeley were being accepted, whereas only 35% or female applicants were. This was obviously a pretty serious problem, so the authors decided to have a closer look, to try and see which departments in particular were most guilty of discrimination.&nbsp;</p>\n<p>As you'll be expecting by now, what they found was that not only were most of the departments not biased at all, in fact, there were more which were biased in favour of women than there were in favour of men! The confounding variable that was found was that women were applying for more competitive departments than men... of course, as we've seen, it's just possible that something else was hiding in the data.</p>\n<p>There are several other real-life examples. You can find a few in the <a href=\"http://en.wikipedia.org/wiki/Simpson's_paradox\">wikipedia article</a> on Simpson's Paradox. Batting averages are a common toy example. It's possible for one player to have a better average than another every season for his entire career, and a worse average overall. Similar phenomena are not particularly unusual in medical data - treatments which are given to patients with more serious ilnesses are always going to look worse in aggregate data. One of my&nbsp;personal favourite examples is that countries which put fluoride in the water have significantly more people who require false teeth than those which don't. As usual, there's a hidden variable lurking.</p>\n<p><strong>References:</strong></p>\n<p>(1) Judea Pearl. Causality: Models, Reasoning, and Inference, Cambridge University Press (2000, 2nd edition 2009)</p>\n<p>(2) P.J. Bickel, E.A. Hammel and J.W. O'Connell (1975). \"Sex Bias in Graduate Admissions: Data From Berkeley\". Science 187 (4175): 398&ndash;404</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 2, "bh7uxTTqmsQ8jZJdB": 5, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7FJRnxbRtT7Sbzizs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 96, "extendedScore": null, "score": 0.000197, "legacy": true, "legacyId": "4827", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 96, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is my first attempt at an elementary&nbsp;<a href=\"/r/discussion/lw/3ag/what_topics_would_you_like_to_see_more_of_on/3564\">statistics post</a>, which I hope is suitable for Less Wrong. I am going to present&nbsp;a discussion of a statistical phenomenon known as Simpson's Paradox. This isn't a paradox, and it wasn't actually discovered by Simpson, but that's the name everybody uses for it, so it's the name I'm going to stick with. Along the way, we'll get some very basic practice at calculating conditional probabilities.</p>\n<h3 id=\"A_worked_example\">A worked example</h3>\n<p>The example I've chosen is an exercise from a university statistics course that I have taught on for the past few years. It is by far the most interesting exercise in the entire course, and it goes as follows:</p>\n<p>You are a doctor in charge of a large hospital, and you have to decide which treatment should be used for a particular disease. You have the following data from last month: there were 390 patients with the disease. Treatment A was given to 160 patients of whom 100 were men and 60 were women; 20 of the men and 40 of the women recovered. Treatment B was given to 230 patients of whom 210 were men and 20 were women; 50 of the men and 15 of the women recovered. Which treatment would you recommend we use for people with the disease in future?</p>\n<p>The simplest way to represent these sort of data is to draw a table, we can then pick the relevant numbers out of the table to calculate the required conditional probabilities.</p>\n<p><strong>Overall</strong>&nbsp;</p>\n<p>\n</p><table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>60</td>\n<td>65</td>\n</tr>\n<tr>\n<td>died</td>\n<td>100</td>\n<td>165</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>The probability that a randomly chosen person survived if they were given treatment A is 60/160 = 0.375</p>\n<p>The probability that a randomly chosen person survived if they were given treatment B is 65/230 = 0.283</p>\n<p>So a randomly chosen person given treatment A was more likely to surive than a randomly chosen person given treatment B. Looks like we'd better give people treatment A.</p>\n<p>However, since were given a breakdown of the data by gender, let's look and see if treatment A is better for both genders, or if it gets all of its advantage from one or the other.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Women\">Women</strong></p>\n<p>\n</p><table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>40</td>\n<td>15</td>\n</tr>\n<tr>\n<td>died</td>\n<td>20</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>&nbsp;</p>\n<p>The probability that a randomly chosen woman survived given that they were given treatment A is 40/60 = 0.67</p>\n<p>The probability that a randomly chosen woman survived given that they were given treatment B is 15/20 = 0.75</p>\n<p>So it looks like treatment B is better for women. Guess that means treatment A must be much better for men, in order to be better overall. Let's take a closer look.</p>\n<p><strong id=\"Men\">Men</strong></p>\n<p>\n</p><table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>20</td>\n<td>50</td>\n</tr>\n<tr>\n<td>died</td>\n<td>80</td>\n<td>160</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>The probability that a randomly chosen man survived given that they were given treatment A is 20/100 = 0.2</p>\n<p>The probability that a randomly chosen man survived given that they were given treatment B is 50/210 = 0.238</p>\n<p>So a randomly chosen man was more likely to survive if given treatment B than treatment A. What is going on here?</p>\n<p><strong id=\"Treatment_A__which_seemed_better_in_the_overall_data__was_worse_for_both_men_and_women_when_considered_separately__\">Treatment A, which seemed better in the overall data, was worse for both men and women when considered separately.&nbsp;</strong></p>\n<p>This, in essence, is Simpson's Paradox, partitioning data can result in a reversal of the correlations present in the aggregated data. Why does this happen? Well, essentially for two reasons. Firstly, the treatments were given to different numbers of people - treatment A was used much less often than treatment B in the example data, and secondly (and probably more importantly) the aggregation is hiding a confounding variable. Treatment B was much more likely to be given to men than to women, and men are much less likely than women to survive the disease, this obviously makes treatment B look worse in the aggregated data.&nbsp;</p>\n<p>So, you might think, we've sorted things out. Gender was the missing variable, and we now know that we can safely give everyone treatment B. Well, if I were writing the exercises for the course I teach on, I would have included the following follow-up question.</p>\n<h3 id=\"Yet_Another_Variable\">Yet Another Variable</h3>\n<p>It turns out that gender wasn't the only data that were collected about the patients. For the men, we also noted whether they were had any family history of heart disease. of the men given treatment A, 80 had a family history of heart disease, 10 of these survived. Of the men given treatment B, 55 had a family history of heart disease, 5 of these survived. The data now break down as follows:</p>\n<p><strong id=\"History_of_heart_disease\">History of heart disease</strong></p>\n<p>\n</p><table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>10</td>\n<td>5</td>\n</tr>\n<tr>\n<td>died</td>\n<td>70</td>\n<td>50</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p><strong id=\"No_history_of_heart_disease\">No history of heart disease</strong></p>\n<p>\n</p><table border=\"1\" width=\"25%\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>A</td>\n<td>B</td>\n</tr>\n<tr>\n<td>lived</td>\n<td>10</td>\n<td>45</td>\n</tr>\n<tr>\n<td>died</td>\n<td>10</td>\n<td>110</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>This time I will leave the calculations as an exercise to the reader but, as you can see, things have changed again. We can keep playing this game all day.&nbsp;</p>\n<h3 id=\"Which_data_to_use_\">Which data to use?</h3>\n<p>This leaves us with the important question, which data should we use when making our decisions? Given a randomly chosen person, it looks like treatment A is better than treatment B. But any randomly chosen person is either a man or a woman, and whichever they are, treatment B is better than treatment A. But let's say the randomly chosen person is a man, then we could ask them whether or not they have a family history of heart disease and whichever answer they give, we will prefer to give them treatment A. &nbsp;</p>\n<p>It may appear that the partitioned data always give a better answer than the aggregated data. Unfortunately, this just isn't true. I made up the numbers in the previous example five minutes ago in order to reverse the correlation in the original exercise. Similarly, for just about any given set of data, you can find some partition which reverses the apparent correlation. How are we to decide which partitions are useful? If someone tells us that women born under Aries, Leo or Sagittarius do better with treatment A, as do those born under the Earth, Air and Water signs, would we really be willing to switch treatments?</p>\n<p>As you might expect, Judea Pearl has an answer to this problem (in chapter 6 of [1]). If we draw the relevant causal networks, we can formally decide which variables are confounding and so which partitions we should use (he quotes a further famous examples in which it is shown that you might want to use different versions of the <em>same data </em>depending on how they were acquired!), but that's another post for another time (and probably for someone better acquainted with Pearl than I am). In the meantime we should take Simpson's Paradox as a further warning of the dangers of drawing causal conclusions from data without understanding where the causes come from.</p>\n<h3 id=\"In_Real_Life\">In Real Life</h3>\n<p>I'll finish with a famous real life example. In 1975, there was a study published [2] which demonstrated that 44% of male graduate applicants for graduate programmes at Berkeley were being accepted, whereas only 35% or female applicants were. This was obviously a pretty serious problem, so the authors decided to have a closer look, to try and see which departments in particular were most guilty of discrimination.&nbsp;</p>\n<p>As you'll be expecting by now, what they found was that not only were most of the departments not biased at all, in fact, there were more which were biased in favour of women than there were in favour of men! The confounding variable that was found was that women were applying for more competitive departments than men... of course, as we've seen, it's just possible that something else was hiding in the data.</p>\n<p>There are several other real-life examples. You can find a few in the <a href=\"http://en.wikipedia.org/wiki/Simpson's_paradox\">wikipedia article</a> on Simpson's Paradox. Batting averages are a common toy example. It's possible for one player to have a better average than another every season for his entire career, and a worse average overall. Similar phenomena are not particularly unusual in medical data - treatments which are given to patients with more serious ilnesses are always going to look worse in aggregate data. One of my&nbsp;personal favourite examples is that countries which put fluoride in the water have significantly more people who require false teeth than those which don't. As usual, there's a hidden variable lurking.</p>\n<p><strong id=\"References_\">References:</strong></p>\n<p>(1) Judea Pearl. Causality: Models, Reasoning, and Inference, Cambridge University Press (2000, 2nd edition 2009)</p>\n<p>(2) P.J. Bickel, E.A. Hammel and J.W. O'Connell (1975). \"Sex Bias in Graduate Admissions: Data From Berkeley\". Science 187 (4175): 398\u2013404</p>", "sections": [{"title": "A worked example", "anchor": "A_worked_example", "level": 1}, {"title": "Women", "anchor": "Women", "level": 2}, {"title": "Men", "anchor": "Men", "level": 2}, {"title": "Treatment A, which seemed better in the overall data, was worse for both men and women when considered separately.\u00a0", "anchor": "Treatment_A__which_seemed_better_in_the_overall_data__was_worse_for_both_men_and_women_when_considered_separately__", "level": 2}, {"title": "Yet Another Variable", "anchor": "Yet_Another_Variable", "level": 1}, {"title": "History of heart disease", "anchor": "History_of_heart_disease", "level": 2}, {"title": "No history of heart disease", "anchor": "No_history_of_heart_disease", "level": 2}, {"title": "Which data to use?", "anchor": "Which_data_to_use_", "level": 1}, {"title": "In Real Life", "anchor": "In_Real_Life", "level": 1}, {"title": "References:", "anchor": "References_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "58 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-13T09:32:24.651Z", "modifiedAt": null, "url": null, "title": "[LINK] What should a reasonable person believe about the Singularity?", "slug": "link-what-should-a-reasonable-person-believe-about-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:54.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2XZju58cP82Fv776N/link-what-should-a-reasonable-person-believe-about-the", "pageUrlRelative": "/posts/2XZju58cP82Fv776N/link-what-should-a-reasonable-person-believe-about-the", "linkUrl": "https://www.lesswrong.com/posts/2XZju58cP82Fv776N/link-what-should-a-reasonable-person-believe-about-the", "postedAtFormatted": "Thursday, January 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20What%20should%20a%20reasonable%20person%20believe%20about%20the%20Singularity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20What%20should%20a%20reasonable%20person%20believe%20about%20the%20Singularity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XZju58cP82Fv776N%2Flink-what-should-a-reasonable-person-believe-about-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20What%20should%20a%20reasonable%20person%20believe%20about%20the%20Singularity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XZju58cP82Fv776N%2Flink-what-should-a-reasonable-person-believe-about-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XZju58cP82Fv776N%2Flink-what-should-a-reasonable-person-believe-about-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 631, "htmlBody": "<p><a href=\"http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/\">http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/</a></p>\r\n<p>Michael Nielsen, a pioneer in the field of quantum computation (from <a href=\"http://michaelnielsen.org/blog/michael-a-nielsen/\">his website</a>: <em>Together with Ike Chuang of MIT, he wrote the standard text on quantum computation. This is the most highly cited physics publication of the last 25 years, and one of the ten most highly cited physics books of all time (Source: Google Scholar, December 2007). He is the author of more than fifty scientific papers, including invited contributions to Nature and Scientific American</em>) has a pretty good essay about the probability of the Singularity. He starts off from Vinge's definition of the Singularity, and says that it's essentially the proposition that the three following assumptions are true:</p>\r\n<blockquote>\r\n<p>A: We will build computers of at least human intelligence at some time in the future, let&rsquo;s say within 100 years.<br /><br />B: Those computers will be able to rapidly and repeatedly increase their own intelligence, quickly resulting in computers that are far more intelligent than human beings.<br /><br />C: This will cause an enormous transformation of the world, so much so that it will become utterly unrecognizable, a phase Vinge terms the &ldquo;post-human era&rdquo;. This event is the Singularity.</p>\r\n</blockquote>\r\n<p>Then he goes on to define the probability of the Singularity within the next 100 years as the probability p(C|B)p(B|A)p(A), and gives what he thinks are reasonable ranges for the values p(A), p(B) and p(C)</p>\r\n<blockquote>\r\n<p>I&rsquo;m not going to argue for specific values for these probabilities. Instead, I&rsquo;ll argue for ranges of probabilities that I believe a person might reasonably assert for each probability on the right-hand side. I&rsquo;ll consider both a hypothetical skeptic, who is pessimistic about the possibility of the Singularity, and also a hypothetical enthusiast for the Singularity. In both cases I&rsquo;ll assume the person is reasonable, i.e., a person who is willing to acknowledge limits to our present-day understanding of the human brain and computer intelligence, and who is therefore not overconfident in their own predictions. By combining these ranges, we&rsquo;ll get a range of probabilities that a reasonable person might assert for the probability of the Singularity..</p>\r\n</blockquote>\r\n<p>&nbsp;In the end, he finds that the Singularity should be considered a serious probability:</p>\r\n<blockquote>\r\n<p>If we put all those ranges together, we get a &ldquo;reasonable&rdquo; probability for the Singularity somewhere in the range of 0.2 percent &ndash; one in 500 &ndash; up to just over 70 perecent. I regard both those as extreme positions, indicating a very strong commitment to the positions espoused. For more moderate probability ranges, I&rsquo;d use (say) 0.2 &lt; p(A) &lt; 0.8, 0.2 &lt; p(b) &lt; 0.8, and 0.3 &lt; p(c) &lt; 0.8. So I believe a moderate person would estimate a probability roughly in the range of 1 to 50 percent.<br /><br />These are interesting probability ranges. In particular, the 0.2 percent lower bound is striking. At that level, it's true that the Singularity is pretty darned unlikely. But it's still edging into the realm of a serious possibility. And to get this kind of probability estimate requires a person to hold quite an extreme set of positions, a range of positions that, in my opinion, while reasonable, requires considerable effort to defend. A less extreme person would end up with a probability estimate of a few percent or more. Given the remarkable nature of the Singularity, that's quite high. In my opinion, the main reason the Singularity has attracted some people's scorn and derision is superficial: it seems at first glance like an outlandish, science-fictional proposition. The end of the human era! It's hard to imgaine, and easy to laugh at. But any thoughtful analysis either requires one to consider the Singularity as a serious possibility, or demands a deep and carefully argued insight into why it won't happen.</p>\r\n</blockquote>\r\n<p>Hat tip to <a href=\"/user/Risto_Saarelma/\">Risto Saarelma</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 2, "sYm3HiWcfZvrGu3ui": 2, "Ng8Gice9KNkncxqcj": 2, "oiRp4T6u5poc8r9Tj": 2, "zHjC29kkPmsdo7WTr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2XZju58cP82Fv776N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 38, "extendedScore": null, "score": 6.66982976633081e-07, "legacy": true, "legacyId": "4846", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-13T12:25:25.611Z", "modifiedAt": null, "url": null, "title": "Fields related to Friendliness philosophy", "slug": "fields-related-to-friendliness-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HtJi9HwkdMrqByyBG/fields-related-to-friendliness-philosophy", "pageUrlRelative": "/posts/HtJi9HwkdMrqByyBG/fields-related-to-friendliness-philosophy", "linkUrl": "https://www.lesswrong.com/posts/HtJi9HwkdMrqByyBG/fields-related-to-friendliness-philosophy", "postedAtFormatted": "Thursday, January 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fields%20related%20to%20Friendliness%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFields%20related%20to%20Friendliness%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHtJi9HwkdMrqByyBG%2Ffields-related-to-friendliness-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fields%20related%20to%20Friendliness%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHtJi9HwkdMrqByyBG%2Ffields-related-to-friendliness-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHtJi9HwkdMrqByyBG%2Ffields-related-to-friendliness-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 619, "htmlBody": "<p>I just realized that my 'Interests' list on The Facebook made an okay ad hoc list of fields potentially related to Friendliness-like philosophy. They sorta kinda flow into each other by relatedness and are not particularly prioritized. This is my own incomplete list, though it was largely inspired by many conversations with Singularity Institute folk. Plus signs mean I'm moderately more certain that the existing field (or some rationalist re-interpretation of it) has useful insights.</p>\n<p>Friendliness philosophy:</p>\n<ul>\n<li>+Epistemology (formal, Bayesian, reflective, group)</li>\n<li>Axiology</li>\n<li>+Singularity (seed AI, universal AI drives, neuromorphic/emulation/de novo/kludge timelines, etc)</li>\n<li>+Cosmology (Tegmark-like stuff, shake vigorously with decision theory, don't get attached to ontologies/intuitions)</li>\n<li>Physics (Quantum MWI, etc)</li>\n<li>Metaphysics</li>\n<li>+Ontology of agency (Yudkowsky (kind of), Parfit, Buddha; limited good condensed stuff seemingly)</li>\n<li>+Ontology&nbsp;(probably grounded in algorithmic information theory / theoretical computer science ideas)</li>\n<li>Ontologyology (abstract Turing equivalence, et cetera)</li>\n<li>+Metaphilosophy (teaching ourselves to teach an AI to do philosophy)</li>\n<li>+Cognitive science (computational cognitive science especially)</li>\n<li>Neuroscience (affective neuroscience)</li>\n<li>Machine learning (reinforcement learners, Monte Carlo)</li>\n<li>+Computer science (super theoretical)</li>\n<li>+Algorithmic probability theory (algorithmic information theory, universal induction, etc)</li>\n<li>+Decision theory (updateless-like)</li>\n<li>Optimal control theory (stochastic, distributed; interestingly harder than it looks)</li>\n<li>+Bayesian probability theory (for building intuitions, mostly, but generally useful)</li>\n<li>Rationality</li>\n<li>Dynamical systems (attractors, stability)</li>\n<li>+Complex systems (multilevel selection, hierarchical stuff, convergent patterns / self-similarity)</li>\n<li>Cybernetics (field kind of disintegrated AFAIK, complex systems took over)</li>\n<li>Microeconomics (AGI negotiation stuff, human preference negotiation at different levels of organization)</li>\n<li>+Meta-ethics (Bostrom)</li>\n<li>Morality (Parfit)</li>\n<li>Moral psychology</li>\n<li>Evolutionary game theory</li>\n<li>+Evolutionary psychology (where human preferences come from (although again, universal/convergent patterns))</li>\n<li>+Evolutionary biology (how preferences evolve, convergent features, etc)</li>\n<li>Evolutionary developmental biology</li>\n<li>Dual inheritance theory (where preferences come from, different ontology and level of organization, see also memetics)</li>\n<li>Computational sociology (how cultures' preferences change over time)</li>\n<li>Epidemiology (for getting intuitions about how beliefs/preferences (memes) spread)</li>\n<li>Aesthetics (elegance, Occam-ness, useful across many domains)</li>\n<li>Buddhism (Theravada, to a lesser extent Zen; basically rationality with a different ontology and more emphasis on understanding oneself/onenotself)</li>\n<li>Jungian psychology (mostly archetypes)</li>\n<li>Psychoanalysis (id/ego/super-ego, defense mechanisms)</li>\n<li>Transpersonal psychology (Maslow's hierarchy, convergent spiritual experiences, convergent superstimuli for reinforcement learners, etc)</li>\n<li>Et cetera</li>\n</ul>\n<ul>\n</ul>\n<div>However I don't suggest you start hacking away at these fields until you have at least one sound ontology from which you can bootstrap by adding concepts in a coherent fashion (without becoming attached to that ontology or implicit metaontology). Unfortunately, this branch of rationality has not been discussed much on Less Wrong. In the meantime, just reading a huge effing amount of diverse material and looking for interesting connections and patterns is probably the next best bet, though I'm not really sure. Reading the Wikipedia articles on all of the above fields seems like a decent place to start, though I give no assurance of quality.</div>\n<div>As far as I know, nobody in the world is making a systematic effort to do Friendliness philosophy; though some have at least started to ask some fundamental questions. What is a preference? What is a decision? What is reality? We do not yet know how to get an AI to do that kind of philosophy for us, nor how to give the AI metaphilosophy. Meanwhile, none of these problems seem like obstacles for those whose accidental aim is uFAI. Just some food for thought.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HtJi9HwkdMrqByyBG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 8, "extendedScore": null, "score": 6.670275003739126e-07, "legacy": true, "legacyId": "4847", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-13T17:44:38.067Z", "modifiedAt": null, "url": null, "title": "Link: The Uncertain Future - \"The Future According to You\"", "slug": "link-the-uncertain-future-the-future-according-to-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.799Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yMb34LxmfCytKJ8gB/link-the-uncertain-future-the-future-according-to-you", "pageUrlRelative": "/posts/yMb34LxmfCytKJ8gB/link-the-uncertain-future-the-future-according-to-you", "linkUrl": "https://www.lesswrong.com/posts/yMb34LxmfCytKJ8gB/link-the-uncertain-future-the-future-according-to-you", "postedAtFormatted": "Thursday, January 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20The%20Uncertain%20Future%20-%20%22The%20Future%20According%20to%20You%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20The%20Uncertain%20Future%20-%20%22The%20Future%20According%20to%20You%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyMb34LxmfCytKJ8gB%2Flink-the-uncertain-future-the-future-according-to-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20The%20Uncertain%20Future%20-%20%22The%20Future%20According%20to%20You%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyMb34LxmfCytKJ8gB%2Flink-the-uncertain-future-the-future-according-to-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyMb34LxmfCytKJ8gB%2Flink-the-uncertain-future-the-future-according-to-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<blockquote>\n<p><strong>Visualizing \"The Future According to You\"</strong></p>\n<p>The Uncertain Future is a future technology and world-modeling project by the <a href=\"http://intelligence.org/\">Singularity Institute for Artificial Intelligence</a>. Its goal is to allow those interested in future technology to form their own rigorous, mathematically consistent model of how the development of advanced technologies will affect the evolution of civilization over the next hundred years. To facilitate this, we have gathered data on what experts think is going to happen, in such fields as semiconductor development, biotechnology, global security, Artificial Intelligence and neuroscience. We invite you, the user, to read about the opinions of these experts, and then come to your own conclusion about the likely destiny of mankind.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://theuncertainfuture.com/\">theuncertainfuture.com</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yMb34LxmfCytKJ8gB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 13, "extendedScore": null, "score": 6.671094218064344e-07, "legacy": true, "legacyId": "4849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-13T23:02:24.846Z", "modifiedAt": null, "url": null, "title": "I want to learn economics", "slug": "i-want-to-learn-economics", "viewCount": null, "lastCommentedAt": "2012-12-08T00:29:40.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ee5WB55MzRcjnvXby/i-want-to-learn-economics", "pageUrlRelative": "/posts/Ee5WB55MzRcjnvXby/i-want-to-learn-economics", "linkUrl": "https://www.lesswrong.com/posts/Ee5WB55MzRcjnvXby/i-want-to-learn-economics", "postedAtFormatted": "Thursday, January 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20want%20to%20learn%20economics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20want%20to%20learn%20economics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe5WB55MzRcjnvXby%2Fi-want-to-learn-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20want%20to%20learn%20economics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe5WB55MzRcjnvXby%2Fi-want-to-learn-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe5WB55MzRcjnvXby%2Fi-want-to-learn-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>I would like to learn more about economics but I don't know where to start. Can lesswrong suggest specific areas of economics that are particularly useful for understanding and optimising the world? Specific suggestions such as reading lists and resources would also be much appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ee5WB55MzRcjnvXby", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "4851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-14T01:43:57.834Z", "modifiedAt": null, "url": null, "title": "Treatment Tips for Used up Costume outfit Jewelry ", "slug": "treatment-tips-for-used-up-costume-outfit-jewelry", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jewelryshop2011", "createdAt": "2010-12-27T06:42:32.840Z", "isAdmin": false, "displayName": "jewelryshop2011"}, "userId": "Erq5A4fPhRxDwwSQG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KQzLHwa4A3h54wbmg/treatment-tips-for-used-up-costume-outfit-jewelry", "pageUrlRelative": "/posts/KQzLHwa4A3h54wbmg/treatment-tips-for-used-up-costume-outfit-jewelry", "linkUrl": "https://www.lesswrong.com/posts/KQzLHwa4A3h54wbmg/treatment-tips-for-used-up-costume-outfit-jewelry", "postedAtFormatted": "Friday, January 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Treatment%20Tips%20for%20Used%20up%20Costume%20outfit%20Jewelry%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATreatment%20Tips%20for%20Used%20up%20Costume%20outfit%20Jewelry%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKQzLHwa4A3h54wbmg%2Ftreatment-tips-for-used-up-costume-outfit-jewelry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Treatment%20Tips%20for%20Used%20up%20Costume%20outfit%20Jewelry%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKQzLHwa4A3h54wbmg%2Ftreatment-tips-for-used-up-costume-outfit-jewelry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKQzLHwa4A3h54wbmg%2Ftreatment-tips-for-used-up-costume-outfit-jewelry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 364, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:DrawingGridVerticalSpacing>7.8 \u78c5</w:DrawingGridVerticalSpacing> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>2</w:DisplayVerticalDrawingGridEvery> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:SpaceForUL /> <w:BalanceSingleByteDoubleByteWidth /> <w:DoNotLeaveBackslashAlone /> <w:ULTrailSpace /> <w:DoNotExpandShiftReturn /> <w:AdjustLineHeightInTable /> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\" \" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\u666e\u901a\u8868\u683c;\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-qformat:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\n\tmso-para-margin:0cm;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.5pt;\n\tmso-bidi-font-size:11.0pt;\n\tfont-family:\"Calibri\",\"sans-serif\";\n\tmso-ascii-font-family:Calibri;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-hansi-font-family:Calibri;\n\tmso-hansi-theme-font:minor-latin;\n\tmso-bidi-font-family:\"Times New Roman\";\n\tmso-bidi-theme-font:minor-bidi;\n\tmso-font-kerning:1.0pt;}\n</style>\n<![endif]--><span style=\"font-size: 10.5pt; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;\" lang=\"EN-US\">When you have old, worn out, found better days outfit jewelry just already there gathering dust really don't throw it out. You can enliven those dirty, obsolete and broken bits by using a few simple steps:Worn out gold or silver plating:Simply because typical costume diamond jewelry plating is very thin it could eventually rub off from wear or grow to be damaged from the by using harsh chemicals, products, or perfumes. Should this happen there are several options throughout gold and silver re-plating solutions that is found online. Some are fairly simple and can possibly be performed without the use of heat up or electricity whilst others are a little more complicated. I found one web-site in particular that offers a decent variety of plating options in addition to general plating instructions with regard to precious metals including jewelry, silver, platinum, plus rhodium. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p><span style=\"font-size: 10.5pt; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;\" lang=\"EN-US\">Click the contents as well as search box and plating gateway. Keep your eyes open: I recommend buying only cyanide zero cost plating solutions as cyanide is usually poisonous. For metallic lovers there is one particular product that I have individually used that has labored well for me and it is called Silver Brite. This kind of easy to use cream polishes, cleans and re-plates in one move. It is very easy to use as well as fairly inexpensive. Simply just rub the product on with an applicator cloth or sponge or a Q-tip for little items and rub off with the buffing cloth that's included with the system. The result is a sparkling, clean silver accomplish.Tarnished or dirty:Cleaning costume diamond jewelry is simple but pick your cleaner smartly. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p><span style=\"font-size: 10.5pt; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;\" lang=\"EN-US\">Only use a rings cleaner that precisely states that it is protected for costume jewellery and read the ingredients to make sure that it does not contain alcoholic beverages, acids, or ammonia as these can strip this gold or silver plate. Work with a children's soft bristle tooth brush for cleaning around fancy dress jewelry stones. I don't recommend submerging rhinestone jewelry since the stones may become loosened and fall out.The light tarnish can be easily buffed away using a soft jewelers pad. A good jewelry develope works great in restoring heavily tarnished solutions.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KQzLHwa4A3h54wbmg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4854", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-14T04:41:04.177Z", "modifiedAt": null, "url": null, "title": "Link - total body transplant", "slug": "link-total-body-transplant", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CnyEbLEZhjCc2xAGQ/link-total-body-transplant", "pageUrlRelative": "/posts/CnyEbLEZhjCc2xAGQ/link-total-body-transplant", "linkUrl": "https://www.lesswrong.com/posts/CnyEbLEZhjCc2xAGQ/link-total-body-transplant", "postedAtFormatted": "Friday, January 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%20-%20total%20body%20transplant&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%20-%20total%20body%20transplant%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnyEbLEZhjCc2xAGQ%2Flink-total-body-transplant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%20-%20total%20body%20transplant%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnyEbLEZhjCc2xAGQ%2Flink-total-body-transplant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnyEbLEZhjCc2xAGQ%2Flink-total-body-transplant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p>Amazing video. This is really \"out there\" medical research that actually worked, and not surprisingly, was closed down.</p>\n<p>http://www.vbs.tv/newsroom/dr-white-s-total-body-transplant-2-of-2--2</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CnyEbLEZhjCc2xAGQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 6.672779451885087e-07, "legacy": true, "legacyId": "4869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-14T06:49:46.793Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 7", "slug": "harry-potter-and-the-methods-of-rationality-discussion-11", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:36.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ae2kq3JmKvL4YPgk/harry-potter-and-the-methods-of-rationality-discussion-11", "pageUrlRelative": "/posts/6ae2kq3JmKvL4YPgk/harry-potter-and-the-methods-of-rationality-discussion-11", "linkUrl": "https://www.lesswrong.com/posts/6ae2kq3JmKvL4YPgk/harry-potter-and-the-methods-of-rationality-discussion-11", "postedAtFormatted": "Friday, January 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%207&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%207%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ae2kq3JmKvL4YPgk%2Fharry-potter-and-the-methods-of-rationality-discussion-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%207%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ae2kq3JmKvL4YPgk%2Fharry-potter-and-the-methods-of-rationality-discussion-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ae2kq3JmKvL4YPgk%2Fharry-potter-and-the-methods-of-rationality-discussion-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<p><strong>Update: Discussion has moved on to a <a href=\"/r/discussion/tag/harry_potter/\">new thread</a>.</strong></p>\n<p>The <em>load more comments</em> links are getting annoying (at least if you're not logged in), so it's time for a new <em><a href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em> discussion thread.&nbsp; We're also approaching the traditional 500-comment mark, but I think that hidden comments provide more appropriate joints to carve these threads at.&nbsp; So as of chapter 67, this is the place to share your thoughts about Eliezer Yudkowsky's Harry Potter fanfic.<br /><br />The first 5 discussion threads are on the main page under the <a href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the <a href=\"/r/discussion/tag/harry_potter/\">discussion section</a> using its separate tag system.&nbsp; Also: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>, <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">six</a>.&nbsp; The <a href=\"http://www.fanfiction.net/u/2269863/Less_Wrong\">fanfiction.net author page</a> is the central author-controlled HPMOR clearinghouse with links to the RSS feed, pdf version, TV Tropes pages, fan art, and more, and AdeleneDawner has kept an <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author's Notes</a>.<br /><br />As a reminder, it's often useful to start your comment by indicating which chapter you are commenting on.<br /><br /><strong>Spoiler Warning</strong>:&nbsp; this thread is full of spoilers.&nbsp; With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp; <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).<br /><br />If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that \"Eliezer said X is true\" unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ae2kq3JmKvL4YPgk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 6.673109971871561e-07, "legacy": true, "legacyId": "4871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 510, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-14T09:37:11.613Z", "modifiedAt": null, "url": null, "title": "Rational Repentance", "slug": "rational-repentance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:08.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u7weERYExjKwtJ8Fx/rational-repentance", "pageUrlRelative": "/posts/u7weERYExjKwtJ8Fx/rational-repentance", "linkUrl": "https://www.lesswrong.com/posts/u7weERYExjKwtJ8Fx/rational-repentance", "postedAtFormatted": "Friday, January 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Repentance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Repentance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu7weERYExjKwtJ8Fx%2Frational-repentance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Repentance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu7weERYExjKwtJ8Fx%2Frational-repentance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu7weERYExjKwtJ8Fx%2Frational-repentance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2670, "htmlBody": "<p>Related to: <a href=\"/lw/3pl/branches_of_rationality/\">Branches of Rationality</a>, <a href=\"/lw/3oj/a_lesswrong_rationality_workbook_idea/\">Rationality Workbook</a></p>\n<p>Changing your <em>behavior</em> to match new evidence could be harder than simply updating your beliefs and then mustering your willpower, because (a) we are in denial about how often we change our minds, (b) cognitive dissonance is tolerable in the medium-term, and (c) the additional monitoring required to verify that your actions as well as your beliefs have changed makes it easier for you to pretend that your actions are suitable to your reality. It might help to (1) specify a quitting point in advance, (2) demonstrate your new opinion with symbolic action, or (3) activate your emotions by reading non-rational propaganda. Additional solutions are eagerly solicited.</p>\n<p><a id=\"more\"></a><strong>Disclaimer:</strong></p>\n<p>This post contains examples drawn from politics and current events. I do not hope to change anyone's mind about any specific political belief, I know that <a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-killer</a>, I have tried to use non-inflammatory language, and I have a good faith belief that this post contains actual content on rationalism sufficient to justify its potentially controversial examples. Equally powerful but less controversial examples will be cheerfully substituted if anyone can bring them to my attention.</p>\n<p><strong>Review:</strong></p>\n<p>As has been amply discussed <a href=\"http://www.lesswrong.com/lw/i4/belief_in_belief/\">in</a> <a href=\"http://www.lesswrong.com/lw/36i/belief_in_belief_vs_internalization/\">the</a> <a href=\"http://www.godlessgeeks.com/LINKS/Dragon.htm\">sequences</a>, a key tool for overcoming the human tendency to irrationally defend prior beliefs simply because they are comfortable is to ask what, if anything, would cause you to <a href=\"/lw/o4/leave_a_line_of_retreat/\">abandon</a> those beliefs. For example, in the &ldquo;invisible dragon in the garage&rdquo; parable, it quickly becomes clear to neutral observers that there is no potential evidence that could convince an invisible-dragon-fundamentalist that the dragon is fictional. If you test for breathing noises, it turns out that the dragon is inaudible. If you test for ecological impact, it turns out that the dragon lives off of invisible hamsters, etc. Thus we say that the belief in the dragon is unfalsifiable; there is no way to falsify your hypothesis that there is a dragon in your garage, and so your belief in the dragon does not pay rent in anticipated experiences.<br />&nbsp;<br />There is a second human bias that causes you to cache an unrealistically high summary statistic for how often you <a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think/\">change your mind</a>: you think you change your mind, in general, pretty often, but unless you are an expert, highly-practiced rationalist, odds are that you do not. As evidence, try thinking of the last time you changed your mind about something and force yourself to specify what you believed beforehand and what you believed afterward. Me personally, I haven't changed my mind about anything that I can remember since about November 10th, 2010, and I'm sure I've expressed thousands of opinions since then. The odds are long.</p>\n<p><strong>The Problem:</strong></p>\n<p>There is a third human bias that causes you to tell yourself that you have successfully changed your mind when you have not really done so. The adherent of the Reformed Church of Dragon leaves the garage door open, and cheerfully admits to anyone who asks that there is probably no such thing as an invisible dragon, yet she is unaccountably cautious about actually parking her car in the garage. Thus it is worth knowing not just how to change your mind, but how to change your habits in response to new information. This is a distinct skill from simply knowing how to fight <a href=\"http://en.wikipedia.org/wiki/Akrasia\">akrasia</a>, i.e., how to <a href=\"/lw/1sm/akrasia_tactics_review/\">muster</a> the <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">willpower</a> to change your habits in general.<br />&nbsp;<br />One <a href=\"http://www.slate.com/blogs/blogs/thewrongstuff/archive/2010/12/16/my-country-right-or-wrong-conscientious-objector-josh-stieber-on-being-wrong-about-the-military.aspx\">example</a> of this failure mode, recently reported by Slate.com, involves American troops in Iraq: there are at least some regions in Iraq where many people strongly prefer not to have American troops around, and yet American troops persist in residing and operating there. In one such region, according to a former American soldier who was there, the people greeted the incoming foreigners with a large, peaceful protest, politely asking the Yankees to go home. When the request was ignored, locals began attacking the Americans with snipers and roadside bombs. According to the ex-soldier, Josh Steiber, the Americans responded not by leaving the region, but by ordering troops to shoot whoever happened to be around when a bomb went off, as a sort of reprisal killing. At that point, <a href=\"http://en.wikipedia.org/wiki/Cognitive_dissonance\">cognitive dissonance</a> finally kicked in for Josh, who had volunteered for the military out of a sense of idealism, and he changed his mind about whether he should be in Iraq: he stopped following orders, went home, and sought conscientious objector status.</p>\n<p>The interesting thing is that his comrades didn't, even after seeing his example. The same troops in the same town confronted with the same evidence that their presence was unwelcome all continued to blame and kill the locals. One of Josh's commanders wound up coming around to Josh's point of view to the extent of being able to agree to disagree and give Josh a hug, but still kept ordering people to kill the locals. One wonders: what would it take to get the commander to change not just his mind, but his actions? What evidence would someone in his position have to observe before he would stop killing Iraqis? The theory is that American military presence in Iraq is good for Iraqis because it helps them build democracy, or security, or their economy, or some combination. It's moderately challenging to concede that the theory could be flawed. But, assuming you have the rationalist chops to admit your doubt, how do you go about changing your actions to reflect that doubt? The answer isn't to sit at home and do nothing; there are probably wars, or at the very least nonviolent humanitarian interventions, that are worth sending people abroad for (or going yourself, if you're not busy). But if you can't change your behavior once you arrive on the scene, your doubt is practically worthless -- we could replace you with an unthinking, unquestioning patriot and get the same results.&nbsp;</p>\n<p>Another <a href=\"http://www.billmckibben.com/deep-economy.html\">example</a> was reported by Bill McKibben, author of Deep Economy, who says he happened to be in the organic farming region of Gorasin, Bangladesh the day an international food expert arrived to talk about genetically engineered \"golden rice,\" which, unlike ordinary rice, is rich in Vitamin A and can prevent certain nutritional deficiency syndromes. \"The villagers listened for a few minutes, and then they started muttering. Unlike most of us in the West who worried about eating genetically modified organisms, they weren't much concerned about 'frankenfood.' Instead, they instantly realized that the new rice would require fertilizer and pesticide, meaning both illness and debt. More to the point, they kept saying, they had no need of golden rice because the leafy vegetables they could now grow in their organic fields provided all the nutrition they needed. 'When we cook the green vegetables, we are aware not to throw out the water,' said one woman. 'Yes,' said another. 'And we don't like to eat rice only. It tastes better with green vegetables.'\"</p>\n<p>Bill doesn't say how the story ended, but one can imagine that there are many places like Gorasin where the villagers ended up with GMOs anyway. The November/December 2010 issue of Foreign Affairs has a pretty good <a href=\"http://www.foreignaffairs.com/articles/66827/roger-thurow/the-fertile-continent\">article</a> (partial paywall) about how international food donors have persisted in shipping grain -- sometimes right past huts full of soon-to-rot local stockpiles -- when what is really needed are roads, warehouses, and loans. One could argue that the continued funding of food aid at 100 times the ratio of food infrastructure aid, or the continued encouragement of miracle mono-crops in the face of local disinterest, simply reflects the financial incentives of large agricultural corporations. Considering how popular farmers are and how unpopular foreign aid is, though, there are doubtless easier ways for Monsanto and ConAgra to get their government subsidies. At least some of the political support for these initiatives has to come from well-intentioned leaders who have reason to know that their policies are counterproductive but who are unable or unwilling to change their behavior to reflect that knowledge.</p>\n<p>It sounds stupid when people act this stubbornly on the global stage, but it is surprisingly difficult not to be stubborn. What if anything, would convince you to stop (or start) eating animals? Not merely to admit, verbally, that it is an acceptable thing for others to do, or even the moral or prudent thing for you to do, but to actually start trying to do it? What, if anything, would convince you to stop (or start) expecting monogamy in your romantic relationships? To save (or borrow) significant amounts of money? To drop one hobby and pick up another? To move across the country?</p>\n<p>And, here's the real sore spot: how do you know? Suppose you said that you would save $1,000 a year if the real interest rate were above 15%. Would you really? What is your reference class for predicting your own behavior? Have you made a change like that before in your life? How did the strength of the evidence you <em>thought</em> it would take to change your behavior compare to the evidence it <em>actually</em> took to change your behavior? How often do you make comparably drastic changes? How often do you try to make such changes? Which are you more likely to remember -- the successful changes, or the failed and quickly aborted attempts?</p>\n<p><strong>Solutions:</strong></p>\n<p>Having just recently become explicitly aware of this problem, I'm hardly an expert on how to solve it. However, for whatever it is worth, here are some potential coping mechanisms. Additional solutions are strongly encouraged in the comments section!</p>\n<p>1) <em>Specify a quitting point in advance</em>. If you know ahead of time what sort of evidence, E, would convince you that your conduct is counterproductive or strictly dominated by some other course of conduct, then switching to that other course of conduct when you observe that evidence will feel like part of your strategy. Instead of seeing yourself as adopting strategy A and then being forced to switch to strategy B because strategy A failed, you can see yourself as adopting the conditional strategy C, which calls for strategy A in circumstance E and for strategy B in circumstance ~E. That way your success is not dependent on your commitment, which should help reduce your commitment down toward an optimal level.</p>\n<p>Without a pre-determined quitting point, you run the risk of making excuses for an endless series of marginal increases in the strength of the evidence required to make a change of action appropriate. <a href=\"/en.wikipedia.org/wiki/Sunk_costs\">Sunk costs</a> may be an economic fallacy, but they're a psychological reality.</p>\n<p>2) <em>Demonstrate your new opinion with symbolic action. </em>Have you decided to move to San Francisco, even though your parents and significant other swear they'll never visit you there? Great! We have nice weather here; look forward to seeing you as soon as you can land a job. Meanwhile, buy a great big map of our beautiful city and put it on your bedroom wall. The map, in and of itself, doesn't get you a damn bit closer to moving here. It doesn't purport to influence your incentives the way a <a href=\"http://www.stickk.com/\">commitment contract</a> would. What it does do is help you internalize your conscious decision so the decision is more <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/?sort=new\">broadly</a> endorsed by the various aspects of your psyche.</p>\n<p>I remember at one point a religious camp counselor caught me using a glowstick on the Sabbath, and advised me to throw the glowstick away, on the theory that kindling a new light on the Sabbath violated the applicable religious laws. I asked him what good throwing away the light would do, seeing as it had already been kindled and would keep on burning its fixed supply of fuel no matter where I put it. He said that even though throwing away the light wouldn't stop the light from having been kindled (there were limits to his magical thinking, despite his religious convictions), it would highlight (har har) my agreement with the principle that kindling lights is wrong and make it easier not to do it again next time...the very sense that it is strange to throw away a lit glowstick helps put cognitive dissonance to work <em>for</em> changing your mind instead of against it: if you didn't strongly believe in the importance of not kindling glowsticks, why on earth would you have thrown it away? But you did throw it away, and so you must believe, and so on. Also, not reaping the benefits of the wrongly kindled light makes kindling lights seem to provide fewer benefits, and makes it easier to resist kindling it the next time -- if you know, in the moment of temptation, that even if you kindle the glowstick you might repent and not let yourself enjoy its light, you'll factor that into your utility function and be more likely to decide that the no-longer-certain future benefit of the light isn't worth the immediate guilt.</p>\n<p>Anyway, this is a fairly weird example; I certainly don't care whether people light glowsticks, on a particular tribe's Sabbath or otherwise. I think it probably does help, though, to be a bit of a drama queen. If you buy a cake while you're dieting, don't just resolve not to eat it; physically throw it out the second-story balcony. If you've just admitted to yourself that your erstwhile political enemies actually have some pretty good points, write your favorite ex-evil candidate a letter of support or a $5 check and physically put it in the mail. As much as possible, bring your whole self into the process of changing your actions.</p>\n<p>3) <em>Over-correct your opinion by reading propaganda</em>. Propaganda is dangerous when you read it in order to help you form an opinion, and a deontological evil when you publish it to hack into other peoples' minds (which, depending on circumstances and your philosophy, may or may not be justified by the good consequences that you expect will follow). But when you've already carefully considered the rational evidence for and against a proposition, and you feel like you've changed your mind, and yet you're still acting as if you hadn't changed your mind, propaganda might be just what you need. Read an essay that forcefully argues for a position even more extreme than the one you've just adopted, even if the essay is full of logical cul-de-sacs. In this limited context alone, gleefully give yourself temporary permission to ignore the fact that reading the essay makes you notice that you are <a href=\"/lw/if/your_strength_as_a_rationalist/\">confused</a>. Bask in the rightness of the essay and the guilt/shame/foolishness/low-status that people who disagree with it should feel. If you gauge the dosage correctly, the propaganda might nudge your opinion just enough to make you actually adopt the new action that you felt would adequately reflect your new beliefs, but not enough to drive you over the cliff into madness.</p>\n<p>As an example, I recently became convinced that eating industrially raised animals while living in San Francisco before the apocalypse can't ever be morally justified, and, yet, lo and behold, I still ate turkey sandwiches at Subway 5 times a week. Obviously I could have just used some of the tactics from the <a href=\"/lw/1sm/akrasia_tactics_review/\">Akrasia Review</a> to make eating less factory-meat a conscious goal...but I'm busy using those tools for other goals, and I think that there are probably at least some contexts in which willpower is limited, or at least a variable-sum game. So I read Peter Singer's book on Animal Liberation, and blamed all the woes of the world on steak for a few hours while slowly eating a particularly foul-tasting beef stew that was ruined by some Thai hole-in-the-wall, to reinforce the message. I'm doing a little bit better...I'm more likely to cross the street and eat vegetarian or pay for the free-range stuff, and I'm down to about 3 Subway footlongs a week, without any noticeable decrease in my willpower reserves.</p>\n<p>Your mileage may vary; please use this tactic carefully.</p>\n<p>4) <em>Your suggestions.</em></p>\n<p>Seriously; most of my point in posting here is to gather more suggestions. If I thought of the three best solutions in two hours with no training, I'll eat my shirt. And I will, too -- it'll help me repent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HXA9WxPpzZCCEwXHT": 1, "mQbxDKHxPcKKRG4mb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u7weERYExjKwtJ8Fx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 44, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "4872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"/lw/3pl/branches_of_rationality/\">Branches of Rationality</a>, <a href=\"/lw/3oj/a_lesswrong_rationality_workbook_idea/\">Rationality Workbook</a></p>\n<p>Changing your <em>behavior</em> to match new evidence could be harder than simply updating your beliefs and then mustering your willpower, because (a) we are in denial about how often we change our minds, (b) cognitive dissonance is tolerable in the medium-term, and (c) the additional monitoring required to verify that your actions as well as your beliefs have changed makes it easier for you to pretend that your actions are suitable to your reality. It might help to (1) specify a quitting point in advance, (2) demonstrate your new opinion with symbolic action, or (3) activate your emotions by reading non-rational propaganda. Additional solutions are eagerly solicited.</p>\n<p><a id=\"more\"></a><strong>Disclaimer:</strong></p>\n<p>This post contains examples drawn from politics and current events. I do not hope to change anyone's mind about any specific political belief, I know that <a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-killer</a>, I have tried to use non-inflammatory language, and I have a good faith belief that this post contains actual content on rationalism sufficient to justify its potentially controversial examples. Equally powerful but less controversial examples will be cheerfully substituted if anyone can bring them to my attention.</p>\n<p><strong id=\"Review_\">Review:</strong></p>\n<p>As has been amply discussed <a href=\"http://www.lesswrong.com/lw/i4/belief_in_belief/\">in</a> <a href=\"http://www.lesswrong.com/lw/36i/belief_in_belief_vs_internalization/\">the</a> <a href=\"http://www.godlessgeeks.com/LINKS/Dragon.htm\">sequences</a>, a key tool for overcoming the human tendency to irrationally defend prior beliefs simply because they are comfortable is to ask what, if anything, would cause you to <a href=\"/lw/o4/leave_a_line_of_retreat/\">abandon</a> those beliefs. For example, in the \u201cinvisible dragon in the garage\u201d parable, it quickly becomes clear to neutral observers that there is no potential evidence that could convince an invisible-dragon-fundamentalist that the dragon is fictional. If you test for breathing noises, it turns out that the dragon is inaudible. If you test for ecological impact, it turns out that the dragon lives off of invisible hamsters, etc. Thus we say that the belief in the dragon is unfalsifiable; there is no way to falsify your hypothesis that there is a dragon in your garage, and so your belief in the dragon does not pay rent in anticipated experiences.<br>&nbsp;<br>There is a second human bias that causes you to cache an unrealistically high summary statistic for how often you <a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think/\">change your mind</a>: you think you change your mind, in general, pretty often, but unless you are an expert, highly-practiced rationalist, odds are that you do not. As evidence, try thinking of the last time you changed your mind about something and force yourself to specify what you believed beforehand and what you believed afterward. Me personally, I haven't changed my mind about anything that I can remember since about November 10th, 2010, and I'm sure I've expressed thousands of opinions since then. The odds are long.</p>\n<p><strong id=\"The_Problem_\">The Problem:</strong></p>\n<p>There is a third human bias that causes you to tell yourself that you have successfully changed your mind when you have not really done so. The adherent of the Reformed Church of Dragon leaves the garage door open, and cheerfully admits to anyone who asks that there is probably no such thing as an invisible dragon, yet she is unaccountably cautious about actually parking her car in the garage. Thus it is worth knowing not just how to change your mind, but how to change your habits in response to new information. This is a distinct skill from simply knowing how to fight <a href=\"http://en.wikipedia.org/wiki/Akrasia\">akrasia</a>, i.e., how to <a href=\"/lw/1sm/akrasia_tactics_review/\">muster</a> the <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">willpower</a> to change your habits in general.<br>&nbsp;<br>One <a href=\"http://www.slate.com/blogs/blogs/thewrongstuff/archive/2010/12/16/my-country-right-or-wrong-conscientious-objector-josh-stieber-on-being-wrong-about-the-military.aspx\">example</a> of this failure mode, recently reported by Slate.com, involves American troops in Iraq: there are at least some regions in Iraq where many people strongly prefer not to have American troops around, and yet American troops persist in residing and operating there. In one such region, according to a former American soldier who was there, the people greeted the incoming foreigners with a large, peaceful protest, politely asking the Yankees to go home. When the request was ignored, locals began attacking the Americans with snipers and roadside bombs. According to the ex-soldier, Josh Steiber, the Americans responded not by leaving the region, but by ordering troops to shoot whoever happened to be around when a bomb went off, as a sort of reprisal killing. At that point, <a href=\"http://en.wikipedia.org/wiki/Cognitive_dissonance\">cognitive dissonance</a> finally kicked in for Josh, who had volunteered for the military out of a sense of idealism, and he changed his mind about whether he should be in Iraq: he stopped following orders, went home, and sought conscientious objector status.</p>\n<p>The interesting thing is that his comrades didn't, even after seeing his example. The same troops in the same town confronted with the same evidence that their presence was unwelcome all continued to blame and kill the locals. One of Josh's commanders wound up coming around to Josh's point of view to the extent of being able to agree to disagree and give Josh a hug, but still kept ordering people to kill the locals. One wonders: what would it take to get the commander to change not just his mind, but his actions? What evidence would someone in his position have to observe before he would stop killing Iraqis? The theory is that American military presence in Iraq is good for Iraqis because it helps them build democracy, or security, or their economy, or some combination. It's moderately challenging to concede that the theory could be flawed. But, assuming you have the rationalist chops to admit your doubt, how do you go about changing your actions to reflect that doubt? The answer isn't to sit at home and do nothing; there are probably wars, or at the very least nonviolent humanitarian interventions, that are worth sending people abroad for (or going yourself, if you're not busy). But if you can't change your behavior once you arrive on the scene, your doubt is practically worthless -- we could replace you with an unthinking, unquestioning patriot and get the same results.&nbsp;</p>\n<p>Another <a href=\"http://www.billmckibben.com/deep-economy.html\">example</a> was reported by Bill McKibben, author of Deep Economy, who says he happened to be in the organic farming region of Gorasin, Bangladesh the day an international food expert arrived to talk about genetically engineered \"golden rice,\" which, unlike ordinary rice, is rich in Vitamin A and can prevent certain nutritional deficiency syndromes. \"The villagers listened for a few minutes, and then they started muttering. Unlike most of us in the West who worried about eating genetically modified organisms, they weren't much concerned about 'frankenfood.' Instead, they instantly realized that the new rice would require fertilizer and pesticide, meaning both illness and debt. More to the point, they kept saying, they had no need of golden rice because the leafy vegetables they could now grow in their organic fields provided all the nutrition they needed. 'When we cook the green vegetables, we are aware not to throw out the water,' said one woman. 'Yes,' said another. 'And we don't like to eat rice only. It tastes better with green vegetables.'\"</p>\n<p>Bill doesn't say how the story ended, but one can imagine that there are many places like Gorasin where the villagers ended up with GMOs anyway. The November/December 2010 issue of Foreign Affairs has a pretty good <a href=\"http://www.foreignaffairs.com/articles/66827/roger-thurow/the-fertile-continent\">article</a> (partial paywall) about how international food donors have persisted in shipping grain -- sometimes right past huts full of soon-to-rot local stockpiles -- when what is really needed are roads, warehouses, and loans. One could argue that the continued funding of food aid at 100 times the ratio of food infrastructure aid, or the continued encouragement of miracle mono-crops in the face of local disinterest, simply reflects the financial incentives of large agricultural corporations. Considering how popular farmers are and how unpopular foreign aid is, though, there are doubtless easier ways for Monsanto and ConAgra to get their government subsidies. At least some of the political support for these initiatives has to come from well-intentioned leaders who have reason to know that their policies are counterproductive but who are unable or unwilling to change their behavior to reflect that knowledge.</p>\n<p>It sounds stupid when people act this stubbornly on the global stage, but it is surprisingly difficult not to be stubborn. What if anything, would convince you to stop (or start) eating animals? Not merely to admit, verbally, that it is an acceptable thing for others to do, or even the moral or prudent thing for you to do, but to actually start trying to do it? What, if anything, would convince you to stop (or start) expecting monogamy in your romantic relationships? To save (or borrow) significant amounts of money? To drop one hobby and pick up another? To move across the country?</p>\n<p>And, here's the real sore spot: how do you know? Suppose you said that you would save $1,000 a year if the real interest rate were above 15%. Would you really? What is your reference class for predicting your own behavior? Have you made a change like that before in your life? How did the strength of the evidence you <em>thought</em> it would take to change your behavior compare to the evidence it <em>actually</em> took to change your behavior? How often do you make comparably drastic changes? How often do you try to make such changes? Which are you more likely to remember -- the successful changes, or the failed and quickly aborted attempts?</p>\n<p><strong id=\"Solutions_\">Solutions:</strong></p>\n<p>Having just recently become explicitly aware of this problem, I'm hardly an expert on how to solve it. However, for whatever it is worth, here are some potential coping mechanisms. Additional solutions are strongly encouraged in the comments section!</p>\n<p>1) <em>Specify a quitting point in advance</em>. If you know ahead of time what sort of evidence, E, would convince you that your conduct is counterproductive or strictly dominated by some other course of conduct, then switching to that other course of conduct when you observe that evidence will feel like part of your strategy. Instead of seeing yourself as adopting strategy A and then being forced to switch to strategy B because strategy A failed, you can see yourself as adopting the conditional strategy C, which calls for strategy A in circumstance E and for strategy B in circumstance ~E. That way your success is not dependent on your commitment, which should help reduce your commitment down toward an optimal level.</p>\n<p>Without a pre-determined quitting point, you run the risk of making excuses for an endless series of marginal increases in the strength of the evidence required to make a change of action appropriate. <a href=\"/en.wikipedia.org/wiki/Sunk_costs\">Sunk costs</a> may be an economic fallacy, but they're a psychological reality.</p>\n<p>2) <em>Demonstrate your new opinion with symbolic action. </em>Have you decided to move to San Francisco, even though your parents and significant other swear they'll never visit you there? Great! We have nice weather here; look forward to seeing you as soon as you can land a job. Meanwhile, buy a great big map of our beautiful city and put it on your bedroom wall. The map, in and of itself, doesn't get you a damn bit closer to moving here. It doesn't purport to influence your incentives the way a <a href=\"http://www.stickk.com/\">commitment contract</a> would. What it does do is help you internalize your conscious decision so the decision is more <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/?sort=new\">broadly</a> endorsed by the various aspects of your psyche.</p>\n<p>I remember at one point a religious camp counselor caught me using a glowstick on the Sabbath, and advised me to throw the glowstick away, on the theory that kindling a new light on the Sabbath violated the applicable religious laws. I asked him what good throwing away the light would do, seeing as it had already been kindled and would keep on burning its fixed supply of fuel no matter where I put it. He said that even though throwing away the light wouldn't stop the light from having been kindled (there were limits to his magical thinking, despite his religious convictions), it would highlight (har har) my agreement with the principle that kindling lights is wrong and make it easier not to do it again next time...the very sense that it is strange to throw away a lit glowstick helps put cognitive dissonance to work <em>for</em> changing your mind instead of against it: if you didn't strongly believe in the importance of not kindling glowsticks, why on earth would you have thrown it away? But you did throw it away, and so you must believe, and so on. Also, not reaping the benefits of the wrongly kindled light makes kindling lights seem to provide fewer benefits, and makes it easier to resist kindling it the next time -- if you know, in the moment of temptation, that even if you kindle the glowstick you might repent and not let yourself enjoy its light, you'll factor that into your utility function and be more likely to decide that the no-longer-certain future benefit of the light isn't worth the immediate guilt.</p>\n<p>Anyway, this is a fairly weird example; I certainly don't care whether people light glowsticks, on a particular tribe's Sabbath or otherwise. I think it probably does help, though, to be a bit of a drama queen. If you buy a cake while you're dieting, don't just resolve not to eat it; physically throw it out the second-story balcony. If you've just admitted to yourself that your erstwhile political enemies actually have some pretty good points, write your favorite ex-evil candidate a letter of support or a $5 check and physically put it in the mail. As much as possible, bring your whole self into the process of changing your actions.</p>\n<p>3) <em>Over-correct your opinion by reading propaganda</em>. Propaganda is dangerous when you read it in order to help you form an opinion, and a deontological evil when you publish it to hack into other peoples' minds (which, depending on circumstances and your philosophy, may or may not be justified by the good consequences that you expect will follow). But when you've already carefully considered the rational evidence for and against a proposition, and you feel like you've changed your mind, and yet you're still acting as if you hadn't changed your mind, propaganda might be just what you need. Read an essay that forcefully argues for a position even more extreme than the one you've just adopted, even if the essay is full of logical cul-de-sacs. In this limited context alone, gleefully give yourself temporary permission to ignore the fact that reading the essay makes you notice that you are <a href=\"/lw/if/your_strength_as_a_rationalist/\">confused</a>. Bask in the rightness of the essay and the guilt/shame/foolishness/low-status that people who disagree with it should feel. If you gauge the dosage correctly, the propaganda might nudge your opinion just enough to make you actually adopt the new action that you felt would adequately reflect your new beliefs, but not enough to drive you over the cliff into madness.</p>\n<p>As an example, I recently became convinced that eating industrially raised animals while living in San Francisco before the apocalypse can't ever be morally justified, and, yet, lo and behold, I still ate turkey sandwiches at Subway 5 times a week. Obviously I could have just used some of the tactics from the <a href=\"/lw/1sm/akrasia_tactics_review/\">Akrasia Review</a> to make eating less factory-meat a conscious goal...but I'm busy using those tools for other goals, and I think that there are probably at least some contexts in which willpower is limited, or at least a variable-sum game. So I read Peter Singer's book on Animal Liberation, and blamed all the woes of the world on steak for a few hours while slowly eating a particularly foul-tasting beef stew that was ruined by some Thai hole-in-the-wall, to reinforce the message. I'm doing a little bit better...I'm more likely to cross the street and eat vegetarian or pay for the free-range stuff, and I'm down to about 3 Subway footlongs a week, without any noticeable decrease in my willpower reserves.</p>\n<p>Your mileage may vary; please use this tactic carefully.</p>\n<p>4) <em>Your suggestions.</em></p>\n<p>Seriously; most of my point in posting here is to gather more suggestions. If I thought of the three best solutions in two hours with no training, I'll eat my shirt. And I will, too -- it'll help me repent.</p>", "sections": [{"title": "Review:", "anchor": "Review_", "level": 1}, {"title": "The Problem:", "anchor": "The_Problem_", "level": 1}, {"title": "Solutions:", "anchor": "Solutions_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "154 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xvAkpCSdqgtYhEceo", "gPSCQC3WMeJnW9q4W", "9weLK2AJ9JEt2Tt8f", "CqyJzDZWvGhhFJ7dY", "ZQe33HWeFW3tSEQcx", "3XgYbghWruBMrPTAL", "buixYfcXBah9hbSNZ", "rRmisKb45dN7DK4BW", "22HfpjsydDS2A6JhH", "5JDkW4MYXit2CquLs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-14T21:21:55.020Z", "modifiedAt": null, "url": null, "title": "Note on Terminology: \"Rationality\", not \"Rationalism\"", "slug": "note-on-terminology-rationality-not-rationalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EbELiWfdE449DWYAk/note-on-terminology-rationality-not-rationalism", "pageUrlRelative": "/posts/EbELiWfdE449DWYAk/note-on-terminology-rationality-not-rationalism", "linkUrl": "https://www.lesswrong.com/posts/EbELiWfdE449DWYAk/note-on-terminology-rationality-not-rationalism", "postedAtFormatted": "Friday, January 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Note%20on%20Terminology%3A%20%22Rationality%22%2C%20not%20%22Rationalism%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANote%20on%20Terminology%3A%20%22Rationality%22%2C%20not%20%22Rationalism%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbELiWfdE449DWYAk%2Fnote-on-terminology-rationality-not-rationalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Note%20on%20Terminology%3A%20%22Rationality%22%2C%20not%20%22Rationalism%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbELiWfdE449DWYAk%2Fnote-on-terminology-rationality-not-rationalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbELiWfdE449DWYAk%2Fnote-on-terminology-rationality-not-rationalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>I feel that the term \"rationalism\", as opposed to \"rationality\", or \"study of rationality\", has <a href=\"/lw/4h/when_truth_isnt_enough/\">undesirable</a> connotations. My concerns are presented well by Eric Drexler in the article&nbsp;<a href=\"http://metamodern.com/2009/12/31/for-darwin%E2%80%99s-sake-reject-%E2%80%9Cdarwin-ism%E2%80%9D-and-other-pernicious-terms/\">For Darwin&rsquo;s sake, reject \"Darwin-<em>ism</em>\" (and other pernicious terms)</a>:</p>\n<blockquote>\n<p>To call something an &ldquo;ism&rdquo; suggests that it is a matter ideology or faith, like Trotskyism or creationism. In the evolution wars, the term &ldquo;evolutionism&rdquo; is used to insinuate that the modern understanding of the principles, mechanisms, and pervasive consequences of evolution is no more than the dogma of a sect within science. It creates a false equivalence between a mountain of knowledge and the emptiness called &ldquo;creationism&rdquo;.</p>\n</blockquote>\n<p>So, my suggestion is to use \"rationality\" consistently and to avoid using \"rationalism\". Via similarity to \"scientist\" and \"physicist\", \"rationalist\" doesn't seem to have the same problem. Discuss.</p>\n<p>(Typical usage on Less Wrong is this way already, <a href=\"http://www.google.com/#q=site:lesswrong.com+rationality\">3720</a> Google results for \"rationality\" and <a href=\"http://www.google.com/#q=site:lesswrong.com+rationalist\">1210</a> for \"rationalist\", against <a href=\"http://www.google.com/#q=site:lesswrong.com+rationalism\">251</a> for \"rationalism\". I've made this post as a reference for when someone uses \"rationalism\".)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "aa3Qg7Qrp9LM7QMaz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EbELiWfdE449DWYAk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 37, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "4873", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9hR2RmpJmxT8dyPo4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T00:45:28.621Z", "modifiedAt": null, "url": null, "title": "Peer review me", "slug": "peer-review-me", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tiiba", "createdAt": "2009-02-27T06:55:57.544Z", "isAdmin": false, "displayName": "Tiiba"}, "userId": "FngsS7fwH2r3ikxTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o9bmYTxEWFTDEoeGf/peer-review-me", "pageUrlRelative": "/posts/o9bmYTxEWFTDEoeGf/peer-review-me", "linkUrl": "https://www.lesswrong.com/posts/o9bmYTxEWFTDEoeGf/peer-review-me", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Peer%20review%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeer%20review%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo9bmYTxEWFTDEoeGf%2Fpeer-review-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Peer%20review%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo9bmYTxEWFTDEoeGf%2Fpeer-review-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo9bmYTxEWFTDEoeGf%2Fpeer-review-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>I wrote an article that I hoped to post on the main page, but then I got stage fright and was afraid to even put it here. So I guess I'm just going to show it to whichever of you is willing to review it privately.</p>\r\n<p>Any takers? Qualifications: must be a fan of Z-movies.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o9bmYTxEWFTDEoeGf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.675873428934904e-07, "legacy": true, "legacyId": "4874", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T16:47:33.255Z", "modifiedAt": null, "url": null, "title": "\"Bad-for-the-world-ipedia?\"", "slug": "bad-for-the-world-ipedia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cTZsgpdxvkZ7o4YMY/bad-for-the-world-ipedia", "pageUrlRelative": "/posts/cTZsgpdxvkZ7o4YMY/bad-for-the-world-ipedia", "linkUrl": "https://www.lesswrong.com/posts/cTZsgpdxvkZ7o4YMY/bad-for-the-world-ipedia", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Bad-for-the-world-ipedia%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Bad-for-the-world-ipedia%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcTZsgpdxvkZ7o4YMY%2Fbad-for-the-world-ipedia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Bad-for-the-world-ipedia%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcTZsgpdxvkZ7o4YMY%2Fbad-for-the-world-ipedia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcTZsgpdxvkZ7o4YMY%2Fbad-for-the-world-ipedia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>Lately I've been thinking about all of the various services and products I consume and how pretty much all of them are bad for the world in one way or another, large or small. Some of the problems associated with them I am less concerned about. Some of them could be construed as good things (i.e. sweat shop labor DOES provide jobs, whatever impact it might or might not have on the overall quality of life).</p>\n<p>In general I'd like to live my life having as minimal a negative impact on the world as possible. But \"negative impact\" is a hugely broad topic and there are a million variables to consider and I just don't have time.&nbsp;</p>\n<p>The best solution, I think, would be to have a wikipedia-like website where individual people with knowledge of specific problems can start tagging specific products with the types of negative consequences associated with them, and (somehow) sort those consequences into categories that individuals can decide how much to worry about. Over time it could eventually become a fairly efficient way to track the utility value of things.</p>\n<p>I'm sort of hoping something like this already exists, even if in an infant form, and that someone here knows about it. But I doubt it, so the I guess this falls mostly under the post category of \"hey someone other than me should devote a bunch of time and energy to this project that I myself am not qualified to do.\" But maybe a few people here at least have a better idea than I do of the scope of the requirements for it, so the idea can be refined a bit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cTZsgpdxvkZ7o4YMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "4895", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T19:01:32.749Z", "modifiedAt": null, "url": null, "title": "Image: Another uninformed perspective on risks from AI (humor)", "slug": "image-another-uninformed-perspective-on-risks-from-ai-humor", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:35.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ChusoceeA9fqsrLE8/image-another-uninformed-perspective-on-risks-from-ai-humor", "pageUrlRelative": "/posts/ChusoceeA9fqsrLE8/image-another-uninformed-perspective-on-risks-from-ai-humor", "linkUrl": "https://www.lesswrong.com/posts/ChusoceeA9fqsrLE8/image-another-uninformed-perspective-on-risks-from-ai-humor", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Image%3A%20Another%20uninformed%20perspective%20on%20risks%20from%20AI%20(humor)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImage%3A%20Another%20uninformed%20perspective%20on%20risks%20from%20AI%20(humor)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FChusoceeA9fqsrLE8%2Fimage-another-uninformed-perspective-on-risks-from-ai-humor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Image%3A%20Another%20uninformed%20perspective%20on%20risks%20from%20AI%20(humor)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FChusoceeA9fqsrLE8%2Fimage-another-uninformed-perspective-on-risks-from-ai-humor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FChusoceeA9fqsrLE8%2Fimage-another-uninformed-perspective-on-risks-from-ai-humor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<p>Here is another example of an outsider perspective on risks from AI. I think such examples can serve as a way to fathom the <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a> between the SIAI and its<span style=\"border-collapse: separate; color: #000000; font-family: 'Times New Roman'; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; font-size: medium;\"><span style=\"border-collapse: collapse; font-family: Arial,Verdana,sans-serif; font-size: 14px; text-align: left;\"> </span></span>target audience as to consequently fine tune their material and general approach.</p>\n<p><img src=\"http://3.bp.blogspot.com/_nIWiKIscZJY/TTHgqE-nyAI/AAAAAAAAC1g/8AXIZT3gFlo/s1600/20110114.png\" alt=\"\" width=\"492\" height=\"1600\" /></p>\n<p><sub>via <a href=\"http://www.sentientdevelopments.com/2011/01/when-robots-achieve-sentience-humor.html\">sentientdevelopments.com</a></sub></p>\n<p>This shows again that people are generally aware of potential risks but either do not take them seriously or don't see why risks from AI are <em><a href=\"http://wiki.lesswrong.com/wiki/Mind_design_space\">the rule</a></em> rather than an exception. So rather than making people aware that <em>there</em> <em>are </em>risks you have to tell them <em><a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">what are</a></em> the risks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ChusoceeA9fqsrLE8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 6.678688727211743e-07, "legacy": true, "legacyId": "4896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T19:46:25.253Z", "modifiedAt": null, "url": null, "title": "SMBC self-fulfilling prophecy", "slug": "smbc-self-fulfilling-prophecy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qsmi6kRNRbdQgPtie/smbc-self-fulfilling-prophecy", "pageUrlRelative": "/posts/qsmi6kRNRbdQgPtie/smbc-self-fulfilling-prophecy", "linkUrl": "https://www.lesswrong.com/posts/qsmi6kRNRbdQgPtie/smbc-self-fulfilling-prophecy", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SMBC%20self-fulfilling%20prophecy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASMBC%20self-fulfilling%20prophecy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqsmi6kRNRbdQgPtie%2Fsmbc-self-fulfilling-prophecy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SMBC%20self-fulfilling%20prophecy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqsmi6kRNRbdQgPtie%2Fsmbc-self-fulfilling-prophecy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqsmi6kRNRbdQgPtie%2Fsmbc-self-fulfilling-prophecy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>An AI should kill all humans if all humans <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2124\">expect that an AI will want to kill all humans</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qsmi6kRNRbdQgPtie", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -2, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4897", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T21:32:36.050Z", "modifiedAt": null, "url": null, "title": "People Neglect Who They Really Are When Predicting Their Own Future Happiness [link]", "slug": "people-neglect-who-they-really-are-when-predicting-their-own", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:25.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dreaded_Anomaly", "createdAt": "2010-12-30T06:38:34.106Z", "isAdmin": false, "displayName": "Dreaded_Anomaly"}, "userId": "sBHF4CXWBLakPFzfu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wbZ4xrNMdSxuhF27m/people-neglect-who-they-really-are-when-predicting-their-own", "pageUrlRelative": "/posts/wbZ4xrNMdSxuhF27m/people-neglect-who-they-really-are-when-predicting-their-own", "linkUrl": "https://www.lesswrong.com/posts/wbZ4xrNMdSxuhF27m/people-neglect-who-they-really-are-when-predicting-their-own", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20People%20Neglect%20Who%20They%20Really%20Are%20When%20Predicting%20Their%20Own%20Future%20Happiness%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeople%20Neglect%20Who%20They%20Really%20Are%20When%20Predicting%20Their%20Own%20Future%20Happiness%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbZ4xrNMdSxuhF27m%2Fpeople-neglect-who-they-really-are-when-predicting-their-own%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=People%20Neglect%20Who%20They%20Really%20Are%20When%20Predicting%20Their%20Own%20Future%20Happiness%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbZ4xrNMdSxuhF27m%2Fpeople-neglect-who-they-really-are-when-predicting-their-own", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbZ4xrNMdSxuhF27m%2Fpeople-neglect-who-they-really-are-when-predicting-their-own", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p><a href=\"http://www.sciencedaily.com/releases/2011/01/110112110436.htm\">People Neglect Who They Really Are When Predicting Their Own Future Happiness (article @ ScienceDaily)</a></p>\n<p>The scientists who conducted this interesting study...</p>\n<blockquote>\n<p>found that our natural sunny or negative dispositions might be a more powerful predictor of future happiness than any specific event. They also discovered that most of us ignore our own personalities when we think about what lies ahead -- and thus miscalculate our future feelings.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wbZ4xrNMdSxuhF27m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.679079814527393e-07, "legacy": true, "legacyId": "4898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T23:03:14.552Z", "modifiedAt": null, "url": null, "title": "Why is consistency often considered to be an intellectual virtue?", "slug": "why-is-consistency-often-considered-to-be-an-intellectual", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:21.483Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CEcdbmaztS7kNpPkh/why-is-consistency-often-considered-to-be-an-intellectual", "pageUrlRelative": "/posts/CEcdbmaztS7kNpPkh/why-is-consistency-often-considered-to-be-an-intellectual", "linkUrl": "https://www.lesswrong.com/posts/CEcdbmaztS7kNpPkh/why-is-consistency-often-considered-to-be-an-intellectual", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20is%20consistency%20often%20considered%20to%20be%20an%20intellectual%20virtue%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20is%20consistency%20often%20considered%20to%20be%20an%20intellectual%20virtue%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCEcdbmaztS7kNpPkh%2Fwhy-is-consistency-often-considered-to-be-an-intellectual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20is%20consistency%20often%20considered%20to%20be%20an%20intellectual%20virtue%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCEcdbmaztS7kNpPkh%2Fwhy-is-consistency-often-considered-to-be-an-intellectual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCEcdbmaztS7kNpPkh%2Fwhy-is-consistency-often-considered-to-be-an-intellectual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 575, "htmlBody": "<p>Now, of course, it's important for a scientific theory to be consistent across all cases in its domain. Otherwise, the theory is useless. Of course, we can specify the preconditions for consistency, and we may be able to argue for a expansion (or reduction) in its domain as time goes on.</p>\n<p>But what about behavioral consistency? Certainly, behavioral consistency makes it easier for people to predict what you'll do in the future. So people who are behaviorally consistent are easier to trust, so to speak.</p>\n<p>But what if we want to be right? If we want to be consistent, we have to stick with a certain behavior and *assume* that the \"utility\" of holding to that behavior will be stable across time. But sometimes, we will have special cases where the \"utility\" from deviation will be greater than the \"utility\" from non-deviation (in many of these special cases, the optimal situation may be one where you hide that special case from the eyes of everyone else, maybe in the interests of \"fairness\" - since many people get outraged by violations in \"fairness\"). Of course, we can specify what these special cases are beforehand (for example, you may require a security clearance for access to certain types of information, or a license to create certain types of drugs). But we cannot reliably predict each and every one of these special cases (there may be cases, for example, where you would increase \"utility\" if you gave out information X to someone who didn't have clearance, for example)</p>\n<p>You could call those \"double standards\". People will often accuse you of hypocrisy if you're inconsistent, but you could always guard yourself against those accusations by introducing double standards. Whether those double standards are defensible or not - that, of course, depends on other factors. There are many behaviors, for example, that one may have to prohibit the entire population from pursuing. But some of these behaviors can be responsibly pursued by a small subset of the population (the only problem is that if you use that phrase, potentially irresponsible people will pursue these behaviors, as people are prone to overestimate their abilities to self-regulate themselves).</p>\n<p>Now, in terms of beliefs, some of these double standards come with updated information. You might say that \"action X is 'bad'\" across all possible cases, but then update your belief and say that \"action X is 'bad'\" in most cases, with a few exceptions. If you do that, some people may accuse you of straying from consistency. And intellectually, the most desirable option is to refrain from making blanket statements such as \"action X is 'bad' across all possible cases\". But in terms of consequence, blanket statements are easier to enforce than non-blanket statements, and blanket statements also have a psychological effect that non-blanket statements do not have (it is very easy to forget about the non-blanket statements). As a book from Simonton (2009) once said, the most famous psychologists (in history) are not the ones who are necessarily right, but those who held extreme views. Now, of course, fame is independent of being \"less wrong\". But at the same time, if you want to change the world (or \"increase utility\"), you have to have some attention to yourself. Furthermore, explaining the \"exceptions to the rule\" is often tl;dr to most people. And people might be less inclined to trust you if you keep updating your views (especially if you try to assign probability values to beliefs)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CEcdbmaztS7kNpPkh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.679312962763276e-07, "legacy": true, "legacyId": "4899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T23:03:25.044Z", "modifiedAt": null, "url": null, "title": "Which are the useful areas of AI study?", "slug": "which-are-the-useful-areas-of-ai-study", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PeterisP", "createdAt": "2010-10-24T12:04:42.682Z", "isAdmin": false, "displayName": "PeterisP"}, "userId": "W6xx9mM4WsMEoQvoc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WsBHfGdtbcEjxbNPk/which-are-the-useful-areas-of-ai-study", "pageUrlRelative": "/posts/WsBHfGdtbcEjxbNPk/which-are-the-useful-areas-of-ai-study", "linkUrl": "https://www.lesswrong.com/posts/WsBHfGdtbcEjxbNPk/which-are-the-useful-areas-of-ai-study", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20are%20the%20useful%20areas%20of%20AI%20study%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20are%20the%20useful%20areas%20of%20AI%20study%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWsBHfGdtbcEjxbNPk%2Fwhich-are-the-useful-areas-of-ai-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20are%20the%20useful%20areas%20of%20AI%20study%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWsBHfGdtbcEjxbNPk%2Fwhich-are-the-useful-areas-of-ai-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWsBHfGdtbcEjxbNPk%2Fwhich-are-the-useful-areas-of-ai-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>&nbsp;&nbsp;I'm stuck wondering on a peculiar question lately - which are the useful areas of AI study?&nbsp;What got me thinking is the opinion occasionally stated (or implied) by Eliezer here that performing general AI research might likely have negative utility, due to indirectly facilitating a chance of unfriendly AI being developed. I've been chewing on the implications of this for quite a while, as acceptance of these arguments would require quite a change in my behavior.</p>\n<p>&nbsp;</p>\n<p>Right now I'm about to start my CompSci PhD studies soon, and had initially planned to focus on unsupervised domain-specific knowledge extraction from the internet, as my current research background is mostly with narrow AI issues in computational linguistics, such as machine-learning, formation of concepts and semantics extraction. However, in the last year my expectations of singularity and existential risks of unfriendly AI have lead me to believe that focusing my efforts on Friendly AI concepts would be a more valuable choice; as a few years of studies in the area would increase the chance of me making some positive contribution later on.</p>\n<p>&nbsp;</p>\n<p>What is your opinion?</p>\n<p>Do studies of general AI topics and research in the area carry a positive or negative utility ? What are the research topics that would be of use to Friendly AI, but still are narrow and shallow enough to make some measurable progress by a single individual/tiny team in the course of a few years of PhD thesis preparation?&nbsp;Are there specific research areas that should be better avoided until&nbsp;more progress has been made on&nbsp;Friendliness research ?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WsBHfGdtbcEjxbNPk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.679310836005998e-07, "legacy": true, "legacyId": "4900", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-15T23:05:17.734Z", "modifiedAt": null, "url": null, "title": "Link: Edge: THE WORLD QUESTION CENTER 2011: \u201cWhat Scientific Concept Would Improve Everybody\u2019s Cognitive Toolkit? (edge.org)", "slug": "link-edge-the-world-question-center-2011-what-scientific", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2xNkYYtAZgYXZFdmG/link-edge-the-world-question-center-2011-what-scientific", "pageUrlRelative": "/posts/2xNkYYtAZgYXZFdmG/link-edge-the-world-question-center-2011-what-scientific", "linkUrl": "https://www.lesswrong.com/posts/2xNkYYtAZgYXZFdmG/link-edge-the-world-question-center-2011-what-scientific", "postedAtFormatted": "Saturday, January 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Edge%3A%20THE%20WORLD%20QUESTION%20CENTER%202011%3A%20%E2%80%9CWhat%20Scientific%20Concept%20Would%20Improve%20Everybody%E2%80%99s%20Cognitive%20Toolkit%3F%20(edge.org)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Edge%3A%20THE%20WORLD%20QUESTION%20CENTER%202011%3A%20%E2%80%9CWhat%20Scientific%20Concept%20Would%20Improve%20Everybody%E2%80%99s%20Cognitive%20Toolkit%3F%20(edge.org)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xNkYYtAZgYXZFdmG%2Flink-edge-the-world-question-center-2011-what-scientific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Edge%3A%20THE%20WORLD%20QUESTION%20CENTER%202011%3A%20%E2%80%9CWhat%20Scientific%20Concept%20Would%20Improve%20Everybody%E2%80%99s%20Cognitive%20Toolkit%3F%20(edge.org)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xNkYYtAZgYXZFdmG%2Flink-edge-the-world-question-center-2011-what-scientific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xNkYYtAZgYXZFdmG%2Flink-edge-the-world-question-center-2011-what-scientific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p><span style=\"color: #336699;\"><a href=\"http://www.edge.org/q2011/q11_1.html \">http://www.edge.org/q2011/q11_1.html</a></span></p>\n<p>It just got out. I've read this series for 5 years by now, and I've noticed many interesting quotes about rationality on that site. Feel free to post the ones you liked the most in this thread. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2xNkYYtAZgYXZFdmG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 6.679318243606924e-07, "legacy": true, "legacyId": "4901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-16T01:56:20.152Z", "modifiedAt": null, "url": null, "title": "UFAI Web Comic", "slug": "ufai-web-comic", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:20.401Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SRStarin", "createdAt": "2010-12-17T15:04:05.504Z", "isAdmin": false, "displayName": "SRStarin"}, "userId": "ww27zvssCFWLZXRpt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SHKMKcusjYn6TKCMo/ufai-web-comic", "pageUrlRelative": "/posts/SHKMKcusjYn6TKCMo/ufai-web-comic", "linkUrl": "https://www.lesswrong.com/posts/SHKMKcusjYn6TKCMo/ufai-web-comic", "postedAtFormatted": "Sunday, January 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20UFAI%20Web%20Comic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUFAI%20Web%20Comic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSHKMKcusjYn6TKCMo%2Fufai-web-comic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=UFAI%20Web%20Comic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSHKMKcusjYn6TKCMo%2Fufai-web-comic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSHKMKcusjYn6TKCMo%2Fufai-web-comic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>I realize just linking to something else is not the norm here. Still, folks here interested in U/FAI will probably appreciate this <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2124#comic\">comic.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SHKMKcusjYn6TKCMo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -7, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "4903", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-16T07:12:54.643Z", "modifiedAt": null, "url": null, "title": "Trying to hide bad signaling? To the Dark Side, lead you it will. ", "slug": "trying-to-hide-bad-signaling-to-the-dark-side-lead-you-it", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.942Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CwN3rCNZ54CCYnYNG/trying-to-hide-bad-signaling-to-the-dark-side-lead-you-it", "pageUrlRelative": "/posts/CwN3rCNZ54CCYnYNG/trying-to-hide-bad-signaling-to-the-dark-side-lead-you-it", "linkUrl": "https://www.lesswrong.com/posts/CwN3rCNZ54CCYnYNG/trying-to-hide-bad-signaling-to-the-dark-side-lead-you-it", "postedAtFormatted": "Sunday, January 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trying%20to%20hide%20bad%20signaling%3F%20To%20the%20Dark%20Side%2C%20lead%20you%20it%20will.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrying%20to%20hide%20bad%20signaling%3F%20To%20the%20Dark%20Side%2C%20lead%20you%20it%20will.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwN3rCNZ54CCYnYNG%2Ftrying-to-hide-bad-signaling-to-the-dark-side-lead-you-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trying%20to%20hide%20bad%20signaling%3F%20To%20the%20Dark%20Side%2C%20lead%20you%20it%20will.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwN3rCNZ54CCYnYNG%2Ftrying-to-hide-bad-signaling-to-the-dark-side-lead-you-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwN3rCNZ54CCYnYNG%2Ftrying-to-hide-bad-signaling-to-the-dark-side-lead-you-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 431, "htmlBody": "<p><strong>Edit:</strong> This is<strong> </strong><a href=\"http://lesswrong.com/r/discussion/lw/eqa/open_thread_october_115_2012/7l85\">old material</a>. It may be out of date.<a href=\"http://lesswrong.com/r/discussion/lw/eqa/open_thread_october_115_2012/7l85\"><br /></a></p>\n<p>Or is that just a point of view?</p>\n<p>I'm going to assume familiary with the common use of the following two terms on this site:</p>\n<ul>\n<li><a title=\"Dark Arts\" href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Dark Arts</a></li>\n<li><a title=\"Signaling\" href=\"http://wiki.lesswrong.com/wiki/Signaling\">Signaling</a></li>\n</ul>\n<p>Otherwise don't worry, I've hanged out here for ages and I still need to update my cache of terms quite often. If you have questions about either after reading the wiki please feel free to ask since there are people much more knowledgeable than me that will probably answer them. I don't know if other users agree, but the Discussion section seems like the best place to ask questions that might have been already covered elsewhere for people who have trouble despite extensive study, in a way this OP is basically an example of this.</p>\n<p>I'm also making the following assumptions:</p>\n<ol>\n<li>People are more often rational than otherwise when the rational answer happens to say good things about them.</li>\n<li>I hope people here agree that learning to be more rational will necessarily at least in some areas change your beliefs (unlikely any one person is right about everything). </li>\n<li>If one hasn't changed any beliefs its likely that they haven't employed rationality where it would do them the most good. </li>\n<li>People are better at defending or promoting a position when they think its true. </li>\n<li>From the above points it seems to follow that people who become more rational than average will send also more bad signals than they would if they hadn't become more rational. </li>\n<li>Most people would try to avoid 5., it represents a disincentive for becoming more rational.&nbsp; </li>\n</ol>\n<p>The main question of this thread:</p>\n<p><strong>How can one work around 5. without employing Dark Arts to sanitize the feelings accompanying a conclusion?</strong> Is it even possible? Can or should we talk about this and try to find and catalogue ways to do this since many of us are not skilled at social interactions (higher than average self identified non-neurotypicals visit LW)?</p>\n<p>&nbsp;</p>\n<p><strong>Notes:</strong></p>\n<p>- I also wish to emphasise that not only do some conclusions send bad signals, wanting to open *some* topics to rational inquiry in itself often sends bad signals even if you do eventually end up with a conclusion that sends good signals.</p>\n<p>- I feel that, even if it isn't possible to hide bad signalling, the better map of reality one enjoys will off set these costs in other ways. Despite this, considering we are social animals I think many people would like to avoid this particular cost quite strongly, myself included.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CwN3rCNZ54CCYnYNG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.680570157169143e-07, "legacy": true, "legacyId": "4904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-16T08:30:57.953Z", "modifiedAt": "2022-04-17T15:30:34.894Z", "url": null, "title": "The Best Textbooks on Every Subject", "slug": "the-best-textbooks-on-every-subject", "viewCount": null, "lastCommentedAt": "2022-04-17T15:29:19.110Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lukeprog", "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject", "pageUrlRelative": "/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject", "linkUrl": "https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject", "postedAtFormatted": "Sunday, January 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Best%20Textbooks%20on%20Every%20Subject&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Best%20Textbooks%20on%20Every%20Subject%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxg3hXCYQPJkwHyik2%2Fthe-best-textbooks-on-every-subject%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Best%20Textbooks%20on%20Every%20Subject%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxg3hXCYQPJkwHyik2%2Fthe-best-textbooks-on-every-subject", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxg3hXCYQPJkwHyik2%2Fthe-best-textbooks-on-every-subject", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2161, "htmlBody": "<p>For years, my self-education was stupid and wasteful. I learned by consuming blog posts, Wikipedia articles, classic texts, podcast episodes, popular books, <a href=\"http://academicearth.org/\">video lectures</a>, peer-reviewed papers, <a href=\"http://www.teach12.com/\">Teaching Company</a> courses, and Cliff's Notes. How inefficient!</p>\n<p>I've since discovered that <em>textbooks</em> are usually the quickest and best way to learn new material. That's what they are <em>designed</em> to be, after all. Less Wrong <a href=\"/lw/2xt/learning_the_foundations_of_math/\">has</a> <a href=\"/lw/ow/the_beauty_of_settled_science/\">often</a> <a href=\"/lw/jv/recommended_rationalist_reading/fcg?c=1\">recommended</a> the \"read textbooks!\" method. <a href=\"/lw/1ul/for_progress_to_be_by_accumulation_and_not_by/\">Make progress by accumulation, not random walks</a>.</p>\n<p>But textbooks vary widely in quality. I was forced to read some awful textbooks in college. The ones on American history and sociology were memorably bad, in my case. Other textbooks are exciting, accurate, fair, well-paced, and immediately useful.</p>\n<p>What if we could compile a list of the best textbooks on every subject? That would be <em>extremely</em> useful.</p>\n<p>Let's do it.</p>\n<p>There have been <a href=\"/lw/jv/recommended_rationalist_reading/\">other</a> <a href=\"/lw/12d/recommended_reading_for_new_rationalists/\">pages</a> of <a href=\"/lw/2un/references_resources_for_lesswrong/\">recommended</a> <a href=\"/lw/2xt/learning_the_foundations_of_math/\">reading</a> on Less Wrong before (and <a href=\"http://ask.metafilter.com/71101/What-single-book-is-the-best-introduction-to-your-field-or-specialization-within-your-field-for-laypeople\">elsewhere</a>), but this post is unique. <a name=\"rules\"></a>Here are <strong>the rules</strong>:</p>\n<ol>\n<li>Post the title of your favorite textbook on a given subject.</li>\n<li>You must have read at least two other textbooks on that same subject.</li>\n<li>You must briefly name the other books you've read on the subject and explain why you think your chosen textbook is superior to them.</li>\n</ol>\n<p>Rules #2 and #3 are to protect against recommending a bad book that only seems impressive because it's the only book you've read on the subject. Once, a popular author on Less Wrong recommended Bertrand Russell's <em>A History of Western Philosophy</em>&nbsp;to me, but when I noted that it was more polemical and inaccurate than the other major histories of philosophy, he admitted he hadn't really done much other reading in the field, and only liked the book because it was exciting.</p>\n<p>I'll start the list with three of my own recommendations...</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><strong><a name=\"history_philosophy\"></a>Subject</strong>: History of Western Philosophy</p>\n<p><strong>Recommendation</strong>: <em><a href=\"http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Great Conversation</a></em>, 6th edition, by Norman Melchert</p>\n<p><strong>Reason</strong>: The most popular history of western philosophy is Bertrand Russell's <em><a href=\"http://www.amazon.com/History-Western-Philosophy-Bertrand-Russell/dp/0671201581/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A History of Western Philosophy</a></em>, which is exciting but also polemical and <a href=\"http://www.the-philosopher.co.uk/reviews/brussel.htm\">inaccurate</a>. More accurate but dry and dull is Frederick Copelston's 11-volume <em><a href=\"http://en.wikipedia.org/wiki/A_History_of_Philosophy_(Copleston)\">A History of Philosophy</a></em>. Anthony Kenny's recent 4-volume history, collected into one book as <em><a href=\"http://www.amazon.com/New-History-Western-Philosophy/dp/0199589887/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A New History of Western Philosophy</a></em>, is both exciting and accurate, but perhaps too long (1000 pages) and technical for a first read on&nbsp;the history of philosophy. Melchert's textbook, <em><a href=\"http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Great Conversation</a></em>,&nbsp;is accurate but also the easiest to read, and has the clearest explanations of the important positions and debates, though of course it has its weaknesses (it spends too many pages on ancient Greek mythology but barely mentions Gottlob Frege, the father of analytic philosophy and of the philosophy of language). Melchert's history is also the only one to seriously cover the dominant mode of Anglophone philosophy done today: <a href=\"http://www.amazon.com/Understanding-Naturalism-Movements-Modern-Thought/dp/1844650790/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">naturalism</a> (what Melchert calls \"physical realism\"). Be sure to get the 6th edition, which has major improvements over the 5th edition.</p>\n<p>&nbsp;</p>\n<p><strong><a name=\"cognitive_science\"></a>Subject</strong>: Cognitive Science</p>\n<p><strong>Recommendation</strong>: <em><a href=\"http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Cognitive Science</a></em>, by Jose Luis Bermudez</p>\n<p><strong>Reason</strong>: Jose Luis Bermudez's&nbsp;<em><a href=\"http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Cognitive Science: An Introduction to the Science of Mind</a></em>&nbsp;does an excellent job setting the historical and conceptual context for cognitive science, and draws fairly from all the fields involved in this heavily interdisciplinary science. Bermudez does a good job of making himself invisible, and the explanations here are some of the clearest available. In contrast,&nbsp;Paul Thagard's <em><a href=\"http://www.amazon.com/Mind-Introduction-Cognitive-Science-2nd/dp/026270109X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Mind: Introduction to Cognitive Science</a></em>&nbsp;skips the context and jumps right into a systematic comparison (by explanatory merit) of the leading theories of mental representation: logic, rules, concepts, analogies, images, and neural networks. The book is only 270 pages long, and is also more idiosyncratic than Bermudez's; for example, Thagard refers to the dominant paradigm in cognitive science as the \"computational-representational understanding of mind,\" which as far as I can tell is used only by him and people drawing from his book. In truth, the term refers to a set of competing theories, for example the <a href=\"http://en.wikipedia.org/wiki/Computational_theory_of_mind\">computational theory</a> and the <a href=\"http://en.wikipedia.org/wiki/Representational_theory_of_mind\">representational theory</a>. While not the best place to start, Thagard's book is a decent follow-up to Bermudez's text. Better, though, is Kolak et. al.'s <em><a href=\"http://www.amazon.com/Cognitive-Science-Introduction-Mind-Brain/dp/0415221013/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Cognitive Science: An Introduction to Mind and Brain</a></em>. It contains more information than Bermudez's book, but I prefer Bermudez's flow, organization and content selection. Really, though, both Bermudez and Kolak offer excellent introductions to the field, and Thagard offers a more systematic and narrow investigation that is worth reading after Bermudez and Kolak.</p>\n<p>&nbsp;</p>\n<p><strong><a name=\"logic\"></a>Subject</strong>: Introductory Logic for Philosophy</p>\n<p><strong>Recommendation</strong>: <em><a href=\"http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Meaning and Argument</a></em>&nbsp;by Ernest Lepore</p>\n<p><strong>Reason</strong>: For years, the standard textbook on logic was Copi's <em><a href=\"http://www.amazon.com/Introduction-Logic-13th-Irving-Copi/dp/0136141390/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Introduction to Logic</a></em>, a comprehensive textbook that has chapters on language, definitions, fallacies, deduction, induction, syllogistic logic, symbolic logic, inference, and probability. It spends too much time on methods that are rarely used today, for example Mill's methods of inductive inference. Amazingly, the chapter on probability does not mention Bayes (as of the 11th edition, anyway). Better is the&nbsp;current standard in classrooms: Patrick Hurley's&nbsp;<em style=\"font-style: italic;\"><a href=\"http://www.amazon.com/Concise-Introduction-Logic-CourseCard/dp/0840034172/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A Concise Introduction to Logic</a>.</em>&nbsp;It has a table at the front of the book that tells you which sections to read depending on whether you want (1) a traditional logic course, (2) a critical reasoning course, or (3) a course on modern formal logic. The single chapter on induction and probability moves too quickly, but is excellent for its length. Peter Smith's&nbsp;<a style=\"font-style: italic;\" href=\"http://www.amazon.com/gp/product/0521008042/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">An Introduction to Formal Logic</a>&nbsp;instead focuses tightly on the usual methods used by today's philosophers: propositional logic and predicate logic. My favorite in this less comprehensive mode, however, is Ernest Lepore's <em><a href=\"http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Meaning and Argument</a></em>, because it (a) is highly efficient, and (b) focuses not so much on the manipulation of symbols in a formal system but on the arguably trickier matter of translating English sentences into symbols in a formal system in the first place.</p>\n<p>&nbsp;</p>\n<p>I would love to read recommendations from experienced readers on the following subjects: physics, chemistry, biology, psychology, sociology, probability theory, economics, statistics, calculus, decision theory, cognitive biases, artificial intelligence, neuroscience, molecular biochemistry, medicine, epistemology, philosophy of science, meta-ethics, and much more.</p>\n<p>Please, post your own recommendations! And, follow <a href=\"#rules\">the rules</a>.</p>\n<p>&nbsp;</p>\n<p><strong>Recommendations so far</strong> (that follow <a href=\"#rules\">the rules</a>; this list updated 02-25-2017):</p>\n<ul>\n<li>On <strong>history of western philosophy</strong>, lukeprog <a href=\"#history_philosophy\">recommends</a>&nbsp;Melchert's <em><a href=\"http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Great Conversation</a></em>&nbsp;over Russell's <em>A History of Western Philosophy</em>, Copelston's <em>History of Philosophy</em>, and Kenney's <em>A New History of Western Philosophy</em>.</li>\n<li>On <strong>cognitive science</strong>, lukeprog <a href=\"#cognitive_science\">recommends</a> Bermudez's <em><a href=\"http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Cognitive Science</a></em>&nbsp;over Thagard's <em>Mind: Introduction to Cognitive Science</em>&nbsp;and Kolak's <em>Cognitive Science</em>.</li>\n<li>On <strong>introductory logic for philosophy</strong>, lukeprog <a href=\"#logic\">recommends</a> Lepore's <em><a href=\"http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Meaning and Argument</a></em>&nbsp;over Copi's <em>Introduction to Logic</em>, Hurley's <em>A Concise Introduction to Logic</em>, and Smith's <em>An Introduction to Formal Logic</em>.</li>\n<li>On <strong>economics</strong>, michaba03m <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cd9\">recommends</a> Mankiw's <em><a href=\"http://www.amazon.com/dp/1429218878/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Macroeconomics</a></em> over Varian's <em>Intermediate Microeconomics</em> and Katz &amp; Rosen's <em>Macroeconomics</em>.</li>\n<li>On <strong>economics</strong>, realitygrill <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3coo\">recommends</a>&nbsp;McAfee's <em><a href=\"https://open.umn.edu/opentextbooks/textbooks/47\">Introduction to Economic Analysis</a></em>&nbsp;over Mankiw's <em>Principles of Microeconomics</em>&nbsp;and Case &amp; Fair's <em>Principles of Macroeconomics</em>.</li>\n<li>On <strong>representation theory</strong>, SarahC <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cez\">recommends</a> Sternberg's <em><a href=\"http://www.amazon.com/Group-Theory-Physics-S-Sternberg/dp/0521558859/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Group Theory and Physics</a></em>&nbsp;over Lang's <em>Algebra</em>, Weyl's <em>The Theory of Groups and Quantum Mechanics</em>, and Fulton &amp; Harris' <em>Representation Theory: A First Course</em>.</li>\n<li>On <strong>statistics</strong>, madhadron&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject/64mz\">recommends</a> Kiefer's&nbsp;<em><a href=\"http://www.amazon.com/Introduction-Statistical-Inference-Springer-Statistics/dp/0387964207/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Introduction to Statistical Inference</a></em> over Hogg &amp; Craig's <em>Introduction to Mathematical Statistics</em>, Casella &amp; Berger's <em>Statistical Inference</em>, and others.</li>\n<li>On <strong>advanced Bayesian statistics</strong>, Cyan <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cg2\">recommends</a>&nbsp;Gelman's <em><a href=\"http://www.amazon.com/dp/158488388X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Bayesian Data Analysis</a></em>&nbsp;over Jaynes' <em>Probability Theory: The Logic of Science</em>&nbsp;and Bernardo's <em>Bayesian Theory</em>.</li>\n<li>On <strong>basic Bayesian statistics</strong>, jsalvatier <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3clc\">recommends</a>&nbsp;Skilling &amp; Sivia's <em><a href=\"http://www.amazon.com/Data-Analysis-Bayesian-Tutorial-ebook/dp/B001E5II36/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Data Analysis: A Bayesian Tutorial</a></em>&nbsp;over Gelman's <em>Bayesian Data Analysis</em>, Bolstad's <em>Bayesian Statistics</em>, and Robert's <em>The Bayesian Choice</em>.</li>\n<li>On <strong>real analysis</strong>, paper-machine <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cll\">recommends</a>&nbsp;Bartle's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/Modern-Integration-Graduate-Studies-Mathematics/dp/0821808451/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A Modern Theory of Integration</a>&nbsp;over Rudin's <em>Real and Complex Analysis</em>&nbsp;and Royden's <em>Real Analysis</em>.</li>\n<li>On <strong>non-relativistic quantum mechanics</strong>, wbcurry <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cmj\">recommends</a> Sakurai &amp; Napolitano's <em><a href=\"http://www.amazon.com/Modern-Quantum-Mechanics-2nd-Sakurai/dp/0805382917/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Modern Quantum Mechanics</a></em>&nbsp;over Messiah's <em>Quantum Mechanics</em>,&nbsp;Cohen-Tannoudji's <em>Quantum Mechanics</em>, and&nbsp;Greiner's <em>Quantum Mechanics: An Introduction</em>.</li>\n<li>On <strong>music theory</strong>, komponisto <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cmp\">recommends</a> Westergaard's <em><a href=\"http://www.amazon.com/Introduction-Tonal-Theory-Peter-Westergaard/dp/0393093425/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">An Introduction to Tonal Theory</a></em>&nbsp;over Piston's <em>Harmony</em>, Aldwell and Schachter's <em>Harmony and Voice Leading</em>, and&nbsp;Kotska and Payne's <em>Tonal Harmony</em>.</li>\n<li>On <strong>business</strong>, joshkaufman <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cny\">recommends</a> Kaufman's <em><a href=\"http://www.amazon.com/gp/product/1591843529/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Personal MBA: Master the Art of Business</a></em>&nbsp;over Bevelin's <em>Seeking Wisdom</em>&nbsp;and Munger's <em>Poor Charlie's Alamanack</em>.</li>\n<li>On <strong>machine learning</strong>, alexflint <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cp0\">recommends</a>&nbsp;Bishop's&nbsp;<em><a href=\"http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Pattern Recognition and Machine Learning</a></em> over Russell &amp; Norvig's <em>Artificial Intelligence: A Modern Approach</em>&nbsp;and Thrun et. al.'s <em>Probabilistic Robotics</em>.</li>\n<li>On <strong>algorithms</strong>, gjm <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cpz\">recommends</a>&nbsp;Cormen et. al.'s <em><a href=\"http://www.amazon.com/Introduction-Algorithms-Third-Thomas-Cormen/dp/0262033844/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Introduction to Algorithms</a></em>&nbsp;over Knuth's <em>The Art of Computer Programming</em>&nbsp;and Sedgwick's <em>Algorithms</em>.</li>\n<li>On <strong>electrodynamics</strong>, Alex_Altair <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cr1\">recommends</a>&nbsp;Griffiths' <em><a href=\"http://www.amazon.com/Introduction-Electrodynamics-3rd-David-Griffiths/dp/013805326X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Introduction to Electrodynamics</a></em>&nbsp;over Jackson's <em>Electrodynamics</em>&nbsp;and Feynman's <em>Lectures on Physics</em>.</li>\n<li>On <strong>electrodynamics</strong>, madhadron&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject/64mz\">recommends</a> Purcell's <em><a href=\"http://www.amazon.com/Electricity-Magnetism-Edward-Purcell/dp/1107013607/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Electricity and Magnetism</a></em>&nbsp;over Griffith's <em>Introduction to Electrodynamics</em>, Feynman's <em>Lectures on Physics</em>, and others.</li>\n<li>On <strong>systems theory</strong>, Davidmanheim <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3ctm\">recommends</a>&nbsp;Meadows' <em><a href=\"http://www.amazon.com/Thinking-Systems-Donella-H-Meadows/dp/1603580557/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Thinking in Systems: A Primer</a></em>&nbsp;over Senge's&nbsp;<em>The Fifth Discipline: The Art &amp; Practice of The Learning Organization</em> and Kim's&nbsp;<em>Introduction to Systems Thinking</em>.</li>\n<li>On&nbsp;<strong>self-help</strong>, lukeprog <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3dcq\">recommends</a> Weiten, Dunn, and Hammer's <em><a href=\"http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life</a></em>&nbsp;over Santrock's <em>Human Adjustment</em>&nbsp;and Tucker-Ladd's <em>Psychological Self-Help</em>.</li>\n<li>On <strong>probability theory</strong>, SarahC <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3f00\">recommends</a>&nbsp;Feller's <em><a href=\"http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257087/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">An Introduction to Probability Theory</a></em>&nbsp;+ <em><a href=\"http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257095/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Vol. 2</a></em> over Ross' <em>A First Course in Probability</em>&nbsp;and Koralov &amp; Sinai's <em>Theory of Probability and Random Processes</em>.</li>\n<li>On <strong>probability theory</strong>, madhadron <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/64mz\">recommends</a>&nbsp;Grimmett &amp; Stirzaker's <em><a href=\"http://www.amazon.com/Probability-Random-Processes-Geoffrey-Grimmett/dp/0198572220/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Probability and Random Processes</a></em>&nbsp;over Feller's <em>Introduction to Probability Theory and Its Applications</em>&nbsp;and Nelson's&nbsp;<em>Radically Elementary Probability Theory</em>.</li>\n<li>On <strong>topology</strong>, jsteinhardt <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3ff2\">recommends</a>&nbsp;Munkres' <em><a href=\"http://www.amazon.com/Topology-2nd-James-Munkres/dp/0131816292/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Topology</a></em>&nbsp;over Armstrong's <em>Topology</em>&nbsp;and Massey's <em>Algebraic Topology</em>.</li>\n<li>On <strong>linguistics</strong>, etymologik <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3fs1\">recommends</a> O'Grady et al.'s <em><a href=\"http://www.amazon.com/Contemporary-Linguistics-William-OGrady/dp/0312555288/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Contemporary Linguistics</a></em>&nbsp;over Hayes et al.'s <em>Linguistics: An Introduction to Linguistic Theory</em>&nbsp;and Carnie's <em>Syntax: A Generative Introduction</em>.</li>\n<li>On <strong>meta-ethics</strong>, lukeprog <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3nvo\">recommends</a> Miller's <em><a href=\"http://www.amazon.com/Introduction-Contemporary-Metaethics-Alex-Miller/dp/074562345X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">An Introduction to Contemporary Metaethics</a></em>&nbsp;over Jacobs' <em>The Dimensions of Moral Theory</em>&nbsp;and Smith's <em>Ethics and the A Priori</em>.</li>\n<li>On <strong>decision-making &amp; biases</strong>, badger <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/490n\">recommends</a> Bazerman &amp; Moore's <em><a href=\"http://www.amazon.com/Judgment-Managerial-Decision-Making-Bazerman/dp/0470049456/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Judgment in Managerial Decision Making</a></em>&nbsp;over Hastie &amp; Dawes' <em>Rational Choice in an Uncertain World</em>, Gilboa's <em>Making Better Decisions</em>, and others.</li>\n<li>On <strong>neuroscience</strong>, kjmiller <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/4zqx\">recommends</a> Bear et al's <em>Neuroscience: Exploring the Brain</em>&nbsp;over Purves et al's&nbsp;<em>Neuroscience</em>&nbsp;and Kandel et al's <em>Principles of Neural Science</em>.</li>\n<li>On <strong>World War II</strong>, Peacewise <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/558w\">recommends</a> Weinberg's <em><a href=\"http://www.amazon.com/World-Arms-Global-History-War/dp/0521618266/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A World at Arms</a></em>&nbsp;over Churchill's <em>The Second World War</em> and Day's <em>The Politics of War</em>.</li>\n<li>On <strong>elliptic curves</strong>, magfrump <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/5zie\">recommends</a> Koblitz'&nbsp;<em><a href=\"http://www.amazon.com/Introduction-Elliptic-Modular-Graduate-Mathematics/dp/0387979662/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Introduction to Elliptic Curves and Modular Forms</a></em> over Silverman's <em>Arithmetic of Elliptic Curves</em> and Cassel's <em>Lectures on Elliptic Curves</em>.</li>\n<li>On <strong>improvisation</strong>,&nbsp;Arepo <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/60l7\">recommends</a> Salinsky &amp; Frances-White's <em><a href=\"http://www.amazon.com/The-Improv-Handbook-Ultimate-Improvising/dp/0826428584/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Improv Handbook</a></em> over Johnstone's <em>Impro</em>, Johnston's <em>The Improvisation Game</em>, and others.</li>\n<li>On <strong>thermodynamics</strong>, madhadron&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject/64mz\">recommends</a>&nbsp;Hatsopoulos &amp; Keenan's <em><a href=\"http://www.amazon.com/Principles-General-Thermodynamics-G-Hatsopoulos/dp/0471359998/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Principles of General Thermodynamics</a></em> over Fermi's <em>Thermodynamics</em>, Sommerfeld's <em>Thermodynamics and Statistical Mechanics</em>, and others.</li>\n<li>On <strong>statistical mechanics</strong>, madhadron&nbsp;<a href=\"/lw/3gu/the_best_textbooks_on_every_subject/64mz\">recommends</a>&nbsp;Landau &amp; Lifshitz' <em><a href=\"http://www.amazon.com/Statistical-Physics-Third-Edition-Part/dp/0750633727/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Statistical Physics, Volume 5</a></em> over Sethna's <em>Entropy, Order Parameters, and Complexity</em> and Reichl's <em>A Modern Course in Statistical Physics</em>.</li>\n<li>On <strong>criminal justice</strong>, strange <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/655d\">recommends</a> Fuller's <em><a href=\"http://www.amazon.com/Criminal-Justice-Mainstream-Crosscurrents-Edition/dp/0135042623/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Criminal Justice: Mainstream and Crosscurrents</a></em>&nbsp;over&nbsp;Neubauer &amp; Fradella's <em>America's Courts and the Criminal Justice System</em> and Albanese' <em>Criminal Justice</em>.</li>\n<li>On <strong>organic chemistry</strong>, rhodium <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/67k1\">recommends</a>&nbsp;Clayden et al's <em><a href=\"http://www.amazon.com/Organic-Chemistry-Jonathan-Clayden/dp/0198503466/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Organic Chemistry</a></em>&nbsp;over&nbsp;McMurry's <em>Organic Chemistry</em> and Smith's <em>Organic Chemistry</em>.</li>\n<li>On <strong>special relativity</strong>, iDante <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/72an\">recommends</a> Taylor &amp; Wheeler's <em><a href=\"http://www.amazon.com/Spacetime-Physics-Edwin-F-Taylor/dp/0716723271/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Spacetime Physics</a></em>&nbsp;over&nbsp;Harris' <em>Modern Physics</em>, French's <em>Special Relativity</em>, and others.</li>\n<li>On <strong>abstract algebra</strong>, Bundle_Gerbe <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/72dq\">recommends</a> Dummit &amp; Foote's <em><a href=\"http://www.amazon.com/Abstract-Algebra-Edition-David-Dummit/dp/0471433349/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Abstract Algebra</a></em> over Lang's <em>Algebra</em> and others.</li>\n<li>On <strong>decision theory</strong>, lukeprog <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/7avv\">recommends</a> Peterson's <em><a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">An Introduction to Decision Theory</a></em>&nbsp;over Resnik's <em>Choices</em>&nbsp;and Luce &amp; Raiffa's <em>Games and Decisions</em>.</li>\n<li>On <strong>calculus</strong>, orthonormal <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/8uec\">recommends</a> Spivak's <em><a href=\"http://smile.amazon.com/Calculus-4th-Michael-Spivak/dp/0914098918\">Calculus</a></em>&nbsp;over Thomas' <em>Calculus</em>&nbsp;and Stewart's <em>Calculus</em>.&nbsp;</li>\n<li>On&nbsp;<strong>analysis in R<sup>n</sup></strong>, orthonormal <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/8uec\">recommends</a>&nbsp;Strichartz's <em><a href=\"http://smile.amazon.com/Analysis-Revised-Jones-Bartlett-Mathematics/dp/0763714976/\">The Way of Analysis</a></em>&nbsp;over&nbsp;Rudin's <em>Principles of Mathematical Analysis</em> and&nbsp;Kolmogorov &amp; Fomin's <em>Introduction to Real Analysis</em>.</li>\n<li>On <strong>real analysis and measure theory</strong>, orthonormal <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/8uec\">recommends</a>&nbsp;Stein &amp; Shakarchi's <em><a href=\"http://smile.amazon.com/Real-Analysis-Integration-Princeton-Lectures/dp/0691113866/\">Measure Theory, Integration, and Hilbert Spaces</a></em> over Royden's <em>Real Analysis</em> and Rudin's <em>Real and Complex Analysis</em>.&nbsp;</li>\n<li>On <strong>partial differential equations</strong>, orthonormal <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/8uec\">recommends</a>&nbsp;Strauss' <em><a href=\"http://smile.amazon.com/Partial-Differential-Equations-Walter-Strauss/dp/0470054565\">Partial Differential Equations</a></em> over Evans' <em>Partial Differential Equations</em> and Hormander's <em>Analysis of Partial Differential Operators</em>.</li>\n<li>On <strong>introductory real analysis</strong>,&nbsp;SatvikBeri <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/9kw2\">recommends</a> Pugh's&nbsp;<a href=\"http://smile.amazon.com/Mathematical-Analysis-Undergraduate-Texts-Mathematics/dp/0387952977/\">Real Mathematical Analysis</a> over Lang's <em>Real and Functional Analysis</em>&nbsp;and Rudin's <em>Principles of Mathematical Analysis</em>.</li>\n<li>On <strong>commutative algebra</strong>, SatvikBeri <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/9kw9\">recommends</a> MacDonald's&nbsp;<em><a href=\"http://smile.amazon.com/Introduction-Commutative-Algebra-Addison-Wesley-Mathematics/dp/0201407515/\">Introduction to Commutative Algebra</a></em> over Lang's <em>Algebra</em>&nbsp;and Eisenbud's&nbsp;<em>Commutative Algebra With a View Towards Algebraic Geometry</em>.</li>\n<li>On <strong>animal behavior</strong>, Natha <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/bke9\">recommends</a>&nbsp;Alcock's <em><a href=\"http://smile.amazon.com/Animal-Behavior-Evolutionary-Approach-Tenth/dp/0878939660\">Animal Behavior, 6th edition</a></em>&nbsp;over&nbsp;Dugatkin's&nbsp;<em>Principles of Animal Behavior</em> and newer editions of the Alcock textbook.</li>\n<li>On <strong>calculus</strong>, Epictetus <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/byee\">recommends</a>&nbsp;Courant's&nbsp;<em><a href=\"http://smile.amazon.com/Differential-Integral-Calculus-Vol-One/dp/4871878384/\">Differential and Integral Calculus</a></em> over&nbsp;Stewart's <em>Calculus</em> and&nbsp;Kline's <em>Calculus</em>.</li>\n<li>On <strong>linear algebra</strong>, Epictetus <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/byee\">recommends</a> Shilov's <em><a href=\"http://smile.amazon.com/Linear-Algebra-Dover-Books-Mathematics/dp/048663518X/\">Linear Algebra</a></em>&nbsp;over&nbsp;Lay's <em>Linear Algebra and its Appications</em> and Axler's <em>Linear Algebra Done Right</em>.</li>\n<li>On <strong>numerical methods</strong>, Epictetus <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/byee\">recommends</a> Press et al.'s&nbsp;<em><a href=\"http://smile.amazon.com/Numerical-Recipes-3rd-Scientific-Computing/dp/0521880688/\">Numerical Recipes</a></em> over Bulirsch &amp; Stoer's <em>Introduction to Numerical Analysis</em>, Atkinson's <em>An Introduction to Numerical Analysis</em>, and Hamming's <em>Numerical Methods of Scientists and Engineers</em>.</li>\n<li>On <strong>ordinary differential equations</strong>, Epictetus <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/byee\">recommends</a>&nbsp;Arnold's <em><a href=\"http://smile.amazon.com/Ordinary-Differential-Equations-V-I-Arnold/dp/0262510189/\">Ordinary Differential Equations</a></em> over Coddington's <em>An Introduction to Ordinary Differential Equations</em> and Enenbaum &amp; Pollard's <em>Ordinary Differential Equations</em>.</li>\n<li>On <strong>abstract algebra</strong>, Epictetus <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/byee\">recommends</a>&nbsp;Jacobson's <em><a href=\"http://smile.amazon.com/Basic-Algebra-Second-Dover-Mathematics/dp/0486471896/\">Basic Algebra</a></em> over Bourbaki's <em>Algebra</em>, Lang's <em>Algebra</em>, and Hungerford's <em>Algebra</em>.</li>\n<li>On&nbsp;<strong>elementary real analysis</strong>, Epictetus <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/byee\">recommends</a> Rudin's <em><a href=\"http://smile.amazon.com/Principles-Mathematical-Analysis-Rudin/dp/1259064786/\">Principles of Mathematical Analysis</a></em> over Ross' <em>Elementary Analysis</em>, Lang's <em>Undergraduate Analysis</em>, and Hardy's <em>A Course of Pure Mathematics</em>.</li>\n</ul>\n<div>If there are no recommendations for the subject you want to learn, you can start by checking the <a href=\"http://www.alibris.com/subjects/subjects-textbooks\">Alibris textbooks</a> category for your subject, and sort by 'Top-selling.' But you'll have to do more research than that. Check which textbooks are asked for in the syllabi of classes on your subject at leading universities. Search Google for recommendations and reviews.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 4, "fF9GEdWXKJ3z73TmB": 12, "fkABsGCJZ6y9qConW": 15, "GQyPQcdEQF4zXhJBq": 4, "4Kcm4etxAJjmeDkHP": 3, "5f5c37ee1b5cdee568cfb2b1": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xg3hXCYQPJkwHyik2", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 452, "baseScore": 548, "extendedScore": null, "score": 0.000978, "legacy": true, "legacyId": "4494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 548, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": "r38pkCm7wF4M44MDQ", "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 390, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["zNCdxFvfbFZggTxDk", "ndGYn7ZFiZyernp9f", "ufBYjpi9gK6uvtkh5", "RiQYixgCdvd8eWsjg", "wfJebLTPGYaK3Gr8W", "TNHQLZK5pHbxdnz4e"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 28, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-01-16T08:30:57.953Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-16T16:00:13.617Z", "modifiedAt": null, "url": null, "title": "Slate article on Efficient Charity (link)", "slug": "slate-article-on-efficient-charity-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bentarm", "createdAt": "2009-03-05T17:59:17.163Z", "isAdmin": false, "displayName": "bentarm"}, "userId": "xdmTZWK4DzchxkyQC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FLR7ncH9pJ8YgMxv2/slate-article-on-efficient-charity-link", "pageUrlRelative": "/posts/FLR7ncH9pJ8YgMxv2/slate-article-on-efficient-charity-link", "linkUrl": "https://www.lesswrong.com/posts/FLR7ncH9pJ8YgMxv2/slate-article-on-efficient-charity-link", "postedAtFormatted": "Sunday, January 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Slate%20article%20on%20Efficient%20Charity%20(link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASlate%20article%20on%20Efficient%20Charity%20(link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLR7ncH9pJ8YgMxv2%2Fslate-article-on-efficient-charity-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Slate%20article%20on%20Efficient%20Charity%20(link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLR7ncH9pJ8YgMxv2%2Fslate-article-on-efficient-charity-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLR7ncH9pJ8YgMxv2%2Fslate-article-on-efficient-charity-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>From the <a href=\"http://www.slate.com/id/2281097/\">article</a>:</p>\n<blockquote><span style=\"font-family: Verdana; font-size: 12px; line-height: 18px;\">Billions of dollars are given and spent on aid and development by individuals and companies each year. Despite this generosity, we simply do not allocate enough resources to solve all of the world's biggest problems. In a world fraught with competing claims on human solidarity, we have a moral obligation to direct additional resources to where they can achieve the most good. And that is as true of our own small-scale charitable donations as it is of governments' or philanthropists' aid budgets.</span></blockquote>\n<blockquote><span style=\"font-family: Verdana; font-size: 12px; line-height: 18px;\">...</span></blockquote>\n<blockquote><span style=\"font-family: Verdana; font-size: 12px; line-height: 18px;\">Guided by their consideration of each option's costs and benefits, and setting aside matters like media attention, the experts identified the best investments: those for which relatively tiny amounts of money could generate significant returns in terms of health, prosperity, and community advantages. These included: increased immunization coverage, initiatives to reduce school dropout rates, community-based nutrition promotion, and micronutrient supplementation.</span></blockquote>\n<p>Their conclusion? Micronutrients for people in poor countries. No, I don't think SIAI was considered as an option.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FLR7ncH9pJ8YgMxv2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 6.681929833755965e-07, "legacy": true, "legacyId": "4905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-16T17:26:44.619Z", "modifiedAt": null, "url": null, "title": "Meta: Rationality News Area", "slug": "meta-rationality-news-area", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:21.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xsWYX2wv2sw6cGsB9/meta-rationality-news-area", "pageUrlRelative": "/posts/xsWYX2wv2sw6cGsB9/meta-rationality-news-area", "linkUrl": "https://www.lesswrong.com/posts/xsWYX2wv2sw6cGsB9/meta-rationality-news-area", "postedAtFormatted": "Sunday, January 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20Rationality%20News%20Area&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20Rationality%20News%20Area%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxsWYX2wv2sw6cGsB9%2Fmeta-rationality-news-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20Rationality%20News%20Area%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxsWYX2wv2sw6cGsB9%2Fmeta-rationality-news-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxsWYX2wv2sw6cGsB9%2Fmeta-rationality-news-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 334, "htmlBody": "<p>A lot of links and media content has been posted lately. Would it make sense to create another area where people can submit relevant links, images or videos? If the discussion area was meant to keep the community blog clean and focused then one could devote the discussion area to questions, proofreading and mediocre posts while an additional media section could be destined for <em>third party</em> content:</p>\n<p><strong>Community Blog </strong>(main area)</p>\n<ul>\n<li>A <a href=\"http://wiki.lesswrong.com/wiki/FAQ#What_are_appropriate_topics_for_top-level_articles.3F\">set of</a> <a href=\"http://wiki.lesswrong.com/wiki/FAQ#What_is_Less_Wrong.3F\">agreed upon topics</a>. </li>\n<li>Temporary announcements (meetups etc.)</li>\n</ul>\n<p><strong>Discussion Area</strong></p>\n<ul>\n<li>Questions</li>\n<li>Proofreading</li>\n<li>Mediocre content (not yet ready or not suitable for the main area)</li>\n<li>Meta discussions about the community</li>\n<li>Miscellaneous topics (e.g. AI, recommendations and questions about nutrition etc.)</li>\n</ul>\n<p><strong>Rationality News </strong>(recommended third party content)</p>\n<ul>\n<li>News and media aggregation </li>\n<li>Ranking and discovery</li>\n<li>Relevant scientific research/studies</li>\n<li>Important technological progress</li>\n</ul>\n<p>There is a lot of <a href=\"/r/discussion/lw/3qm/link_what_should_a_reasonable_person_believe/\">good</a> and <em>bad </em>stuff out there to learn from. Yet this content is often discovered by people who don't want, or are not able to write a post about it or start a <em>discussion </em>but would be willing to submit a link to the content. This would allow LW to offer a selection of relevant high quality third party news and media content without having the discussion area (RSS feed) cluttered with it.</p>\n<p>LW could also employ a few moderators that can move posts to the appropriate area. For example, if a post in the main area is being downvoted or near zero after a week it will be moved to the discussion area. Or if someone posts a link to third-party content to the discussion area it is moved to the news (media) area. Users, especially newbies and outsiders would also benefit from it by not being exposed to inappropriate content but rather to the highest quality of content to be expected when browsing a certain area.</p>\n<p>So what do you think, would a rationality news area be a worthwhile feature?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xsWYX2wv2sw6cGsB9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 9, "extendedScore": null, "score": 6.682152539315565e-07, "legacy": true, "legacyId": "4906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2XZju58cP82Fv776N"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-17T01:13:44.233Z", "modifiedAt": null, "url": null, "title": "The annoyingness of New Atheists: declaring God Dead makes you a Complete Monster?", "slug": "the-annoyingness-of-new-atheists-declaring-god-dead-makes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:25.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jBryseg72Dkgee9dT/the-annoyingness-of-new-atheists-declaring-god-dead-makes", "pageUrlRelative": "/posts/jBryseg72Dkgee9dT/the-annoyingness-of-new-atheists-declaring-god-dead-makes", "linkUrl": "https://www.lesswrong.com/posts/jBryseg72Dkgee9dT/the-annoyingness-of-new-atheists-declaring-god-dead-makes", "postedAtFormatted": "Monday, January 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20annoyingness%20of%20New%20Atheists%3A%20declaring%20God%20Dead%20makes%20you%20a%20Complete%20Monster%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20annoyingness%20of%20New%20Atheists%3A%20declaring%20God%20Dead%20makes%20you%20a%20Complete%20Monster%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjBryseg72Dkgee9dT%2Fthe-annoyingness-of-new-atheists-declaring-god-dead-makes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20annoyingness%20of%20New%20Atheists%3A%20declaring%20God%20Dead%20makes%20you%20a%20Complete%20Monster%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjBryseg72Dkgee9dT%2Fthe-annoyingness-of-new-atheists-declaring-god-dead-makes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjBryseg72Dkgee9dT%2Fthe-annoyingness-of-new-atheists-declaring-god-dead-makes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 492, "htmlBody": "<p>I have noticed during my dialectic adventures on the Grid that religious people, no matter how \"reasonable\" (i.e. moderate, unaggressive, unassuming, gentle, etc.), would get very annoyed by an assertive, dry Atheist perspective, which they tend to nickname Hollywood Atheist (interestingly, religious people tend to use this term to atheists that openly make fun of religion and are very assertive and even preachy about their disbelief, while atheists tend to use it to mean people who are atheists for shallow, weak reasons and who do a poor job of defending their stance in an argument). There is also the tendency to compare the certainty of an Atheist with that of a Fundamentalists, when they are fundamentally different in nature (pun unintended), something they do not seem to be able or willing to grasp. Not that atheism hasn't had its fair share of fundamentalists, but that's supposedly the difference between an atheist who is so out of rationalism and one that is so because they hate the Church or because Stalin (glorified be his name) told them to.</p>\n<p>On of the things that irritate them the most is the phrase \"God is Dead\". A phrase that is obviously meaningless in a literal sense (although, of course, God was never a living being in the first place, <a href=\"http://en.wikipedia.org/wiki/Life#Biology\">by the current definition</a>). <em>Figuratively</em>, it means something akin to \"Our Father is dead\": we are now orphans, adults, we don't need a God to tell us what to do, or what to want, or how to see the world: we decide for ourselves, we see for ourselves, we are now free... but it does feel a bit lonely, and, for those who relied on their God or Parent Figure as a crutch, it can be hard to adapt to a world without a reference, without an authority figure. A world where <em>you</em> are the reference, <em>you</em> are responsible for your own moral choices.</p>\n<p>There are other things, specific arguments, methods of approach, that anger them and are counterproductive to the submitting of the message. Of course, the atheist message <em>is</em> a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/BrownNote\">Brown Note</a> of sorts to the religious mind, since it challenges their entire worldview (though in the end it all adds up to normality... except much more seamlessly). However, it would be nice to develop an approach towards theists that avoids the frontal part of their mental shields and gets into the seams, using the minimal force in the points of maximum efficiency, bypassing their knee-jerk defences...</p>\n<p>So, here is my question to you all: how do you get your points across to a theist without pushing any of their Berserk Buttons, without coming off as a condescending and dismissive jerk, and without having to shorten all of the freaking Inferential Distance?</p>\n<p>Developing a general algorithm would help us spread our ideals further, which, as far as I know, we think will be to the benefit of all humanity and might in fact help us avoid extinction. So, suggestions...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jBryseg72Dkgee9dT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -10, "extendedScore": null, "score": 6.683354870494843e-07, "legacy": true, "legacyId": "4907", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-17T11:11:44.028Z", "modifiedAt": null, "url": null, "title": "Meta: Basic and advanced section", "slug": "meta-basic-and-advanced-section", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:21.224Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gtpJ6FqptK4LTJ3j6/meta-basic-and-advanced-section", "pageUrlRelative": "/posts/gtpJ6FqptK4LTJ3j6/meta-basic-and-advanced-section", "linkUrl": "https://www.lesswrong.com/posts/gtpJ6FqptK4LTJ3j6/meta-basic-and-advanced-section", "postedAtFormatted": "Monday, January 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20Basic%20and%20advanced%20section&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20Basic%20and%20advanced%20section%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtpJ6FqptK4LTJ3j6%2Fmeta-basic-and-advanced-section%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20Basic%20and%20advanced%20section%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtpJ6FqptK4LTJ3j6%2Fmeta-basic-and-advanced-section", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtpJ6FqptK4LTJ3j6%2Fmeta-basic-and-advanced-section", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 801, "htmlBody": "<p>lukeprog recently wrote a top-level post exhorting LW to emphasize <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">the basics of rationality</a>. That strikes me as a worthwhile endeavor- if we do actually want to raise the sanity waterline, we need a lot of droplets.</p>\n<p>The suggestion <a href=\"/lw/3mm/back_to_the_basics_of_rationality/3bkm\">I made</a> was to redesign the site to create more specialized areas: a basic one where people aren't expected to have read the sequences, and an advanced one where people are. That's not the division that has to be made, but it seems a natural place to make one. For ease of reading, a page that displays both sections seems like it would be a good plan (but I have no idea if that is easy to make), and it might also be desirable to have it so one article can appear in both sections (maybe there would flags or tags for basic and advanced?). Another option for a division is between video and text posts- I personally have little interest in video posts but I'm sure there are people interested in both or just video posts.</p>\n<p><a id=\"more\"></a>The basic section would focus on fundamental concepts and standalone / modular posts. By modular, I mean replaceable- two posts that cover the same concept from different angles, ideally in enough detail that you could read either of them and be able to go on to the next post, if there is one. There are several (mutually inclusive) ways to go about this- video explanations of old posts, entirely new material, and new restatements (or perspectives) on old material. lukeprog covers videos, so I won't; new material seems pretty self-explanatory.</p>\n<p>The main possible focus of such a section that needs discussion is new perspectives on old concepts. Right now, one of the failures of LW according to the outside view is the sense of cultishness, and I would not be surprised if that comes from the fact that anyone who maintains their desire to read the sequences throughout the entire sequences must be in some sense on the same wavelength as EY. If we had five different explanations of <a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killler</a> rather than just EY's, it seems plausible we could reach more people. If there were more approaches to metaethics than EY's, that might convince a few critics to stick around for other things they do like, and so on.</p>\n<p>While plausible, it's not certain. Right now, the sequences are mostly pretty good. If we invite other people to present their core concepts, the average quality will probably decline, even if the maximum quality for each individual reader will increase (as new options are either better or not). A basic educational fact is that people react differently to different approaches, and what is interesting and straightforward for one person might be boring and opaque to another. The question becomes then an empirical one about max quality minus search cost (will you read enough of the five different explanations to see which one is best for you? How long would that take? How much better are alternatives?) and an ideological one about average quality vs. maximum quality.</p>\n<p>Warning: the ideological question can get ugly. It shares many elements with questions we've seen before- \"what percentage of LW's target audience have we reached?\"- but focuses on the word \"target.\" One quickly notices that \"average quality\" doesn't average over all readers or potential readers, but just the readers that <em>stick around</em>. It's trivial to have a high average when you can pick cherries. Do we want this to be a place full of thinkers who, even if they disagree with EY, like him / the community enough to stick around despite many features that suggest it's an echo chamber? Or do we want this to be a place that grows beyond its roots and branches into many things, too large for one person to dominate? Note that smaller and larger groups each have their own benefits, and it's not clear to me which apply to the situation at hand.</p>\n<p>Another sticking point in this idea is defining the 'advanced' section. It's one thing to say \"ok, everyone who reads the sequences graduates to the advanced section,\" it's another thing to identify topics that only make sense once you've read the sequences. Going back through the list of the last 30 posts to the main LW page, I had trouble identifying which I would put where (except for a generic \"get your AI out of my rationality\" urge). My more reasonable urge was to advertise the <strong>summary break</strong>, to the right of the block quote button and to the left of the bullet points button. If you use that, only the part of your post above the summary break gets posted to the new list (and everything after requires a clickthrough).</p>\n<p>Thoughts, on either the basic/advanced division or the more benign video section suggestion?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gtpJ6FqptK4LTJ3j6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 6.68489504301677e-07, "legacy": true, "legacyId": "4848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gfexKxsBDM6v2sCMo", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-17T13:20:12.658Z", "modifiedAt": null, "url": null, "title": "Your best arguments for risks from AI", "slug": "your-best-arguments-for-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:09.692Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/otF5QbX2etBWn3dBA/your-best-arguments-for-risks-from-ai", "pageUrlRelative": "/posts/otF5QbX2etBWn3dBA/your-best-arguments-for-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/otF5QbX2etBWn3dBA/your-best-arguments-for-risks-from-ai", "postedAtFormatted": "Monday, January 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Your%20best%20arguments%20for%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYour%20best%20arguments%20for%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotF5QbX2etBWn3dBA%2Fyour-best-arguments-for-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Your%20best%20arguments%20for%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotF5QbX2etBWn3dBA%2Fyour-best-arguments-for-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotF5QbX2etBWn3dBA%2Fyour-best-arguments-for-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1700, "htmlBody": "<p>An easy to read list of evidence and simple arguments in support of risks from artificial intelligence could help to raise awareness. Such a list could be the first step to draw attention, to spark interest and make people read some more advanced papers or the sequences. To my knowledge so far nobody has put the evidence, various arguments and indications together in one place.</p>\n<p>My intention is to enable people interested to mitigate risks from AI to be able to offer a brochure that allows them to raise awareness without having to spend a lot of time explaining the details or tell people to read through hundreds of posts of marginal importance. There has to be some promotional literature that provides a summary of the big picture and some strong arguments for action. Such a brochure has to be simple and concise enough to arouse interest in the reader even if they just skim over the text.</p>\n<p><span style=\"color: #ff0000;\"><strong>Post a comment with the the best argument(s) for risks from AI</strong></span></p>\n<p><strong>Some rules:</strong></p>\n<ul>\n<li>The argument or summary has to be simple and concise.</li>\n<li>Allow for some <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a>. Make your arguments <em>self-evident</em>.</li>\n<li>If possible cite an example, provide references or link to further reading.</li>\n<li>Disclosure: Note if your argument is controversial and account for it.</li>\n</ul>\n<p>For starters I wrote a quick draft below. But there sure do exist a lot of <a href=\"/lw/qk/that_alien_message/\">other</a> <a href=\"/lw/wp/what_i_think_if_not_why/\">arguments</a> and <a href=\"http://www.hplusmagazine.com/articles/ai/singularity-already-happening-goldman-sachs\">indications</a> for why risks from artificial intelligence should be taken serious. What convinced <em>you</em>?</p>\n<hr />\n<p><strong>Claim:</strong> Creation of general intelligence <a href=\"http://en.wikipedia.org/wiki/Human_evolution\">is possible</a>.</p>\n<p><strong>Status:</strong> Uncontroversial<strong></strong></p>\n<p><strong>Claim:</strong> Intelligence can be <a href=\"http://en.wikipedia.org/wiki/War#List_of_wars_by_death_toll\">destructive</a>.<strong></strong></p>\n<p><strong>Status:</strong> Uncontroversial<strong></strong></p>\n<p><strong>Claim:</strong> Algorithmic intelligence can be creative and inventive.<strong></strong></p>\n<p><strong>Status:</strong> Uncontroversial<sup>1</sup><strong></strong></p>\n<p><strong>Claim:</strong> Improvements of algorithms can in many cases lead to dramatic performance gains.<strong></strong></p>\n<p><strong>Status:</strong> Uncontroversial<sup>2</sup><strong></strong></p>\n<p><strong>Claim:</strong> Human-level intelligence is not the maximum.</p>\n<p><strong>Status: </strong>Very likely<sup>3</sup><strong></strong></p>\n<p><strong>Claim:</strong> Any sufficiently advanced AI will do everything to continue to keep pursuing terminal goals indefinitely.</p>\n<p><strong>Status:</strong> Controversial but a possibility to be taken seriously.<sup>4 </sup>We don't yet have a good understanding of intelligence but given all that we know there are no good reasons to rule out this possibility. Overconfidence can have fatal consequences in this case.<sup>5</sup><strong></strong></p>\n<p><strong>Claim:</strong> Morality is fragile and not imperative (i.e. is <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">not a natural law</a>)<strong></strong></p>\n<p><strong>Status:</strong><em> </em>Uncontroversial.<sup>6 </sup>Even humans who have been honed by <a title=\"Social evolution\" href=\"http://en.wikipedia.org/wiki/Social_evolution\">social evolution</a> to consider the well-being of other agents can overcome their instincts and commit large scale atrocities in favor of various peculiar instrumental goals.</p>\n<hr />\n<p>1.<strong> </strong></p>\n<blockquote>\n<p>We report the development of Robot Scientist &ldquo;Adam,&rdquo; which advances the automation of both. <strong>Adam has autonomously generated functional genomics hypotheses about the yeast Saccharomyces cerevisiae and experimentally tested these hypotheses by using laboratory automation.</strong></p>\n</blockquote>\n<p>&mdash; <a rel=\"nofollow\" href=\"http://www.sciencemag.org/content/324/5923/85.abstract\">The Automation of Science</a></p>\n<blockquote>\n<p>Without any prior knowledge about physics, kinematics, or geometry, <strong>the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation</strong>. The <strong>discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems</strong>, gradually uncovering the &ldquo;alphabet&rdquo; used to describe those systems.</p>\n</blockquote>\n<p>&mdash; <a rel=\"nofollow\" href=\"http://www.wired.com/wiredscience/2009/04/newtonai/\">Computer Program Self-Discovers Laws of Physics</a></p>\n<blockquote>\n<p>This aim was achieved within 3000 generations, but <strong>the success was even greater than had been anticipated</strong>. The evolved system uses far fewer cells than anything a human engineer could have designed, and it <strong>does not even need the most critical component of human-built systems</strong> - a clock. <strong>How does it work? Thompson has no idea</strong>, though he has traced the input signal through a complex arrangement of feedback loops within the evolved circuit. In fact, out of the 37 logic gates the final product uses, five of them are not even connected to the rest of the circuit in any way - yet if their power supply is removed, the circuit stops working. It seems that evolution has exploited some subtle electromagnetic effect of these cells to come up with its solution, <strong>yet the exact workings of the complex and intricate evolved structure remain a mystery</strong>. (Davidson 1997)</p>\n</blockquote>\n<blockquote>\n<p>When the GA was applied to this problem, the evolved results for three, four and five-satellite constellations were unusual, highly asymmetric orbit configurations, with the satellites spaced by alternating large and small gaps rather than equal-sized gaps as conventional techniques would produce. However, this solution significantly reduced both average and maximum revisit times, in some cases by up to 90 minutes. In a news article about the results, Dr. William Crossley noted that <strong>\"engineers with years of aerospace experience were surprised by the higher performance offered by the unconventional design\". </strong>(Williams, Crossley and Lang 2001)</p>\n</blockquote>\n<p>&mdash; <a href=\"http://www.talkorigins.org/faqs/genalg/genalg.html\">Genetic Algorithms and Evolutionary Computation</a></p>\n<blockquote>\n<p>UC Santa Cruz emeritus professor David Cope is ready to introduce computer software that creates original, modern music.</p>\n</blockquote>\n<p>&mdash; <a href=\"http://www.miller-mccune.com/culture-society/triumph-of-the-cyborg-composer-8507/\">Triumph of the Cyborg Composer</a></p>\n<p>2.</p>\n<blockquote>\n<p>Everyone knows Moore&rsquo;s Law &ndash; a prediction made in 1965 by Intel co-founder Gordon Moore that the&nbsp;density of transistors in integrated circuits would continue to double every 1 to 2 years.&nbsp;(&hellip;) &nbsp;Even more remarkable &ndash; and even less widely understood &ndash; is that in many areas, <strong>performance gains due&nbsp;to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased&nbsp;processor speed.</strong></p>\n<p>The algorithms that we use today for speech recognition, for natural language translation, for chess&nbsp;playing, for logistics planning, have evolved remarkably in the past decade. &nbsp;It&rsquo;s difficult to quantify the&nbsp;improvement, though, because it is as much in the realm of quality as of execution time.</p>\n<p>In the field of numerical algorithms, however, the improvement can be quantified. &nbsp;Here is just one&nbsp;example, provided by Professor Martin Gr&ouml;tschel of Konrad-Zuse-Zentrum f&uuml;r Informationstechnik Berlin. &nbsp;Gr&ouml;tschel, an expert in optimization, observes that a benchmark production planning model solved&nbsp;using linear programming would have taken 82 years to solve in 1988, using the computers and the linear&nbsp;programming algorithms of the day. &nbsp;<strong>Fifteen years later &ndash; in 2003 &ndash; this same model could be solved in&nbsp;roughly 1 minute, an improvement by a factor of roughly 43 million. &nbsp;Of this, a factor of roughly 1,000 was&nbsp;due to increased processor speed, whereas a factor of roughly 43,000 was due to improvements in algorithms!</strong> &nbsp;Gr&ouml;tschel also cites an algorithmic improvement of roughly 30,000 for mixed integer programming&nbsp;between 1991 and 2008.</p>\n</blockquote>\n<p>&mdash; Page 71, Progress in Algorithms Beats Moore&rsquo;s Law (<a href=\"http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf\">Report to the President and Congress: Designing a Digital Future: Federally FUnded R&amp;D in Networking and IT</a>)</p>\n<p>3.</p>\n<p>The following argument is not directly applicable but can similarly be made for human intelligence, computational capacity or processing speed.</p>\n<blockquote>\n<p>An AI might go from infrahuman to transhuman in <em>less than a week?</em>&nbsp; But a week is 10^49 Planck intervals - if you just look at the exponential scale that stretches from the Planck time to the age of the universe, there's nothing special about the timescale that 200Hz humans happen to live on.</p>\n<p>If we're talking about a starting population of 2GHz processor cores, then any given AI that FOOMs at all, is likely to FOOM in less than 10^15 sequential operations or more than 10^19 sequential operations, because the region between 10^15 and 10^19 isn't all that wide a target.&nbsp; So less than a week or more than a century, and in the latter case that AI will be trumped by one of a shorter timescale.</p>\n</blockquote>\n<p>&mdash; <a href=\"/lw/wm/disjunctions_antipredictions_etc/\">Disjunctions, Antipredictions, Etc.</a></p>\n<p>4.</p>\n<blockquote>\n<p>There are &ldquo;basic AI drives&rdquo; we can expect to emerge in sufficiently advanced AIs, almost regardless of their initial programming. Across a <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">wide range of top goals</a>, any AI that uses decision theory will want to 1) self-improve, 2) have an accurate model of the world and consistent preferences (be rational), 3) preserve their utility functions, 4) prevent counterfeit utility, 5) be self-protective, and 6) acquire resources and use them efficiently. Any AI with a sufficiently open-ended utility function (absolutely necessary if you want to avoid having human beings double-check every decision the AI makes) will pursue all these instrumental goals <em>indefinitely</em> as long as it can eke out a little more utility from doing so. AIs will <em>not</em> have built in satiation points where they say, &ldquo;I&rsquo;ve had enough&rdquo;. We have to program those in, and if there&rsquo;s a potential satiation point we miss, the AI will just keep pursuing instrumental goals indefinitely. The only way we can keep an AI from continuously expanding like an endless nuclear explosion is to make it to <em>want</em> to be constrained</p>\n</blockquote>\n<p>&mdash; Basic AI Drives, <a href=\"http://www.acceleratingfuture.com/michael/blog/2011/01/yes-the-singularity-is-the-biggest-threat-to-humanity/\">Yes, The Singularity is the Biggest Threat to Humanity<br /></a></p>\n<p>5.</p>\n<blockquote>\n<p>I&rsquo;m not going to argue for specific values for these probabilities. Instead, I&rsquo;ll argue for <em>ranges</em> of probabilities that I believe a person might reasonably assert for each probability on the right-hand side. I&rsquo;ll consider both a hypothetical skeptic, who is pessimistic about the possibility of the Singularity, and also a hypothetical enthusiast for the Singularity. In both cases I&rsquo;ll assume the person is reasonable, i.e., a person who is willing to acknowledge limits to our present-day understanding of the human brain and computer intelligence, and who is therefore not overconfident in their own predictions. By combining these ranges, we&rsquo;ll get a range of probabilities that a reasonable person might assert for the probability of the Singularity.</p>\n</blockquote>\n<p>&mdash; <a href=\"http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/\">What should a reasonable person believe about the Singularity?</a></p>\n<p>6.</p>\n<blockquote>\n<p>The Patrician took a sip of his beer. &ldquo;I have told this to few people, gentlemen, and I suspect I never will again, but one day when I was a young boy on holiday in Uberwald I was walking along the bank of a stream when I saw a mother otter with her cubs. A very endearing sight, I&rsquo;m sure you will agree, and even as I watched, the mother otter dived into the water and came up with a plump salmon, which she subdued and dragged onto a half-submerged log. As she ate it, while of course it was still alive, the body split and I remember to its day the sweet pinkness of its roes as they spilled out, much to the delight of the baby otters who scrambled over themselves to feed on the delicacy. One of nature&rsquo;s wonders, gentlemen: mother and children dining upon mother and children. And that&rsquo;s when I first learned about evil. It is built in to the very nature of the universe. Every world spins in pain. If there is any kind of supreme being, I told myself, it is up to all of us to become his moral superior.</p>\n</blockquote>\n<p>&mdash; Terry Pratchett, <em>Unseen Academicals</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "otF5QbX2etBWn3dBA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 6.685226024929458e-07, "legacy": true, "legacyId": "4930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5wMcKNAwB6X4mp9og", "z3kYdw54htktqt9Jb", "yzzoWR33S9C3m75e8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-17T17:46:48.453Z", "modifiedAt": null, "url": null, "title": "Link: \"Top 10 Mistakes in Behavior Change\"", "slug": "link-top-10-mistakes-in-behavior-change", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.491Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7FxZ57gpXNktyYFF8/link-top-10-mistakes-in-behavior-change", "pageUrlRelative": "/posts/7FxZ57gpXNktyYFF8/link-top-10-mistakes-in-behavior-change", "linkUrl": "https://www.lesswrong.com/posts/7FxZ57gpXNktyYFF8/link-top-10-mistakes-in-behavior-change", "postedAtFormatted": "Monday, January 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20%22Top%2010%20Mistakes%20in%20Behavior%20Change%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20%22Top%2010%20Mistakes%20in%20Behavior%20Change%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FxZ57gpXNktyYFF8%2Flink-top-10-mistakes-in-behavior-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20%22Top%2010%20Mistakes%20in%20Behavior%20Change%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FxZ57gpXNktyYFF8%2Flink-top-10-mistakes-in-behavior-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FxZ57gpXNktyYFF8%2Flink-top-10-mistakes-in-behavior-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p><a href=\"http://www.slideshare.net/captology/stanford-6401325\">Linky</a>. <a href=\"http://news.ycombinator.com/item?id=2106553\">HN discussion</a>. My summary with the negatives flipped:</p>\n<p>If you want to change your behavior effectively,</p>\n<p>\n<ul>\n<li>think of a small step,</li>\n<li>that doesn't require willpower every time,</li>\n<li>uses the environment for encouragement,</li>\n<li>involves a new behavior, not stopping an old one,</li>\n<li>is relatively easy to do,</li>\n<li>is prompted by an external trigger,</li>\n<li>(which shouldn't be just learning some new information),</li>\n<li>is a concrete behavior rather than an abstract goal,</li>\n<li>and resolve to adopt it for some fixed period of time, not forever.</li>\n</ul>\n</p>\n<p>Then the behavior change will be easy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7FxZ57gpXNktyYFF8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 32, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "4931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-17T23:58:21.854Z", "modifiedAt": null, "url": null, "title": "Eliezer to speak in Oxford, 8pm Jan 25th", "slug": "eliezer-to-speak-in-oxford-8pm-jan-25th", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:59.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tgBLnoeid98AceP7s/eliezer-to-speak-in-oxford-8pm-jan-25th", "pageUrlRelative": "/posts/tgBLnoeid98AceP7s/eliezer-to-speak-in-oxford-8pm-jan-25th", "linkUrl": "https://www.lesswrong.com/posts/tgBLnoeid98AceP7s/eliezer-to-speak-in-oxford-8pm-jan-25th", "postedAtFormatted": "Monday, January 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eliezer%20to%20speak%20in%20Oxford%2C%208pm%20Jan%2025th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEliezer%20to%20speak%20in%20Oxford%2C%208pm%20Jan%2025th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtgBLnoeid98AceP7s%2Feliezer-to-speak-in-oxford-8pm-jan-25th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eliezer%20to%20speak%20in%20Oxford%2C%208pm%20Jan%2025th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtgBLnoeid98AceP7s%2Feliezer-to-speak-in-oxford-8pm-jan-25th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtgBLnoeid98AceP7s%2Feliezer-to-speak-in-oxford-8pm-jan-25th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p>Next Tuesday Eliezer will be giving a talk in conjunction with the <a href=\"http://www.facebook.com/group.php?gid=265828309853\">Oxford Transhumanists</a> entitled \"<a href=\"http://www.facebook.com/event.php?eid=182377671794308\">Smarter-Than-Human Artificial Intelligence: Predictably the Most Important Thing in the History of Ever</a>\". The talk is open for everybody and it would be great to meet any of you that are in the area and can come down.&nbsp;The talk will be followed by drinks at the Turf Tavern, and hopefully we will be able to drag Eliezer with us for further discussion.</p>\n<p><strong>When</strong>: 8pm, Tuesday January 25th<br /><strong>Where</strong>: <a href=\"http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=ox13dp&amp;sll=51.752279,-1.255884&amp;sspn=0.14155,0.363579&amp;ie=UTF8&amp;hq=&amp;hnear=Oxford+OX1+3DP,+United+Kingdom&amp;z=16\">Saskatchewan Room, Exeter College, Turl Street, OX1 3DP</a>&nbsp;(there will be ushers to guide you to the room from the college entrance)<br /><strong>RSVP</strong>:&nbsp;<a href=\"http://www.facebook.com/event.php?eid=182377671794308\">http://www.facebook.com/event.php?eid=182377671794308</a></p>\n<p>Eliezer doesn't often speak in this part of the world so don't miss it!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tgBLnoeid98AceP7s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 6.686870460891022e-07, "legacy": true, "legacyId": "4932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-18T03:19:33.866Z", "modifiedAt": null, "url": null, "title": "Statistical Prediction Rules Out-Perform Expert Human Judgments", "slug": "statistical-prediction-rules-out-perform-expert-human", "viewCount": null, "lastCommentedAt": "2021-03-24T02:43:56.122Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CKW8c2Bngz9yXibSk/statistical-prediction-rules-out-perform-expert-human", "pageUrlRelative": "/posts/CKW8c2Bngz9yXibSk/statistical-prediction-rules-out-perform-expert-human", "linkUrl": "https://www.lesswrong.com/posts/CKW8c2Bngz9yXibSk/statistical-prediction-rules-out-perform-expert-human", "postedAtFormatted": "Tuesday, January 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Statistical%20Prediction%20Rules%20Out-Perform%20Expert%20Human%20Judgments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStatistical%20Prediction%20Rules%20Out-Perform%20Expert%20Human%20Judgments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKW8c2Bngz9yXibSk%2Fstatistical-prediction-rules-out-perform-expert-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Statistical%20Prediction%20Rules%20Out-Perform%20Expert%20Human%20Judgments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKW8c2Bngz9yXibSk%2Fstatistical-prediction-rules-out-perform-expert-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKW8c2Bngz9yXibSk%2Fstatistical-prediction-rules-out-perform-expert-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1469, "htmlBody": "<p>A parole board considers the release of a prisoner: Will he be violent again? A hiring officer considers a job candidate: Will she be a valuable asset to the company? A young couple considers marriage: Will they have a happy marriage?</p>\n<p>The <a href=\"/lw/k5/cached_thoughts/\">cached wisdom</a> for making such high-stakes predictions is to have experts gather as much evidence as possible, weigh this evidence, and make a judgment. But 60 years of research has shown that in hundreds of cases, a simple formula called a <strong><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Swets-Better-Decisions-Through-Science.pdf\">statistical prediction rule</a> (SPR)</strong> makes better predictions than leading experts do. Or, more exactly:</p>\n<blockquote>\n<p>When based on the same evidence, the predictions of SPRs are at least as reliable as, and are typically more reliable than, the predictions of human experts for problems of social prediction.<sup>1</sup></p>\n</blockquote>\n<p>For example, one SPR developed in 1995 <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Ashenfelter-Bordeaux-Wine-Vintage-Quality-and-the-Weather.pdf\">predicts</a> the price of mature Bordeaux red wines at auction better than expert wine tasters do. Reaction from the wine-tasting industry to such wine-predicting SPRs has been \"<a href=\"http://www.nytimes.com/1990/03/04/us/wine-equation-puts-some-noses-out-of-joint.html\">somewhere between violent and hysterical</a>.\"</p>\n<p>How does the SPR work? This particular SPR is called a&nbsp;<em>proper linear model</em>, which has the form:</p>\n<p>P = w<sub>1</sub>(c<sub>1</sub>) + w<sub>2</sub>(c<sub>2</sub>)&nbsp;+ w<sub>3</sub>(c<sub>3</sub>) + ...w<sub>n</sub>(c<sub>n</sub>)</p>\n<p>The model calculates the summed result P, which aims to predict a <em>target property</em> such as wine price, on the basis of a series of <em>cues</em>. Above, c<sub>n</sub> is the <em>value</em> of the n<sup>th</sup> cue, and w<sub>n</sub> is the <em>weight</em> assigned to the n<sup>th</sup> cue.<sup>2</sup></p>\n<p>In the wine-predicting SPR, c<sub>1</sub> reflects the age of the vintage, and other cues reflect relevant climatic features where the grapes were grown. The weights for the cues were assigned on the basis of a comparison of these cues to a large set of data on past market prices for mature Bordeaux wines.<sup>3</sup></p>\n<p>There are other ways to construct SPRs, but rather than survey these details, I will instead survey the incredible success of SPRs.</p>\n<p><a id=\"more\"></a></p>\n<ul>\n<li>Howard and Dawes (1976) found they can reliably predict marital happiness with one of the simplest SPRs ever conceived, using only two cues: P = [rate of lovemaking] - [rate of fighting]. The reliability of this SPR was confirmed by Edwards &amp; Edwards (1977) and by Thornton (1979).</li>\n<li>Unstructured interviews reliably <em>degrade</em>&nbsp;the decisions of gatekeepers (e.g. hiring and admissions officers, parole boards, etc.). Gatekeepers (and SPRs) make better decisions on the basis of dossiers alone than on the basis of dossiers and unstructured interviews. (Bloom and Brundage 1947, DeVaul et. al. 1957, Oskamp 1965, Milstein et. al. 1981; Hunter &amp; Hunter 1984; Wiesner &amp; Cronshaw 1988). If you're hiring, you're probably better off <em>not</em>&nbsp;doing interviews.</li>\n<li>Wittman (1941) constructed an SPR that predicted the success of electroshock therapy for patients more reliably than the medical or psychological staff.</li>\n<li>Carroll et. al. (1988) found an SPR that predicts criminal recidivism better than expert criminologists.</li>\n<li>An SPR constructed by Goldberg (1968) did a better job of diagnosing patients as neurotic or psychotic than did trained clinical psychologists.</li>\n<li>SPRs regularly predict academic performance better than admissions officers, whether for medical schools (DeVaul et. al. 1957), law schools (Swets, Dawes and Monahan 2000), or graduate school in psychology (Dawes 1971).</li>\n<li>SPRs predict loan and credit risk better than bank officers (Stillwell et. al. 1983).</li>\n<li><span style=\"font-family: verdana, Arial, Helvetica, sans-serif, georgia; \">SPRs predict newborns at risk for Sudden Infant Death Syndrome better than human experts do (Lowry 1975; Carpenter et. al. 1977; Golding et. al. 1985).</span></li>\n<li><span style=\"font-family: verdana, Arial, Helvetica, sans-serif, georgia; \">SPRs are better at predicting who is prone to violence than are forensic psychologists (Faust &amp; Ziskin 1988).</span></li>\n<li><span style=\"font-family: verdana, Arial, Helvetica, sans-serif, georgia; \">Libby (1976) found a simple SPR that predicted firm bankruptcy better than experienced loan officers.</span></li>\n</ul>\n<p>And that is barely scratching the surface.</p>\n<p>If this is not amazing enough, consider the fact that even when experts are <em>given</em>&nbsp;the results of SPRs, they <em>still</em>&nbsp;can't outperform those SPRs (Leli &amp; Filskov 1985; Goldberg 1968).</p>\n<p>So why aren't SPRs in use <em>everywhere</em>?&nbsp;Probably, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Bishop-Trout-50-years-of-successful-predictive-modeling-should-be-enough-Lessons-for-philosophy-of-science.pdf\">suggest Bishop &amp; Trout</a>, we deny or ignore the success of SPRs <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Bishop-Trout-50-years-of-successful-predictive-modeling-should-be-enough-Lessons-for-philosophy-of-science.pdf\">because of deep-seated cognitive biases</a>,&nbsp;such as <a href=\"http://wiki.lesswrong.com/wiki/Overconfidence\">overconfidence</a> in our own <a href=\"http://www.amazon.com/Judgment-under-Uncertainty-Heuristics-Biases/dp/0521284147/\">judgments</a>. But if these SPRs work as well as or better than human judgments, shouldn't we <em>use</em>&nbsp;them?</p>\n<p><a href=\"http://video.google.com/videoplay?docid=-1321077408096928789\">Robyn Dawes</a> (2002) drew out the normative implications of such studies:</p>\n<blockquote>\n<p>If a well-validated SPR that is superior to professional judgment exists in a relevant decision making context, professionals should use it, totally absenting themselves from the prediction.</p>\n</blockquote>\n<p>Sometimes, being rational is easy. When there exists a reliable statistical prediction rule for the problem you're considering, you need not waste your brain power trying to make a careful judgment. Just take an&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a> and use the damn SPR.<sup>4</sup></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><em>Recommended Reading</em></p>\n<ul>\n<li><a href=\"http://books.google.com/books?id=3DPNR2AFpkoC&amp;lpg=PA24&amp;dq=bishop%20trout%20amazing%20success%20epistemology&amp;pg=PA24#v=onepage&amp;q&amp;f=false\">Chapter 2</a> of Bishop &amp; Trout, <em><a href=\"http://www.amazon.com/dp/0195162307/\">Epistemology and the Psychology of Human Judgment</a></em></li>\n<li><a href=\"http://books.google.com/books?id=w_fYZZPcgpkC&amp;lpg=PT68&amp;dq=rational%20choice%20in%20an%20uncertain%20world%20how%20do%20statistical%20models%20beat&amp;pg=PT65#v=onepage&amp;q&amp;f=false\">Chapter 3</a> of Dawes &amp; Hastie, <em><a href=\"http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/\">Rational Choice in an Uncertain World</a></em></li>\n<li><a href=\"http://books.google.com/books?id=FfTVDY-zrCoC&amp;lpg=PA719&amp;dq=heuristics%20and%20biases%20clinical%20actuarial%20judgment&amp;pg=PA719#v=onepage&amp;q&amp;f=false\">Chapter 40</a> of (eds.) Gilovich, Griffin, &amp; Kahneman, <em><a href=\"http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/\">Heuristics and Biases: The Psychology of Intuitive Judgment</a></em></li>\n<li>Dawes, \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Dawes-The-Robust-Beauty-of-Improper-Linear-Models-in-Decision-Making2.pdf\">The Robust Beauty of Improper Linear Models in Decision Making</a>\"</li>\n<li><a href=\"http://books.google.com/books?id=J6iq_khf5HkC&amp;lpg=PA75&amp;dq=robyn%20dawes%20prediction%20and%20diagnosis&amp;pg=PA75#v=onepage&amp;q&amp;f=false\">Chapter 3</a> of Dawes, <em><a href=\"http://www.amazon.com/House-Cards-Robyn-Dawes/dp/0684830914/\">House of Cards</a></em></li>\n</ul>\n<p>&nbsp;</p>\n<p><em>Notes</em></p>\n<p><small><sup>1 </sup>Bishop &amp; Trout, <em><a href=\"http://www.amazon.com/dp/0195162307/\">Epistemology and the Psychology of Human Judgment</a></em>, p. 27. The definitive case for this claim is made in a <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/167GroveMeehlClinstix.pdf\">1996 study</a> by Grove &amp; Meehl that surveyed 136 studies yielding 617 comparisons between the judgments of human experts and SPRs (in which humans and SPRs made predictions about the same cases and the SPRs never had more information than the humans). Grove &amp; Meehl found that of the 136 studies, 64 favored the SPR, 64 showed roughly equal accuracy, and 8 favored human judgment. Since these last 8 studies \"do not form a pocket of predictive excellent in which [experts] could profitably specialize,\" Grove and Meehl speculated that these 8 outliers may be due to random sampling error.</small></p>\n<p><small><sup>2</sup> Readers of Less Wrong may recognize SPRs as a relatively simple type of <a href=\"http://en.wikipedia.org/wiki/Expert_system\">expert system</a>.</small></p>\n<p><small><sup>3</sup> But, see <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/3cu1\">Anatoly_Vorobey's fine objections</a>.</small></p>\n<p><small><sup>4</sup> There are occasional exceptions, usually referred to as \"broken leg\" cases. Suppose an SPR reliably predicts an individual's movie attendance, but then you learn he has a broken leg. In this case it may be wise to abandon the SPR. The problem is that there is no general rule for when experts should abandon the SPR. When they are allowed to do so, they abandon the SPR far too frequently, and thus would have been better off sticking strictly to the SPR, even for legitimate \"broken leg\" instances&nbsp;(Goldberg 1968; Sawyer 1966; Leli and Filskov 1984).</small></p>\n<p><small>&nbsp;</small></p>\n<p><em>References</em></p>\n<p><small>Bloom &amp; Brundage (1947). \"Predictions of Success in Elementary School for Enlisted Personnel\", <em>Personnel Research and Test Development in the Natural Bureau of Personnel</em>, ed. D.B. Stuit, 233-61. Princeton: Princeton University Press.</small></p>\n<p><small>Carpenter, Gardner, McWeeny, &amp; Emery (1977). \"Multistage scory systemfor identifying infants at risk of unexpected death\", <em>Arch. Dis. Childh.,</em> 53: 606&minus;612.</small></p>\n<p><small>Carroll, Winer, Coates, Galegher, &amp; Alibrio (1988). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Carroll-Evaluation-Diagnosis-and-Prediction-in-Parole-Decision-Making.pdf\">Evaluation, Diagnosis, and Prediction in Parole Decision-Making</a>\", <em>Law and Society Review,</em>&nbsp;17: 199-228.</small></p>\n<p><small>Dawes (1971). \"A Case Study of Graduate Admissions: Applications of Three Principles of Human Decision-Making\", <em>American Psychologist</em>, 26: 180-88.</small></p>\n<p><small>Dawes (2002). \"T<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Dawes-The-ethics-of-using-or-not-using-statistical-prediction-rules.pdf\">he Ethics of Using or Not Using Statistical Prediction Rules in Psychological Practice and Related Consulting Activities</a>\", <em>Philosophy of Science</em>, 69:&nbsp;S178-S184.</small></p>\n<p><small>DeVaul, Jervey, Chappell, Carver, Short, &amp; O'Keefe (1957). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/DeVaul-School-Performance-of-Initially-Rejected-Students.pdf\">Medical School Performance of Initially Rejected Students</a>\", <em>Journal of the American Medical Association</em>, 257: 47-51.</small></p>\n<p><small>Faust &amp; Ziskin (1988). \"The expert witness in psychology and psychiatry\", <em>Science</em>, 241: 1143&minus;1144.</small></p>\n<p><small>Goldberg (1968). \"Simple Models of Simple Process? Some Research on Clinical Judgments\", <em>American Psychologist</em>, 23: 483-96.</small></p>\n<p><small>Golding, Limerick, &amp; MacFarlane (1985). <em>Sudden Infant Death</em>. Somerset: Open Books.</small></p>\n<p><small>Edwards &amp; Edwards (1977). \"Marriage: Direct and Continuous Measurement\", <em>Bulletin of the Psychonomic Society</em>, 10: 187-88.</small></p>\n<p><small>Howard &amp; Dawes (1976). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Howard-Dawes-Linear-Prediction-of-Marital-Happiness.pdf\">Linear Prediction of Marital Happiness</a>\", <em>Personality and Social Psychology Bulletin</em>, 2: 478-80.</small></p>\n<p><small>Hunter &amp; Hunter (1984). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Hunter-Validity-and-Utility-of-alternative-predictors-of-job-performance.pdf\">Validity and utility of alternate predictors of job performance</a>\", <em>Psychological Bulletin</em>, 96: 72-98</small></p>\n<p><small>Leli &amp; Filskov (1984). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Leli-Clinical-detection-of-intellectual-deterioration-associated-with-brain-damage.pdf\">Clinical Detection of Intellectual Deterioration Associated with Brain Damage</a>\", <em>Journal of Clinical Psychology</em>, 40: 1435&ndash;1441.</small></p>\n<p><small>Libby (1976). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Libby-Man-versus-model-of-man-some-conflicting-evidence.pdf\">Man versus model of man: Some conflicting evidence</a>\", <em>Organizational Behavior and Human Performance</em>, 16: 1-12.</small></p>\n<p><small>Lowry (1975). \"The identification of infants at high risk of early death\", <em>Med. Stats. Report</em>, London School of Hygiene and Tropical Medicine.</small></p>\n<p><small>Milstein, Wildkinson, Burrow, &amp; Kessen (1981). \"Admission Decisions and Performance during Medical School\", <em>Journal of Medical Education</em>, 56: 77-82.</small></p>\n<p><small>Oskamp (1965). \"Overconfidence in Case Study Judgments\", <em>Journal of Consulting Psychology,</em>&nbsp;63: 81-97.</small></p>\n<p><small>Sawyer (1966). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Sawyer-Measurement-and-prediction-clinical-and-statistical.pdf\">Measurement and Prediction, Clinical and Statistical</a>\", <em>Psychological Bulletin</em>, 66: 178-200.</small></p>\n<p><small>Stillwell, Barron, &amp; Edwards (1983). \"Evaluating Credit Applications: A Validation of Multiattribute Utility Weight Elicitation Techniques\", <em>Organizational Behavior and Human Performance</em>, 32: 87-108.</small></p>\n<p><small>Swets, Dawes, &amp; Monahan (2000). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Swets-Psychological-science-can-improve-diagnostic-decisions.pdf\">Psychological Science Can Improve Diagnostic Decisions</a>\",&nbsp;<em>Psychological Science in the Public Interest</em>, 1: 1&ndash;26.</small></p>\n<p><small>Thornton (1977). \"Linear Prediction of Marital Happiness: A Replication\", <em>Personality and Social Psychology Bulletin</em>, 3: 674-76.</small></p>\n<p><small>Wiesner &amp; Cronshaw (1988). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Wiesner-Cronshaw-A-meta-analytic-investigation-of-the-impact-of-interview-format-and-degree-of-structure-on-the-validity-of-the-employment-interview.pdf\">A meta-analytic investigation of the impact of interview format and degree of structure on the validity of the employment interview</a>\", <em>Journal of Applied Psychology</em>, 61: 275-290.</small></p>\n<p><small>Wittman (1941). \"A Scale for Measuring Prognosis in Schizophrenic Patients\", <em>Elgin Papers</em>&nbsp;4: 20-33.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CKW8c2Bngz9yXibSk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 93, "baseScore": 92, "extendedScore": null, "score": 0.000169, "legacy": true, "legacyId": "4495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 92, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 199, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MD3NMLBPCqPfnfre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-18T04:50:20.454Z", "modifiedAt": null, "url": null, "title": "January 2011 Southern California Meetup", "slug": "january-2011-southern-california-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kjAdjv9kkY4AdSuXP/january-2011-southern-california-meetup", "pageUrlRelative": "/posts/kjAdjv9kkY4AdSuXP/january-2011-southern-california-meetup", "linkUrl": "https://www.lesswrong.com/posts/kjAdjv9kkY4AdSuXP/january-2011-southern-california-meetup", "postedAtFormatted": "Tuesday, January 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20January%202011%20Southern%20California%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJanuary%202011%20Southern%20California%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjAdjv9kkY4AdSuXP%2Fjanuary-2011-southern-california-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=January%202011%20Southern%20California%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjAdjv9kkY4AdSuXP%2Fjanuary-2011-southern-california-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjAdjv9kkY4AdSuXP%2Fjanuary-2011-southern-california-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<p>There will be a meetup for Southern California this Sunday, January 23, 2011 at 4PM and running for three to five hours.&nbsp; The meetup is happening at <a href=\"http://www.marcoswesthollywood.com/\">Marco's Trattoria</a>.&nbsp; The address is:</p>\n<p><a href=\"http://maps.google.com/maps?q=8200+Santa+Monica+Blvd+West+Hollywood,+CA+90046&amp;sll=37.0625,-95.677068&amp;sspn=24.791593,71.630859&amp;z=14\"><span class=\"street-address\">8200 Santa Monica Blvd</span><br /><span class=\"locality\">West Hollywood</span>, <span class=\"region\">CA</span> <span class=\"postal-code\">90046</span></a></p>\n<p>If all the people (including guests and high end group estimates) show up we'll be at the limit of the space with 24 attendees.&nbsp; Previous meetups had room for walk-ins and future meetups should as well, but this one is full.&nbsp; If you didn't RSVP in time for this one but want to get an email reminder when the <em>February</em> meetup is scheduled <a href=\"/message/compose/?to=JenniferRM\">send me a PM</a> with contact info.</p>\n<p><a id=\"more\"></a>For those interested in carpooling, see comments for: <a href=\"/r/JenniferRM-drafts/lw/3te/january_2011_southern_california_meetup/3cr6\">San Diego</a>, <a href=\"/lw/3te/january_2011_southern_california_meetup/3crd\">Lake Forest</a>.</p>\n<p>The format for past meetups has varied based on the number of attendees and their interests. At various points we have either tried or considered: <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">paranoid debating</a>, small group \"dinner party conversations\", <a href=\"/lw/2ps/september_2010_southern_california_meetup/2mbq?c=1\">structured rationality exercises</a>, large discussions with people sharing personal experiences with sleep and \"nutraceutical\" interventions for intelligence augmentation, and specialized subprojects to develop tools for quantitatively estimating the value of things like <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a> or <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">existential risk</a> interventions.<br /><br />People at these meetups are generally up for being subjects of fun experiments in group or individual rationality.&nbsp; Also, past experience indicates that interesting top level articles are inspired by conversations that happen at meetups.&nbsp; Expect something awesome to happen... or bring something neat to <em>make</em> something awesome happen!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kjAdjv9kkY4AdSuXP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 6.687623083947609e-07, "legacy": true, "legacyId": "4946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-18T15:56:21.053Z", "modifiedAt": null, "url": null, "title": "Some Morals from the Study of Human Irrationality [Link]", "slug": "some-morals-from-the-study-of-human-irrationality-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gLdrh9HBquhGiWNsa/some-morals-from-the-study-of-human-irrationality-link", "pageUrlRelative": "/posts/gLdrh9HBquhGiWNsa/some-morals-from-the-study-of-human-irrationality-link", "linkUrl": "https://www.lesswrong.com/posts/gLdrh9HBquhGiWNsa/some-morals-from-the-study-of-human-irrationality-link", "postedAtFormatted": "Tuesday, January 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20Morals%20from%20the%20Study%20of%20Human%20Irrationality%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20Morals%20from%20the%20Study%20of%20Human%20Irrationality%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgLdrh9HBquhGiWNsa%2Fsome-morals-from-the-study-of-human-irrationality-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20Morals%20from%20the%20Study%20of%20Human%20Irrationality%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgLdrh9HBquhGiWNsa%2Fsome-morals-from-the-study-of-human-irrationality-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgLdrh9HBquhGiWNsa%2Fsome-morals-from-the-study-of-human-irrationality-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p><span class=\"author vcard fn\">Luke Muehlhauser</span> posted <em>a selection of the &ldquo;morals&rdquo;</em> from Stuart Sutherland&rsquo;s <a href=\"http://www.amazon.com/dp/1905177070/ref=nosim?tag=lukeprogcom-20\">Irrationality</a>.</p>\n<p><strong>Link:</strong> <a href=\"http://commonsenseatheism.com/?p=13556\">commonsenseatheism.com/?p=13556</a></p>\n<p>I was wondering if the same could be done for <a href=\"http://wiki.lesswrong.com/wiki/Sequences\"><em>the Sequences</em></a> and if it would be a good idea or rather hold people off from reading them at full length.</p>\n<p>Is it even possible to summarize each post in one sentence? An example would be <a href=\"/lw/pb/belief_in_the_implied_invisible/\">Belief in the Implied Invisible</a> which could be summarized by a quote from the post <a href=\"/lw/q4/decoherence_is_falsifiable_and_testable/\">Decoherence is Falsifiable and Testable</a>:</p>\n<blockquote>\n<p>If P(Y|X) &asymp; 1, then P(X&and;Y) &asymp; P(X). Which is to say, believing extra details doesn't cost you extra probability when they are logical implications of general beliefs you already have.</p>\n</blockquote>\n<p>One could then compile a quote paper and cite the accompanying posts. As Luke wrote, <em>\"If you want to know his&nbsp;reasons for giving all this advice, <a href=\"http://www.amazon.com/dp/1905177070/ref=nosim?tag=lukeprogcom-20\">read the book</a>.\"</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gLdrh9HBquhGiWNsa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 6.689340408087787e-07, "legacy": true, "legacyId": "4956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3XMwPNMSbaPm2suGz", "DFxoaWGEh9ndwtZhk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-19T00:27:33.631Z", "modifiedAt": null, "url": null, "title": "Meditations on first philosophy", "slug": "meditations-on-first-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:22.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Craig_Heldreth", "createdAt": "2010-06-14T23:30:28.110Z", "isAdmin": false, "displayName": "Craig_Heldreth"}, "userId": "hhKowsjZBQSyBE6c5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/96mPStbubtnQ3FE56/meditations-on-first-philosophy", "pageUrlRelative": "/posts/96mPStbubtnQ3FE56/meditations-on-first-philosophy", "linkUrl": "https://www.lesswrong.com/posts/96mPStbubtnQ3FE56/meditations-on-first-philosophy", "postedAtFormatted": "Wednesday, January 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meditations%20on%20first%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeditations%20on%20first%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96mPStbubtnQ3FE56%2Fmeditations-on-first-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meditations%20on%20first%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96mPStbubtnQ3FE56%2Fmeditations-on-first-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96mPStbubtnQ3FE56%2Fmeditations-on-first-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 912, "htmlBody": "<p><em>I put this post up on my blog this morning (<a href=\"http://craigsspringbranchblog.blogspot.com/2011/01/meditations-on-first-philosophy.html\">link</a>) and some here might like to see it or discuss it.</em></p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.amazon.com/Meditations-First-Philosophy-immortality-demonstrated/dp/1615340289/ref=sr_1_8?s=books&amp;ie=UTF8&amp;qid=1295361617&amp;sr=1-8\">Meditations on first philosophy</a> is the title of the first book ever written in contemporary western  Philosophy, according to the university course I took. It was written by  Rene Descartes in the seventeenth century, who began with the premise  that he wanted to find true factual indubitable knowledge and that the  way to find this was to begin by doubting <span style=\"font-style: italic;\">everything, </span>and  retaining only what could be defended by rigorous logic. So by these  meditations he reasoned to such respected elements as \"I think,  therefore I am.\" This short book continues to be relied on hundreds of  years later and modern (especially French) philosophers such as Sartre,  Foucault and Derrida all explicitly acknowledge the importance of  Descartes' achievement.<br /><br />I am occupied with a similar project.  This was not entirely a conscious choice or any sort of Cartesian magnum  opus. In the past five years I have slowly realized that almost nothing  I think or do is too trivial to closely examine, and I am now of the  attitude that I am willing to examine anything and everything, taking  absolutely nothing for granted.<br /><br />The first major recent instance  changed my note taking practice. This happened in 2005 and was entirely  accidental. I found myself in a seminar that did not absorb my entire  attention, and to entertain myself I experimented with notation methods.  It is no understatement to say that I learned how to take notes all  over again as if for the first time in my life, which is a little odd  considering I was over forty years old at the time. By the end of the  seminar, I still was not engaged in it very attentively, but everybody  else in the seminar felt they had to have a copy of my notes. One of my  fellow attendees referred to them as the <span style=\"font-style: italic;\">Golden</span> notes. I later learned that a similar system was employed by Michel Foucault, which he referred to as <a href=\"http://craigsspringbranchblog.blogspot.com/2010/04/hypomnemata-hypnerotomachia-handycrafts.html\">hypomnemata</a>.<br /><br />The  second big event concerned my vocabulary. In 2006 I thought I had  almost as much vocabulary as I was ever going to need, that this was a  basic skill like tying one's shoes or riding a bicycle that you learn  once and don't worry about it after. In retrospect, that was obviously a  silly attitude, but it was not until I took a vocabulary test which I  thought I had aced and found out I had not aced it that I realized this.  Details are <a href=\"http://craigsspringbranchblog.blogspot.com/2010/07/what-do-you-want-to-be-when-you-grow-up.html\">here</a>.<br /><br />It  was the third of my discoveries which shook me to Cartesian doubting  level. This was months in the making, and thanks to my hypomnemata I  have exact records of its genesis and development. On the 31 March 2010 I  became self-employed, and one of the first tasks I embarked upon was an  overhaul of my diet and workout practices. I took advantage of the  freedom to ignore the clock, eat when hungry, and sleep when tired.  Things seemed great in the beginning and actually for several months  when I noticed that I had lost quite a few pounds. I bought a scale and  weighed myself and was surprised at how little I weighed. It was  actually a little bit alarming.<br /><br />This was in late September. I  began weighing myself daily. Within a couple of weeks I concluded that  eating when hungry was not consistent with the level of working out I  was doing, that I had to eat breakfast immediately upon waking and  continue to feed myself as soon as possible in order to maintain weight.  After a couple of months (and gaining weight very slowly) I did the  numbers. It was a revelation. Here are the constraints:<br /><br />Eight hours sleep per night.<br />Three meals per day.<br />Ninety minutes workout per day.<br />Five hours to fully digest dinner before laying down to sleep.<br /><br />To  my amazement this sums up (including to the minute how much time is  required to prepare and eat breakfast, lunch and dinner) to 24 hours  within 2%; i. e. <span style=\"font-weight: bold;\">there is virtually no slack time in my schedule at all</span>.  After six months and nine days of thinking I was indulging myself in  complete freedom, on the 10th of November 2010 I discovered that the  simple mechanics of running my organism in its proper operational  fashion requires rigid adherence to a fixed daily schedule. Up at 6:00  A. M. every single day without fail. Directly to the kitchen to ingest  my breakfast. After four hours digestion, directly into workout with no  delay. After workout, directly to the kitchen to prepare and consume  lunch. After four hours digestion, directly to the kitchen to prepare  and consume dinner. Five hours after dinner, directly to bed to sleep  for eight hours. I am self-employed and I have no boss.<br /><br />Ha! My  body is the most rigid task master I have ever known. A fact which I  could have, should have known my entire life but it took me six months  of being unemployed or self-employed (however you prefer to frame it) to  figure it out. And so now I am wondering what else is ripe for making  over, and I am inclined to consider anything--speaking, reading,  walking, standing,  sitting, listening. Even breathing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "96mPStbubtnQ3FE56", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 1, "extendedScore": null, "score": 6.690659104904618e-07, "legacy": true, "legacyId": "4957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-19T01:44:13.376Z", "modifiedAt": null, "url": null, "title": "Erroneous Visualizations", "slug": "erroneous-visualizations", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:22.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BecomingMyself", "createdAt": "2011-01-15T16:35:07.919Z", "isAdmin": false, "displayName": "BecomingMyself"}, "userId": "57B3d3KkqqRPsut57", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FCjQZY9ditqRWEWR5/erroneous-visualizations", "pageUrlRelative": "/posts/FCjQZY9ditqRWEWR5/erroneous-visualizations", "linkUrl": "https://www.lesswrong.com/posts/FCjQZY9ditqRWEWR5/erroneous-visualizations", "postedAtFormatted": "Wednesday, January 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Erroneous%20Visualizations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AErroneous%20Visualizations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCjQZY9ditqRWEWR5%2Ferroneous-visualizations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Erroneous%20Visualizations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCjQZY9ditqRWEWR5%2Ferroneous-visualizations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCjQZY9ditqRWEWR5%2Ferroneous-visualizations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 411, "htmlBody": "<p>Buried somewhere among Eliezer's writings is something essentially the same as the following phrase:</p>\n<p>\"Intentional causes are made of neurons. Evolutionary causes are made of ancestors.\"</p>\n<p>I remember this quite well because of my strange reaction to it. I understood what it meant pretty well, but upon seeing it, some demented part of my brain immediately constructed a mental image of what it thought an \"evolutionary cause\" looked like. The result was something like a mountain of fused-together bodies (the ancestors) with gears and levers and things (the causation) scattered throughout. \"This,\" said that part of my brain, \"is what an evolutionary cause looks like, and <a href=\"/lw/ir/science_as_attire/\">like a good reductionist</a> I know it is physically implicit in the structure of my brain.\" Luckily it didn't take me long to realize what I was doing and reject that model, though I am just now realizing that the one I replaced it with still had some physical substance called \"causality\" flowing from ancient humans to my brain.</p>\n<p>This is actually a common error for me. I remember I used to think of computer programs as these glorious steampunk assemblies of wheels and gears and things (apparently gears are a common visual metaphor in my brain for things it labels as complex) floating just outside the universe with all the other platonic concepts, somehow exerting their patterns upon the computers that ran them. It took me forever to figure out that these strange thingies were physical systems in the computers themselves, and a bit longer to realize that they didn't look anything like what I thought they did. (I still haven't bothered to find out what they really are, despite having a non-negligible desire to know.) And even before that -- long before I started reading Less Wrong, or even adopted empiricism (which may or may not have come earlier), I decided that because&nbsp;the human brain performs computation,&nbsp;and&nbsp;(it seemed to me) all computations were embodiments of some platonic ideal, souls must exist. Which could have been semi-okay, if I had realized that calling it a \"soul\" shouldn't allow you to <a href=\"/lw/ny/sneaking_in_connotations/\">assume it has properties that you ascribe to \"souls\" but not to \"platonic ideals of computation\"</a>.</p>\n<p>Are errors like this common? I talked to a friend about it and she doesn't make this mistake, but one person is hardly a good sample. If anyone else is like this, I'd like to know how often it causes really big misconceptions and whether you have a way to control it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FCjQZY9ditqRWEWR5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "4955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4Bwr6s9dofvqPWakn", "yuKaWPRTxZoov4z8K"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-19T06:12:02.271Z", "modifiedAt": null, "url": null, "title": "\"Manna\" by Marshall Brain", "slug": "manna-by-marshall-brain", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:19.915Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Yt4XLvn9hreMRYk9K/manna-by-marshall-brain", "pageUrlRelative": "/posts/Yt4XLvn9hreMRYk9K/manna-by-marshall-brain", "linkUrl": "https://www.lesswrong.com/posts/Yt4XLvn9hreMRYk9K/manna-by-marshall-brain", "postedAtFormatted": "Wednesday, January 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Manna%22%20by%20Marshall%20Brain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Manna%22%20by%20Marshall%20Brain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYt4XLvn9hreMRYk9K%2Fmanna-by-marshall-brain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Manna%22%20by%20Marshall%20Brain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYt4XLvn9hreMRYk9K%2Fmanna-by-marshall-brain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYt4XLvn9hreMRYk9K%2Fmanna-by-marshall-brain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p><a href=\"http://www.marshallbrain.com/manna1.htm\">Oldie but goodie</a>. A piece of fiction describing how a computer system can do the job of human managers at fast food restaurants (scarily plausible), how this leads to a dystopia (slowly getting implausible), and how to avoid this scenario and reach utopia (give me a break).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Yt4XLvn9hreMRYk9K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 6.691546673954087e-07, "legacy": true, "legacyId": "4970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-19T21:45:59.444Z", "modifiedAt": null, "url": null, "title": "Starcraft AI Competition", "slug": "starcraft-ai-competition", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DjTqPBP4wChcAcGbj/starcraft-ai-competition", "pageUrlRelative": "/posts/DjTqPBP4wChcAcGbj/starcraft-ai-competition", "linkUrl": "https://www.lesswrong.com/posts/DjTqPBP4wChcAcGbj/starcraft-ai-competition", "postedAtFormatted": "Wednesday, January 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Starcraft%20AI%20Competition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStarcraft%20AI%20Competition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjTqPBP4wChcAcGbj%2Fstarcraft-ai-competition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Starcraft%20AI%20Competition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjTqPBP4wChcAcGbj%2Fstarcraft-ai-competition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjTqPBP4wChcAcGbj%2Fstarcraft-ai-competition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 693, "htmlBody": "<p>Ars Technica has an article about <a href=\"http://arstechnica.com/gaming/news/2011/01/skynet-meets-the-swarm-how-the-berkeley-overmind-won-the-2010-starcraft-ai-competition.ars\">A Starcraft AI competition.</a>. While this is clearly narrow AI there are some details which may interest people at LW. The article is about the best performing AI, the \"Berkeley Overmind.\" (The AI in question only played as Zerg, one of the three possible sides in Starcraft. In fact, it seems that the AIs in general were all specialized for a single one of the three sides. While human players are often much better at one specific side, they are not nearly this specialized).</p>\n<p>Highlights from the article:</p>\n<blockquote>\n<p><em>StarCraft</em> was released in 1998, an eternity ago by video  game standards.  Over those years Blizzard Entertainment, the game&rsquo;s  creator, has continually updated it so that it&rsquo;s one of the most finely  tuned and balanced Real Time Strategy (RTS) games ever made. It has  three playable races: the human-like Terrans, with familiar tanks and  starships, the alien Zerg, with large swarms of organic creatures, and  the Protoss, technologically advanced aliens reliant on powerful but  expensive units. Each race has different units and gameplay  philosophies, yet no one race or combination of units has an unbeatable  advantage. Player skill, ingenuity, and the ability to react  intelligently to enemy actions determine victory.</p>\n<p>This refinement and complexity makes&nbsp;<em>StarCraft</em> an ideal  environment for conducting AI research. In an RTS game, events unfold in  real-time and players&rsquo; orders are carried out immediately. Resources  have to be gathered so fighting units can be produced and commanded into  battle. The map is shrouded in fog-of-war, so enemy units and buildings  are only visible when they&rsquo;re near friendly buildings or units. A <em>StarCraft</em> player has to acquire and allocate resources to create units,  coordinate those units in combat, discover, reason about and react to  enemy actions, and do all this in real-time. These are all hard problems  for a computer to solve.</p>\n</blockquote>\n<p>Note, that using the interface that humans need to use was not one of the restrictions. This was an advantage that the Berkeley group used to full effect, as did other AIs in the comptetion.</p>\n<blockquote>\n<p>We had to limit ourselves. David Burkett, another of Dan&rsquo;s PhD  students and the other team lead, says, &ldquo;It turns out building control  nodes for units is hard, so there&rsquo;s a huge cost associated with building  more than one [type of] unit. So we started asking: what one unit type  [would be] the most effective overall?&rdquo;</p>\n<p>We focused our efforts on Zerg mutalisks: fast, dragon-like flying  creatures that can attack both air and ground targets. Their mobility is  unmatched, and we suspected they would be particularly amenable to  computer control. Mutalisks are cheap for their strength, but large  numbers are rarely seen in human play because it&rsquo;s hard for a human to  manage mutalisks without clumping them and making them easy prey for  enemies with area attacks (attacks that do damage to all units in an  area instead of a single target). A computer would have no such  limitations.</p>\n</blockquote>\n<p>The programmers then used a series of potential fields to control what the mutalisks did, with different entities and events creating different potential fields. A major issue became how to weigh these fields:</p>\n<blockquote>\n<p>Using <em>StarCraft</em>&rsquo;s  map editor, we built Valhalla for the Overmind, where it could  repeatedly and automatically run through different combat scenarios. By  running repeated trials in Valhalla and varying the potential field  strengths, the agent learned the best combination of parameters for each  kind of engagement.</p>\n</blockquote>\n<p>The article unfortunately doesn't go into great detail about the exact learning mechanism. Note however that this implies that the Overmind should be able to learn how to respond to other unit types.</p>\n<p>There are other details in the article that are also interesting. For example, they replaced the standard path tracing algorithm that units do automatically with their own algorithms.</p>\n<p>The final form of the AI can play well against very skilled human players, but it isn't at the top of the game. Note also that the Overmind is designed for one-on-one games. It should be interesting to see how this AI and similar AIs improve over the next few years. I'd be very curious how an AIXI would do in this sort of situation.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DjTqPBP4wChcAcGbj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 6.693958908065517e-07, "legacy": true, "legacyId": "4988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-19T22:02:58.267Z", "modifiedAt": null, "url": null, "title": "Should we have secular churches?", "slug": "should-we-have-secular-churches", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:56.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DnjBQAWE7jYhYytmr/should-we-have-secular-churches", "pageUrlRelative": "/posts/DnjBQAWE7jYhYytmr/should-we-have-secular-churches", "linkUrl": "https://www.lesswrong.com/posts/DnjBQAWE7jYhYytmr/should-we-have-secular-churches", "postedAtFormatted": "Wednesday, January 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20we%20have%20secular%20churches%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20we%20have%20secular%20churches%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnjBQAWE7jYhYytmr%2Fshould-we-have-secular-churches%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20we%20have%20secular%20churches%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnjBQAWE7jYhYytmr%2Fshould-we-have-secular-churches", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDnjBQAWE7jYhYytmr%2Fshould-we-have-secular-churches", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 270, "htmlBody": "<p>In the <a href=\"/lw/3sb/the_annoyingness_of_new_atheists_declaring_god/3cna\" target=\"_self\">comments of a recent thread</a>, another poster pointed out that religious individuals tend to report higher levels of happiness than nonreligious individuals. I suggested that the social network of churches, rather than the direct effects of theistic belief, might be responsible for this difference, and after doing a bit of searching around to see if the available studies support such an explanation, <a href=\"http://www.livescience.com/health/religion-happiness-church-friends-101207.html\" target=\"_self\">found a study that indicates that this is indeed the case</a>.</p>\n<p>Religious churches may be far from optimal in the services they provide to communities, but they have a great positive impact on the lives of many individuals. And not just as friendly social gatherings and occasional providers of community service; I've known priests who were superb community organizers and motivational speakers, who played an important role for their congregations to which I know of no existing secular analogue.</p>\n<p>It seems probable that a secular organization could effectively play the same role in a community, but would anyone be likely to take it seriously? Since people who're already religious may be inclined to reject the value of a secular authority filling the role of a church, and atheistic individuals may not be inclined to attend, either due to reversing the stupidity of religion, or due to <a href=\"/lw/3h/why_our_kind_cant_cooperate/\" target=\"_self\">asocial and anticooperative values</a>, it's uncertain whether a secular organization that adequately filled the role of a church would get off the ground in the first place in the present social climate.</p>\n<p>So, what are your feelings on the prospect of secular church analogues? Do you think that they're appropriate or practical? Do you expect them ever to become common in real life?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DnjBQAWE7jYhYytmr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 6.694002756015018e-07, "legacy": true, "legacyId": "4989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T00:18:34.164Z", "modifiedAt": null, "url": null, "title": "Theists are wrong; is theism?", "slug": "theists-are-wrong-is-theism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:09.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a57ZERSKF7oHwW46j/theists-are-wrong-is-theism", "pageUrlRelative": "/posts/a57ZERSKF7oHwW46j/theists-are-wrong-is-theism", "linkUrl": "https://www.lesswrong.com/posts/a57ZERSKF7oHwW46j/theists-are-wrong-is-theism", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Theists%20are%20wrong%3B%20is%20theism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATheists%20are%20wrong%3B%20is%20theism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa57ZERSKF7oHwW46j%2Ftheists-are-wrong-is-theism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Theists%20are%20wrong%3B%20is%20theism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa57ZERSKF7oHwW46j%2Ftheists-are-wrong-is-theism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa57ZERSKF7oHwW46j%2Ftheists-are-wrong-is-theism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 583, "htmlBody": "<p>Many folk here on LW take the simulation argument (in its more general forms) seriously. Many others take Singularitarianism<sup>1</sup> seriously. Still others take Tegmark cosmology (and related big universe hypotheses) seriously. But then I see them proceed to self-describe as atheist (instead of omnitheist, theist, deist, having a predictive distribution over states of religious belief, et cetera), and many tend to be overtly dismissive of theism. Is this signalling cultural affiliation, an attempt to communicate a point estimate, or what?</p>\n<p>I am especially confused that the theism/atheism debate is considered a closed question on Less Wrong. Eliezer's reformulations of the Problem of Evil in terms of Fun Theory provided a fresh look at theodicy, but I do not find those arguments conclusive. A look at&nbsp;<a href=\"http://commonsenseatheism.com/\">Luke&nbsp;Muehlhauser's blog</a>&nbsp;surprised me; the arguments against theism are just not nearly as convincing as I'd been brought up to believe<sup>2</sup>, nor nearly convincing enough to cause what I saw as massive overconfidence on the part of most atheists, aspiring rationalists or no.</p>\n<p>It may be that theism is in the class of hypotheses that we have yet to develop a strong enough practice of rationality to handle, even if the hypothesis has non-negligible probability given our best understanding of the evidence. We are becoming adept at wielding Occam's razor, but it may be that we are still too foolhardy to wield <span style=\"text-decoration: line-through;\">Solomonoff's lightsaber</span> Tegmark's Black Blade of Disaster without chopping off our own arm. The literature on cognitive biases gives us every reason to believe we are poorly equipped to reason about infinite cosmology, decision theory, the motives of superintelligences, or our place in the universe.</p>\n<p>Due to these considerations, it is unclear if we should go ahead doing the equivalent of philosoraptorizing amidst these poorly asked questions so far outside the realm of science. This is not the sort of domain where one should tread if one is feeling insecure in one's sanity, and it is possible that no one should tread here. Human philosophers are probably not as good at philosophy as hypothetical Friendly AI philosophers (though we've seen in the cases of decision theory and utility functions that not everything can be left for the AI to solve). I don't want to stress your epistemology too much, since it's not like your immortal soul<sup>3</sup> matters very much. Does it?</p>\n<p><strong>Added: </strong>By theism I do not mean the hypothesis that Jehovah created the universe. (Well, mostly.) I am talking about the possibility of agenty processes in general creating this universe, as opposed to impersonal math-like processes like cosmological natural selection.</p>\n<p><strong>Added: </strong>The answer to the question raised by the post is \"Yes, theism is wrong, and we don't have good words for the thing that looks a lot like theism but has less unfortunate connotations, but we do know that calling it theism would be stupid.\" As to whether this universe gets most of its reality fluid from agenty creators... perhaps we will come back to that argument on a day with less distracting terminology on the table.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><sup>1</sup>&nbsp;Of either the 'AI-go-FOOM' or 'someday we'll be able to do lots of brain emulations' variety.</p>\n<p><sup>2 </sup>I was never a theist, and only recently began to question some old assumptions about the likelihood of various Creators. This perhaps either lends credibility to my interest, or lends credibility to the idea that I'm insane.</p>\n<p><sup>3&nbsp;</sup>Or the set of things that would have been translated to Archimedes by the Chronophone as the equivalent of an immortal soul (id est, whatever concept ends up being actually significant).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a57ZERSKF7oHwW46j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 92, "baseScore": 2, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "4987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 539, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T01:06:32.742Z", "modifiedAt": null, "url": null, "title": "Steve Jobs' medical leave, riches and longevity", "slug": "steve-jobs-medical-leave-riches-and-longevity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.769Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sfb", "createdAt": "2010-04-20T23:33:34.298Z", "isAdmin": false, "displayName": "sfb"}, "userId": "x3wHkQG26AsREHbsw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/btoJ8qH4jGssigHtd/steve-jobs-medical-leave-riches-and-longevity", "pageUrlRelative": "/posts/btoJ8qH4jGssigHtd/steve-jobs-medical-leave-riches-and-longevity", "linkUrl": "https://www.lesswrong.com/posts/btoJ8qH4jGssigHtd/steve-jobs-medical-leave-riches-and-longevity", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Steve%20Jobs'%20medical%20leave%2C%20riches%20and%20longevity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASteve%20Jobs'%20medical%20leave%2C%20riches%20and%20longevity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtoJ8qH4jGssigHtd%2Fsteve-jobs-medical-leave-riches-and-longevity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Steve%20Jobs'%20medical%20leave%2C%20riches%20and%20longevity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtoJ8qH4jGssigHtd%2Fsteve-jobs-medical-leave-riches-and-longevity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtoJ8qH4jGssigHtd%2Fsteve-jobs-medical-leave-riches-and-longevity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 408, "htmlBody": "<p>Heinlein imagined the Howard Foundation as a group founded by a millionaire dying of \"old age\" in mid-life; founded to encourage long lived people to have children together using financial incentives with the goal of breeding extra long-lived humans.</p>\n<p>Our world has many billionaires, typically older rather than younger people ( <a title=\"Forbes list\" href=\"http://www.forbes.com/lists/2010/10/billionaires-2010_The-Worlds-Billionaires_Rank.html\">Forbes list</a> - few under 50 ); middle aged Bill Gates has donated his fortune to normal kinds of charity, elder Warren Buffett has as well, youthful Mark Zuckerberg has pledged his too, all healthy. Steve Jobs is worth over a billion dollars and has been criticised for his lack of public philanthropy, he's also CEO of a company with $60 billion in reserve, and suffering serious health problems.</p>\n<p>In short, we live in a world where there are rich people, and where you hear the idea of \"rich old white men spending a lot on medical treatments to benefit rich old white men\" but at the same time, Aubrey De Gray style serious discussion of longevity is rare&nbsp; and much medical spending goes on alleviating and curing the problems of old age rather than avoiding them.</p>\n<p>Sergey Brin has donated <a href=\"http://www.wired.com/magazine/2010/06/ff_sergeys_search/all/1\">$50 million</a> towards Parkinson's research based on DNA tests showing he has a 50% chance of getting it, yet at the moment he has a much higher probability of getting old-age and a net worth of $10-15 billion.</p>\n<p>Is our world one where something analagous to the Howard Foundation will appear? Let's pull some numbers from thin air and say that means someone dying and leaving pretty much all of their estate of more than $200 million to fund longevity research/treatment in some way. If so, it might be something done in private that we would not hear of, so what would be indicators that it might be about to happen, or might have happened already? And if an ill middle aged technology billionaire with change-the-world drive doesn't do it, then who?</p>\n<p>&nbsp;</p>\n<p>(I consider significant increases in lifespan (100 healthy years, 150 total years, or more) nearly inevitable at some nonspecific time in the future, given a world where humans continue to develop technology improvements, remain primarily biological, and avert or avoid existential risks and government restrictions on it. I also consider that dying and leaving money to The Gates Foundation is a good cause arguably much better than longevity research, wheras dying and leaving hundreds of millions to heirs / dogs homes / art / etc. is not).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "btoJ8qH4jGssigHtd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 18, "extendedScore": null, "score": 6.694476829071695e-07, "legacy": true, "legacyId": "4990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T02:04:58.129Z", "modifiedAt": null, "url": null, "title": "Networks, Crowds, and Markets textbook", "slug": "networks-crowds-and-markets-textbook", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ft9n7hjYG69QXWBf5/networks-crowds-and-markets-textbook", "pageUrlRelative": "/posts/Ft9n7hjYG69QXWBf5/networks-crowds-and-markets-textbook", "linkUrl": "https://www.lesswrong.com/posts/Ft9n7hjYG69QXWBf5/networks-crowds-and-markets-textbook", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Networks%2C%20Crowds%2C%20and%20Markets%20textbook&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANetworks%2C%20Crowds%2C%20and%20Markets%20textbook%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFt9n7hjYG69QXWBf5%2Fnetworks-crowds-and-markets-textbook%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Networks%2C%20Crowds%2C%20and%20Markets%20textbook%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFt9n7hjYG69QXWBf5%2Fnetworks-crowds-and-markets-textbook", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFt9n7hjYG69QXWBf5%2Fnetworks-crowds-and-markets-textbook", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>I thought some people might like to see a free textbook on networks. &nbsp;This is <em>not</em>&nbsp;a graph theory course; it covers an enormous breadth of topics including game theory, and it's written in plain English with almost no mathematical technicality. &nbsp;There are a lot of examples related to the social sciences, frequently citing and summarizing specific studies, and this is the real value of the book, I think -- it's almost an overview of the field of \"math applied to the social sciences.\" &nbsp;Some of the examples are fascinating -- for example, I now have the first convincing explanation I've yet seen for why WWI began. &nbsp;</p>\n<p>Should be a fairly quick read; I'm up to Ch. 7 in a few hours this evening.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.cs.cornell.edu/home/kleinber/networks-book/\" target=\"_blank\">Networks, Crowds, and Markets, by Easley and Kleinberg</a>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ft9n7hjYG69QXWBf5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.694627716400151e-07, "legacy": true, "legacyId": "4992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T03:46:47.006Z", "modifiedAt": null, "url": null, "title": "I Want to Learn About Education", "slug": "i-want-to-learn-about-education", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.559Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5mFBqN2wYv7DzC5rQ/i-want-to-learn-about-education", "pageUrlRelative": "/posts/5mFBqN2wYv7DzC5rQ/i-want-to-learn-about-education", "linkUrl": "https://www.lesswrong.com/posts/5mFBqN2wYv7DzC5rQ/i-want-to-learn-about-education", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20Want%20to%20Learn%20About%20Education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20Want%20to%20Learn%20About%20Education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5mFBqN2wYv7DzC5rQ%2Fi-want-to-learn-about-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20Want%20to%20Learn%20About%20Education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5mFBqN2wYv7DzC5rQ%2Fi-want-to-learn-about-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5mFBqN2wYv7DzC5rQ%2Fi-want-to-learn-about-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p>I want to learn what's well-understood about education. I expect to launch myself into some endeavors in teaching the first few levels of epistemic and instrumental rationality - ie., critical thinking and problem solving. I'm a little suspicious, though, of the scattered educational texts that I've so far read. In particular, education seems like a field where it's easy to have motivated thoughts, and hard to gather good data.</p>\n<p>With my background (Math and CS) I'm a little at sea in educational literature. Does anyone know of good, reductionist-grade or evidential-grade, introductory texts in education?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5mFBqN2wYv7DzC5rQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 6.694890683969401e-07, "legacy": true, "legacyId": "4996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T06:22:13.396Z", "modifiedAt": null, "url": null, "title": "Meta: A 5 karma requirement to post in discussion", "slug": "meta-a-5-karma-requirement-to-post-in-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:09.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/24W5yFDBLEQiuKcmM/meta-a-5-karma-requirement-to-post-in-discussion", "pageUrlRelative": "/posts/24W5yFDBLEQiuKcmM/meta-a-5-karma-requirement-to-post-in-discussion", "linkUrl": "https://www.lesswrong.com/posts/24W5yFDBLEQiuKcmM/meta-a-5-karma-requirement-to-post-in-discussion", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20A%205%20karma%20requirement%20to%20post%20in%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20A%205%20karma%20requirement%20to%20post%20in%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24W5yFDBLEQiuKcmM%2Fmeta-a-5-karma-requirement-to-post-in-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20A%205%20karma%20requirement%20to%20post%20in%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24W5yFDBLEQiuKcmM%2Fmeta-a-5-karma-requirement-to-post-in-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24W5yFDBLEQiuKcmM%2Fmeta-a-5-karma-requirement-to-post-in-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Admins have been doing a decent, timely job taking down the spam that comes up in the Discussion section. But it is an eyesore for any period of time and there seems to be more and more of it. And there is an easy solution: a small karma requirement for discussion section posts. I think 5 would about right. A reasonable, literate person can get 5 karma pretty easily. \"Hi, I'm new\" usually does it. That plus a half-way insightful comment about something almost definitely will. This would screen out the spammers. As for the occasional genuine user that posts in discussion before commenting at all, I don't know how many there have been but my sense is that delaying them from posting until they can get five upvotes is almost certainly a <em>good</em> thing.</p>\n<p>Thoughts? Or is changing this actually a difficult task that requires rewriting the site's code and that's why it hasn't been done already?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "24W5yFDBLEQiuKcmM", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 66, "extendedScore": null, "score": 6.695292190520967e-07, "legacy": true, "legacyId": "5005", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-04-30T02:20:06.403Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T09:08:05.761Z", "modifiedAt": null, "url": null, "title": "Eliezer is dead (not really)", "slug": "eliezer-is-dead-not-really", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.733Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Miller", "createdAt": "2010-11-11T22:17:48.680Z", "isAdmin": false, "displayName": "Miller"}, "userId": "Kb4oLoRnX2oxKZYe3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rg8dzgtB8qgEpZb95/eliezer-is-dead-not-really", "pageUrlRelative": "/posts/Rg8dzgtB8qgEpZb95/eliezer-is-dead-not-really", "linkUrl": "https://www.lesswrong.com/posts/Rg8dzgtB8qgEpZb95/eliezer-is-dead-not-really", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eliezer%20is%20dead%20(not%20really)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEliezer%20is%20dead%20(not%20really)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRg8dzgtB8qgEpZb95%2Feliezer-is-dead-not-really%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eliezer%20is%20dead%20(not%20really)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRg8dzgtB8qgEpZb95%2Feliezer-is-dead-not-really", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRg8dzgtB8qgEpZb95%2Feliezer-is-dead-not-really", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>edit: Getting bombed and going on a rant on Less Wrong seemed like a good idea at the time.. I've placed a line in this post below which the value of the content decays rapidly.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>I bet he spends less than 4 hours a day on his book. Where is he?</p>\n<p>These communities decay without obvious leadership, and it doesn't exist here. Let's acknowledge this fact about human collaboration that it requires denoted leadership. If I had an insanely, objectively awesome idea about how to modify this site it would be an uphill battle to convert that to implementation. Robin Hanson was happy to go solo on his site because it is the strength of the man at the top that matters not the strength of his commenting readership. He wouldn't give two shits about particular people dropping off his readership including Eliezer apparently (which is indeed a loss, but recoverable.)</p>\n<p>Let's acknowledge the value of aristocracy. Where is Eliezer? Should we appoint some leadership and declare him dead? Can we?</p>\n<p>Web 2.0 circa 2004-now is all about entrepreneurs generating value from the donated efforts of others. I don't plan on being one of those slaves.</p>\n<p>edit: the first vote was negative. I clearly expect that as a superficial response. I am quite adaptable to argument, give me reasons please.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rg8dzgtB8qgEpZb95", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": -22, "extendedScore": null, "score": -1.3e-05, "legacy": true, "legacyId": "5015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T09:18:10.037Z", "modifiedAt": null, "url": null, "title": "Who are these spammers? ", "slug": "who-are-these-spammers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rxCEqL64YiY45JCoo/who-are-these-spammers", "pageUrlRelative": "/posts/rxCEqL64YiY45JCoo/who-are-these-spammers", "linkUrl": "https://www.lesswrong.com/posts/rxCEqL64YiY45JCoo/who-are-these-spammers", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Who%20are%20these%20spammers%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWho%20are%20these%20spammers%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrxCEqL64YiY45JCoo%2Fwho-are-these-spammers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Who%20are%20these%20spammers%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrxCEqL64YiY45JCoo%2Fwho-are-these-spammers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrxCEqL64YiY45JCoo%2Fwho-are-these-spammers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p>The <a href=\"r/discussion/lw/3v1/meta_a_5_karma_requirement_to_post_in_discussion/\">proposal</a> for a minimum karma of 5 to post in discussion might solve the current  spam problem. Or it might just slow the spammers down. But all this spam  is coming from just one source, because it all advertises the same  thing - \"Pandora charm bracelets\". In principle, therefore, one might  seek a more permanent solution. Doing that, and perhaps even just  talking about it, has a danger of its own - what if the spammers call on  their friends and colleagues? What if they're egosurfing - checking on  the image of their \"brand\" - and run across the discussion? Maybe the  wise course of action for an intellectual community trying to have  serious discussions, undisturbed, is to do the minimum thing necessary  to block the source of noise, but not to <a href=\"http://threatpost.com/en_us/blogs/report-zdnets-danchev-hospitalized-011711\">provoke  it</a>.</p>\n<p>Nonetheless, knowledge is supposed to be power, and it must be  possible to discover something of who these spammers are, where they are  physically located, what their methods are, what their history is, and  what recourse the victims of their harassment have. Just today there  have been posts from \"pandorabracelet\", \"pandoracharm\", \"charmthomassabo\",  \"pandorabraceletsuk\", and \"PandoraJewelryuk\". There are no links visible  in the messages, presumably because their methods aren't quite tuned to  the peculiarities of LW's markup syntax. But it turns out that  pandora-jewelry.uk.com is an existing domain. <a href=\"http://www.ipillion.com/site/pandora-jewelry.uk.com\">WHOIS  information</a> reveals a Russian IP, a registrant with a fake British address but a  Chinese-sounding email address, and an <a href=\"http://icannwiki.com/index.php/Billy_Watenpaugh\">American  registrar</a>.</p>\n<p>Meanwhile,  in case you were wondering: the real Pandora is a <a href=\"http://en.wikipedia.org/wiki/Pandora_%28jewelry%29\">Danish jewelry  company</a>. But these spammers are promoting a scam which has nothing to do with the real Pandora. It is unclear to me whether their business model is to sell fakes, to just pocket the money and not send anything at all, or whether they're actually collecting credit-card information - it may be some combination of these - but <a href=\"http://www.complaintsboard.com/complaints/pandora-jewelleryukcom-c370689.html\">the victims speak here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rxCEqL64YiY45JCoo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 6.695746711574459e-07, "legacy": true, "legacyId": "5017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["24W5yFDBLEQiuKcmM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-20T20:44:43.868Z", "modifiedAt": null, "url": null, "title": "Scientific Self-Help: The State of Our Knowledge", "slug": "scientific-self-help-the-state-of-our-knowledge", "viewCount": null, "lastCommentedAt": "2020-06-22T16:47:29.159Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/33KewgYhNSxFpbpXg/scientific-self-help-the-state-of-our-knowledge", "pageUrlRelative": "/posts/33KewgYhNSxFpbpXg/scientific-self-help-the-state-of-our-knowledge", "linkUrl": "https://www.lesswrong.com/posts/33KewgYhNSxFpbpXg/scientific-self-help-the-state-of-our-knowledge", "postedAtFormatted": "Thursday, January 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scientific%20Self-Help%3A%20The%20State%20of%20Our%20Knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScientific%20Self-Help%3A%20The%20State%20of%20Our%20Knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F33KewgYhNSxFpbpXg%2Fscientific-self-help-the-state-of-our-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scientific%20Self-Help%3A%20The%20State%20of%20Our%20Knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F33KewgYhNSxFpbpXg%2Fscientific-self-help-the-state-of-our-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F33KewgYhNSxFpbpXg%2Fscientific-self-help-the-state-of-our-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3018, "htmlBody": "<h5><span style=\"font-weight: normal;\">Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></span></h5>\n<p><a href=\"/lw/9p/rationality_its_not_that_great/\">Some</a> <a href=\"/lw/bg/instrumental_rationality_is_a_chimera/\">have</a> <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">suggested</a> <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l5h\">that</a> the Less Wrong community could improve readers' <a href=\"/lw/2p5/humans_are_not_automatically_strategic\">instrumental rationality</a> more effectively if it first&nbsp;<a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">caught up with the scientific literature</a> on productivity and self-help, and then enabled readers to deliberately practice <a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">self-help skills</a> and apply what they've learned <a href=\"/lw/34m/what_ive_learned_from_less_wrong/\">in real life</a>.</p>\n<p>I think that's a good idea. My contribution today is a quick overview of scientific self-help: what professionals call \"the psychology of adjustment.\" First I'll review the state of the industry and the scientific literature, and then I'll briefly summarize the scientific data available on three topics in self-help: study methods, productivity, and happiness.</p>\n<h4>The industry and the literature</h4>\n<p>As you probably know, much of <a href=\"http://www.amazon.com/Self-Help-Inc-Makeover-Culture-American/dp/0195171241/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">the self-help</a> <a href=\"http://www.amazon.com/Oracle-Supermarket-American-Preoccupation-Self-Help/dp/0765809648/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">industry</a> is a <a href=\"http://www.amazon.com/Sham-Self-Help-Movement-America-Helpless/dp/1400054095/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">sham</a>, <a href=\"http://www.amazon.com/Lost-Cosmos-Last-Self-Help-Book/dp/0312253990/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">ripe</a> for <a href=\"http://www.amazon.com/Secrets-SuperOptimist-W-R-Morton/dp/0977480704/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">parody</a>.&nbsp;Most self-help books are written to <em>sell</em>, <a href=\"http://www.amazon.com/Im-Dysfunctional-Youre-Recovery-Self-Help/dp/0201570629/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">not to <em>help</em></a>. <a href=\"http://www.amazon.com/Fools-Paradise-Unreal-World-Psychology/dp/1566636280/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Pop psychology</a> may be&nbsp;<a href=\"http://www.amazon.com/Great-Myths-Popular-Psychology-Misconceptions/dp/1405131128/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">more myth</a> <a href=\"http://www.amazon.com/House-Cards-Robyn-Dawes/dp/0684830914/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">than fact</a>. As Christopher Buckley (1999) writes, \"The more people read [self-help books], the more they think they need them... [it's] more like an addiction than an alliance.<span style=\"font-size: 11px;\">\"</span></p>\n<p>Where can you turn for reliable, empirically-based self-help advice? A few leading therapeutic psychologists (e.g., <a href=\"http://www.amazon.com/Albert-Ellis/e/B000APLUAE/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Albert Ellis</a>, <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=arnold+lazarus&amp;x=0&amp;y=0&amp;tag=lesswrong-20\">Arnold Lazarus</a>, <a href=\"http://www.amazon.com/Martin-E.-P.-Seligman/e/B001ILOB78/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Martin Seligman</a>) have written self-help books based on decades of research, but even these works tend to give recommendations that are still debated, because they aren't yet part of <a href=\"/lw/ow/the_beauty_of_settled_science/\"><em>settled</em>&nbsp;science</a>.</p>\n<p>Lifelong self-help researcher <a href=\"http://psychcentral.com/blog/archives/2010/01/08/a-psychologist-pioneer-clay-tucker-ladd-phd-78/\">Clayton Tucker-Ladd</a> wrote and updated <em><a href=\"http://www.psychologicalselfhelp.org/\">Psychological Self-Help</a></em>&nbsp;(<a href=\"http://www.psychologicalselfhelp.org/download/\">pdf</a>) over several decades. It's a summary of what scientists do and don't know about self-help methods (as of about 2003), but it's also more than 2,000 pages long, and much of it surveys scientific <em>opinion </em>rather than experimental results, because on many subjects there <em>aren't</em>&nbsp;any experimental results yet. The book is associated with an <a href=\"http://forums.psychcentral.com/\">internet community</a> of people sharing what does and doesn't work for them.</p>\n<p>More immediately useful is Richard Wiseman's <em><a href=\"http://www.amazon.com/59-Seconds-Change-Minute-Vintage/dp/0307474860/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">59 Seconds</a></em>. <a href=\"http://en.wikipedia.org/wiki/Richard_Wiseman\">Wiseman</a> is an experimental psychologist and paranormal investigator who gathered together what little self-help research <em>is</em>&nbsp;part of settled science, and put it into a short, fun, and useful <a href=\"http://en.wikipedia.org/wiki/Malcolm_Gladwell\">Malcolm Gladwell-ish</a> book. The next best popular-level <em>general</em> self-help book is perhaps Martin Seligman's <em><a href=\"http://www.amazon.com/What-You-Change-Cant-Self-Improvement/dp/1400078407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">What You Can Change and What You Can't</a></em>.</p>\n<p><a id=\"more\"></a></p>\n<p>Two <a href=\"http://www.amazon.com/Authoritative-Self-Help-Resources-Revised-Clinicians/dp/1572308397/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">large</a>&nbsp;<a href=\"http://www.amazon.com/dp/0838906524/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">books</a>&nbsp;rate hundreds of popular self-help books according to what professional psychologists think of them, and offer advice on <a href=\"http://lukeprog.com/selfhelp/self_help_books.html\">how to choose self-help books</a>. Unfortunately, this may not mean much because even professional psychologists very often have opinions that depart from the empirical data, as documented extensively by <a href=\"http://www.psychology.emory.edu/clinical/lilienfeld/index.html\">Scott Lilienfeld</a> and others in <em><a href=\"http://www.amazon.com/Science-Pseudoscience-Clinical-Psychology-Lilienfeld/dp/1593850700/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Science and Pseudoscience in Clinical Psychology</a> </em>and <em><a href=\"http://www.amazon.com/Navigating-Mindfield-Separating-Science-Pseudoscience/dp/1591024676/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Navigating the Mindfield</a></em>. These two books are helpful in assessing what is and isn't known according to <em>empirical research</em> (rather than according to&nbsp;<em>expert opinion</em>). Lilienfeld also edits the useful journal&nbsp;<a style=\"font-style: italic;\" href=\"http://www.srmhp.org/current-issue.html\">Scientific Review of Mental Health Practice</a>, and has compiled a list of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Lilienfeld-Psychological-Treatments-That-Cause-Harm.pdf\">harmful psychological treatments</a>. Also see Nathan and Gorman's <em><a href=\"http://www.amazon.com/Guide-Treatments-that-Work/dp/0195304144/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A Guide to Treatments That Work</a></em>, Roth &amp; Fonagy's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/What-Works-Whom-Second-Psychotherapy/dp/159385272X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">What Works for Whom?</a><em>,</em>&nbsp;and, more generally, Stanovich's <em><a href=\"http://www.amazon.com/How-Think-Straight-About-Psychology/dp/0205685900/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">How to Think Straight about Psychology</a></em>.</p>\n<p>Many self-help books are written as \"one size fits all,\" but of course this is rarely appropriate in psychology, and this leads to reader disappointment (Norem &amp; Chang, 2000).&nbsp;But psychologists <em>have </em>tested the effectiveness of reading particular&nbsp;<em>problem-focused</em> self-help books (\"bibliotherapy\").<sup>1 </sup>For example, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Anderson-Self-Help-Books-for-Depression.pdf\">it appears that</a> reading David Burns'&nbsp;<em><a href=\"http://www.amazon.com/Feeling-Good-Therapy-Revised-Updated/dp/0380810336/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Feeling Good</a></em> can be as effective for treating depression as individual or group therapy. Results vary from book to book.</p>\n<p>There are at least four university textbooks that teach basic scientific self-help. The first is Weiten, Dunn, and Hammer's&nbsp;<em><a href=\"http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life: Adjustment in the 21st Century</a></em>. It's expensive, but you can <a href=\"http://www.coursesmart.com/9781111186630/Ch02\">preview it here.</a>&nbsp;Others are are Santrock's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/Human-Adjustment--Psych-CD-ROM-Santrock/dp/0073111910/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Human Adjustment</a>, Duffy et al.'s <em><a href=\"http://www.amazon.com/Psychology-Living-Adjustment-Behavior-MyPsychKit/dp/0205790364/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology for Living</a></em>, and Nevid &amp; Rathus' <em><a href=\"http://www.amazon.com/Psychology-Challenges-Life-Jeffrey-Nevid/dp/0470383623/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology and the Challenges of Life</a></em>.</p>\n<p>If you read only one book of self-help in your life, I recommend Weiten, Dunn, and Hammer's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life</a>.<sup>2</sup> Unfortunately, like Tucker-Ladd's&nbsp;<em>Psychological Self-Help</em>, many sections of the book are an overview of scientific <em>opinion</em> rather than <em>experimental result</em>, because so few experimental studies on the subject have been done!</p>\n<p>In private correspondance with me, Weiten remarked:</p>\n<blockquote>\n<p>You are looking for substance in what is ultimately a black hole of empirical research&nbsp;...Basically, almost everything written on the topic emphasizes the complete lack of evidence.</p>\n<p>Perhaps I am overly cynical, but I suspect that empirical tests are nonexistent because the authors of self-help and time-management titles are not at all confident that the results would be favorable. Hence, they have no incentive to pursue such research because it is likely to undermine their sales and their ability to write their next book. Another issue is that many of the authors who crank out these titles have little or no background in research. In a less cynical vein, another issue is that this research would come with all the formidable complexities of the research evaluating the effectiveness of different approaches to therapy. Efficacy trials for therapies are extremely difficult to conduct in a clean fashion and because of these complexities require big bucks in the way of grants.</p>\n</blockquote>\n<p>Other leading researchers in the psychology of adjustment expressed much the same opinion of the field when I contacted them.</p>\n<p>&nbsp;</p>\n<h4>A sampling of scientific self-help advice</h4>\n<p>Still, perhaps scientific psychology can offer&nbsp;<em>some</em>&nbsp;useful self-help advice.&nbsp;I'll focus on two areas of <em>particular</em> interest to the Less Wrong community -&nbsp;<a href=\"/lw/2un/references_resources_for_lesswrong/\">studying</a>&nbsp;and&nbsp;<a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">productivity</a>&nbsp;- and on one area of <em>general</em> interest: <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">happiness</a>.</p>\n<p>&nbsp;</p>\n<p><em><a name=\"study\"></a><a name=\"study\"></a>Study methods</em></p>\n<p>Organize for clarity the information you want to learn, for example in an outline (Einstein &amp; McDaniel 2004; Tigner 1999; McDaniel et al. 1996).&nbsp;Cramming doesn't work (Wong 2006). Set up a schedule for studying (Allgood et al. 2000). <em>Test</em> yourself on the material (Karpicke &amp; Roediger 2003; Roediger &amp; Karpicke 2006a; Roediger &amp; Karpicke 2006b; Agarwal et al. 2008; Butler &amp; Roediger 2008), and do so repeatedly, with 24 hours or more between study sessions (Rohrer &amp; Taylor 2006; Seabrook et al 2005; Cepeda et al. 2006; Rohrer et al. 2005; Karpicke &amp; Roediger 2007). Basically: <a href=\"/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/\"><strong>use Anki</strong></a>.</p>\n<p>To retain studied information more effectively, try <a href=\"http://en.wikipedia.org/wiki/Acrostic\">acrostics</a> (Hermann et al. 2002), the <a href=\"http://www.brighthub.com/education/homework-tips/articles/41610.aspx\">link method</a> (Iaccino 1996; Worthen 1997); and the <a href=\"http://en.wikipedia.org/wiki/Method_of_loci\">method of loci</a> (Massen &amp; Vaterrodt-Plunnecke 2006; Moe &amp; De Beni 2004; Moe &amp; De Beni 2005).</p>\n<p>&nbsp;</p>\n<p><em>Productivity</em></p>\n<p>Unfortunately, there have been fewer experimental studies on effective productivity and time management methods than there have been on effective study methods. For an overview of scientific <em>opinion</em>&nbsp;on productivity, I recommend pages 121-126 of <em><a href=\"http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life</a></em>. According to those pages, common advice from professionals includes:</p>\n<ol>\n<li>Doing the <em>right</em>&nbsp;tasks is more important than doing your tasks <em>efficiently</em>. In fact, too much concern for efficiency is a leading cause of procrastination. Say \"no\" more often, and use your time for tasks that really matter.</li>\n<li>Delegate responsibility as often as possible. Throw away unimportant tasks and items.</li>\n<li>Keep a record of your time use. (<a href=\"http://quantifiedself.com/\">Quantified Self</a> can help.)</li>\n<li>Write down your goals. Break them down into smaller goals, and break these into manageable tasks. Schedule these tasks into your calendar.</li>\n<li>Process notes and emails only once. Tackle one task at a time, and group similar tasks together.</li>\n<li>Make use of your downtime (plane rides, bus rides, doctor's office waitings). These days, many of your tasks can be completed on your smartphone.</li>\n</ol>\n<p>Why the dearth of experimental research on productivity? A leading researcher on the topic, <a href=\"http://haskayne.ucalgary.ca/profiles/piers-steel\">Piers Steel</a>, explained to me in personal communication:</p>\n<blockquote>Fields tend to progress from description to experimentation, and the procrastination field is just starting to move towards that direction. There really isn&rsquo;t very much directly done on procrastination, but there is more for the broader field of self-regulation... it should transfer as the fundamentals are the same. For example, I would bet everything I own that goal setting works, as there [are] about [a thousand studies] on it in the motivational field (just not specifically on procrastination). On the other hand, we are building a behavioral lab so we can test many of these techniques head to head, something that sorely needs to be done.</blockquote>\n<p>Steel's book on the subject is <a style=\"font-style: italic;\" href=\"http://webapps2.ucalgary.ca/~steel/\">The Procrastination Equation</a>, which I highly recommend.</p>\n<p>&nbsp;</p>\n<p><em>Happiness</em></p>\n<p>There is an abundance of research on factors that correlate with <em>subjective well-being</em> (individuals' own assessments of their happiness and life satisfaction).</p>\n<p>Factors that <em>don't correlate</em> much with happiness include: age,<sup>3</sup> gender,<sup>4</sup> parenthood,<sup>5</sup> intelligence,<sup>6</sup> physical attractiveness,<sup>7</sup> and money<sup>8</sup> (as long as you're above the poverty line). Factors that <em>correlate moderately</em>&nbsp;with happiness include: health,<sup>9</sup> social activity,<sup>10</sup> and religiosity.<sup>11</sup> Factors that <em>correlate strongly</em> with happiness include: genetics,<sup>12</sup> love and relationship satisfaction,<sup>13</sup> and work satisfaction.<sup>14</sup></p>\n<p>For many of these factors, a <em>causal</em>&nbsp;link to happiness has also been demonstrated with some confidence, but that story is too complicated to tell in this short article.</p>\n<p>&nbsp;</p>\n<h4>Conclusions</h4>\n<p>Many compassionate professionals have modeled their careers after George Miller's (1969) call to \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Epstein-Giving-Psychology-Away.pdf\">give psychology away</a>\" to the masses as a means of promoting human welfare. As a result, hundreds of experimental studies have been done to test which self-help methods work, and which do not. We humans can use this knowledge to achieve our goals.</p>\n<p>But much work remains to be done. Many features of human psychology and behavior are not well-understood, and many self-help methods recommended by popular and academic authors have not yet been experimentally tested. If you are considering psychology research as a career path, and you want to (1) improve human welfare, (2) get research funding, (3) explore an under-developed area of research, and (4) have the chance to write a best-selling self-help book once you've done some of your research, then please consider a career of <em>experimentally testing different self-help methods</em>. Humanity will thank you for it.</p>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Notes</h4>\n<p><small><sup>1</sup><sup>&nbsp;</sup>Read a nice overview of the literature in Bergsma, \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Bergsma-Do-Self-Help-Books-Help.pdf\">Do Self-Help Books Help?</a>\" (2008).</small></p>\n<p><small><sup>2</sup> I recommend the 10th edition, which has large improvements over the 9th edition, including 4500 new citations.</small></p>\n<p><small><sup>3</sup>&nbsp;Age and happiness are unrelated (Lykken 1999), age accounting for less than 1% of the variation in people's happiness (Inglehart 1990; Myers &amp; Diener 1997).</small></p>\n<p><small><sup>4</sup> Despite being treated for depressive disorders twice as often as men (Nolen-Hoeksema 2002), women report just as high levels of well-being as men do (Myers 1992).</small></p>\n<p><small><sup>5</sup> Apparently, the joys and stresses of parenthood balance each other out, as people with and without children are equally happy (Argyle 2001).</small></p>\n<p><small><sup>6</sup>&nbsp;Both IQ and educational attainment appear to be unrelated to happiness (Diener et al. 2009; Ross &amp; Van Willigen 1997).</small></p>\n<p><small><sup>7</sup> Good-looking people enjoy huge advantages, but do not report greater happiness than others (Diener et al. 1995).</small></p>\n<p><small><sup>8</sup> The correlation between income and happiness is surprisingly weak (Diener &amp; Seligman 2004; Diener et al. 1993; Johnson &amp; Krueger 2006). One problem may be that higher income contributes to greater materialism, which impedes happiness (Frey &amp; Stutzer 2002; Kasser et al. 2004; Solberg et al. 2002; Kasser 2002; Van Boven 2005; Nickerson et al. 2003; Kahneman et al. 2006).</small></p>\n<p><small><sup>9</sup> Those with disabling health conditions are happier than you might think (Myers 1992; Riis et al. 2005; Argyle 1999).</small></p>\n<p><small><sup>10</sup> Those who are satisfied with their social life are moderately more happy than others (Diener &amp; Seligman 2004; Myers 1999; Diener &amp; Seligman 2002).</small></p>\n<p><small><sup>11</sup> Religiosity correlates with happiness (Abdel-Kahlek 2005; Myers 2008), though it may be religious attendance and not religious belief that matters (Chida et al. 2009).</small></p>\n<p><small><sup>12</sup> Past happiness is the best predictor of future happiness (Lucas &amp; Diener 2008). Happiness is surprisingly unmoved by external factors (Lykken &amp; Tellegen 1996), because the genetics accounts for about 50% of the variance in happiness (Lyubomirsky et al. 2005; Stubbe et al. 2005).</small></p>\n<p><small><sup>13</sup>&nbsp;Married people are happier than those who are single or divorced (Myers &amp; Diener 1995; Diener et al. 2000), and marital satisfaction predicts happiness (Proulx et al. 2007).</small></p>\n<p><small><sup>14</sup>&nbsp;Unemployment makes people very unhappy (Argyle 2001), and job satisfaction is strongly correlated with happiness (Judge &amp; Klinger 2008; Warr 1999).</small></p>\n<p><small>&nbsp;</small></p>\n<h4>References</h4>\n<p><small>Abdel-Khalek (2006). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Abdel-Khalek-Happiness-health-and-religiosity-Significant-relations.pdf\">Happiness, health, and religiosity: Significant relations</a>.\" <em>Mental Health</em>, 9(1): 85-97.</small></p>\n<p><small>Agarwal, Karpicke, Kang, Roediger, &amp; McDermott (2008). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Agarwal-Examining-the-Testing-Effect.pdf\">Examining the testing effect with open- and closed-book tests.</a>\" <em>Applied Cognitive Psychology</em>, 22: 861-876.</small></p>\n<p><small>Allgood, Risko, Alvarez, &amp; Fairbanks (2000). \"Factors that influence study.\" In Flippo &amp; Caverly, (Eds.), <em>Handbook of college reading and study strategy research</em>. Mahwah, NJ: Erlbaum.</small></p>\n<p><small>Argyle (1999). \"Causes and correlates of happiness.\" In Kahneman, Diener, &amp; Schwartz (Eds.), <em>Well-being: The foundations of hedonic psychology</em>. New York: Sage.</small></p>\n<p><small>Argyle (2001). <em>The Psychology of Happiness</em>&nbsp;(2nd ed.). New York: Routledge.</small></p>\n<p><small>Buckley (1998). <em>God is My Broker:&nbsp;A Monk-Tycoon Reveals the 7 1/2 Laws of Spiritual and Financial Growth</em>. New York: Random House.</small></p>\n<p><small>Butler &amp; Roediger (2008). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Butler-Feedback-enhances-the-positive-effects.pdf\">Feedback enhances the positive effects and reduces the negative effects of multiple-choice testing</a>.\" <em>Memory &amp; Cognition</em>, 36(3).</small></p>\n<p><small>Chida, Steptoe, &amp; Powell (2009). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chida-Religiosity-Spirituality-and-Mortality.pdf\">Religiosity/Spirituality and Mortality</a>.\" <em>Psychotherapy and Psychosomatics</em>, 78(2): 81-90.</small></p>\n<p><small>Cepeda, Pashler, Vul, Wixted, &amp; Rohrer (2006). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Cepeda-Distributed-Practice-in-Verbal-Recall-Trasks.pdf\">Distributed practice in verbal recall tasks: A review and quantitative synthesis</a>.\" <em>Psychological Bulletin</em>, 132: 354-380.</small></p>\n<p><small>Diener, Sandvik, Seidlitz, &amp; Diener (1993). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-The-relationship-between-income-and-subjective-well-being-Relative-or-absolute.pdf\">The relationship between income and subjective well-being: Relative or absolute?</a>\" <em>Social Indicators Research</em>, 28: 195-223.</small></p>\n<p><small>Diener, Wolsic, &amp; Fujita (1995). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Physical-Attractiveness-and-Subjective-Well-Being.pdf\">Physical attractiveness and subjective well-being</a>.\" <em>Journal of Personality and Social Psychology</em>, 69: 120-129.</small></p>\n<p><small>Diener, Gohm, Suh, &amp; Oishi (2000). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Similarity-of-the-relations-between-marital-status-and-subjective-well-being-across-cultures.pdf\">Similarity of the relations between marital status and subjective well-being across cultures</a>.\" <em>Journal of Cross-Cultural Psychology</em>, 31: 419-436.</small></p>\n<p><small>Diener &amp; Seligman (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Very-Happy-People.pdf\">Very happy people</a>.\" <em>Psychological Science</em>, 13: 80-83.</small></p>\n<p><small>Diener &amp; Seligman (2004). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Beyond-money.pdf\">Beyond money: Toward an economy of well-being</a>.\" <em>Psychological Science in the Public Interest</em>, 5(1): 1-31.</small></p>\n<p><small>Diener, Kesebir, &amp; Tov (2009). \"Happiness\" In Leary &amp; Hoyle (Eds.), <em>Handbook of Individual Differences in Social Behavior</em>&nbsp;(pp. 147-160). New York: Guilford.</small></p>\n<p><small>Einstein &amp; McDaniel (2004). <em>Memory Fitness: A Guide for Successful Aging</em>. New Haven, CT: Yale University Press.</small></p>\n<p><small>Frey &amp; Stutzer (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Frey-What-can-economists-learn-from-happiness-research.pdf\">What can economists learn from happiness research?</a>\" <em>Journal of Economic Literature</em>, 40: 402-435.</small></p>\n<p><small>Hermann, Raybeck, &amp; Gruneberg (2002). <em>Improving memory and study skills: Advances in theory and practice.</em>&nbsp;Ashland, OH: Hogrefe &amp; Huber.</small></p>\n<p><small>Iaccino (1996). \"A further examination of the bizarre imagery mnemonic: Its effectiveness with mixed context and delayed testing. <em>Perceptual &amp; Motor Skills</em>, 83: 881-882.</small></p>\n<p><small>Inglehart (1990). <em>Culture shift in advanced industrial society</em>. Princeton, NJ: Princeton University Press.</small></p>\n<p><small>Johnson &amp; Krueger (2006). \"How money buys happiness: Genetic and environmental processes linking finances and life satisfaction.\" <em>Journal of Personality and Social Psychology</em>, 90: 680-691.</small></p>\n<p><small>Judge &amp; Klinger (2008). \"Job satisfaction: Subjective well-being at work.\" In Eid &amp; Larsen (Eds.), <em>The science of subjective well-being</em>&nbsp;(pp. 393-413). New York: Guilford.</small></p>\n<p><small>Kahneman, Krueger, Schkade, Schwarz, &amp; Stone (2006). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Kahneman-Would-you-be-happier-if-you-were-richer-A-focusing-illusion.pdf\">Would you be happier if you were richer? A focusing illusion</a>.\" <em>Science</em>, 312: 1908-1910.</small></p>\n<p><small>Kasser (2002). <em>The high prices of materialism</em>. Cambridge, MA: MIT Press.</small></p>\n<p><small>Kasser, Ryan, Couchman, &amp; Sheldon (2004). \"Materialistic values: Their causes and consequences.\" In Kasser &amp; Kanner (Eds.), <em>Psychology and consumer culture: The struggle for a good life in a materialistic world</em>. Washington DC: American Psychological Association.</small></p>\n<p><small>Karpicke &amp; Roediger (2003). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Karpicke-The-critical-importance-of-retrieval-for-learning.pdf\">The critical importance of retrieval for learning</a>.\"&nbsp;<em>Science</em>, 319: 966-968.&nbsp;</small></p>\n<p><small>Karpicke &amp; Roediger (2007). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Karpicke-Expanding-retrieval-practice-promotes-short-term-retention-but-equally-spaced-retrieval-enhances-long-term-retention.pdf\">Expanding retrieval practice promotes short-term retention, but equally spaced retrieval enhances long-term retention</a>.\" <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, 33(4): 704-719.</small></p>\n<p><small>Lucas &amp; Diener (2008). \"Personality and subjective well-being.\" In John, Robins, &amp; Pervin (Eds.), <em>Handbook of personality: Theory and research</em>&nbsp;(pp. 796-814). New York: Guilford.</small></p>\n<p><small>Lyubomirsky, Sheldon, &amp; Schkade (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Lyubomirsky-Pursuing-happiness-The-architecture-of-sustainable-change.pdf\">Pursuing happiness: The architecture of sustainable change</a>.\" <em>Review of General Psychology</em>, 9(2), 111-131.</small></p>\n<p><small>Lykken &amp; Tellegen (1996). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Lykken-Happiness-Is-a-Stochastic-Phenomenon.pdf\">Happiness is a stochastic phenomenon</a>.\" <em>Psychological Science</em>, 7: 186-189.</small></p>\n<p><small>Lykken (1999). <em>Happiness: The nature and nurture of joy and contentment</em>. New York: St. Martin's.</small></p>\n<p><small>Massen &amp; Vaterrodt-Plunnecke (2006). \"The role of proactive interference in mnemonic techniques.\" <em>Memory</em>, 14: 189-196.</small></p>\n<p><small>McDaniel, Waddill, &amp; Shakesby (1996). \"Study strategies, interest, and learning from Text: The application of material appropriate processing.\" In Herrmann, McEvoy, Hertzog, Hertel, &amp; Johnson (Eds.), <em>Basic and applied memory research: Theory in context</em>&nbsp;(Vol 1). Mahwah, NJ: Erlbaum.</small></p>\n<p><small>Miller (1969).&nbsp;\"On turning psychology over to the unwashed.\" <em>Psychology Today</em>, 3(7), 53&ndash;54, 66&ndash;68, 70, 72, 74.</small></p>\n<p><small>Moe &amp; De Beni (2004). \"Studying passages with the loci method: Are subject-generated more effective than experimenter-supplied loci?\" <em>Journal of Mental Imagery</em>, 28(3-4): 75-86.</small></p>\n<p><small>Moe &amp; De Beni (2005).&nbsp;\"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Moe-Stressing-the-efficacy-of-the-Loci-method.pdf\">Stressing the efficacy of the Loci method: oral presentation and the subject-generation of the Loci pathway with expository passages</a>.\" <em>Applied Cognitive Psychology</em>, 19(1): 95-106.</small></p>\n<p><small>Myers (1992). <em>The pursuit of happiness: Who is happy, and why</em>. New York: Morrow.</small></p>\n<p><small>Myers &amp; Diener (1995). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Myers-who-is-happy.pdf\">Who is happy?</a>\" <em>Psychological Science</em>, 6: 10-19.</small></p>\n<p><small>Myers &amp; Diener (1997). \"The pursuit of happiness.\" <em>Scientific American, Special Issue 7</em>: 40-43.</small></p>\n<p><small>Myers (1999). \"Close relationships and quality of life.\" In Kahnemann, Diener, &amp; Schwarz (Eds.), <em>Well-being: The foundations of hedonic psychology</em>. New York: Sage.</small></p>\n<p><small>Myers (2008). \"Religion and human flourishing.\" In Eid &amp; Larsen (Eds.), <em>The science of subjective well-being</em>&nbsp;(pp. 323-346). New York: Guilford.</small></p>\n<p><small>Nickerson, Schwartz, Diener, &amp; Kahnemann (2003). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Nickerson-Zeroing-in-on-the-dark-side-of-the-american-dream.pdf\">Zeroing in on the dark side of the American dream: A closer look at the negative consequences of the goal for financial success</a>.\" <em>Psychological Science</em>, 14(6): 531-536.</small></p>\n<p><small>Nolen-Hoeksema (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Nolen-Hoeksema-Gender-differences-in-depression.pdf\">Gender differences in depression</a>.\" In Gotlib &amp; Hammen (Eds.), <em>Handbook of Depression</em>. New York: Guilford.</small></p>\n<p><small>Proulx, Helms, &amp; Cheryl (2007). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Proulx-Marital-Quality-and-Personal-Well\u2010Being-A-Meta\u2010Analysis.pdf\">Marital quality and personal well-being: A Meta-analysis</a>.\" <em>Journal of Marriage and Family</em>, 69: 576-593.</small></p>\n<p><small>Roediger &amp; Karpicke (2006a). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Roediger-Test-Enhanced-Learning.pdf\">Test-enhanced learning: Taking memory tests improves long-term retention</a>.\"&nbsp;<em>Psychological Science</em>, 17: 249-255.</small></p>\n<p><small>Roediger &amp; Karpicke (2006b). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Roediger-The-power-of-testing-memory.pdf\">The power of testing memory: Basic research and implications for educational practice</a>.\"&nbsp;<em>Perspectives on Psychological Science</em>, 1(3): 181-210.</small></p>\n<p><small>Riis, Loewenstein, Baron, Jepson, Fagerlin, &amp; Ubel (2005). \"Ignorance of hedonic adaptation to hemodialysis: A study using ecological momentary assessment.\" <em>Journal of Experimental Psychology: General</em>, 134: 3-9.</small></p>\n<p><small>Rohrer &amp; Taylor (2006). \"The effects of over-learning and distributed practice on the retention of mathematics knowlege.&nbsp;<em style=\"font-style: italic;\">Applied Cognitive Psychology</em>, 20: 1209-1224.&nbsp;</small></p>\n<p><small>Rohrer, Taylor, Pashler, Wixted, &amp; Cepeda (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Rohrer-The-Effect-of-Overlearning-on-Long-Term-Retention.pdf\">The Effect of Overlearning on Long-Term Retention</a>.\"&nbsp;<em style=\"font-style: italic;\">Applied Cognitive Psychology</em>, 19: 361-374.</small></p>\n<p><small>Ross &amp; Van Willigen (1997). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Ross-Education-and-the-subjective-quality-of-life.pdf\">Education and the subjective quality of life</a>.\" <em>Journal of Health &amp; Social Behavior</em>, 38: 275-297.</small></p>\n<p><small>Seabrook, Brown, &amp; Solity (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Saebrook-Distributed-and-massed-practice-From-laboratory-to-classroom.pdf\">Distributed and massed practice: From laboratory to class-room</a>.\" <em>Applied Cognitive Psychology</em>, 19(1): 107-122.</small></p>\n<p><small>Solberg, Diener, Wirtz, Lucas, &amp; Oishi (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Solberg-Wanting-Having-and-Satisfaction-Examining-the-Role-of-Desire-Discrepancies-in-Satisfaction-With-Income.pdf\">Wanting, having, and satisfaction: Examining the role of desire discrepancies in satisfaction with income</a>.\" <em>Journal of Personality and Social Psychology</em>, 83(3): 725-734.</small></p>\n<p><small>Stubbe, Posthuma, Boomsa, &amp; De Geus (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Stubbe-Heritability-of-life-satisfaction-in-adults-A-twin-family-study.pdf\">Heritability and life satisfaction in adults: A twin-family study</a>.\" <em>Psychological Medicine</em>, 35: 1581-1588.</small></p>\n<p><small>Tigner (1999). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Tigner-Putting-Memory-Research-to-Good-Use.pdf\">Putting memory research to good use: Hints from cognitive psychology</a>.\"&nbsp;<em>College Teaching</em>, 47(4): 149-151.</small></p>\n<p><small>Van Boven (2005). \"Experientialism, materialism, and the pursuit of happiness.\" <em>Review of General Psychology</em>, 9(2): 132-142.</small></p>\n<p><small>Warr (1999). \"Well-being and the workplace.\" In Kahneman, Diener, &amp; Schwartz (Eds.), <em>Well-being: The foundations of hedonic psychology</em>. New York: Sage.</small></p>\n<p><small>Wong (2006). <em>Essential Study Skills</em>. Boston: Houghton Mifflin.</small></p>\n<p><small>Worthen (1997). \"Resiliency of bizarreness effects under varying conditions of verbal and imaginal elaboration and list composition. <em>Journal of Mental Imagery</em>, 21: 167-194.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 9, "XqykXFKL9t38pbSEm": 2, "Jzm2mYuuDBCNWq8hi": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "33KewgYhNSxFpbpXg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 180, "baseScore": 204, "extendedScore": null, "score": 0.000367, "legacy": true, "legacyId": "4739", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 205, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h5><span style=\"font-weight: normal;\">Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></span></h5>\n<p><a href=\"/lw/9p/rationality_its_not_that_great/\">Some</a> <a href=\"/lw/bg/instrumental_rationality_is_a_chimera/\">have</a> <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">suggested</a> <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l5h\">that</a> the Less Wrong community could improve readers' <a href=\"/lw/2p5/humans_are_not_automatically_strategic\">instrumental rationality</a> more effectively if it first&nbsp;<a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">caught up with the scientific literature</a> on productivity and self-help, and then enabled readers to deliberately practice <a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">self-help skills</a> and apply what they've learned <a href=\"/lw/34m/what_ive_learned_from_less_wrong/\">in real life</a>.</p>\n<p>I think that's a good idea. My contribution today is a quick overview of scientific self-help: what professionals call \"the psychology of adjustment.\" First I'll review the state of the industry and the scientific literature, and then I'll briefly summarize the scientific data available on three topics in self-help: study methods, productivity, and happiness.</p>\n<h4 id=\"The_industry_and_the_literature\">The industry and the literature</h4>\n<p>As you probably know, much of <a href=\"http://www.amazon.com/Self-Help-Inc-Makeover-Culture-American/dp/0195171241/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">the self-help</a> <a href=\"http://www.amazon.com/Oracle-Supermarket-American-Preoccupation-Self-Help/dp/0765809648/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">industry</a> is a <a href=\"http://www.amazon.com/Sham-Self-Help-Movement-America-Helpless/dp/1400054095/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">sham</a>, <a href=\"http://www.amazon.com/Lost-Cosmos-Last-Self-Help-Book/dp/0312253990/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">ripe</a> for <a href=\"http://www.amazon.com/Secrets-SuperOptimist-W-R-Morton/dp/0977480704/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">parody</a>.&nbsp;Most self-help books are written to <em>sell</em>, <a href=\"http://www.amazon.com/Im-Dysfunctional-Youre-Recovery-Self-Help/dp/0201570629/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">not to <em>help</em></a>. <a href=\"http://www.amazon.com/Fools-Paradise-Unreal-World-Psychology/dp/1566636280/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Pop psychology</a> may be&nbsp;<a href=\"http://www.amazon.com/Great-Myths-Popular-Psychology-Misconceptions/dp/1405131128/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">more myth</a> <a href=\"http://www.amazon.com/House-Cards-Robyn-Dawes/dp/0684830914/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">than fact</a>. As Christopher Buckley (1999) writes, \"The more people read [self-help books], the more they think they need them... [it's] more like an addiction than an alliance.<span style=\"font-size: 11px;\">\"</span></p>\n<p>Where can you turn for reliable, empirically-based self-help advice? A few leading therapeutic psychologists (e.g., <a href=\"http://www.amazon.com/Albert-Ellis/e/B000APLUAE/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Albert Ellis</a>, <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=arnold+lazarus&amp;x=0&amp;y=0&amp;tag=lesswrong-20\">Arnold Lazarus</a>, <a href=\"http://www.amazon.com/Martin-E.-P.-Seligman/e/B001ILOB78/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Martin Seligman</a>) have written self-help books based on decades of research, but even these works tend to give recommendations that are still debated, because they aren't yet part of <a href=\"/lw/ow/the_beauty_of_settled_science/\"><em>settled</em>&nbsp;science</a>.</p>\n<p>Lifelong self-help researcher <a href=\"http://psychcentral.com/blog/archives/2010/01/08/a-psychologist-pioneer-clay-tucker-ladd-phd-78/\">Clayton Tucker-Ladd</a> wrote and updated <em><a href=\"http://www.psychologicalselfhelp.org/\">Psychological Self-Help</a></em>&nbsp;(<a href=\"http://www.psychologicalselfhelp.org/download/\">pdf</a>) over several decades. It's a summary of what scientists do and don't know about self-help methods (as of about 2003), but it's also more than 2,000 pages long, and much of it surveys scientific <em>opinion </em>rather than experimental results, because on many subjects there <em>aren't</em>&nbsp;any experimental results yet. The book is associated with an <a href=\"http://forums.psychcentral.com/\">internet community</a> of people sharing what does and doesn't work for them.</p>\n<p>More immediately useful is Richard Wiseman's <em><a href=\"http://www.amazon.com/59-Seconds-Change-Minute-Vintage/dp/0307474860/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">59 Seconds</a></em>. <a href=\"http://en.wikipedia.org/wiki/Richard_Wiseman\">Wiseman</a> is an experimental psychologist and paranormal investigator who gathered together what little self-help research <em>is</em>&nbsp;part of settled science, and put it into a short, fun, and useful <a href=\"http://en.wikipedia.org/wiki/Malcolm_Gladwell\">Malcolm Gladwell-ish</a> book. The next best popular-level <em>general</em> self-help book is perhaps Martin Seligman's <em><a href=\"http://www.amazon.com/What-You-Change-Cant-Self-Improvement/dp/1400078407/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">What You Can Change and What You Can't</a></em>.</p>\n<p><a id=\"more\"></a></p>\n<p>Two <a href=\"http://www.amazon.com/Authoritative-Self-Help-Resources-Revised-Clinicians/dp/1572308397/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">large</a>&nbsp;<a href=\"http://www.amazon.com/dp/0838906524/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">books</a>&nbsp;rate hundreds of popular self-help books according to what professional psychologists think of them, and offer advice on <a href=\"http://lukeprog.com/selfhelp/self_help_books.html\">how to choose self-help books</a>. Unfortunately, this may not mean much because even professional psychologists very often have opinions that depart from the empirical data, as documented extensively by <a href=\"http://www.psychology.emory.edu/clinical/lilienfeld/index.html\">Scott Lilienfeld</a> and others in <em><a href=\"http://www.amazon.com/Science-Pseudoscience-Clinical-Psychology-Lilienfeld/dp/1593850700/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Science and Pseudoscience in Clinical Psychology</a> </em>and <em><a href=\"http://www.amazon.com/Navigating-Mindfield-Separating-Science-Pseudoscience/dp/1591024676/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Navigating the Mindfield</a></em>. These two books are helpful in assessing what is and isn't known according to <em>empirical research</em> (rather than according to&nbsp;<em>expert opinion</em>). Lilienfeld also edits the useful journal&nbsp;<a style=\"font-style: italic;\" href=\"http://www.srmhp.org/current-issue.html\">Scientific Review of Mental Health Practice</a>, and has compiled a list of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Lilienfeld-Psychological-Treatments-That-Cause-Harm.pdf\">harmful psychological treatments</a>. Also see Nathan and Gorman's <em><a href=\"http://www.amazon.com/Guide-Treatments-that-Work/dp/0195304144/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">A Guide to Treatments That Work</a></em>, Roth &amp; Fonagy's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/What-Works-Whom-Second-Psychotherapy/dp/159385272X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">What Works for Whom?</a><em>,</em>&nbsp;and, more generally, Stanovich's <em><a href=\"http://www.amazon.com/How-Think-Straight-About-Psychology/dp/0205685900/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">How to Think Straight about Psychology</a></em>.</p>\n<p>Many self-help books are written as \"one size fits all,\" but of course this is rarely appropriate in psychology, and this leads to reader disappointment (Norem &amp; Chang, 2000).&nbsp;But psychologists <em>have </em>tested the effectiveness of reading particular&nbsp;<em>problem-focused</em> self-help books (\"bibliotherapy\").<sup>1 </sup>For example, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Anderson-Self-Help-Books-for-Depression.pdf\">it appears that</a> reading David Burns'&nbsp;<em><a href=\"http://www.amazon.com/Feeling-Good-Therapy-Revised-Updated/dp/0380810336/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Feeling Good</a></em> can be as effective for treating depression as individual or group therapy. Results vary from book to book.</p>\n<p>There are at least four university textbooks that teach basic scientific self-help. The first is Weiten, Dunn, and Hammer's&nbsp;<em><a href=\"http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life: Adjustment in the 21st Century</a></em>. It's expensive, but you can <a href=\"http://www.coursesmart.com/9781111186630/Ch02\">preview it here.</a>&nbsp;Others are are Santrock's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/Human-Adjustment--Psych-CD-ROM-Santrock/dp/0073111910/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Human Adjustment</a>, Duffy et al.'s <em><a href=\"http://www.amazon.com/Psychology-Living-Adjustment-Behavior-MyPsychKit/dp/0205790364/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology for Living</a></em>, and Nevid &amp; Rathus' <em><a href=\"http://www.amazon.com/Psychology-Challenges-Life-Jeffrey-Nevid/dp/0470383623/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology and the Challenges of Life</a></em>.</p>\n<p>If you read only one book of self-help in your life, I recommend Weiten, Dunn, and Hammer's <a style=\"font-style: italic;\" href=\"http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life</a>.<sup>2</sup> Unfortunately, like Tucker-Ladd's&nbsp;<em>Psychological Self-Help</em>, many sections of the book are an overview of scientific <em>opinion</em> rather than <em>experimental result</em>, because so few experimental studies on the subject have been done!</p>\n<p>In private correspondance with me, Weiten remarked:</p>\n<blockquote>\n<p>You are looking for substance in what is ultimately a black hole of empirical research&nbsp;...Basically, almost everything written on the topic emphasizes the complete lack of evidence.</p>\n<p>Perhaps I am overly cynical, but I suspect that empirical tests are nonexistent because the authors of self-help and time-management titles are not at all confident that the results would be favorable. Hence, they have no incentive to pursue such research because it is likely to undermine their sales and their ability to write their next book. Another issue is that many of the authors who crank out these titles have little or no background in research. In a less cynical vein, another issue is that this research would come with all the formidable complexities of the research evaluating the effectiveness of different approaches to therapy. Efficacy trials for therapies are extremely difficult to conduct in a clean fashion and because of these complexities require big bucks in the way of grants.</p>\n</blockquote>\n<p>Other leading researchers in the psychology of adjustment expressed much the same opinion of the field when I contacted them.</p>\n<p>&nbsp;</p>\n<h4 id=\"A_sampling_of_scientific_self_help_advice\">A sampling of scientific self-help advice</h4>\n<p>Still, perhaps scientific psychology can offer&nbsp;<em>some</em>&nbsp;useful self-help advice.&nbsp;I'll focus on two areas of <em>particular</em> interest to the Less Wrong community -&nbsp;<a href=\"/lw/2un/references_resources_for_lesswrong/\">studying</a>&nbsp;and&nbsp;<a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">productivity</a>&nbsp;- and on one area of <em>general</em> interest: <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">happiness</a>.</p>\n<p>&nbsp;</p>\n<p><em><a name=\"study\"></a><a name=\"study\"></a>Study methods</em></p>\n<p>Organize for clarity the information you want to learn, for example in an outline (Einstein &amp; McDaniel 2004; Tigner 1999; McDaniel et al. 1996).&nbsp;Cramming doesn't work (Wong 2006). Set up a schedule for studying (Allgood et al. 2000). <em>Test</em> yourself on the material (Karpicke &amp; Roediger 2003; Roediger &amp; Karpicke 2006a; Roediger &amp; Karpicke 2006b; Agarwal et al. 2008; Butler &amp; Roediger 2008), and do so repeatedly, with 24 hours or more between study sessions (Rohrer &amp; Taylor 2006; Seabrook et al 2005; Cepeda et al. 2006; Rohrer et al. 2005; Karpicke &amp; Roediger 2007). Basically: <a href=\"/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/\"><strong>use Anki</strong></a>.</p>\n<p>To retain studied information more effectively, try <a href=\"http://en.wikipedia.org/wiki/Acrostic\">acrostics</a> (Hermann et al. 2002), the <a href=\"http://www.brighthub.com/education/homework-tips/articles/41610.aspx\">link method</a> (Iaccino 1996; Worthen 1997); and the <a href=\"http://en.wikipedia.org/wiki/Method_of_loci\">method of loci</a> (Massen &amp; Vaterrodt-Plunnecke 2006; Moe &amp; De Beni 2004; Moe &amp; De Beni 2005).</p>\n<p>&nbsp;</p>\n<p><em>Productivity</em></p>\n<p>Unfortunately, there have been fewer experimental studies on effective productivity and time management methods than there have been on effective study methods. For an overview of scientific <em>opinion</em>&nbsp;on productivity, I recommend pages 121-126 of <em><a href=\"http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life</a></em>. According to those pages, common advice from professionals includes:</p>\n<ol>\n<li>Doing the <em>right</em>&nbsp;tasks is more important than doing your tasks <em>efficiently</em>. In fact, too much concern for efficiency is a leading cause of procrastination. Say \"no\" more often, and use your time for tasks that really matter.</li>\n<li>Delegate responsibility as often as possible. Throw away unimportant tasks and items.</li>\n<li>Keep a record of your time use. (<a href=\"http://quantifiedself.com/\">Quantified Self</a> can help.)</li>\n<li>Write down your goals. Break them down into smaller goals, and break these into manageable tasks. Schedule these tasks into your calendar.</li>\n<li>Process notes and emails only once. Tackle one task at a time, and group similar tasks together.</li>\n<li>Make use of your downtime (plane rides, bus rides, doctor's office waitings). These days, many of your tasks can be completed on your smartphone.</li>\n</ol>\n<p>Why the dearth of experimental research on productivity? A leading researcher on the topic, <a href=\"http://haskayne.ucalgary.ca/profiles/piers-steel\">Piers Steel</a>, explained to me in personal communication:</p>\n<blockquote>Fields tend to progress from description to experimentation, and the procrastination field is just starting to move towards that direction. There really isn\u2019t very much directly done on procrastination, but there is more for the broader field of self-regulation... it should transfer as the fundamentals are the same. For example, I would bet everything I own that goal setting works, as there [are] about [a thousand studies] on it in the motivational field (just not specifically on procrastination). On the other hand, we are building a behavioral lab so we can test many of these techniques head to head, something that sorely needs to be done.</blockquote>\n<p>Steel's book on the subject is <a style=\"font-style: italic;\" href=\"http://webapps2.ucalgary.ca/~steel/\">The Procrastination Equation</a>, which I highly recommend.</p>\n<p>&nbsp;</p>\n<p><em>Happiness</em></p>\n<p>There is an abundance of research on factors that correlate with <em>subjective well-being</em> (individuals' own assessments of their happiness and life satisfaction).</p>\n<p>Factors that <em>don't correlate</em> much with happiness include: age,<sup>3</sup> gender,<sup>4</sup> parenthood,<sup>5</sup> intelligence,<sup>6</sup> physical attractiveness,<sup>7</sup> and money<sup>8</sup> (as long as you're above the poverty line). Factors that <em>correlate moderately</em>&nbsp;with happiness include: health,<sup>9</sup> social activity,<sup>10</sup> and religiosity.<sup>11</sup> Factors that <em>correlate strongly</em> with happiness include: genetics,<sup>12</sup> love and relationship satisfaction,<sup>13</sup> and work satisfaction.<sup>14</sup></p>\n<p>For many of these factors, a <em>causal</em>&nbsp;link to happiness has also been demonstrated with some confidence, but that story is too complicated to tell in this short article.</p>\n<p>&nbsp;</p>\n<h4 id=\"Conclusions\">Conclusions</h4>\n<p>Many compassionate professionals have modeled their careers after George Miller's (1969) call to \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Epstein-Giving-Psychology-Away.pdf\">give psychology away</a>\" to the masses as a means of promoting human welfare. As a result, hundreds of experimental studies have been done to test which self-help methods work, and which do not. We humans can use this knowledge to achieve our goals.</p>\n<p>But much work remains to be done. Many features of human psychology and behavior are not well-understood, and many self-help methods recommended by popular and academic authors have not yet been experimentally tested. If you are considering psychology research as a career path, and you want to (1) improve human welfare, (2) get research funding, (3) explore an under-developed area of research, and (4) have the chance to write a best-selling self-help book once you've done some of your research, then please consider a career of <em>experimentally testing different self-help methods</em>. Humanity will thank you for it.</p>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\">Notes</h4>\n<p><small><sup>1</sup><sup>&nbsp;</sup>Read a nice overview of the literature in Bergsma, \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Bergsma-Do-Self-Help-Books-Help.pdf\">Do Self-Help Books Help?</a>\" (2008).</small></p>\n<p><small><sup>2</sup> I recommend the 10th edition, which has large improvements over the 9th edition, including 4500 new citations.</small></p>\n<p><small><sup>3</sup>&nbsp;Age and happiness are unrelated (Lykken 1999), age accounting for less than 1% of the variation in people's happiness (Inglehart 1990; Myers &amp; Diener 1997).</small></p>\n<p><small><sup>4</sup> Despite being treated for depressive disorders twice as often as men (Nolen-Hoeksema 2002), women report just as high levels of well-being as men do (Myers 1992).</small></p>\n<p><small><sup>5</sup> Apparently, the joys and stresses of parenthood balance each other out, as people with and without children are equally happy (Argyle 2001).</small></p>\n<p><small><sup>6</sup>&nbsp;Both IQ and educational attainment appear to be unrelated to happiness (Diener et al. 2009; Ross &amp; Van Willigen 1997).</small></p>\n<p><small><sup>7</sup> Good-looking people enjoy huge advantages, but do not report greater happiness than others (Diener et al. 1995).</small></p>\n<p><small><sup>8</sup> The correlation between income and happiness is surprisingly weak (Diener &amp; Seligman 2004; Diener et al. 1993; Johnson &amp; Krueger 2006). One problem may be that higher income contributes to greater materialism, which impedes happiness (Frey &amp; Stutzer 2002; Kasser et al. 2004; Solberg et al. 2002; Kasser 2002; Van Boven 2005; Nickerson et al. 2003; Kahneman et al. 2006).</small></p>\n<p><small><sup>9</sup> Those with disabling health conditions are happier than you might think (Myers 1992; Riis et al. 2005; Argyle 1999).</small></p>\n<p><small><sup>10</sup> Those who are satisfied with their social life are moderately more happy than others (Diener &amp; Seligman 2004; Myers 1999; Diener &amp; Seligman 2002).</small></p>\n<p><small><sup>11</sup> Religiosity correlates with happiness (Abdel-Kahlek 2005; Myers 2008), though it may be religious attendance and not religious belief that matters (Chida et al. 2009).</small></p>\n<p><small><sup>12</sup> Past happiness is the best predictor of future happiness (Lucas &amp; Diener 2008). Happiness is surprisingly unmoved by external factors (Lykken &amp; Tellegen 1996), because the genetics accounts for about 50% of the variance in happiness (Lyubomirsky et al. 2005; Stubbe et al. 2005).</small></p>\n<p><small><sup>13</sup>&nbsp;Married people are happier than those who are single or divorced (Myers &amp; Diener 1995; Diener et al. 2000), and marital satisfaction predicts happiness (Proulx et al. 2007).</small></p>\n<p><small><sup>14</sup>&nbsp;Unemployment makes people very unhappy (Argyle 2001), and job satisfaction is strongly correlated with happiness (Judge &amp; Klinger 2008; Warr 1999).</small></p>\n<p><small>&nbsp;</small></p>\n<h4 id=\"References\">References</h4>\n<p><small>Abdel-Khalek (2006). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Abdel-Khalek-Happiness-health-and-religiosity-Significant-relations.pdf\">Happiness, health, and religiosity: Significant relations</a>.\" <em>Mental Health</em>, 9(1): 85-97.</small></p>\n<p><small>Agarwal, Karpicke, Kang, Roediger, &amp; McDermott (2008). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Agarwal-Examining-the-Testing-Effect.pdf\">Examining the testing effect with open- and closed-book tests.</a>\" <em>Applied Cognitive Psychology</em>, 22: 861-876.</small></p>\n<p><small>Allgood, Risko, Alvarez, &amp; Fairbanks (2000). \"Factors that influence study.\" In Flippo &amp; Caverly, (Eds.), <em>Handbook of college reading and study strategy research</em>. Mahwah, NJ: Erlbaum.</small></p>\n<p><small>Argyle (1999). \"Causes and correlates of happiness.\" In Kahneman, Diener, &amp; Schwartz (Eds.), <em>Well-being: The foundations of hedonic psychology</em>. New York: Sage.</small></p>\n<p><small>Argyle (2001). <em>The Psychology of Happiness</em>&nbsp;(2nd ed.). New York: Routledge.</small></p>\n<p><small>Buckley (1998). <em>God is My Broker:&nbsp;A Monk-Tycoon Reveals the 7 1/2 Laws of Spiritual and Financial Growth</em>. New York: Random House.</small></p>\n<p><small>Butler &amp; Roediger (2008). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Butler-Feedback-enhances-the-positive-effects.pdf\">Feedback enhances the positive effects and reduces the negative effects of multiple-choice testing</a>.\" <em>Memory &amp; Cognition</em>, 36(3).</small></p>\n<p><small>Chida, Steptoe, &amp; Powell (2009). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chida-Religiosity-Spirituality-and-Mortality.pdf\">Religiosity/Spirituality and Mortality</a>.\" <em>Psychotherapy and Psychosomatics</em>, 78(2): 81-90.</small></p>\n<p><small>Cepeda, Pashler, Vul, Wixted, &amp; Rohrer (2006). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Cepeda-Distributed-Practice-in-Verbal-Recall-Trasks.pdf\">Distributed practice in verbal recall tasks: A review and quantitative synthesis</a>.\" <em>Psychological Bulletin</em>, 132: 354-380.</small></p>\n<p><small>Diener, Sandvik, Seidlitz, &amp; Diener (1993). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-The-relationship-between-income-and-subjective-well-being-Relative-or-absolute.pdf\">The relationship between income and subjective well-being: Relative or absolute?</a>\" <em>Social Indicators Research</em>, 28: 195-223.</small></p>\n<p><small>Diener, Wolsic, &amp; Fujita (1995). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Physical-Attractiveness-and-Subjective-Well-Being.pdf\">Physical attractiveness and subjective well-being</a>.\" <em>Journal of Personality and Social Psychology</em>, 69: 120-129.</small></p>\n<p><small>Diener, Gohm, Suh, &amp; Oishi (2000). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Similarity-of-the-relations-between-marital-status-and-subjective-well-being-across-cultures.pdf\">Similarity of the relations between marital status and subjective well-being across cultures</a>.\" <em>Journal of Cross-Cultural Psychology</em>, 31: 419-436.</small></p>\n<p><small>Diener &amp; Seligman (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Very-Happy-People.pdf\">Very happy people</a>.\" <em>Psychological Science</em>, 13: 80-83.</small></p>\n<p><small>Diener &amp; Seligman (2004). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Beyond-money.pdf\">Beyond money: Toward an economy of well-being</a>.\" <em>Psychological Science in the Public Interest</em>, 5(1): 1-31.</small></p>\n<p><small>Diener, Kesebir, &amp; Tov (2009). \"Happiness\" In Leary &amp; Hoyle (Eds.), <em>Handbook of Individual Differences in Social Behavior</em>&nbsp;(pp. 147-160). New York: Guilford.</small></p>\n<p><small>Einstein &amp; McDaniel (2004). <em>Memory Fitness: A Guide for Successful Aging</em>. New Haven, CT: Yale University Press.</small></p>\n<p><small>Frey &amp; Stutzer (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Frey-What-can-economists-learn-from-happiness-research.pdf\">What can economists learn from happiness research?</a>\" <em>Journal of Economic Literature</em>, 40: 402-435.</small></p>\n<p><small>Hermann, Raybeck, &amp; Gruneberg (2002). <em>Improving memory and study skills: Advances in theory and practice.</em>&nbsp;Ashland, OH: Hogrefe &amp; Huber.</small></p>\n<p><small>Iaccino (1996). \"A further examination of the bizarre imagery mnemonic: Its effectiveness with mixed context and delayed testing. <em>Perceptual &amp; Motor Skills</em>, 83: 881-882.</small></p>\n<p><small>Inglehart (1990). <em>Culture shift in advanced industrial society</em>. Princeton, NJ: Princeton University Press.</small></p>\n<p><small>Johnson &amp; Krueger (2006). \"How money buys happiness: Genetic and environmental processes linking finances and life satisfaction.\" <em>Journal of Personality and Social Psychology</em>, 90: 680-691.</small></p>\n<p><small>Judge &amp; Klinger (2008). \"Job satisfaction: Subjective well-being at work.\" In Eid &amp; Larsen (Eds.), <em>The science of subjective well-being</em>&nbsp;(pp. 393-413). New York: Guilford.</small></p>\n<p><small>Kahneman, Krueger, Schkade, Schwarz, &amp; Stone (2006). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Kahneman-Would-you-be-happier-if-you-were-richer-A-focusing-illusion.pdf\">Would you be happier if you were richer? A focusing illusion</a>.\" <em>Science</em>, 312: 1908-1910.</small></p>\n<p><small>Kasser (2002). <em>The high prices of materialism</em>. Cambridge, MA: MIT Press.</small></p>\n<p><small>Kasser, Ryan, Couchman, &amp; Sheldon (2004). \"Materialistic values: Their causes and consequences.\" In Kasser &amp; Kanner (Eds.), <em>Psychology and consumer culture: The struggle for a good life in a materialistic world</em>. Washington DC: American Psychological Association.</small></p>\n<p><small>Karpicke &amp; Roediger (2003). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Karpicke-The-critical-importance-of-retrieval-for-learning.pdf\">The critical importance of retrieval for learning</a>.\"&nbsp;<em>Science</em>, 319: 966-968.&nbsp;</small></p>\n<p><small>Karpicke &amp; Roediger (2007). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Karpicke-Expanding-retrieval-practice-promotes-short-term-retention-but-equally-spaced-retrieval-enhances-long-term-retention.pdf\">Expanding retrieval practice promotes short-term retention, but equally spaced retrieval enhances long-term retention</a>.\" <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, 33(4): 704-719.</small></p>\n<p><small>Lucas &amp; Diener (2008). \"Personality and subjective well-being.\" In John, Robins, &amp; Pervin (Eds.), <em>Handbook of personality: Theory and research</em>&nbsp;(pp. 796-814). New York: Guilford.</small></p>\n<p><small>Lyubomirsky, Sheldon, &amp; Schkade (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Lyubomirsky-Pursuing-happiness-The-architecture-of-sustainable-change.pdf\">Pursuing happiness: The architecture of sustainable change</a>.\" <em>Review of General Psychology</em>, 9(2), 111-131.</small></p>\n<p><small>Lykken &amp; Tellegen (1996). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Lykken-Happiness-Is-a-Stochastic-Phenomenon.pdf\">Happiness is a stochastic phenomenon</a>.\" <em>Psychological Science</em>, 7: 186-189.</small></p>\n<p><small>Lykken (1999). <em>Happiness: The nature and nurture of joy and contentment</em>. New York: St. Martin's.</small></p>\n<p><small>Massen &amp; Vaterrodt-Plunnecke (2006). \"The role of proactive interference in mnemonic techniques.\" <em>Memory</em>, 14: 189-196.</small></p>\n<p><small>McDaniel, Waddill, &amp; Shakesby (1996). \"Study strategies, interest, and learning from Text: The application of material appropriate processing.\" In Herrmann, McEvoy, Hertzog, Hertel, &amp; Johnson (Eds.), <em>Basic and applied memory research: Theory in context</em>&nbsp;(Vol 1). Mahwah, NJ: Erlbaum.</small></p>\n<p><small>Miller (1969).&nbsp;\"On turning psychology over to the unwashed.\" <em>Psychology Today</em>, 3(7), 53\u201354, 66\u201368, 70, 72, 74.</small></p>\n<p><small>Moe &amp; De Beni (2004). \"Studying passages with the loci method: Are subject-generated more effective than experimenter-supplied loci?\" <em>Journal of Mental Imagery</em>, 28(3-4): 75-86.</small></p>\n<p><small>Moe &amp; De Beni (2005).&nbsp;\"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Moe-Stressing-the-efficacy-of-the-Loci-method.pdf\">Stressing the efficacy of the Loci method: oral presentation and the subject-generation of the Loci pathway with expository passages</a>.\" <em>Applied Cognitive Psychology</em>, 19(1): 95-106.</small></p>\n<p><small>Myers (1992). <em>The pursuit of happiness: Who is happy, and why</em>. New York: Morrow.</small></p>\n<p><small>Myers &amp; Diener (1995). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Myers-who-is-happy.pdf\">Who is happy?</a>\" <em>Psychological Science</em>, 6: 10-19.</small></p>\n<p><small>Myers &amp; Diener (1997). \"The pursuit of happiness.\" <em>Scientific American, Special Issue 7</em>: 40-43.</small></p>\n<p><small>Myers (1999). \"Close relationships and quality of life.\" In Kahnemann, Diener, &amp; Schwarz (Eds.), <em>Well-being: The foundations of hedonic psychology</em>. New York: Sage.</small></p>\n<p><small>Myers (2008). \"Religion and human flourishing.\" In Eid &amp; Larsen (Eds.), <em>The science of subjective well-being</em>&nbsp;(pp. 323-346). New York: Guilford.</small></p>\n<p><small>Nickerson, Schwartz, Diener, &amp; Kahnemann (2003). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Nickerson-Zeroing-in-on-the-dark-side-of-the-american-dream.pdf\">Zeroing in on the dark side of the American dream: A closer look at the negative consequences of the goal for financial success</a>.\" <em>Psychological Science</em>, 14(6): 531-536.</small></p>\n<p><small>Nolen-Hoeksema (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Nolen-Hoeksema-Gender-differences-in-depression.pdf\">Gender differences in depression</a>.\" In Gotlib &amp; Hammen (Eds.), <em>Handbook of Depression</em>. New York: Guilford.</small></p>\n<p><small>Proulx, Helms, &amp; Cheryl (2007). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Proulx-Marital-Quality-and-Personal-Well\u2010Being-A-Meta\u2010Analysis.pdf\">Marital quality and personal well-being: A Meta-analysis</a>.\" <em>Journal of Marriage and Family</em>, 69: 576-593.</small></p>\n<p><small>Roediger &amp; Karpicke (2006a). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Roediger-Test-Enhanced-Learning.pdf\">Test-enhanced learning: Taking memory tests improves long-term retention</a>.\"&nbsp;<em>Psychological Science</em>, 17: 249-255.</small></p>\n<p><small>Roediger &amp; Karpicke (2006b). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Roediger-The-power-of-testing-memory.pdf\">The power of testing memory: Basic research and implications for educational practice</a>.\"&nbsp;<em>Perspectives on Psychological Science</em>, 1(3): 181-210.</small></p>\n<p><small>Riis, Loewenstein, Baron, Jepson, Fagerlin, &amp; Ubel (2005). \"Ignorance of hedonic adaptation to hemodialysis: A study using ecological momentary assessment.\" <em>Journal of Experimental Psychology: General</em>, 134: 3-9.</small></p>\n<p><small>Rohrer &amp; Taylor (2006). \"The effects of over-learning and distributed practice on the retention of mathematics knowlege.&nbsp;<em style=\"font-style: italic;\">Applied Cognitive Psychology</em>, 20: 1209-1224.&nbsp;</small></p>\n<p><small>Rohrer, Taylor, Pashler, Wixted, &amp; Cepeda (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Rohrer-The-Effect-of-Overlearning-on-Long-Term-Retention.pdf\">The Effect of Overlearning on Long-Term Retention</a>.\"&nbsp;<em style=\"font-style: italic;\">Applied Cognitive Psychology</em>, 19: 361-374.</small></p>\n<p><small>Ross &amp; Van Willigen (1997). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Ross-Education-and-the-subjective-quality-of-life.pdf\">Education and the subjective quality of life</a>.\" <em>Journal of Health &amp; Social Behavior</em>, 38: 275-297.</small></p>\n<p><small>Seabrook, Brown, &amp; Solity (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Saebrook-Distributed-and-massed-practice-From-laboratory-to-classroom.pdf\">Distributed and massed practice: From laboratory to class-room</a>.\" <em>Applied Cognitive Psychology</em>, 19(1): 107-122.</small></p>\n<p><small>Solberg, Diener, Wirtz, Lucas, &amp; Oishi (2002). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Solberg-Wanting-Having-and-Satisfaction-Examining-the-Role-of-Desire-Discrepancies-in-Satisfaction-With-Income.pdf\">Wanting, having, and satisfaction: Examining the role of desire discrepancies in satisfaction with income</a>.\" <em>Journal of Personality and Social Psychology</em>, 83(3): 725-734.</small></p>\n<p><small>Stubbe, Posthuma, Boomsa, &amp; De Geus (2005). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Stubbe-Heritability-of-life-satisfaction-in-adults-A-twin-family-study.pdf\">Heritability and life satisfaction in adults: A twin-family study</a>.\" <em>Psychological Medicine</em>, 35: 1581-1588.</small></p>\n<p><small>Tigner (1999). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Tigner-Putting-Memory-Research-to-Good-Use.pdf\">Putting memory research to good use: Hints from cognitive psychology</a>.\"&nbsp;<em>College Teaching</em>, 47(4): 149-151.</small></p>\n<p><small>Van Boven (2005). \"Experientialism, materialism, and the pursuit of happiness.\" <em>Review of General Psychology</em>, 9(2): 132-142.</small></p>\n<p><small>Warr (1999). \"Well-being and the workplace.\" In Kahneman, Diener, &amp; Schwartz (Eds.), <em>Well-being: The foundations of hedonic psychology</em>. New York: Sage.</small></p>\n<p><small>Wong (2006). <em>Essential Study Skills</em>. Boston: Houghton Mifflin.</small></p>\n<p><small>Worthen (1997). \"Resiliency of bizarreness effects under varying conditions of verbal and imaginal elaboration and list composition. <em>Journal of Mental Imagery</em>, 21: 167-194.</small></p>", "sections": [{"title": "The industry and the literature", "anchor": "The_industry_and_the_literature", "level": 1}, {"title": "A sampling of scientific self-help advice", "anchor": "A_sampling_of_scientific_self_help_advice", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "502 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 502, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LgavAYtzFQZKg95WC", "qTSRpyuuu6i9gGmWY", "uFYQaGCRwt3wKtyZP", "PBRWb2Em5SNeWYwwB", "64FdKLwmea8MCLWkE", "9o3QBg2xJXcRCxGjS", "qGEqpy7J78bZh3awf", "ndGYn7ZFiZyernp9f", "TNHQLZK5pHbxdnz4e", "synsRtBKDeAFuo7e3", "2iwT298d6A7QgiLPe", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-21T00:06:36.829Z", "modifiedAt": null, "url": null, "title": "Do Corporations Have a Right to Privacy?", "slug": "do-corporations-have-a-right-to-privacy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.225Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Allen", "createdAt": "2010-08-12T18:39:43.418Z", "isAdmin": false, "displayName": "David_Allen"}, "userId": "yKNx2drs5QMLT6iqu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WN65GCMtErEF7QNCf/do-corporations-have-a-right-to-privacy", "pageUrlRelative": "/posts/WN65GCMtErEF7QNCf/do-corporations-have-a-right-to-privacy", "linkUrl": "https://www.lesswrong.com/posts/WN65GCMtErEF7QNCf/do-corporations-have-a-right-to-privacy", "postedAtFormatted": "Friday, January 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20Corporations%20Have%20a%20Right%20to%20Privacy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20Corporations%20Have%20a%20Right%20to%20Privacy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWN65GCMtErEF7QNCf%2Fdo-corporations-have-a-right-to-privacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20Corporations%20Have%20a%20Right%20to%20Privacy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWN65GCMtErEF7QNCf%2Fdo-corporations-have-a-right-to-privacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWN65GCMtErEF7QNCf%2Fdo-corporations-have-a-right-to-privacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 488, "htmlBody": "<p><a title=\"Do Corporations Have a Right to Privacy?\" href=\"http://www.schneier.com/blog/archives/2011/01/do_corporations.html\">The link to Bruce Schneier's original post.</a></p>\n<blockquote>\n<p>This week, the U.S. Supreme Court will hear arguments about whether or not corporations have the same rights to \"personal privacy\" that individuals do.</p>\n</blockquote>\n<p>The Electronic Privacy Information Center (<a title=\"EPIC\" href=\"http://epic.org/\">EPIC</a>) has filed a <a title=\"Friend of the Court\" href=\"http://en.wikipedia.org/wiki/Amicus_curiae\">amicus curiae</a> <a title=\"FCC v. AT&amp;T\" href=\"http://epic.org/amicus/fccvatt/EPIC_FCCvATT%20Brief_final.pdf\">brief</a>&nbsp;in the case.</p>\n<p>The brief makes legal and philosophical arguments for privacy as an important human right, and that it is not a corporate right, and does not need to be. It also contains a number of&nbsp;scholarly&nbsp;references on the topic.</p>\n<p>I find the legal arguments against a corporate right to privacy convincing. Corporations in our current legal context are intentionally organized to provide certain types of public accountability.</p>\n<p>However, I am not convinced by the philosophical arguments for restricting the right to privacy to individuals.</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a><em></em></p>\n<p><em>To summarize the utility of an individual right to privacy, as discussed in the brief:</em></p>\n<p>A right to privacy is&nbsp;necessary&nbsp;for individuals to enjoy the feelings of autonomy, control and security, which are important for healthy&nbsp;cognitive&nbsp;development.&nbsp;It protects the seclusion needed for&nbsp;experimentation, developing&nbsp;identity, establishing intimate relationships, and for making meaningful&nbsp;choices free of <a title=\"Ex-Ante\" href=\"http://en.wikipedia.org/wiki/Ex-ante\">ex-ante</a> manipulation and coercion.&nbsp;It provides the&nbsp;opportunity&nbsp;to structure life in unconventional ways, freeing&nbsp;a person from the stultifying effects of scrutiny and approbation or&nbsp;disapprobation.&nbsp;It helps to avoid&nbsp;embarrassment, preserving dignity and respect, avoiding harm to social development and growth. It helps to prevent the&nbsp;appropriation of a person's name or likeness.&nbsp;It sets the balance of power between a person,&nbsp;and the world around them.</p>\n<p><em>To summarize the philosophical argument against a corporate right to privacy:</em></p>\n<p>Corporate entities are incapable of experiencing hurt feelings.</p>\n<div><br /></div>\n<div>I think these&nbsp;arguments provide an unwarranted special status to human emotion. Emotions are body states related to expected utility. They bias decision making toward choices that maximize reward and minimize punishment.</div>\n<div>\n<div>Although corporate entities do not experience body states in the same way that humans do, they demonstrate responses similar to individuals in equivalent situations.</div>\n<div>Corporate entities have strong survival related incentives for protecting their public image and will respond to social pressures. They will actively defend themselves from legal and market threats. They need to understand the limits of their autonomy, control and security so they can evaluate risks and make good decisions. They benefit from seclusion for experimentation, developing identity, establishing high trust relationships, and for making meaningful choices free of ex-ante manipulation and coercion. They benefit from opportunities to structure their organization in unconventional ways. That need to protect their name and likeness.</div>\n</div>\n<div>\n<div><br /></div>\n<div>Another argument against a corporate right to privacy that I have heard, is that corporate entities&nbsp;have more power than individuals, and therefore they require more transparency and accountability.</div>\n<div>Power however is not restricted to corporate entities. Individuals can also&nbsp;wield&nbsp;the power of money, position and influence. By this argument they should also be forced to endure greater&nbsp;transparency and accountability.</div>\n<div>Also, there are&nbsp;corporate entities that have no more power than common individuals. Additional accountability would&nbsp;burden&nbsp;these entities, with no meaningful benefit to the community.</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WN65GCMtErEF7QNCf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -10, "extendedScore": null, "score": -2e-05, "legacy": true, "legacyId": "5018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-21T11:07:56.256Z", "modifiedAt": null, "url": null, "title": "Politics is a fact of life", "slug": "politics-is-a-fact-of-life", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:38.196Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CFBDgcpt3irn87waD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/66JS2y8zx9yR7CRWE/politics-is-a-fact-of-life", "pageUrlRelative": "/posts/66JS2y8zx9yR7CRWE/politics-is-a-fact-of-life", "linkUrl": "https://www.lesswrong.com/posts/66JS2y8zx9yR7CRWE/politics-is-a-fact-of-life", "postedAtFormatted": "Friday, January 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politics%20is%20a%20fact%20of%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitics%20is%20a%20fact%20of%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F66JS2y8zx9yR7CRWE%2Fpolitics-is-a-fact-of-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politics%20is%20a%20fact%20of%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F66JS2y8zx9yR7CRWE%2Fpolitics-is-a-fact-of-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F66JS2y8zx9yR7CRWE%2Fpolitics-is-a-fact-of-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>There is a tendency to downvote articles and commentaries with a political subtext with a remark on how politics is the mind-killer. I completely understand that nobody wants his mind to be killed, however, I disagree on the employed methods. I don't think anybody can really afford to ignore politics. It's a fact about any group of even a handful of people. Thus instead of shunning politics I think it's better to build one's rational defenses. Understanding that politics is a problem is only the first step. If you stop there, there will always be a big part of life where you are not rational. Therefore I suggest that, as long as it doesn't get out of hands, there should always be room for political discussions if not on the main site at least in the discussion section.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "66JS2y8zx9yR7CRWE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 14, "extendedScore": null, "score": 6.699752636513559e-07, "legacy": true, "legacyId": "5055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-21T19:06:05.325Z", "modifiedAt": null, "url": null, "title": "Hugo Awards - HP:MoR", "slug": "hugo-awards-hp-mor", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y9nxFGLdrwu572f8f/hugo-awards-hp-mor", "pageUrlRelative": "/posts/y9nxFGLdrwu572f8f/hugo-awards-hp-mor", "linkUrl": "https://www.lesswrong.com/posts/y9nxFGLdrwu572f8f/hugo-awards-hp-mor", "postedAtFormatted": "Friday, January 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hugo%20Awards%20-%20HP%3AMoR&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHugo%20Awards%20-%20HP%3AMoR%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy9nxFGLdrwu572f8f%2Fhugo-awards-hp-mor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hugo%20Awards%20-%20HP%3AMoR%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy9nxFGLdrwu572f8f%2Fhugo-awards-hp-mor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy9nxFGLdrwu572f8f%2Fhugo-awards-hp-mor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>This year I'm a supporting member of WorldCon for the first time, and I noticed that the Hugo's have a&nbsp;category&nbsp;for Best Fan Writer (\"<span style=\"font-family: Trebuchet, Verdana, Arial; font-size: 13px;\">Any person whose writing has appeared in semiprozines or fanzines or in generally available electronic media during 2010.\"). I do believe Harry Potter and the Methods of Rationality counts, and I plan on nominating it. &nbsp;Even making it onto the ballot will probably expose it to a wider audience. &nbsp;Is anyone else here a WorldCon member and thinking of nominating MoR?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y9nxFGLdrwu572f8f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 66, "extendedScore": null, "score": 0.00013811272008027893, "legacy": true, "legacyId": "5056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-22T06:41:33.992Z", "modifiedAt": null, "url": null, "title": "The Illusion of Sameness", "slug": "the-illusion-of-sameness", "viewCount": null, "lastCommentedAt": "2011-02-03T21:04:50.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Elizabeth", "createdAt": "2010-06-13T03:33:52.050Z", "isAdmin": false, "displayName": "Elizabeth"}, "userId": "ZCSWTn7aaWZMKasDB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KsB2BDeXtn92LntYd/the-illusion-of-sameness", "pageUrlRelative": "/posts/KsB2BDeXtn92LntYd/the-illusion-of-sameness", "linkUrl": "https://www.lesswrong.com/posts/KsB2BDeXtn92LntYd/the-illusion-of-sameness", "postedAtFormatted": "Saturday, January 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Illusion%20of%20Sameness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Illusion%20of%20Sameness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKsB2BDeXtn92LntYd%2Fthe-illusion-of-sameness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Illusion%20of%20Sameness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKsB2BDeXtn92LntYd%2Fthe-illusion-of-sameness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKsB2BDeXtn92LntYd%2Fthe-illusion-of-sameness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<p>I have lately been pondering two contradictory human beliefs: the belief in our own exceptionalism and the belief in our own ordinariness.&nbsp; We model the world with ourselves as prototypical humans, using our own emotions and reactions and thought processes to run a program predicting the behavior of others.&nbsp; That is, after all, what our mirror neurons evolved for.&nbsp; However, when it comes to our abilities or our intelligence or our problems, we believe we are something out of the ordinary.&nbsp; The second bias is easier to compensate for than the first, but it is the first that interests me, because even when confronted with significant evidence of your own difference, it is extremely difficult to really internalize it and change your model of the world.</p>\n<p>I consider it likely that among those reading this, more of you grew up in families of intelligent, educated people than the national average.&nbsp; It is also likely that the number of readers who grew up in liberal and nonreligious families exceeds the norm.&nbsp; Not all, certainly.&nbsp; Quite possibly not even the majority.&nbsp; However, I think there must be many among you who understand the shock of leaving your family and community, perhaps to go to college, and slowly discovering that there are people who <em>don't think like you</em>.&nbsp; People who don't share your foundational knowledge, who trust different authorities, who have completely different default settings.</p>\n<p>There are two separate pieces to this: the first is our default setting for the beliefs and authorities of others as similar to our own, and the second is our modeling of other people's mental processes as similar to our own.&nbsp; The second is seldom run across in everyday life, unless engaging in a discussion of mental processes as in the comments on <a href=\"/lw/o9/words_as_mental_paintbrush_handles/\" target=\"_blank\">this post</a>.&nbsp; The first is run across fairly frequently, but here I must apologize for bringing up the mind-killer, for it is most apparent in politics.&nbsp; I will endeavor to keep my example brief.</p>\n<p>In the spring of 2010 I was substitute teaching in a rural area of upstate New York.&nbsp; I was in the teacher's room eating lunch, with ten or so other teachers, when the subject of the BP oil spill arose, as it was the major current event at the time.&nbsp; My experience dictated that the conversation would start with \"Isn't this a terrible thing?\" and proceed to \"Oil companies shouldn't be allowed to make a mess they can't clean up.\" or \"Shouldn't we invest in clean energy?\"&nbsp; However, though the conversation began as I expected, I was subsequently informed that the oil companies <em>were</em> fully capable of cleaning it up, and that the reason it had not been cleaned up already was a conspiracy on the part of President Obama.</p>\n<p>This was particularly shocking to me because there were no warning signs.&nbsp; These were people who were all educated to a Master's Degree level.&nbsp; I had spoken to several on more innocuous topics, and they seemed both interesting and intelligent. (I realize that this reveals a potential bias on my part regarding a correlation between intelligence and a liberal bent).&nbsp; They seemed, in every respect, to be people like me.</p>\n<p>How could I have better predicted this?&nbsp; I remain at a loss.&nbsp; The only significant difference between that group and the people who react according to my model is region of origin, but that oversimplifies the question.&nbsp; I am not only confused, I am viscerally uncomfortable.&nbsp; How do we model for people whose cultural contexts and information delivering authorities are fundamentally different from our own, without lumping them into a faceless group?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KsB2BDeXtn92LntYd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 17, "extendedScore": null, "score": 6.702789113328041e-07, "legacy": true, "legacyId": "4428", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YF9HB6cWCJrDK5pBM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-22T21:11:19.278Z", "modifiedAt": null, "url": null, "title": "Could/would an FAI recreate people who are information-theoretically dead by modern standards?", "slug": "could-would-an-fai-recreate-people-who-are-information", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.143Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mNqyFZT2M9fh2oRPx/could-would-an-fai-recreate-people-who-are-information", "pageUrlRelative": "/posts/mNqyFZT2M9fh2oRPx/could-would-an-fai-recreate-people-who-are-information", "linkUrl": "https://www.lesswrong.com/posts/mNqyFZT2M9fh2oRPx/could-would-an-fai-recreate-people-who-are-information", "postedAtFormatted": "Saturday, January 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Could%2Fwould%20an%20FAI%20recreate%20people%20who%20are%20information-theoretically%20dead%20by%20modern%20standards%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACould%2Fwould%20an%20FAI%20recreate%20people%20who%20are%20information-theoretically%20dead%20by%20modern%20standards%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmNqyFZT2M9fh2oRPx%2Fcould-would-an-fai-recreate-people-who-are-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Could%2Fwould%20an%20FAI%20recreate%20people%20who%20are%20information-theoretically%20dead%20by%20modern%20standards%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmNqyFZT2M9fh2oRPx%2Fcould-would-an-fai-recreate-people-who-are-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmNqyFZT2M9fh2oRPx%2Fcould-would-an-fai-recreate-people-who-are-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>If someone gets cremated or buried long enough for eir brain to fully decompose into dirt, it becomes extremely difficult to revive em. Nothing short of a vastly superhuman intelligence would have a chance of doing it. I suspect that it would be possible for a superintelligence to do it, but unless there's a more efficient way to do it, it would require recomputing the Earth's history from the time the AGI is activated back to the death of the last person it intends to save. Not only does this require immense computational resources that could be used to the benefit of people who are still alive, it also requires simulating people experiencing pain (backwards). On the other hand, this saves people's lives. Does anyone have any compelling arguments on why an FAI would or would not recreate me if I die, decompose, and then the singularity occurs a long time after my death?</p>\n<p>Why do I want to know? Well, aside from the question being interesting in its own right, it is an important factor in deciding whether or not cryonics is worth-while.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mNqyFZT2M9fh2oRPx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 8, "extendedScore": null, "score": 6.705039667043306e-07, "legacy": true, "legacyId": "5079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-22T23:40:17.572Z", "modifiedAt": null, "url": null, "title": "Link: Chessboxing could help train automatic emotion regulation", "slug": "link-chessboxing-could-help-train-automatic-emotion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:25.591Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2oFZXTr6idLjyPxtA/link-chessboxing-could-help-train-automatic-emotion", "pageUrlRelative": "/posts/2oFZXTr6idLjyPxtA/link-chessboxing-could-help-train-automatic-emotion", "linkUrl": "https://www.lesswrong.com/posts/2oFZXTr6idLjyPxtA/link-chessboxing-could-help-train-automatic-emotion", "postedAtFormatted": "Saturday, January 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Chessboxing%20could%20help%20train%20automatic%20emotion%20regulation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Chessboxing%20could%20help%20train%20automatic%20emotion%20regulation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2oFZXTr6idLjyPxtA%2Flink-chessboxing-could-help-train-automatic-emotion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Chessboxing%20could%20help%20train%20automatic%20emotion%20regulation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2oFZXTr6idLjyPxtA%2Flink-chessboxing-could-help-train-automatic-emotion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2oFZXTr6idLjyPxtA%2Flink-chessboxing-could-help-train-automatic-emotion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>EDIT: Argh, I really failed to read this closely. Rewriting...</p>\n<p>Just saw this over at Not Exactly Rocket Science. <a href=\"http://www.scientificamerican.com/blog/post.cfm?id=chessboxing-is-fighting-for-good-be-2011-01-10\">Chessboxing (or similar games) could help train automatic emotion regulation.</a>&nbsp; <span style=\"text-decoration: line-through;\">Obviously this should generalize.&nbsp; Has this - by which I mean finding things that can help train automatic emotion regulation - been done before?</span> This doesn't seem to be anything new - and this is extrapolation, not experimental results - but it's a neat application.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2oFZXTr6idLjyPxtA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.70542679320575e-07, "legacy": true, "legacyId": "5080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T02:17:14.029Z", "modifiedAt": null, "url": null, "title": "Easy Intelligence Augmentation or Internet Wackaloonery?", "slug": "easy-intelligence-augmentation-or-internet-wackaloonery", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rDQJWoFtp7pvCHraA/easy-intelligence-augmentation-or-internet-wackaloonery", "pageUrlRelative": "/posts/rDQJWoFtp7pvCHraA/easy-intelligence-augmentation-or-internet-wackaloonery", "linkUrl": "https://www.lesswrong.com/posts/rDQJWoFtp7pvCHraA/easy-intelligence-augmentation-or-internet-wackaloonery", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Easy%20Intelligence%20Augmentation%20or%20Internet%20Wackaloonery%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEasy%20Intelligence%20Augmentation%20or%20Internet%20Wackaloonery%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDQJWoFtp7pvCHraA%2Feasy-intelligence-augmentation-or-internet-wackaloonery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Easy%20Intelligence%20Augmentation%20or%20Internet%20Wackaloonery%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDQJWoFtp7pvCHraA%2Feasy-intelligence-augmentation-or-internet-wackaloonery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDQJWoFtp7pvCHraA%2Feasy-intelligence-augmentation-or-internet-wackaloonery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p>On January 4, PJ Eby sent around an email linking an... interesting... website. The claim on the particular <a title=\"I'm not convinced.\" href=\"http://www.winwenger.com/ebooks/guaran3.htm\">webpage he linked</a> was as follows:</p>\r\n<ul>\r\n<li>the normal span of your breath is critical to how well your mental faculties can function</li>\r\n<li>the best activity for increasing your breath span is held-breath underwater swimming</li>\r\n<li>this also results in an&nbsp;increase in intelligence caused by a permanent increase in blood flow to the brain</li>\r\n<li>being fully underwater is important to the practice because it induces the diving reflex response</li>\r\n</ul>\r\n<p>This site is part of a sales pitch, so many of the claims are stated in hyperbolic language. I've already noted one factual error: the webpage claims that being underwater triggers the diving reflex, while in fact (or at least, according to Wikipedia) the <a title=\"Wikipedia -- so reliable!\" href=\"http://en.wikipedia.org/wiki/Mammalian_diving_reflex\">diving reflex</a> is triggered when one's face is immersed in water colder that 21&nbsp;&deg;C.</p>\r\n<p>But there is a testable claim here: learn to hold your breath for longer periods of time -- particularly in conditions that elicit the diving reflex -- and you will see increased intelligence. I know that some readers of LW regularly train and test their intelligence, so I offer this as an easily implemented potential method. The possible gains seem to me to outweigh the costs of the training and the low prior probability of the claim.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rDQJWoFtp7pvCHraA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.705833307011174e-07, "legacy": true, "legacyId": "5082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T02:52:58.572Z", "modifiedAt": null, "url": null, "title": "[Comic]: The Singularity in Dinosaur Comics", "slug": "comic-the-singularity-in-dinosaur-comics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rh7bdRbCTWprLojK6/comic-the-singularity-in-dinosaur-comics", "pageUrlRelative": "/posts/rh7bdRbCTWprLojK6/comic-the-singularity-in-dinosaur-comics", "linkUrl": "https://www.lesswrong.com/posts/rh7bdRbCTWprLojK6/comic-the-singularity-in-dinosaur-comics", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BComic%5D%3A%20The%20Singularity%20in%20Dinosaur%20Comics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BComic%5D%3A%20The%20Singularity%20in%20Dinosaur%20Comics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frh7bdRbCTWprLojK6%2Fcomic-the-singularity-in-dinosaur-comics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BComic%5D%3A%20The%20Singularity%20in%20Dinosaur%20Comics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frh7bdRbCTWprLojK6%2Fcomic-the-singularity-in-dinosaur-comics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frh7bdRbCTWprLojK6%2Fcomic-the-singularity-in-dinosaur-comics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>Look, Dinosaur Comics has a <a href=\"http://www.qwantz.com/index.php?comic=1883\">strip about the singularity</a>. And it's pretty much in line with the Singularity Institute's views on the matter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rh7bdRbCTWprLojK6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 1, "extendedScore": null, "score": 6.705924598565178e-07, "legacy": true, "legacyId": "5083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T04:05:50.605Z", "modifiedAt": null, "url": null, "title": "Hyperlinks and Less Wrong", "slug": "hyperlinks-and-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:27.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q3Lkz6hsfnyH9NEJd/hyperlinks-and-less-wrong", "pageUrlRelative": "/posts/Q3Lkz6hsfnyH9NEJd/hyperlinks-and-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/Q3Lkz6hsfnyH9NEJd/hyperlinks-and-less-wrong", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hyperlinks%20and%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHyperlinks%20and%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3Lkz6hsfnyH9NEJd%2Fhyperlinks-and-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hyperlinks%20and%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3Lkz6hsfnyH9NEJd%2Fhyperlinks-and-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ3Lkz6hsfnyH9NEJd%2Fhyperlinks-and-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1174, "htmlBody": "<p>Just prior to learning about Less Wrong, I read an article that discussed internet usage, hyperlinks, and reading comprehension. The focus of the article was on numerous studies showing that people's reading comprehension goes down when they are searching the web, or viewing multimedia presentations. People reading an article with straight text and no distractions were able to answer more questions about it than people viewing a page that contained the same information, but which included video and hyperlinks. Even if you don't click on the hyperlinks, every time you see one you have to consciously make the decision whether to click, which is distracting.</p>\n<p>I will link the article at the end of my post. If you want you can go read it now.</p>\n<p>In my personal experience, I find it not only distracting but frustrating. Each link is a tantalizing window into interesting-sounding-new-information, and I know that if I don't click on it immediately I probably won't bother to go back to it later, but that if DO click on it immediately I'm probably going to lose track of what I'm currently reading. It can be fun to link-crawl through Wikipedia, starting out with an article about prepositions, and somehow ending on an article about animal sexuality. But what's fun is not the same thing as useful for education.</p>\n<p>Enter Less Wrong. My initial reaction was \"This is Wikipedia on Crack.\" Not only do a lot of articles here feature a bajillion hyperlinks, but each link often goes to another lengthy article full of fascinating information that I don't know, some of which is necessary to understand the first article, but none of which is easily summarized. With Wikipedia, if I run across a new word with a hyperlink it's at least possible for me to glance at the hyperlink, get a quick sense of what it's about, then return to my original reading. On Less Wrong that is often impossible. My first foray into this website felt like drowning in amazing ideas. It was a lot of fun, I definitely learned some things, but it was very confusing and hard to focus. Eventually I realized what the Sequences were and started reading them, but even they have a fair amount of interconnectedness that makes it hard to focus. (A related issue is that oftentimes I WAS already familiar with the introductory stuff, so I skipped ahead to the later articles, and then THOSE would link to various articles that I wasn't familiar with).</p>\n<p>I've been meaning to post about this for a while. I was particularly motivated to after the Scientific Self Help article, which was a) really interesting and important to me, and b) filled with an absurd number of hyperlinks, with varying degrees of usefulness. The problem is that even among the useful ones, I don't have time to read *all* of them and still be able to really read the original article. I think the web in general and Less Wrong in particular would benefit from a new standard technique to provide relevant information that takes advantage of the web's interconnectivity, without being as distracting. Part of that can be accomplished by including better summaries of information instead of linking entire articles. Part of it might require fundamental changes in site coding, which is more work than I am capable of doing. (And yes I'm aware of the \"hey guys there this awesome idea oh by the way I don't have time to do it\" issue, but I think there's legitimate work that non-programmers can contribute and I plan to if other people are interested).</p>\n<p><strong>Solutions?</strong></p>\n<p>I'm not sure what the ideal alternative is. Hyperlinks are used mainly for two reasons, both useful enough that simply saying \"don't do it\" isn't practical. One is to provide easy access to content the reader might not be familiar with but which is outside the purview of your article. The other is to craft a sort of <a href=\"http://www.ahajokes.com/\">joke</a>, where the content isn't actually important, it's just funny that you pretended it was. Which isn't strictly *necessary*, and probably is overused and would easily be replaced with alternate ways to be funny or poignant, but I'm not going to dismiss it out of hand.</p>\n<p>The main alternate approach to hyperlinks is footnotes, and those have the opposite problem: by the time I get to them, I often forget what the footnote was about. What was interesting to me about the Scientific Self Help article was that it contained a LOAD of links to amazon.com books in the middle of the article. Then at the end it had a References section which listed a lot of books but DIDN'T include links to acquire them. (There were *some* links there, but they seemed to go to actual webpages, not places where I could acquire books).</p>\n<p>I can think of some possible browser apps/extensions that might alleviate the issue. (Maybe let you click on a hyperlink and store it, along with the surrounding context, in another window, so you can store the links you wanted to remember and then read them all after the initial piece). But forcing people to acquire a particular app isn't the best of solutions, especially when it's not clear to a visiting user that they are supposed to do so. It probably wouldn't actually solve the measurable reading comprehension problems either.</p>\n<p>Edit: Yes, I&nbsp; know about tabbed browsing. But when you get ten+ hyperlinks per article, by the time you get to the end it's hard to remember the context for the pages I opened. With ten+ links it also becomes difficult to read the titles of the pages so finding the ones I'm looking for is a pain.</p>\n<p>A fairly simple solution (for some cases) is to replace or supplement hyperlinks with <a title=\"Which lets you include a summary of the site you're linking. People can read the summary to get the basics if they aren't familiar, then move on\" href=\"http://www.web-source.net/html_mouseover_text.htm\">mouseover</a>&nbsp;text. I'm not sure if that'll actually improve the reading comprehension issues. (Probably not, so long as hyperlinks are the norm, so your brain will struggle every time it sees something that even *looks* like one). But Lesswrong could probably benefit from an established norm wherein top level posts come with a *brief* summary (no more than a paragraph) that can be easily referred to.</p>\n<p>Facebook has a feature that goes a step further (and presumably requires some effort to program) wherein entering a link automatically grabs relevant information and attaches it to your post. This is something that works specifically in the context of Facebook though, where posts are short and you don't need to wait before reaching the end for the relevant information. I feel like there should be some way to accomplish something similar for longer posts.</p>\n<p>That's all I have for now. I don't know if other people are as bothered by this as I am, but assuming there is interest in this issue I'll continue to think about it.</p>\n<p>&nbsp;</p>\n<p>Relevant links:</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.wired.com/magazine/2010/05/ff_nicholas_carr/all/1\">The Web Shatters Focus, Rewires Brains (Wired Article)</a></p>\n<p><a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">Scientific Self Help (Example of over-hyperlinked Less Wrong article)<br /></a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q3Lkz6hsfnyH9NEJd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 20, "extendedScore": null, "score": 6.706114656819944e-07, "legacy": true, "legacyId": "5085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["33KewgYhNSxFpbpXg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T06:44:29.137Z", "modifiedAt": null, "url": null, "title": "Gettier in Zombie World", "slug": "gettier-in-zombie-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:06.699Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hairyfigment", "createdAt": "2010-10-14T22:12:59.035Z", "isAdmin": false, "displayName": "hairyfigment"}, "userId": "NesjW63eueLsbKrCY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/diwLfp9oapitH7Fjj/gettier-in-zombie-world", "pageUrlRelative": "/posts/diwLfp9oapitH7Fjj/gettier-in-zombie-world", "linkUrl": "https://www.lesswrong.com/posts/diwLfp9oapitH7Fjj/gettier-in-zombie-world", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gettier%20in%20Zombie%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGettier%20in%20Zombie%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdiwLfp9oapitH7Fjj%2Fgettier-in-zombie-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gettier%20in%20Zombie%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdiwLfp9oapitH7Fjj%2Fgettier-in-zombie-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdiwLfp9oapitH7Fjj%2Fgettier-in-zombie-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3970, "htmlBody": "<p>A Dr. Nigel Thomas has <a title=\"Thomas Trilemma\" href=\"http://www.imagery-imagination.com/zom-abs.htm\">tried to show</a> a logical self-contradiction in Chalmers' <a title=\"EY: Zombies?\" href=\"/lw/p7/zombies_zombies/\">\"Zombie World\"</a> or zombiphile argument, in a way that would convince Chalmers (given perfect rationality, of course). The argument concerns the claim that we can conceive of a near-duplicate of our world sharing all of its physical laws and containing apparent duplicates of us, who copy everything we say or do for the same physical reasons that lead us to do it, but who lack consciousness. Chalmers says this shows the logical possibility of Zombie World (though nobody believes that it actually exists.) He concludes from this that our world has a nonphysical <a title=\"Chalmers answering reviewer Searle\" href=\"http://www.nybooks.com/articles/archives/1997/may/15/consciousness-and-the-philosophers-an-exchange/\">\"bridging law\"</a> saying that \"systems with the same functional organization have the same sort of conscious experiences,\" and we must regard this as logically independent of the world's physical laws. Thomas' response includes the following point:</p>\n<blockquote>\n<p>Thus zombiphiles normally (and plausibly) insist that we know of our own consciousness directly, non-inferentially. Even so, there must be some sort of cognitive process that takes me from the fact of my consciousness to my (true) belief that I am conscious. As my zombie twin is cognitively indiscernible from me, an indiscernible process, functioning in just the same way, must lead it from the fact of its non-consciousness to the equivalent mistaken belief. Given either consciousness or non-consciousness (and the same contextual circumstances: ex hypothesis, ceteris is entirely paribus) the process leads one to believe that one is conscious. It is like a stuck fuel gauge, that reads FULL whether or not there is any gas in the tank.</p>\n</blockquote>\n<p>While I think his full response has some flaws, it seems better than anything I produced on the subject &mdash; perhaps because I <a title=\"not a defense\" href=\"/lw/ui/use_the_try_harder_luke/\">didn't try very hard</a> to find a contradiction. Thomas tries to form a trilemma by arguing that we can't regard statements made by Zombie Chalmers about consciousness as true, or false, or meaningless. (If you think they deserve the full range of probabilistic truth values, then for the moment let \"true\" mean at least as much confidence as we place in our own equivalent statements and let \"false\" mean any lesser value.) But the important lemma requires an account of knowledge in order to work. To run through the other two lemmas: we know the zombie statements have meaning or truth values for us, the people postulating them, so let the rest of the argument apply to those meanings. And if we call the statements true then they must by assumption refer to something other than consciousness &mdash; call it consciousness-Z. But then Zombie Hairyfigment will (by assumption) say, \"I have real consciousness and not just consciousness-Z.\" (This also seems to reduce probabilistically \"false\" statements to ordinary falsity, still by assumption.)<br /><br />The remaining lemma tries to draw a contradiction out of the zombiphile argument's assumption that Chalmers has knowledge of his own consciousness. We need some way to recognize or rule out knowledge in order for this to work. Happily, on this one question standard philosophy seems to point clearly towards the answer we want.<br /><br />(One Robert Bass apparently makes a related point in his paper, <a title=\"link I found on Google\" href=\"http://reocities.com/Athens/olympus/2178/chalmers.pdf\">Chalmers and the Self-Knowledge Problem (pdf)</a>. But I took the argument in a slightly different direction by asking what the philosophers actually meant.)</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h3>Gettier intuitions: How do they work?</h3>\n<p>&nbsp;</p>\n<p>The famous Gettier Problem illustrates a flaw in the verbal definition of knowledge as \"justified true belief\". <a title=\"online text of short paper\" href=\"http://www.ditext.com/gettier/gettier.html\">Gettier's original case</a> takes a situation in the works of Plato involving Socrates (<a title=\"Wikipedia article with link to text\" href=\"http://en.wikipedia.org/wiki/Theaetetus_%28dialogue%29\"><em>Theatetus</em></a> 142d), and modifies it to make the situation clearer. In Gettier's version, S and his friend Jones have both applied for a job. S heard the president of the company say the job would go to Jones. S has also just counted the ten coins in Jones' pocket, and therefore believes that <strong>\"The man who will get the job has ten coins in his pocket.\"</strong> But it turns out the job goes to S, who unbeknown to himself happened to have ten coins in his own pocket. He therefore had a true belief that seems justified, but I don't know of anyone who believes it should count as knowledge.</p>\n<p>Some philosophers responded to this by saying that in addition to justification and truth, a belief needs to have no false lemmas hiding in its past in order to count as knowledge. But this led to the appearance of the following counterexample, as told in the historical summary <a title=\"Gettier Problem problem\" href=\"http://www.unc.edu/~ujanel/Gettier.htm\">here</a>:</p>\n<blockquote>\n<p>Fake Barn Country: Henry is looking at a (real) barn, and has impeccable visual and other evidence that it is a barn. He is not gettiered; his justification is sound in every way. However, in the neighborhood there are a number of fake, papiere-m&acirc;ch&eacute; barns, any of which would have fooled Henry into thinking it was a barn.</p>\n</blockquote>\n<p>Henry does not appear to use any false lemmas in forming his belief, at least not explicit lemmas like S in the first problem. Yet most philosophers do not believe Henry has knowledge when he says, <strong>'Hey, a barn,'</strong> since he would have thought this whether he saw a real barn or a barn facade. Interestingly, <a title=\"order effect on Gettier\" href=\"http://experimentalphilosophy.typepad.com/experimental_philosophy/2006/12/thoughts_about_.html\">a lot of ordinary people may not share this intuition</a> with the philosophers or may take a different position on the matter in different contexts. I will try to spell out what the Gettier intuitions actually point towards before judging them, in the hope that they point to something useful. For now we can call their object 'G-knowledge' or 'G-nosis'. (That part doesn't seem like proper <a title=\"EY again\" href=\"/lw/nu/taboo_your_words\">Rationalist Taboo</a>, but as far as I can tell my laziness has no fatal consequences.)</p>\n<p>At one time I thought we could save the verbal definition and sweep all the Gettier cases into the No False Lemmas basket by requiring S to reject any practical possibility of deception or self-deception before his or her belief could possibly count as knowledge. This however does not work. The reason why it fails gives me an excuse to quote a delightful Gettier case (also from Lycan's linked historical summary) involving an apparent AI who knows better than to take anything humans say at face value:</p>\n<blockquote>\n<p>Noninferential Nogot (Lehrer 1965; 1970). Mr. Nogot in S&rsquo;s office has given S evidence that he, Nogot, owns a Ford. By a single probabilistic inference, S moves directly (without passing through &lsquo;Nogot owns a Ford&rsquo;) to the conclusion that someone in S&rsquo;s office owns a Ford. (As in any such example, Mr. Nogot does not own a Ford, but S&rsquo;s belief happens to be true because Mr. Havit owns one.)<br />Cautious Nogot (Lehrer 1974; sometimes called &lsquo;Clever Reasoner&rsquo;). This is like the previous example, except that here S, not caring at all who it might be that owns the Ford and also being cautious in matters doxastic, deliberately refrains from forming the belief that Nogot owns it.</p>\n</blockquote>\n<p>The Cautious AI has evidently observed a link between claims of Ford ownership and the existence of Fords which seem to 'belong' to some human in the vicinity. But this S believes humans may have a greater tendency to say they own a Ford when somebody nearby owns one. I can think of postulates that would justify this belief, but let's assume none of them hold true. Then S will modify some of its numerical 'assumptions' if it learns the truth about the link. In principle we could keep using my first attempt at a definition if not for this:</p>\n<blockquote>\n<p>And there is the obvious sort of counterexample to the necessity of &lsquo;no-false-lemmas&rsquo; (Saunders and Champawat 1964; Lehrer 1965). Nondefective Chain: If S has at least one epistemically justifying and non-Gettier-defective line of justification, then S knows even if S has other justifying grounds that contain Gettier gaps. For example (Lehrer), suppose S has overwhelming evidence that Nogot owns a Ford and also overwhelming evidence that Havit owns one. S then knows that someone in the office owns a Ford, because S knows that Havit does and performs existential generalization; it does not matter that one of S&rsquo;s grounds (S&rsquo;s belief that Nogot owns a Ford) is false.</p>\n</blockquote>\n<p>By the time I saw this problem I'd already tried to add something about an acceptable margin of error &epsilon;, changing what my definition said about \"practical possibility\" to make it agree with <a title=\"Intuitive Explanation of Bayes\" href=\"http://yudkowsky.net/bayes/bayes.html\">Bayes' Theorem</a>. But at this point I had to ask if the rest of my definition <a title=\"Bayes Structure\" href=\"/lw/o7/searching_for_bayesstructure/\">actually did anything</a>. (No.)</p>\n<p>From this perspective it seems clear that in each Gettier case where S lacks G-nosis, the reader has more information than S and that leads to a different set of probabilities. I'll start nailing down what that means shortly. First let's look at the claim that G-nosis obeys Bayes.</p>\n<p>My new definition leads to a more generous view of Henry or S in the simple case of No Fake Barns. Previously I would have said that S lacked knowledge both in Fake Barn Country and in the more usual case. But assume that S has unstated estimates of probability, which would change if a pig in a cape appeared to fly out of the barn and take to the sky. (If we assume a lack of even potential self-doubt, I have no problem saying that S lacks knowledge.) It looks like in many cases the Gettier intuitions allow vague verbal or even implied estimates, so long as we could translate each into a rough number range, neither end of which differs by more than &epsilon; from the 'correct' value. S would then have G-nosis for sufficiently forgiving values of &epsilon;.<br /><br />And I do mean values, one &epsilon; for each 'number' that S uses. G-nosis must include a valid chain of probabilistic reasoning that starts from the starting point of S, and in which no value anywhere differs by more than &epsilon; from that which an omniscient human reader would assign. If you think that last part hides a problem or five, give yourself a pat on the back. But we can make it seem less circular by taking \"omniscient\" to mean that for every true claim which requires Bayesian adjustment, our reader uses the appropriate numbers.&nbsp; (I'd call the all-knowing reader 'Kyon,' except I suspect we'll wind up excluding too many people even without this.)</p>\n<p>Note for newcomers: this applies to logical evidence as well. We can treat standard true/false logic as a special case or <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Limit_%28mathematics%29\">limit</a> of probability as we approach total certainty. 'Evidence' in probability means any fact that you gave a greater chance of truth on the assumption of some belief A than on the assumption of not-A. This clearly applies to seeing a proof of the belief. If you assume that you'll never see a valid proof of A and then later have to accept not-A as true, then you can just plug in zero for the probability of seeing the proof in the world of not-A and get a probability of 100% for A. So our proposed definition of knowledge seems general enough to include abstract math and concrete everyday 'knowledge'.<br /><br />Another note about our definition, chiefly for newcomers: the phrase \"every true claim\" needs close attention. In principle <a title=\"his theorems\" href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\">G&ouml;del</a> tells us that every useful logical system (including Bayes) will produce statements it can't prove or disprove with logical certainty. In particular it can't prove its own logical self-consistency (unless it actually contradicts itself). If we regard the statement with the greater probability for us as true, that gives us a new system that creates a new unprovable statement, and so on. But none of these new axioms would change the truth values of statements we could prove within the old system. If we treat mathematically proven statements as overwhelmingly likely but not certain &mdash; if we say that for any real-world system that acts like math in certain basic ways, mathematical theorems have overwhelming probability &mdash; then it seems like none of the new axioms would have much effect on the probability of any statement we've been claiming to know (or have G-nosis of). In fact, that seems like our reason for wanting to call the \"axioms\" true. So I don't <em>think</em> they affect any practical case of G-nosis. We already more or less defined our reader as the limit of a human updating by Bayes (as the set of true statements that no longer require changes approaches the set of all true statements). Hopefully for any given statement we could have G-nosis of, we can define a no-more-than-countably-infinite set of true statements or pieces of evidence that get the job done by creating such a limit. I think I'll just assert that <strong>for every such G-knowable statement A, at worst there exists a sequence of evidence such that A's new probabilities (as we take more of the sequence into account) converge to some limit, and this limit does not change for any additional truth(s) we could throw into the sequence.</strong> Humanity's use of math acts like part of a sequence that works in this way, and because of this the set of unprovable G&ouml;del \"axioms\" looks unnecessary. At this point we might not even need the part about a human reader. But I'll keep it for a little longer in case some non-human reader lacks (say) the Gettier intuition regarding counterfactuals. More on that later.<br /><br />Third note chiefly for newcomers: I haven't addressed the problem of getting the numbers we plug in to the equation, or 'priors'. We know that by choosing sufficiently wrong priors, we can resist any push towards the right answer by finite real-world evidence. This seems less important if we use my limit-definition but still feels like a flaw, in theory. In practice, humans seem to form explanations by comparing the world to the output of <a title=\"Comedy of behaviorism\" href=\"/lw/sr/the_comedy_of_behaviorism/\">black boxes</a> that we carry inside of ourselves, and to which we've affixed labels such as \"anger\" or \"computation\". I don't think it matters if we call these boxes 'spiritual' or 'physical' or 'epiphenomenal', so long as we admit that stuff goes in, stuff comes out and for the most part we don't know how the boxes work. Now a vast amount of evidence suggests that if you started from the fundamental nature of reality and tried using it to <a title=\"Occam's Razor\" href=\"/lw/jp/occams_razor/\">duplicate the output</a> of the \"anger\" box (or one of the many 'love' boxes), you'd need to add more conditions or assumptions than the effects of \"computation\" would require. Even if you took the easy way out and tried to copy a truly opaque box without understanding it, you'd need complicated materials and up to nine months of complex labor for the smallest model. (Also, how would a philosopher postulate love without postulating at least the number 2?) New evidence about reality could of course change this premise. Evidence always changes some of the priors for future calculations. But right now, whenever all else seems equal, I have to assign a greater probability to an explanation which uses only my \"computation\" box than to one which uses other boxes. (Very likely the \"beauty\" box plays a role in all cases. But if you tell me you can explain the world through beauty alone, I probably won't believe you.) This means I must tentatively conclude our omniscient reader would use a similar way of assigning priors. And further differences seem unlikely to matter in the limit. Even for real-world scenarios, the assumption of \"sufficiently wrong priors\" now looks implausible. (Humans didn't actually evolve to get the wrong answer. We just didn't evolve to get the right one. Our ancestors slid in under the margin of error.) All of which seems to let our reader assign a meaning to Eliezer's otherwise fallacious comment <a title=\"on the right Sol. prior\" href=\"/lw/qa/the_dilemma_science_or_bayes/k06\">here</a>.<br /><br />(I had a line that I can't bear to remove entirely about the two worlds of <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Parmenides\">Parmenides</a>, prior probability vs the evidence, and <a title=\"EY's main account\" href=\"/lw/qp/timeless_physics/\">timeless physics</a> compared to previous attempts at reconciliation. But I don't think spelling it out further adds to this argument.)<br /><br />Having established the internal consistency of our definition, we still need to look for Gettier counterexamples before we can call it an account of G-nosis. The ambiguity of Nogot allows for one test. If we assume that people do in fact show a greater chance of saying they own a Ford when somebody nearby owns one, and that S would not need to adjust any prior by more than &epsilon;, it seems to me that S does have knowledge. But we need more than hindsight to prove our case. Apparent creationist nut Robert C. Koons has three attempts at a counterexample in his book <em>Realism regained</em> (though he doesn't seem to look at our specific Bayesian definition). We can dismiss one attempt as giving S an obvious false prior. Another says that S would have used a false prior if not for a blow to the head. This implies that S has no Bayes-approved chain of reasoning from his/her actual starting point to the conclusion. Finally, Koons postulates that an \"all-powerful genie\" controlled the evidence for reasons unrelated to the truth of the belief A, and the result happens to lead to the 'correct' value. But if our 'human reader' would not consider this result knowledge then 'correct' must not mean what I've called G-nosis. Apparently the reader imagines the genie making a different whimsical decision, and calculates different results for most of the many other possible whims it could follow. This results in a high reader-assigned probability which we call P(E|&not;A), or P of E if not-A, for the evidence to appear the way it does to S even if one treats S's belief as false. And so S still would not have G-nosis by our definition.<br /><br />This seems to pinpoint the disagreement between intuitions in the case of Fake Barn. People who deny S in Fake Barn knowledge believe that <a title=\"Counterfactuals\" href=\"/lw/sh/can_counterfactuals_be_true/\">P(E|&not;A) has a meaning</a> even if we think P(&not;A)=0 &mdash; in the limit, perhaps, or in a case where someone mistakenly set the prior probability of A at 100% because the evidence seems so directly and clearly known that they counted it twice. Obviously if we also require that no value anywhere differ by too much from the reader's, then a sufficiently 'wrong' P(E|&not;A) rules out G-nosis. (This seems particularly true if it equals P(E|A), since E would no longer count as evidence at all.) If the reader adds what S might call a 'no-silliness' assumption, ruling out any effect of the barn facades on P(E|&not;A), then S does have G-nosis. Though of course this would make less sense if our reader brought out the 'silly' counterfactual in the first place.</p>\n<p>I'd love to see how the belief in knowledge for Fake Barn correlates with <a title=\"The Authoritarians\" href=\"http://home.cc.umanitoba.ca/~altemey/\">getting the wrong answer on a logic test</a> due to judging the conclusion (\"sharks are fish,\") instead of the argument or evidence (\"fish live in the water,\" &amp; \"sharks live in the water.\")</p>\n<p>Finally, the Zombie World story itself has a form similar to a Gettier case. If we just assume two 'logically possible worlds' then the probability of Chalmers arriving at the truth about his own experiences, given only the process he used to reach his beliefs, appears to equal 50%. Clearly this does not count as G-nosis, nor would we expect it to. But let's adjust the number of worlds so that the probability of consciousness, given the process by which Chalmers came to believe in his consciousness, equals 1-&epsilon;. (Technically we should also subtract any uncertainty Chalmers will admit to, but I think he agrees that if this number grows large enough he no longer has G-nosis of the belief in question, rather then the contrary belief or the division of probability between them.) His belief now fits the definition. And it intuitively seems like knowledge, since &epsilon; means the chance of error that I'd find acceptable. This seems like very good news for our definition.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h3>What about the zombies?</h3>\n<p>&nbsp;</p>\n<p>Before going on to the obvious question, I want to make sure we <a title=\"rules\" href=\"http://en.wikipedia.org/wiki/Zombieland#The_rules\">double-tap</a>. Back to the first zombie! Until now I've tried to treat \"G-nosis\" as a strange construct that philosophers happen to value. Now that we have the definition, however, it seems to spell out what we need in order for our beliefs to fit the facts. Even the assumption that P(E|&not;A) obeys a reasonable definition of counterfactuals in the limit seems necessary in that, if we assume this doesn't happen, we suddenly have no reason to think our beliefs fit the facts. \"G-nosis\" illustrates the process of seeking reality. If our beliefs don't fit the definition, then extending this process will bring them into conflict with reality &mdash; unless we somehow arrived at the right answer by chance without having any reason to believe it. I think the zombiphile argument does assume that we have reason to believe in our own consciousness and no omniscient reader could disagree.</p>\n<p>We gave Chalmers a 50% chance of having conscious experiences, what philosophers call \"qualia,\" given his evidence and the assumption of the two worlds. But he would object that he used qualia to reach his conclusion of having them &mdash; not in the sense of causation, but in the sense of giving his conclusion meaning. When he says he knows qualia this sounds exactly like Z-Chalmers' belief, but gains added content from his actual qualia. Our definition however requires him to use probabilistic reasoning on some level before he can trust his belief. This appears to mean that some time elapses between the qualia he (seemingly) uses to form his belief, and the formally acceptable conclusion. If we assume Zombie World exists, then the evidence available when the conclusion appears seems just like the evidence available to Z-Chalmers. So it seems like this added content appears after the fact, like the content we give to statements by Z-Chalmers. And if the real Chalmers treats it as new evidence then the same argument applies. So much for Zombie #1.</p>\n<p>So does the addition of worlds save the zombiphile argument? I don't know. (We always see one bloody zombie left at the end of the film!) But anything that could deceive me about the existence of my own qualia seems able to deceive me about '2+2=4'. I therefore argue that, in this particular case, &epsilon; should not exceed the chance that Descartes' Demon is deceiving me now, or that arithmetic contradicts itself, and 2 plus 2 can equal 3. Obviously it seems like a stretch to call this a logical possibility.</p>\n<p>Even in the zombiphile argument, we can't regard the \"bridging law\" that creates consciousness as wholly independent from physical or inter-subjectively functional causes. We must therefore admit a strong a priori connection between a set of physical processes (a part of what Chalmers calls \"microphysical truths\") and human experience or \"phenomenal truths\". This brings us a lot closer to agreement, since the Bayesian forms of physicalism claim only an overwhelming probability for the claim that necessity links a <em>particular set</em> of causes to consciousness. And if we trust that our own consciousness exists, I argue this shows we must not believe in Zombie World as a meaningful possibility.</p>\n<p>I feel like I should get this posted now, so I won't try to say what this means for a not-so-Giant <a title=\"GAZP vs GLUT\" href=\"/lw/pa/gazp_vs_glut/\">Look-Up Table</a> that simulates a <a title=\"and anthropics\" href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann Brain</a> concluding it has qualia. Feel free to solve that in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "diwLfp9oapitH7Fjj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 6.706525652041377e-07, "legacy": true, "legacyId": "5086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A Dr. Nigel Thomas has <a title=\"Thomas Trilemma\" href=\"http://www.imagery-imagination.com/zom-abs.htm\">tried to show</a> a logical self-contradiction in Chalmers' <a title=\"EY: Zombies?\" href=\"/lw/p7/zombies_zombies/\">\"Zombie World\"</a> or zombiphile argument, in a way that would convince Chalmers (given perfect rationality, of course). The argument concerns the claim that we can conceive of a near-duplicate of our world sharing all of its physical laws and containing apparent duplicates of us, who copy everything we say or do for the same physical reasons that lead us to do it, but who lack consciousness. Chalmers says this shows the logical possibility of Zombie World (though nobody believes that it actually exists.) He concludes from this that our world has a nonphysical <a title=\"Chalmers answering reviewer Searle\" href=\"http://www.nybooks.com/articles/archives/1997/may/15/consciousness-and-the-philosophers-an-exchange/\">\"bridging law\"</a> saying that \"systems with the same functional organization have the same sort of conscious experiences,\" and we must regard this as logically independent of the world's physical laws. Thomas' response includes the following point:</p>\n<blockquote>\n<p>Thus zombiphiles normally (and plausibly) insist that we know of our own consciousness directly, non-inferentially. Even so, there must be some sort of cognitive process that takes me from the fact of my consciousness to my (true) belief that I am conscious. As my zombie twin is cognitively indiscernible from me, an indiscernible process, functioning in just the same way, must lead it from the fact of its non-consciousness to the equivalent mistaken belief. Given either consciousness or non-consciousness (and the same contextual circumstances: ex hypothesis, ceteris is entirely paribus) the process leads one to believe that one is conscious. It is like a stuck fuel gauge, that reads FULL whether or not there is any gas in the tank.</p>\n</blockquote>\n<p>While I think his full response has some flaws, it seems better than anything I produced on the subject \u2014 perhaps because I <a title=\"not a defense\" href=\"/lw/ui/use_the_try_harder_luke/\">didn't try very hard</a> to find a contradiction. Thomas tries to form a trilemma by arguing that we can't regard statements made by Zombie Chalmers about consciousness as true, or false, or meaningless. (If you think they deserve the full range of probabilistic truth values, then for the moment let \"true\" mean at least as much confidence as we place in our own equivalent statements and let \"false\" mean any lesser value.) But the important lemma requires an account of knowledge in order to work. To run through the other two lemmas: we know the zombie statements have meaning or truth values for us, the people postulating them, so let the rest of the argument apply to those meanings. And if we call the statements true then they must by assumption refer to something other than consciousness \u2014 call it consciousness-Z. But then Zombie Hairyfigment will (by assumption) say, \"I have real consciousness and not just consciousness-Z.\" (This also seems to reduce probabilistically \"false\" statements to ordinary falsity, still by assumption.)<br><br>The remaining lemma tries to draw a contradiction out of the zombiphile argument's assumption that Chalmers has knowledge of his own consciousness. We need some way to recognize or rule out knowledge in order for this to work. Happily, on this one question standard philosophy seems to point clearly towards the answer we want.<br><br>(One Robert Bass apparently makes a related point in his paper, <a title=\"link I found on Google\" href=\"http://reocities.com/Athens/olympus/2178/chalmers.pdf\">Chalmers and the Self-Knowledge Problem (pdf)</a>. But I took the argument in a slightly different direction by asking what the philosophers actually meant.)</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h3 id=\"Gettier_intuitions__How_do_they_work_\">Gettier intuitions: How do they work?</h3>\n<p>&nbsp;</p>\n<p>The famous Gettier Problem illustrates a flaw in the verbal definition of knowledge as \"justified true belief\". <a title=\"online text of short paper\" href=\"http://www.ditext.com/gettier/gettier.html\">Gettier's original case</a> takes a situation in the works of Plato involving Socrates (<a title=\"Wikipedia article with link to text\" href=\"http://en.wikipedia.org/wiki/Theaetetus_%28dialogue%29\"><em>Theatetus</em></a> 142d), and modifies it to make the situation clearer. In Gettier's version, S and his friend Jones have both applied for a job. S heard the president of the company say the job would go to Jones. S has also just counted the ten coins in Jones' pocket, and therefore believes that <strong>\"The man who will get the job has ten coins in his pocket.\"</strong> But it turns out the job goes to S, who unbeknown to himself happened to have ten coins in his own pocket. He therefore had a true belief that seems justified, but I don't know of anyone who believes it should count as knowledge.</p>\n<p>Some philosophers responded to this by saying that in addition to justification and truth, a belief needs to have no false lemmas hiding in its past in order to count as knowledge. But this led to the appearance of the following counterexample, as told in the historical summary <a title=\"Gettier Problem problem\" href=\"http://www.unc.edu/~ujanel/Gettier.htm\">here</a>:</p>\n<blockquote>\n<p>Fake Barn Country: Henry is looking at a (real) barn, and has impeccable visual and other evidence that it is a barn. He is not gettiered; his justification is sound in every way. However, in the neighborhood there are a number of fake, papiere-m\u00e2ch\u00e9 barns, any of which would have fooled Henry into thinking it was a barn.</p>\n</blockquote>\n<p>Henry does not appear to use any false lemmas in forming his belief, at least not explicit lemmas like S in the first problem. Yet most philosophers do not believe Henry has knowledge when he says, <strong>'Hey, a barn,'</strong> since he would have thought this whether he saw a real barn or a barn facade. Interestingly, <a title=\"order effect on Gettier\" href=\"http://experimentalphilosophy.typepad.com/experimental_philosophy/2006/12/thoughts_about_.html\">a lot of ordinary people may not share this intuition</a> with the philosophers or may take a different position on the matter in different contexts. I will try to spell out what the Gettier intuitions actually point towards before judging them, in the hope that they point to something useful. For now we can call their object 'G-knowledge' or 'G-nosis'. (That part doesn't seem like proper <a title=\"EY again\" href=\"/lw/nu/taboo_your_words\">Rationalist Taboo</a>, but as far as I can tell my laziness has no fatal consequences.)</p>\n<p>At one time I thought we could save the verbal definition and sweep all the Gettier cases into the No False Lemmas basket by requiring S to reject any practical possibility of deception or self-deception before his or her belief could possibly count as knowledge. This however does not work. The reason why it fails gives me an excuse to quote a delightful Gettier case (also from Lycan's linked historical summary) involving an apparent AI who knows better than to take anything humans say at face value:</p>\n<blockquote>\n<p>Noninferential Nogot (Lehrer 1965; 1970). Mr. Nogot in S\u2019s office has given S evidence that he, Nogot, owns a Ford. By a single probabilistic inference, S moves directly (without passing through \u2018Nogot owns a Ford\u2019) to the conclusion that someone in S\u2019s office owns a Ford. (As in any such example, Mr. Nogot does not own a Ford, but S\u2019s belief happens to be true because Mr. Havit owns one.)<br>Cautious Nogot (Lehrer 1974; sometimes called \u2018Clever Reasoner\u2019). This is like the previous example, except that here S, not caring at all who it might be that owns the Ford and also being cautious in matters doxastic, deliberately refrains from forming the belief that Nogot owns it.</p>\n</blockquote>\n<p>The Cautious AI has evidently observed a link between claims of Ford ownership and the existence of Fords which seem to 'belong' to some human in the vicinity. But this S believes humans may have a greater tendency to say they own a Ford when somebody nearby owns one. I can think of postulates that would justify this belief, but let's assume none of them hold true. Then S will modify some of its numerical 'assumptions' if it learns the truth about the link. In principle we could keep using my first attempt at a definition if not for this:</p>\n<blockquote>\n<p>And there is the obvious sort of counterexample to the necessity of \u2018no-false-lemmas\u2019 (Saunders and Champawat 1964; Lehrer 1965). Nondefective Chain: If S has at least one epistemically justifying and non-Gettier-defective line of justification, then S knows even if S has other justifying grounds that contain Gettier gaps. For example (Lehrer), suppose S has overwhelming evidence that Nogot owns a Ford and also overwhelming evidence that Havit owns one. S then knows that someone in the office owns a Ford, because S knows that Havit does and performs existential generalization; it does not matter that one of S\u2019s grounds (S\u2019s belief that Nogot owns a Ford) is false.</p>\n</blockquote>\n<p>By the time I saw this problem I'd already tried to add something about an acceptable margin of error \u03b5, changing what my definition said about \"practical possibility\" to make it agree with <a title=\"Intuitive Explanation of Bayes\" href=\"http://yudkowsky.net/bayes/bayes.html\">Bayes' Theorem</a>. But at this point I had to ask if the rest of my definition <a title=\"Bayes Structure\" href=\"/lw/o7/searching_for_bayesstructure/\">actually did anything</a>. (No.)</p>\n<p>From this perspective it seems clear that in each Gettier case where S lacks G-nosis, the reader has more information than S and that leads to a different set of probabilities. I'll start nailing down what that means shortly. First let's look at the claim that G-nosis obeys Bayes.</p>\n<p>My new definition leads to a more generous view of Henry or S in the simple case of No Fake Barns. Previously I would have said that S lacked knowledge both in Fake Barn Country and in the more usual case. But assume that S has unstated estimates of probability, which would change if a pig in a cape appeared to fly out of the barn and take to the sky. (If we assume a lack of even potential self-doubt, I have no problem saying that S lacks knowledge.) It looks like in many cases the Gettier intuitions allow vague verbal or even implied estimates, so long as we could translate each into a rough number range, neither end of which differs by more than \u03b5 from the 'correct' value. S would then have G-nosis for sufficiently forgiving values of \u03b5.<br><br>And I do mean values, one \u03b5 for each 'number' that S uses. G-nosis must include a valid chain of probabilistic reasoning that starts from the starting point of S, and in which no value anywhere differs by more than \u03b5 from that which an omniscient human reader would assign. If you think that last part hides a problem or five, give yourself a pat on the back. But we can make it seem less circular by taking \"omniscient\" to mean that for every true claim which requires Bayesian adjustment, our reader uses the appropriate numbers.&nbsp; (I'd call the all-knowing reader 'Kyon,' except I suspect we'll wind up excluding too many people even without this.)</p>\n<p>Note for newcomers: this applies to logical evidence as well. We can treat standard true/false logic as a special case or <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Limit_%28mathematics%29\">limit</a> of probability as we approach total certainty. 'Evidence' in probability means any fact that you gave a greater chance of truth on the assumption of some belief A than on the assumption of not-A. This clearly applies to seeing a proof of the belief. If you assume that you'll never see a valid proof of A and then later have to accept not-A as true, then you can just plug in zero for the probability of seeing the proof in the world of not-A and get a probability of 100% for A. So our proposed definition of knowledge seems general enough to include abstract math and concrete everyday 'knowledge'.<br><br>Another note about our definition, chiefly for newcomers: the phrase \"every true claim\" needs close attention. In principle <a title=\"his theorems\" href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\">G\u00f6del</a> tells us that every useful logical system (including Bayes) will produce statements it can't prove or disprove with logical certainty. In particular it can't prove its own logical self-consistency (unless it actually contradicts itself). If we regard the statement with the greater probability for us as true, that gives us a new system that creates a new unprovable statement, and so on. But none of these new axioms would change the truth values of statements we could prove within the old system. If we treat mathematically proven statements as overwhelmingly likely but not certain \u2014 if we say that for any real-world system that acts like math in certain basic ways, mathematical theorems have overwhelming probability \u2014 then it seems like none of the new axioms would have much effect on the probability of any statement we've been claiming to know (or have G-nosis of). In fact, that seems like our reason for wanting to call the \"axioms\" true. So I don't <em>think</em> they affect any practical case of G-nosis. We already more or less defined our reader as the limit of a human updating by Bayes (as the set of true statements that no longer require changes approaches the set of all true statements). Hopefully for any given statement we could have G-nosis of, we can define a no-more-than-countably-infinite set of true statements or pieces of evidence that get the job done by creating such a limit. I think I'll just assert that <strong>for every such G-knowable statement A, at worst there exists a sequence of evidence such that A's new probabilities (as we take more of the sequence into account) converge to some limit, and this limit does not change for any additional truth(s) we could throw into the sequence.</strong> Humanity's use of math acts like part of a sequence that works in this way, and because of this the set of unprovable G\u00f6del \"axioms\" looks unnecessary. At this point we might not even need the part about a human reader. But I'll keep it for a little longer in case some non-human reader lacks (say) the Gettier intuition regarding counterfactuals. More on that later.<br><br>Third note chiefly for newcomers: I haven't addressed the problem of getting the numbers we plug in to the equation, or 'priors'. We know that by choosing sufficiently wrong priors, we can resist any push towards the right answer by finite real-world evidence. This seems less important if we use my limit-definition but still feels like a flaw, in theory. In practice, humans seem to form explanations by comparing the world to the output of <a title=\"Comedy of behaviorism\" href=\"/lw/sr/the_comedy_of_behaviorism/\">black boxes</a> that we carry inside of ourselves, and to which we've affixed labels such as \"anger\" or \"computation\". I don't think it matters if we call these boxes 'spiritual' or 'physical' or 'epiphenomenal', so long as we admit that stuff goes in, stuff comes out and for the most part we don't know how the boxes work. Now a vast amount of evidence suggests that if you started from the fundamental nature of reality and tried using it to <a title=\"Occam's Razor\" href=\"/lw/jp/occams_razor/\">duplicate the output</a> of the \"anger\" box (or one of the many 'love' boxes), you'd need to add more conditions or assumptions than the effects of \"computation\" would require. Even if you took the easy way out and tried to copy a truly opaque box without understanding it, you'd need complicated materials and up to nine months of complex labor for the smallest model. (Also, how would a philosopher postulate love without postulating at least the number 2?) New evidence about reality could of course change this premise. Evidence always changes some of the priors for future calculations. But right now, whenever all else seems equal, I have to assign a greater probability to an explanation which uses only my \"computation\" box than to one which uses other boxes. (Very likely the \"beauty\" box plays a role in all cases. But if you tell me you can explain the world through beauty alone, I probably won't believe you.) This means I must tentatively conclude our omniscient reader would use a similar way of assigning priors. And further differences seem unlikely to matter in the limit. Even for real-world scenarios, the assumption of \"sufficiently wrong priors\" now looks implausible. (Humans didn't actually evolve to get the wrong answer. We just didn't evolve to get the right one. Our ancestors slid in under the margin of error.) All of which seems to let our reader assign a meaning to Eliezer's otherwise fallacious comment <a title=\"on the right Sol. prior\" href=\"/lw/qa/the_dilemma_science_or_bayes/k06\">here</a>.<br><br>(I had a line that I can't bear to remove entirely about the two worlds of <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Parmenides\">Parmenides</a>, prior probability vs the evidence, and <a title=\"EY's main account\" href=\"/lw/qp/timeless_physics/\">timeless physics</a> compared to previous attempts at reconciliation. But I don't think spelling it out further adds to this argument.)<br><br>Having established the internal consistency of our definition, we still need to look for Gettier counterexamples before we can call it an account of G-nosis. The ambiguity of Nogot allows for one test. If we assume that people do in fact show a greater chance of saying they own a Ford when somebody nearby owns one, and that S would not need to adjust any prior by more than \u03b5, it seems to me that S does have knowledge. But we need more than hindsight to prove our case. Apparent creationist nut Robert C. Koons has three attempts at a counterexample in his book <em>Realism regained</em> (though he doesn't seem to look at our specific Bayesian definition). We can dismiss one attempt as giving S an obvious false prior. Another says that S would have used a false prior if not for a blow to the head. This implies that S has no Bayes-approved chain of reasoning from his/her actual starting point to the conclusion. Finally, Koons postulates that an \"all-powerful genie\" controlled the evidence for reasons unrelated to the truth of the belief A, and the result happens to lead to the 'correct' value. But if our 'human reader' would not consider this result knowledge then 'correct' must not mean what I've called G-nosis. Apparently the reader imagines the genie making a different whimsical decision, and calculates different results for most of the many other possible whims it could follow. This results in a high reader-assigned probability which we call P(E|\u00acA), or P of E if not-A, for the evidence to appear the way it does to S even if one treats S's belief as false. And so S still would not have G-nosis by our definition.<br><br>This seems to pinpoint the disagreement between intuitions in the case of Fake Barn. People who deny S in Fake Barn knowledge believe that <a title=\"Counterfactuals\" href=\"/lw/sh/can_counterfactuals_be_true/\">P(E|\u00acA) has a meaning</a> even if we think P(\u00acA)=0 \u2014 in the limit, perhaps, or in a case where someone mistakenly set the prior probability of A at 100% because the evidence seems so directly and clearly known that they counted it twice. Obviously if we also require that no value anywhere differ by too much from the reader's, then a sufficiently 'wrong' P(E|\u00acA) rules out G-nosis. (This seems particularly true if it equals P(E|A), since E would no longer count as evidence at all.) If the reader adds what S might call a 'no-silliness' assumption, ruling out any effect of the barn facades on P(E|\u00acA), then S does have G-nosis. Though of course this would make less sense if our reader brought out the 'silly' counterfactual in the first place.</p>\n<p>I'd love to see how the belief in knowledge for Fake Barn correlates with <a title=\"The Authoritarians\" href=\"http://home.cc.umanitoba.ca/~altemey/\">getting the wrong answer on a logic test</a> due to judging the conclusion (\"sharks are fish,\") instead of the argument or evidence (\"fish live in the water,\" &amp; \"sharks live in the water.\")</p>\n<p>Finally, the Zombie World story itself has a form similar to a Gettier case. If we just assume two 'logically possible worlds' then the probability of Chalmers arriving at the truth about his own experiences, given only the process he used to reach his beliefs, appears to equal 50%. Clearly this does not count as G-nosis, nor would we expect it to. But let's adjust the number of worlds so that the probability of consciousness, given the process by which Chalmers came to believe in his consciousness, equals 1-\u03b5. (Technically we should also subtract any uncertainty Chalmers will admit to, but I think he agrees that if this number grows large enough he no longer has G-nosis of the belief in question, rather then the contrary belief or the division of probability between them.) His belief now fits the definition. And it intuitively seems like knowledge, since \u03b5 means the chance of error that I'd find acceptable. This seems like very good news for our definition.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h3 id=\"What_about_the_zombies_\">What about the zombies?</h3>\n<p>&nbsp;</p>\n<p>Before going on to the obvious question, I want to make sure we <a title=\"rules\" href=\"http://en.wikipedia.org/wiki/Zombieland#The_rules\">double-tap</a>. Back to the first zombie! Until now I've tried to treat \"G-nosis\" as a strange construct that philosophers happen to value. Now that we have the definition, however, it seems to spell out what we need in order for our beliefs to fit the facts. Even the assumption that P(E|\u00acA) obeys a reasonable definition of counterfactuals in the limit seems necessary in that, if we assume this doesn't happen, we suddenly have no reason to think our beliefs fit the facts. \"G-nosis\" illustrates the process of seeking reality. If our beliefs don't fit the definition, then extending this process will bring them into conflict with reality \u2014 unless we somehow arrived at the right answer by chance without having any reason to believe it. I think the zombiphile argument does assume that we have reason to believe in our own consciousness and no omniscient reader could disagree.</p>\n<p>We gave Chalmers a 50% chance of having conscious experiences, what philosophers call \"qualia,\" given his evidence and the assumption of the two worlds. But he would object that he used qualia to reach his conclusion of having them \u2014 not in the sense of causation, but in the sense of giving his conclusion meaning. When he says he knows qualia this sounds exactly like Z-Chalmers' belief, but gains added content from his actual qualia. Our definition however requires him to use probabilistic reasoning on some level before he can trust his belief. This appears to mean that some time elapses between the qualia he (seemingly) uses to form his belief, and the formally acceptable conclusion. If we assume Zombie World exists, then the evidence available when the conclusion appears seems just like the evidence available to Z-Chalmers. So it seems like this added content appears after the fact, like the content we give to statements by Z-Chalmers. And if the real Chalmers treats it as new evidence then the same argument applies. So much for Zombie #1.</p>\n<p>So does the addition of worlds save the zombiphile argument? I don't know. (We always see one bloody zombie left at the end of the film!) But anything that could deceive me about the existence of my own qualia seems able to deceive me about '2+2=4'. I therefore argue that, in this particular case, \u03b5 should not exceed the chance that Descartes' Demon is deceiving me now, or that arithmetic contradicts itself, and 2 plus 2 can equal 3. Obviously it seems like a stretch to call this a logical possibility.</p>\n<p>Even in the zombiphile argument, we can't regard the \"bridging law\" that creates consciousness as wholly independent from physical or inter-subjectively functional causes. We must therefore admit a strong a priori connection between a set of physical processes (a part of what Chalmers calls \"microphysical truths\") and human experience or \"phenomenal truths\". This brings us a lot closer to agreement, since the Bayesian forms of physicalism claim only an overwhelming probability for the claim that necessity links a <em>particular set</em> of causes to consciousness. And if we trust that our own consciousness exists, I argue this shows we must not believe in Zombie World as a meaningful possibility.</p>\n<p>I feel like I should get this posted now, so I won't try to say what this means for a not-so-Giant <a title=\"GAZP vs GLUT\" href=\"/lw/pa/gazp_vs_glut/\">Look-Up Table</a> that simulates a <a title=\"and anthropics\" href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann Brain</a> concluding it has qualia. Feel free to solve that in the comments.</p>", "sections": [{"title": "Gettier intuitions: How do they work?", "anchor": "Gettier_intuitions__How_do_they_work_", "level": 1}, {"title": "What about the zombies?", "anchor": "What_about_the_zombies_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fdEWWr8St59bXLbQr", "fhEPnveFhb9tmd7Pe", "WBdvyyHLdxZSAMmoz", "QrhAeKBkm2WsdRYao", "9fpWoXpNv83BAHJdc", "f4txACqDWithRi7hs", "rrW7yf42vQYDf8AcH", "dhGGnB2oxBP3m5cBc", "k6EPphHiBH4WWYFCj", "LubwxZHKKvCivYGzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T13:07:39.155Z", "modifiedAt": null, "url": null, "title": "Knowledge doesn't just happen ", "slug": "knowledge-doesn-t-just-happen", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:25.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZszXoB56yC5hBzPPL/knowledge-doesn-t-just-happen", "pageUrlRelative": "/posts/ZszXoB56yC5hBzPPL/knowledge-doesn-t-just-happen", "linkUrl": "https://www.lesswrong.com/posts/ZszXoB56yC5hBzPPL/knowledge-doesn-t-just-happen", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Knowledge%20doesn't%20just%20happen%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKnowledge%20doesn't%20just%20happen%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZszXoB56yC5hBzPPL%2Fknowledge-doesn-t-just-happen%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Knowledge%20doesn't%20just%20happen%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZszXoB56yC5hBzPPL%2Fknowledge-doesn-t-just-happen", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZszXoB56yC5hBzPPL%2Fknowledge-doesn-t-just-happen", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p>It should be obvious that people need to learn what they know, but a tremendous amount of effort goes into blaming them for not knowing whatever would have been useful if only they'd known it.</p>\n<p>A lot of this appears in the form of \"should have known better\". Sometimes (especially after a low-probability disaster), it will be people tormenting themselves about ordinary behavior. If only they had left a little earlier or later, they wouldn't have been in the car crash. As far as I can tell, people are apt to expect themselves to be clairvoyant, and they expect other people to be telepathic.</p>\n<p>The latter comes into play when they're shocked and infuriated that other people don't have the same views they do, or don't understand what is vividly clear to the telepathy expecter. This plays out both politically and personally.</p>\n<p>One thing I've learned from arguing about torture is that moral intuitions don't transfer very well. To some people, it's completely obvious that torture is a bad thing and doesn't work, and to others, it's completely obvious that getting important information for one's own side is an emergency, people can't lie if they're suffering enough, and people on the other side don't deserve careful treatment. (The overlap between what people believe is moral and what they believe is effective is probably worth another essay.) When you're in the grasp of a moral intuition, it can be very hard to believe that people who don't share it aren't trying to hide bad motivations.</p>\n<p>On the personal level, it's probably more complex. You do want to be around people who are reasonably clueful about how you want to be treated, but on the other hand, I've seen and done a certain amount of ranting about how people should just know how to behave. How are they to know it? They just should.</p>\n<p>I suggest that as a matter of epistemic and emotional hygiene, downplaying \"should have known better\". As far as I can tell, if taken literally, it invokes an unavailable counterfactual and just leads to fury. I think I have better sense when I don't deploy it, but it's hard work to inhibit the reaction, and it's tempting to think that other people should know better than to invoke \"should have known better\".</p>\n<p>It's possible that \"should have known better\" is one of those normal people things that geeks need to figure out. I can interpret it as \"I am lowering your status because I want you to never make that mistake again\". I think it can be effective for specific cases, but at a cost of cranking up anxiety and despair because it isn't actually possible to know what one could be blamed for in the future.</p>\n<p>This post is a rough draft. It could probably use more examples, and I'm not sure whether the material has already been covered. I think the idea of knowledge needing a source has been, but the emotional and cultural effects of not having a gut level understanding of the fact haven't been.</p>\n<p>A philosophy professor recently told me that one of the few things philosophers agree on is that there can't be a moral obligation to do the impossible-- ought implies can. On the other hand, there hasn't been significant work done on figuring out what actually is possible.</p>\n<p>On the epistemic side, I've been distracted by whether there's explanation required for how people recognize sound arguments, or if it's enough to say that they just do it some of the time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZszXoB56yC5hBzPPL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 6.70751591961194e-07, "legacy": true, "legacyId": "5088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T19:14:18.465Z", "modifiedAt": null, "url": null, "title": "Put all your eggs in one basket?", "slug": "put-all-your-eggs-in-one-basket", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.142Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/off2RTNFrmvrRyzmq/put-all-your-eggs-in-one-basket", "pageUrlRelative": "/posts/off2RTNFrmvrRyzmq/put-all-your-eggs-in-one-basket", "linkUrl": "https://www.lesswrong.com/posts/off2RTNFrmvrRyzmq/put-all-your-eggs-in-one-basket", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Put%20all%20your%20eggs%20in%20one%20basket%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APut%20all%20your%20eggs%20in%20one%20basket%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foff2RTNFrmvrRyzmq%2Fput-all-your-eggs-in-one-basket%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Put%20all%20your%20eggs%20in%20one%20basket%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foff2RTNFrmvrRyzmq%2Fput-all-your-eggs-in-one-basket", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foff2RTNFrmvrRyzmq%2Fput-all-your-eggs-in-one-basket", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>Having all known life on Earth concentrated on a single planet is an existential risk.&nbsp; So we should try to spread out, right?&nbsp; As soon as possible?</p>\n<p>Yet, if we had advanced civilizations on two planets, that would be two places for unfriendly AI to originate.&nbsp; If, as many people here believe, a single failed trial ruins the universe, you want to have as few places trying it as possible.&nbsp; So you don't want any space colonization until after AI is developed.</p>\n<p>If we apply that logic to countries, you would want as few industrialized nations as possible until AAI (After AI).&nbsp; So instead of trying to help Africa, India, China, and the Middle East develop, you should be trying to suppress them.&nbsp; In fact, if you really believed the calculations I commonly see used in these circles about the probability of unfriendly AI and its consequences, you should be trying to exterminate human life outside of your developed country of choice.&nbsp; Failing to would be immoral.</p>\n<p>And if you apply it within the USA, you need to pick one of MIT and Stanford and Carnegie Mellon, and burn the other two to the ground.</p>\n<p>Of course, doing this will slow the development of AI.&nbsp; But that's a good thing, if UFAI is most likely and has zero utility.</p>\n<p>In fact, if slowing development is good, probably the best thing of all is just to destroy civilization and stop development completely.</p>\n<p>Do you agree with any of this?&nbsp; Is there a point where you think it goes too far?&nbsp; If so, say where it goes too far and explain why.</p>\n<p>I see two main flaws in the reasoning.</p>\n<ul>\n<li>Categorization of outcomes as \"FAI vs UFAI\", with no other possible outcomes recognized, and no gradations within the category of either, and zero utility assigned to UFAI.</li>\n<li>Failing to consider scenarios in which multiple AIs can provide a balance of power.&nbsp; The purpose of this balance of power may not be to keep humans in charge; it may be to put the AIs in an AI society in which human values will be worthwhile.</li>\n<li>ADDED, after being reminded of this by Vladimir Nesov:&nbsp; Re. the final point, stopping completely guarantees Earth life will eventually be eliminated; see his comment below for elaboration.</li>\n</ul>\n<p>ADDED:&nbsp; A number of the comments so far imply that the first AI built will necessarily FOOM immediately.&nbsp; FOOM is an appealing argument.&nbsp; I've <a href=\"/lw/1bm/the_efficaciousness_of_information\">argued in favor of it myself</a>.&nbsp;&nbsp; But it is not a theorem.&nbsp; I don't care who you are; you do not know enough about AI and its future development to bet the future of the universe on your intuition that non-FOOMing AI is impossible.&nbsp; You may even think FOOM is the default case; that does not make it the only case to consider.&nbsp; In this case, even a 1% chance of a non-foom AI, multiplied by astronomical differences in utility, could justify terrible present disutility.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "off2RTNFrmvrRyzmq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 10, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "5092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KqhHhsBRzbf7eckTS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T20:02:17.249Z", "modifiedAt": null, "url": null, "title": "How to Grow a Mind (video)", "slug": "how-to-grow-a-mind-video", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FJzeNz5z9DgKJ2RL3/how-to-grow-a-mind-video", "pageUrlRelative": "/posts/FJzeNz5z9DgKJ2RL3/how-to-grow-a-mind-video", "linkUrl": "https://www.lesswrong.com/posts/FJzeNz5z9DgKJ2RL3/how-to-grow-a-mind-video", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Grow%20a%20Mind%20(video)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Grow%20a%20Mind%20(video)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJzeNz5z9DgKJ2RL3%2Fhow-to-grow-a-mind-video%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Grow%20a%20Mind%20(video)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJzeNz5z9DgKJ2RL3%2Fhow-to-grow-a-mind-video", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJzeNz5z9DgKJ2RL3%2Fhow-to-grow-a-mind-video", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 557, "htmlBody": "<p>From the recent NIPS conference, here's a talk by MIT cognitive scientist Josh Tenenbaum on what he calls \"rich\" machine learning. &nbsp;Should be of some relevance for people who are interested in AI or human cognitive development. &nbsp;I found it really interesting.</p>\n<p><a href=\"http://videolectures.net/nips2010_tenenbaum_hgm/\" target=\"_blank\">http://videolectures.net/nips2010_tenenbaum_hgm/</a></p>\n<p>The gist is: children are able, from a young age, to learn the meanings of words from just a few examples. &nbsp;Adults, given pictures of abstract, made-up objects, and given that some of them are called \"tufas,\" can pick out which other pictures are tufas and which aren't. &nbsp;We can do this much faster than a typical Bayesian estimator can, with less training data. &nbsp;This is partly because we have background knowledge about the world and what sort of categories and structures it forms. &nbsp;</p>\n<p>For instance, we learn fairly young (~2 years) that non-solid objects are defined by substance rather than shape: that \"toothpaste\" is that sticky substance we brush our teeth with, whether it's in the toothpaste tube or on the toothbrush or smeared on the sink, all very different shapes in terms of the raw images hitting our visual cortex. &nbsp;Pour a new liquid called \"floo\" in a glass, and we'll predict that it's still called \"floo\" when you spill it or while it's splashing through the air. On the other hand, some objects are about shape more than color or texture: a chair is a chair no matter what it's made of. &nbsp;Some sets of objects fall into tree-like organization (taxonomy of living things) &nbsp;and some fall into \"flat\" clusters without hierarchy (the ROYGBIV colors). &nbsp;It takes children three years or so to understand that the same object can be both a dog and a mammal. &nbsp;We learn over time which structural organization is best for which sorts of concepts in the world.</p>\n<p>Research in machine learning/computer vision/statistics/related fields often focuses on optimizing the description of data given a certain form, and not assuming that the machine has any \"life experience\" or \"judgment\" about what format is best. &nbsp;Clustering algorithms give the best way to sort data into clusters, if we think it falls into clusters; dimensionality reduction techniques give the best way to sort data onto low-dimensional subspaces, if we think it lies in a subspace; manifold learning techniques give the best way to sort data onto low-dimensional manifolds, if we think it lies on a manifold. &nbsp;There's less attention paid to how we identify the best structural model (and whether the process of identifying the best model can somehow be automated, moved from human judgment to machine judgment.) &nbsp;See <a href=\"http://www.slate.com/id/2279895/\" target=\"_blank\">Jordan Ellenberg's piece in Slate</a>&nbsp;for more on this.</p>\n<p>We usually don't use completely different techniques in computer vision for identifying pictures of cows vs. pictures of trucks. &nbsp;But there's evidence that the human brain <em>does</em>&nbsp;do exactly that -- specialized techniques based on the experiential knowledge that cows, trucks, and faces are different sorts of objects, and are identified by different sets of features.</p>\n<p>I'm sympathetic to Tenenbaum's main point: that we won't achieve computer counterparts to human learning and sensory recognition until we incorporate experiential knowledge and \"learning to learn.\" &nbsp;There is no single all-purpose statistical algorithm that Explains All Of Life -- we're going to have to teach machines to judge between algorithms based on context. &nbsp;That seems intuitively right to me, but I'd like to hear some back-and-forth on whether other folks agree.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FJzeNz5z9DgKJ2RL3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 6.708593197613285e-07, "legacy": true, "legacyId": "5093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T21:51:39.130Z", "modifiedAt": null, "url": null, "title": "Dozens of leading experts answer: \u201cWhat scientific concept would improve everybody\u2019s cognitive toolkit?\"", "slug": "dozens-of-leading-experts-answer-what-scientific-concept", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:24.644Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z28iH9R3nh8xrD96X/dozens-of-leading-experts-answer-what-scientific-concept", "pageUrlRelative": "/posts/Z28iH9R3nh8xrD96X/dozens-of-leading-experts-answer-what-scientific-concept", "linkUrl": "https://www.lesswrong.com/posts/Z28iH9R3nh8xrD96X/dozens-of-leading-experts-answer-what-scientific-concept", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dozens%20of%20leading%20experts%20answer%3A%20%E2%80%9CWhat%20scientific%20concept%20would%20improve%20everybody%E2%80%99s%20cognitive%20toolkit%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADozens%20of%20leading%20experts%20answer%3A%20%E2%80%9CWhat%20scientific%20concept%20would%20improve%20everybody%E2%80%99s%20cognitive%20toolkit%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ28iH9R3nh8xrD96X%2Fdozens-of-leading-experts-answer-what-scientific-concept%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dozens%20of%20leading%20experts%20answer%3A%20%E2%80%9CWhat%20scientific%20concept%20would%20improve%20everybody%E2%80%99s%20cognitive%20toolkit%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ28iH9R3nh8xrD96X%2Fdozens-of-leading-experts-answer-what-scientific-concept", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ28iH9R3nh8xrD96X%2Fdozens-of-leading-experts-answer-what-scientific-concept", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2, "htmlBody": "<p><span style=\"font-family: Georgia, Helvetica, Arial, sans-serif; font-size: 13px; line-height: 18px; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px;\">From Edge.</span></p>\n<p>Highlights:</p>\n<p><ol>\n<li></li>\n</ol></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z28iH9R3nh8xrD96X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "5095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T22:28:06.837Z", "modifiedAt": null, "url": null, "title": "Dozens of leading experts...", "slug": "dozens-of-leading-experts", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JYLn6gGhPN4n7xkw8/dozens-of-leading-experts", "pageUrlRelative": "/posts/JYLn6gGhPN4n7xkw8/dozens-of-leading-experts", "linkUrl": "https://www.lesswrong.com/posts/JYLn6gGhPN4n7xkw8/dozens-of-leading-experts", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dozens%20of%20leading%20experts...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADozens%20of%20leading%20experts...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJYLn6gGhPN4n7xkw8%2Fdozens-of-leading-experts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dozens%20of%20leading%20experts...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJYLn6gGhPN4n7xkw8%2Fdozens-of-leading-experts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJYLn6gGhPN4n7xkw8%2Fdozens-of-leading-experts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.edge.org/q2011/q11_17.html#kahneman\">http://www.edge.org/q2011/q11_17.html#kahneman</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JYLn6gGhPN4n7xkw8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "5096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T22:46:53.570Z", "modifiedAt": null, "url": null, "title": "Don't plan for the future", "slug": "don-t-plan-for-the-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/94AC7TDzSsnEHcFQm/don-t-plan-for-the-future", "pageUrlRelative": "/posts/94AC7TDzSsnEHcFQm/don-t-plan-for-the-future", "linkUrl": "https://www.lesswrong.com/posts/94AC7TDzSsnEHcFQm/don-t-plan-for-the-future", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20plan%20for%20the%20future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20plan%20for%20the%20future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94AC7TDzSsnEHcFQm%2Fdon-t-plan-for-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20plan%20for%20the%20future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94AC7TDzSsnEHcFQm%2Fdon-t-plan-for-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94AC7TDzSsnEHcFQm%2Fdon-t-plan-for-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 281, "htmlBody": "<p>Why do we imagine our actions could have consequences for more than a few million years into the future?</p>\n<p>Unless what we believe about evolution is wrong, or UFAI is unlikely, or we are very very lucky, we should assume there are already a large number of unfriendly AIs in the universe, and probably in our galaxy; and that they will assimilate us within a few million years.</p>\n<p>Therefore, justifications for harming people on Earth today in the name of protecting the entire universe over all time from UFAI in the future, like <a href=\"/lw/3xg/put_all_your_eggs_in_one_basket/3e1x\">this one</a>, should not be done.&nbsp; Our default assumption should be that the offspring of Earth will at best have a short happy life.</p>\n<p>ADDED:&nbsp; If you observe, as many have, that Earth has not yet been assimilated, you can draw one of these conclusions:</p>\n<ol>\n<li>The odds of intelligent life developing on a planet are precisely balanced with the number of suitable planets in our galaxy, such that after billions of years, there is exactly one such instance.&nbsp; This is an extremely low-probability argument.&nbsp; The anthropic argument does not justify this as easily as it justifies observing one low-probability creation of intelligent life.</li>\n<li>The progression (intelligent life &rarr;AI&rarr;expansion and assimilation) is unlikely.</li>\n</ol>\n<p>Surely, for a Bayesian, the more reasonable conclusion is number 2!&nbsp; Conclusion 1 has priors we can estimate numerically.&nbsp; Conclusion 2 has priors we know very little about.</p>\n<p>To say, \"I am so confident in my beliefs about what a superintelligent AI will do, that I consider it more likely that I live on an astronomically lucky planet, than that those beliefs are wrong\", is something I might come up with if asked to draw a <em>caricature</em> of irrationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "94AC7TDzSsnEHcFQm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 2, "extendedScore": null, "score": 6.709018624662538e-07, "legacy": true, "legacyId": "5097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-23T23:02:38.972Z", "modifiedAt": null, "url": null, "title": "Intrapersonal negotiation", "slug": "intrapersonal-negotiation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:10.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "datadataeverywhere", "createdAt": "2010-08-06T03:09:35.035Z", "isAdmin": false, "displayName": "datadataeverywhere"}, "userId": "wKbyzrGvXH77dFfZX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fkYdYJdWY7tEgGXQM/intrapersonal-negotiation", "pageUrlRelative": "/posts/fkYdYJdWY7tEgGXQM/intrapersonal-negotiation", "linkUrl": "https://www.lesswrong.com/posts/fkYdYJdWY7tEgGXQM/intrapersonal-negotiation", "postedAtFormatted": "Sunday, January 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intrapersonal%20negotiation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntrapersonal%20negotiation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkYdYJdWY7tEgGXQM%2Fintrapersonal-negotiation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intrapersonal%20negotiation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkYdYJdWY7tEgGXQM%2Fintrapersonal-negotiation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkYdYJdWY7tEgGXQM%2Fintrapersonal-negotiation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1224, "htmlBody": "<p><em>Related to:&nbsp;</em><span style=\"font-size: 12px; line-height: 18px; font-family: Arial, Helvetica, sans-serif; color: #000000;\"><a style=\"font-size: 12px; line-height: 18px;\" title=\"Akrasia as a collective action problem\" href=\"/lw/38s/akrasia_as_a_collective_action_problem/\">Akrasia as a collective action problem</a></span><em>&nbsp;and&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; color: #000000;\"><a style=\"font-size: 12px; line-height: 18px;\" title=\"Self-empathy as a source of &quot;willpower&quot;\" href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">Self-empathy as a source of \"willpower\"</a>.</span></p>\n<p>The Less Wrong community has discussed negotiating with one's conflicting sub-agents as a method to defeat akrasia and other forms of dynamic inconsistency, with some mix of reactions about how possible or effective that strategy can be. This article presents a successful example in my life, though it is probably an extreme&nbsp;outlier&nbsp;for a number of&nbsp;reasons.</p>\n<p>I have been diagnosed with <a href=\"http://en.wikipedia.org/wiki/Bipolar_II_disorder\">bipolar II disorder</a>. It is one of the most significant challenges in my life, and certainly the one with the most dire implications. I can be fairly well modeled as three major sub-agents<sup>1</sup>:</p>\n<ul>\n<li><strong>Neutral</strong>: asymptomatic. Attempting to control mood swings has resulted in me spending the vast majority of my time as \"neutral\".</li>\n<li><strong>Hypomanic</strong>: anxious and energetic. I have some trouble focusing, but can be extremely productive. I often exercise several hours a day and sleep an average of two hours a night (often six hours every three days). I've written a 70 page paper in three days, and a 12 <a href=\"http://en.wikipedia.org/wiki/Source_lines_of_code\">KLOC</a> compiler in five days.</li>\n<li><strong>Depressed</strong>: listless, and suicidal. I have serious insomnia, but stay in bed all day. I can't stand other people and would really sincerely prefer not to exist. Ironically, I can credit my inability to motivate myself to do anything with not having killed myself in this state. Sometimes I'm very emotional (crying, etc.), sometimes I'm emotionally dead.</li>\n</ul>\n<div>Of course, everything I know is&nbsp;perceived&nbsp;by all three agents, but it is interpreted in vastly different ways. None is rational, but all are biased in different ways. Each agent feels that it is less biased than the others, which is key to successful negotiation, since each thinks that it's getting the better end of the bargain.</div>\n<div>Here are desires, both conflicting and otherwise:</div>\n<div><ol>\n<li>Hypomanic loves being who he is and doing what he wants. The main difficulty of transitioning back to Neutral is unwillingness, rather than inability.</li>\n<li>Hypomanic despises Depressed, and thinks that Neutral is boring. He recognizes that staying Hypomanic for too long will likely result in a crash followed by Depressed taking over, but still wants \"one more day\".</li>\n<li>Depressed loathes all of us. He doesn't like the others more than himself, but is usually willing to trade knowledge and pain for ignorance and happiness. However, he's unmotivated to bother taking the steps to make that happen. Again, the difficulty in forcing a transition is not really about inability, although difficulty and self-discipline are an issue.</li>\n<li>Depressed doesn't really want to kill himself, but he's very&nbsp;desirous&nbsp;of&nbsp;nonexistence.&nbsp;</li>\n<li>Neutral is scared of and pained by Depressed. He is wary of Hypomanic but recognizes his usefulness. He spends much more time thinking of the other two than they think of him, and actively prevents transitions to the other two, which are fairly easy to&nbsp;accidentally&nbsp;trigger otherwise.</li>\n<li>Neutral is somewhat careful for his own sake, but all are motivated by other's concern for him. The latter is the only other significant reason Depressed hasn't gone through with a suicide attempt.</li>\n</ol></div>\n<div>Without further ado, a compromise brokered by Neutral:&nbsp;<strong>Depressed will not take his life, on the condition that Neutral will free Hypomanic to do more things that might kill him.</strong></div>\n<div>This probably sounds like it's a terrible compromise, but it's the most we could agree to. Depressed would really like us to die, but would prefer it look like an accident for the sake of our family and friends. This is actually very unlikely, but becomes most likely when Hypomanic is doing something stupid and dangerous. Neutral thinks that Depressed is overly pessimistic, and is greatly exaggerating how dangerous Hypomanic's activities are<sup>2</sup>. Hypomanic is rather fearless, and dismisses the possibility of dying on accident. Also, although this is something I'm not supposed to say, my own life isn't that important to any of my agents. An accidental death in the near future is not that bothersome to any of us, even if it's not&nbsp;particularly&nbsp;desirable to Neutral or Hypomanic<sup>3</sup>. The prospect of causing grief for those around me weighs much more heavily in my mind, and while any death will cause this, I think a suicide will have a much longer lasting impact and make a lot of people feel guilt for something that isn't their fault.</div>\n<div>I *don't* have faith that Depressed is sincere in his agreement (he's really not), but I *do* have faith and weak evidence that the existence of this compromise is one more excuse that reduces his motivation, and will still reduce the net&nbsp;likelihood that I will die in the near future.&nbsp;Likewise, Neutral doesn't constrain Hypomanic's activities very well to begin with, but the existence of this compromise ironically makes Hypomanic more likely to have second thoughts about the dangerous things he's about to do.</div>\n<p>Neutral feels it necessary to let Hypomanic take control more often to ensure that the compromise has weight to Depressed, but has started using Hypomanic to accomplish goals that are otherwise too exhausting to attain (a several-day code crunch or a need to meet and make a good impression on dozens of people). Meanwhile, Hypomanic has been more responsible lately in&nbsp;relinquishing control within days rather than weeks, partially because of these negotiations, but mostly because of other people in my life who have been conscripted to help monitor and rein me in.</p>\n<p>I do not have a great deal of proven success with this strategy. I started doing this less than a year ago, and have not dealt with a full-blown major depressive episode since then. During that time I have also been more successful than ever at preventing myself from slipping into depression in the first place and treating early depression&nbsp;aggressively. In the end, that makes a much more significant difference, but on the two occasions when I became depressed enough to start feeling suicidal I was positively influenced by this agreement.</p>\n<p>It seems unlikely that this approach will help many people with anything, but I feel like it is interesting in the debate about dynamic inconsistency, and I encourage others to find mutually-beneficial agreements they can make with themselves if they also feel like they deal with mutually incompatible agents from time to time. Also, this is my first post that is more than a link, so please be constructive.</p>\n<p>&nbsp;</p>\n<p><em>Notes</em></p>\n<p><sup>1&nbsp;</sup>I've never used names to refer to myself in different states, and don't think of my major sub-agents as individuals, but I felt that it was useful for didactic purposes to refer to myself in different states as different proper nouns.</p>\n<p><sup>2</sup>&nbsp;I don't race cars, do drugs, or get in fights (except at the dojo). I do push my physical limits farther than I should (do&nbsp;<a href=\"http://en.wikipedia.org/wiki/Parkour\">parkour</a>&nbsp;that I'm not be ready for, run 20km when I usually run 5, etc.), and I have injured myself this way, but just pulled muscles, sprains and once a broken finger.</p>\n<p><sup>3</sup>&nbsp;I haven't heard this argument before, but this is the reason I haven't signed up for cryonics.</p>\n<p>&nbsp;</p>\n<p>If it's not obvious, I was in a neutral state when I wrote this. It would have been impossible for me to do while depressed, and unlikely for me to try while hypomanic. I tried to de-bias myself, but no matter what state I'm in, I prefer my own viewpoint, and speak less highly of the others that diverge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5uHdFgR938LGGxMKQ": 2, "r7qAjcbfhj2256EHH": 2, "jxMoPnsWXBnDzXBwE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fkYdYJdWY7tEgGXQM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 34, "extendedScore": null, "score": 6.709060771690359e-07, "legacy": true, "legacyId": "5094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EYiAoxvnKjgJe8GbN", "22HfpjsydDS2A6JhH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-24T02:24:58.051Z", "modifiedAt": null, "url": null, "title": "Is Less Wrong discouraging less nerdy people from participating? ", "slug": "is-less-wrong-discouraging-less-nerdy-people-from", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GN9KWYYHW54FNtcq7/is-less-wrong-discouraging-less-nerdy-people-from", "pageUrlRelative": "/posts/GN9KWYYHW54FNtcq7/is-less-wrong-discouraging-less-nerdy-people-from", "linkUrl": "https://www.lesswrong.com/posts/GN9KWYYHW54FNtcq7/is-less-wrong-discouraging-less-nerdy-people-from", "postedAtFormatted": "Monday, January 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Less%20Wrong%20discouraging%20less%20nerdy%20people%20from%20participating%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Less%20Wrong%20discouraging%20less%20nerdy%20people%20from%20participating%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGN9KWYYHW54FNtcq7%2Fis-less-wrong-discouraging-less-nerdy-people-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Less%20Wrong%20discouraging%20less%20nerdy%20people%20from%20participating%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGN9KWYYHW54FNtcq7%2Fis-less-wrong-discouraging-less-nerdy-people-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGN9KWYYHW54FNtcq7%2Fis-less-wrong-discouraging-less-nerdy-people-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p>Less Wrong is as a community extremely nerdy. That's true for almost any definition of \"nerd\" that captures anyone's intuition for the word. However, to a&nbsp; large extent, many aspects of nerdiness are not connected to rationality at all, even though nerdiness may be associated with more rationality in some limited aspects. For example, fantasy literature is probably not in any deep way connected to either intelligent or rational thinking except for historical reasons.</p>\n<p>Yet LW is full of references to science fiction, fantasy literature, anime and D&amp;D. In one recent example, <a href=\"/r/discussion/lw/3um/steve_jobs_medical_leave_riches_and_longevity/\"> a post started with an only marginally connected tidbit from Heinlein.</a> Moreover, substantial subthreads have arisen bashing aspects of other subcultures. For example, see <a href=\"/lw/2co/how_to_always_have_interesting_conversations/25ck\"> this subthread where multiple users discuss how spectator sports are \"banal\" and \"pointless\".</a> I suspect that this attitude may be turning away not only non-nerds but even the somewhat nerdy who enjoy watching sports, and see it has harmless tribalist fun,&nbsp; not very different than friends arguing over whether Star Wars or Star Trek is superior which has about the same degree of actual value here.</p>\n<p>There's a related issue which is a serious point about rationality and human cognition: Our hobbies are to a large extent functions of our specific upbringings and surrounding culture. That some people prefer one form of fantastic escapism involving imaginary spaceships isn't at some level very different than the escapism of watching some people throw and catch objects. Looking down on other people because of these sorts of preferences is unhelpful tribalism. It might feel good, and it might be fun, but it isn't helpful.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GN9KWYYHW54FNtcq7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 31, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "5098", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["btoJ8qH4jGssigHtd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-24T11:45:53.461Z", "modifiedAt": null, "url": null, "title": "Some rationalistic aphorisms", "slug": "some-rationalistic-aphorisms", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n4ctAtDBNKnv78kxS/some-rationalistic-aphorisms", "pageUrlRelative": "/posts/n4ctAtDBNKnv78kxS/some-rationalistic-aphorisms", "linkUrl": "https://www.lesswrong.com/posts/n4ctAtDBNKnv78kxS/some-rationalistic-aphorisms", "postedAtFormatted": "Monday, January 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20rationalistic%20aphorisms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20rationalistic%20aphorisms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn4ctAtDBNKnv78kxS%2Fsome-rationalistic-aphorisms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20rationalistic%20aphorisms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn4ctAtDBNKnv78kxS%2Fsome-rationalistic-aphorisms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn4ctAtDBNKnv78kxS%2Fsome-rationalistic-aphorisms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 576, "htmlBody": "<p>I'm no steven0461/stevenkaas, but perhaps some of these are approaching something like insight. Truthiness, not truth.</p>\n<div>(More context: I use Facebook to share short insights with people who have relatively high estimates of my general sanity. The following are things I posted on Facebook. The stevenkaas reference is referencing the awesomeness of stevenkaas's twitter feed, to which my blurbs just cannot compare.)</div>\n<ul>\n<li>Two good words: polymathic and protagonistic.</li>\n<li>Creepy is low status abnormality, weird is normal status abnormality, eccentric is high status abnormality.</li>\n<li>It's unfortunate that scientific studies often treat data averaged from entire nations as the atoms of statistical analysis.</li>\n<li>If you never have akrasia, you're spending too much time on trivial challenges.</li>\n<li>The universe is a big deal. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (hint acausal economy hint)</li>\n<li>Animals look kinda like agents. Memes look kinda like agents. Humans don't look like much like agents because they don't fulfill the preferences of either their genes or their memes nearly as well as they could if they could just choose one.</li>\n<li>People who always go meta will never become complacent in their epistemology, because beyond a certain level they'll always go, \"Well, shit.\"</li>\n<li>One of my goals lately has been to write a thousand words a day. I've noticed that the first three hundred are difficult but the last three thousand come naturally.</li>\n<li>There seems to be a correlation between a person saying that correlation does not imply causation and that person not understanding much about either correlation or causation. Now, I don't mean to imply anything, but...</li>\n<li>I wish people would just update on the expectation of being Dutch booked due to inconsistent preferences and give me money.</li>\n<li>Some people have comparative advantage in having comparative advantage. This applies to cognitive styles as much as professions. Human mindspace has many niches.</li>\n<li>The feeling of knowing that one knows more than others is addicting and almost always wrong.</li>\n<li>On Friendliness:&nbsp;All things lie on the axis of provincialism and universality. Positive affect around any one level of universality is provincial thinking. Societal values have become more universal over time. We may thus intuit that reflective consistency suggests planning for the case where we should have been thinking universally all along, including in the sense that full provincialism and full universality seem arbitrary with respect to each other.</li>\n<li>Debates about ontology are important because choice of ontology does in fact change your expected anticipations because having a certain ontology will lengthen and shorten certain inferential distances. Humans rely much on cached insights and single step inferences.</li>\n<li>Intelligence is seeing implications.</li>\n<li>\"The world is perfect, including your desire to change it.\" (Found in a fortune cookie. My fortune cookies tend to be really meta.)</li>\n<li>The balance of Yin and Yang is timelessly fulfilled by timeful striving.</li>\n<li>Analyze verbs timelessly and nouns timefully. That way you look at the part of the conceptualization that isn't already explicit in its construction.</li>\n<li>Moore's paradox is only a paradox if you assume that people are unified and coherent agents. They aren't.</li>\n<li>Decompartmentalize your knowledge of Turing equivalence, take the principle of charity seriously, and lots more things will start making lots more sense. Added bonus: you can learn smart things from stupid people.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n4ctAtDBNKnv78kxS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 14, "extendedScore": null, "score": 6.711040051346367e-07, "legacy": true, "legacyId": "5117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-24T19:03:16.954Z", "modifiedAt": null, "url": null, "title": "Perfectly Friendly AI", "slug": "perfectly-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rjEYsH9Em58Eei4Qx/perfectly-friendly-ai", "pageUrlRelative": "/posts/rjEYsH9Em58Eei4Qx/perfectly-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/rjEYsH9Em58Eei4Qx/perfectly-friendly-ai", "postedAtFormatted": "Monday, January 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Perfectly%20Friendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APerfectly%20Friendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrjEYsH9Em58Eei4Qx%2Fperfectly-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Perfectly%20Friendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrjEYsH9Em58Eei4Qx%2Fperfectly-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrjEYsH9Em58Eei4Qx%2Fperfectly-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 385, "htmlBody": "<p>Inspired by <a href=\"/r/discussion/lw/3xl/dont_plan_for_the_future/\" target=\"_self\">Don't Plan For the Future</a>.</p>\n<p>For the purposes of discussion on this site, a Friendly AI is assumed to be one that shares our terminal values. It's a <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\" target=\"_self\">safe genie</a> that doesn't need to be told what to do, but anticipates how to best serve the interests of its creators. Since our terminal values are a function of our evolutionary history, it seems reasonable to assume that an FAI created by one intelligent species would not necessarily be friendly to other intelligent species, and that being subsumed by another species' FAI would be fairly catastrophic.</p>\n<p>Except.... doesn't that seem kind of <em>bad?</em> Supposing I were able to create a strong AI, and it created a sound fun-theoretic utopia for human beings, but then proceeded to expand and subsume extraterrestrial intelligences, and subject them to something they considered a fate worse than death, I would have to regard that as a major failing of my design. My utility function assigns value to the desires of beings whose values conflict with my own. I can't allow other values to <em>supersede</em> mine, but absent other considerations, I have to assign negative utility in my own function for creating negative utility in the functions of other existing beings. I'm skeptical that an AI that would impose catastrophe on other thinking beings is really <em>maximizing</em> my utility.</p>\n<p>It seems to me that to truly maximize my utility, an AI would need to have consideration for the utility of other beings. Secondary consideration, perhaps, but it could not maximize my utility simply by treating them as raw material with which to tile the universe with my utopian civilization.</p>\n<p>Perhaps my utility function gives more value than most to beings that don't share my values (full disclosure, I prefer the \"false\" ending of <a href=\"/lw/y4/three_worlds_collide_08/\" target=\"_self\">Three Worlds Collide, </a>although I don't consider it ideal.) However, if an AI imposes truly catastrophic fates on other intelligent beings, my own utility function takes such a hit that I cannot consider it friendly. A true Friendly AI would <em>need</em> to be at least passably friendly to other intelligences to satisfy me.</p>\n<p>I don't know if I've finally come to terms with&nbsp; Eliezer's understanding of how hard Friendly AI is, or made it much, much harder, but it gives me a somewhat humbling perspective of the true scope of the problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rjEYsH9Em58Eei4Qx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 6.712174785174749e-07, "legacy": true, "legacyId": "5122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["94AC7TDzSsnEHcFQm", "4ARaTpNX62uaL86j6", "HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-24T19:29:30.327Z", "modifiedAt": null, "url": null, "title": "What do superintelligences really want? [Link]", "slug": "what-do-superintelligences-really-want-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:57.178Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ssc8ik3ghBrq7bqtA/what-do-superintelligences-really-want-link", "pageUrlRelative": "/posts/Ssc8ik3ghBrq7bqtA/what-do-superintelligences-really-want-link", "linkUrl": "https://www.lesswrong.com/posts/Ssc8ik3ghBrq7bqtA/what-do-superintelligences-really-want-link", "postedAtFormatted": "Monday, January 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20do%20superintelligences%20really%20want%3F%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20do%20superintelligences%20really%20want%3F%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsc8ik3ghBrq7bqtA%2Fwhat-do-superintelligences-really-want-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20do%20superintelligences%20really%20want%3F%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsc8ik3ghBrq7bqtA%2Fwhat-do-superintelligences-really-want-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsc8ik3ghBrq7bqtA%2Fwhat-do-superintelligences-really-want-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 644, "htmlBody": "<blockquote>\n<p><strong><span style=\"color: red;\">In Conclusion:</span></strong></p>\n<p>In the case of humans, everything that we do that <em>seems</em> intelligent is part of a large, complex mechanism in which we are engaged to ensure our survival. This is so hardwired into us that we do not see it easily, and we certainly cannot change it very much. However, superintelligent computer programs are not limited in this way. They understand the way that they work, can change their own code, and are not limited by any particular reward mechanism. I argue that because of this fact, such entities are not self-consistent. In fact, if our superintelligent program has no hard-coded survival mechanism, it is more likely to switch itself off than to destroy the human race willfully.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://physicsandcake.wordpress.com/2011/01/22/pavlovs-ai-what-did-it-mean/\">physicsandcake.wordpress.com/2011/01/22/pavlovs-ai-what-did-it-mean/</a></p>\n<p>Suzanne Gildert basically argues that any AGI that can considerably self-improve would simply alter its reward function directly. I'm not sure how she arrives at the conclusion that such an AGI would likely switch itself off. Even if an abstract general intelligence would tend to alter its reward function, wouldn't it do so indefinitely rather than switching itself off?</p>\n<blockquote>\n<p>So imagine a simple example &ndash; our case from earlier &ndash; where a computer gets an additional &rsquo;1&prime; added to a numerical value for each good thing it does, and it tries to maximize the total by doing more good things. But if the computer program is clever enough, why can&rsquo;t it just rewrite it&rsquo;s own code and replace that piece of code that says &lsquo;add 1&prime; with an &lsquo;add 2&prime;? Now the program gets twice the reward for every good thing that it does! And why stop at 2? Why not 3, or 4? Soon, the program will spend so much time thinking about adjusting its reward number that it will ignore the good task it was doing in the first place!<br /> It seems that being intelligent enough to start modifying your own reward mechanisms is not necessarily a good thing!</p>\n</blockquote>\n<p>If it wants to maximize its reward by increasing a numerical value, why wouldn't it consume the universe doing so? Maybe she had something in mind along the lines of an argument by Katja Grace:</p>\n<blockquote>\n<p>In trying to get to most goals, people don&rsquo;t invest and invest until they explode with investment. Why is this? Because it quickly becomes cheaper to actually fulfil a goal at than it is to invest more and then fulfil it. [...] A creature should only invest in many levels of intelligence improvement when it is pursuing goals significantly more resource intensive than creating many levels of intelligence improvement.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://meteuphoric.wordpress.com/2010/02/06/cheap-goals-not-explosive/\">meteuphoric.wordpress.com/2010/02/06/cheap-goals-not-explosive/</a></p>\n<p>I am not sure if that argument would apply here. I suppose the AI might hit diminishing returns but could again alter its reward function to prevent that, though what would be the incentive for doing so?</p>\n<p><strong>ETA:</strong></p>\n<p>I left a <a href=\"http://physicsandcake.wordpress.com/2011/01/22/pavlovs-ai-what-did-it-mean/#comment-1111\">comment</a> over there:</p>\n<blockquote>\n<p>Because it would consume the whole universe in an effort to encode an even larger reward number? In the case that an AI decides to alter its reward function directly, maximizing its reward by means of improving its reward function becomes its new goal. Why wouldn&rsquo;t it do everything to maximize its payoff, after all it has no incentive to switch itself off? And why would it account for humans in doing so?</p>\n</blockquote>\n<p><strong>ETA #2:</strong></p>\n<p>What else I wrote:</p>\n<blockquote>\n<p>There is absolutely no reason (incentive) for it to do anything except  increasing its reward number. This includes the modification of its  reward function in any way that would not increase the numerical value  that is the reward number.</p>\n<p>We are talking about a general intelligence with the ability to  self-improve towards superhuman intelligence. Of course it would do a  long-term risks-benefits analysis and calculate its payoff and do  everything to increase its reward number maximally. Human values are  complex but superhuman intelligence does not imply complex values. It  has no incentive to alter its goal.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ssc8ik3ghBrq7bqtA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 6.712242826402681e-07, "legacy": true, "legacyId": "5123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p><strong id=\"In_Conclusion_\"><span style=\"color: red;\">In Conclusion:</span></strong></p>\n<p>In the case of humans, everything that we do that <em>seems</em> intelligent is part of a large, complex mechanism in which we are engaged to ensure our survival. This is so hardwired into us that we do not see it easily, and we certainly cannot change it very much. However, superintelligent computer programs are not limited in this way. They understand the way that they work, can change their own code, and are not limited by any particular reward mechanism. I argue that because of this fact, such entities are not self-consistent. In fact, if our superintelligent program has no hard-coded survival mechanism, it is more likely to switch itself off than to destroy the human race willfully.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://physicsandcake.wordpress.com/2011/01/22/pavlovs-ai-what-did-it-mean/\">physicsandcake.wordpress.com/2011/01/22/pavlovs-ai-what-did-it-mean/</a></p>\n<p>Suzanne Gildert basically argues that any AGI that can considerably self-improve would simply alter its reward function directly. I'm not sure how she arrives at the conclusion that such an AGI would likely switch itself off. Even if an abstract general intelligence would tend to alter its reward function, wouldn't it do so indefinitely rather than switching itself off?</p>\n<blockquote>\n<p>So imagine a simple example \u2013 our case from earlier \u2013 where a computer gets an additional \u20191\u2032 added to a numerical value for each good thing it does, and it tries to maximize the total by doing more good things. But if the computer program is clever enough, why can\u2019t it just rewrite it\u2019s own code and replace that piece of code that says \u2018add 1\u2032 with an \u2018add 2\u2032? Now the program gets twice the reward for every good thing that it does! And why stop at 2? Why not 3, or 4? Soon, the program will spend so much time thinking about adjusting its reward number that it will ignore the good task it was doing in the first place!<br> It seems that being intelligent enough to start modifying your own reward mechanisms is not necessarily a good thing!</p>\n</blockquote>\n<p>If it wants to maximize its reward by increasing a numerical value, why wouldn't it consume the universe doing so? Maybe she had something in mind along the lines of an argument by Katja Grace:</p>\n<blockquote>\n<p>In trying to get to most goals, people don\u2019t invest and invest until they explode with investment. Why is this? Because it quickly becomes cheaper to actually fulfil a goal at than it is to invest more and then fulfil it. [...] A creature should only invest in many levels of intelligence improvement when it is pursuing goals significantly more resource intensive than creating many levels of intelligence improvement.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://meteuphoric.wordpress.com/2010/02/06/cheap-goals-not-explosive/\">meteuphoric.wordpress.com/2010/02/06/cheap-goals-not-explosive/</a></p>\n<p>I am not sure if that argument would apply here. I suppose the AI might hit diminishing returns but could again alter its reward function to prevent that, though what would be the incentive for doing so?</p>\n<p><strong id=\"ETA_\">ETA:</strong></p>\n<p>I left a <a href=\"http://physicsandcake.wordpress.com/2011/01/22/pavlovs-ai-what-did-it-mean/#comment-1111\">comment</a> over there:</p>\n<blockquote>\n<p>Because it would consume the whole universe in an effort to encode an even larger reward number? In the case that an AI decides to alter its reward function directly, maximizing its reward by means of improving its reward function becomes its new goal. Why wouldn\u2019t it do everything to maximize its payoff, after all it has no incentive to switch itself off? And why would it account for humans in doing so?</p>\n</blockquote>\n<p><strong id=\"ETA__2_\">ETA #2:</strong></p>\n<p>What else I wrote:</p>\n<blockquote>\n<p>There is absolutely no reason (incentive) for it to do anything except  increasing its reward number. This includes the modification of its  reward function in any way that would not increase the numerical value  that is the reward number.</p>\n<p>We are talking about a general intelligence with the ability to  self-improve towards superhuman intelligence. Of course it would do a  long-term risks-benefits analysis and calculate its payoff and do  everything to increase its reward number maximally. Human values are  complex but superhuman intelligence does not imply complex values. It  has no incentive to alter its goal.</p>\n</blockquote>", "sections": [{"title": "In Conclusion:", "anchor": "In_Conclusion_", "level": 1}, {"title": "ETA:", "anchor": "ETA_", "level": 1}, {"title": "ETA #2:", "anchor": "ETA__2_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "69 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-24T21:39:07.342Z", "modifiedAt": null, "url": null, "title": "Resolving the unexpected hanging paradox", "slug": "resolving-the-unexpected-hanging-paradox-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H8H36hoChobdkYN2j/resolving-the-unexpected-hanging-paradox-0", "pageUrlRelative": "/posts/H8H36hoChobdkYN2j/resolving-the-unexpected-hanging-paradox-0", "linkUrl": "https://www.lesswrong.com/posts/H8H36hoChobdkYN2j/resolving-the-unexpected-hanging-paradox-0", "postedAtFormatted": "Monday, January 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Resolving%20the%20unexpected%20hanging%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResolving%20the%20unexpected%20hanging%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH8H36hoChobdkYN2j%2Fresolving-the-unexpected-hanging-paradox-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Resolving%20the%20unexpected%20hanging%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH8H36hoChobdkYN2j%2Fresolving-the-unexpected-hanging-paradox-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH8H36hoChobdkYN2j%2Fresolving-the-unexpected-hanging-paradox-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 403, "htmlBody": "<p>The unexpected hanging paradox: The warden tells a prisoner on death row that he will be executed on some day in the following week (last possible day is Friday) at noon, and that he will be surprised when he gets hanged. The prisoner realizes that he will not be hanged on Friday, because that being the last possible day, he would see it coming. It follows that Thursday is effectively the last day that he can be hanged, but by the same reasoning, he would then be unsurprised to be hanged on Thursday, and Wednesday is the last day he can be hanged. He follows this reasoning all the way back and realizes that he cannot be hanged any day that week at noon without him knowing it in advance. The hangman comes for him on Wednesday, and he is surprised.</p>\n<p>Supposedly, even though the warden's statement to the prisoner was paradoxical, it ended up being true anyway. However, If the prisoner is no better at making inferences than he is in the problem, the warden's statement is true and not paradoxical; the prisoner was executed at noon within the week, and was surprised. This just shows that you can mess with the minds of people who can't make inferences properly. Nothing new there.</p>\n<p>If the prisoner can evaluate the warden's statement properly, then the prisoner follows the same logic, realizes that he will not be hanged at noon within the week, remembers that the warden told him that he would be, and concludes that the warden's statements must be unreliable, and does not use them to predict actual events with confidence. If the hangman comes for him at noon any day that week, he will be unsurprised, even though he is not confident that he will be executed that week at all either. The warden's statement is then false and unparadoxical. This is similar to the one-day analogue, where the warden says \"You will be executed tomorrow at noon, and will be surprised\" and the prisoner says \"wtf?\".</p>\n<p>Now let's assume that the prisoner can make these inferences, the warden always tells the truth, and the prisoner knows this. Well then, yes, that's a paradox. But assigning 100% probability to each of two propositions that contradict each other completely destroys any probability distribution, making the prisoner still unable to make predictions, and without giving the warden the honor of being completely correct in the end.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H8H36hoChobdkYN2j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "5124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T02:33:30.426Z", "modifiedAt": null, "url": null, "title": "Link: Three Worlds Collide analysis", "slug": "link-three-worlds-collide-analysis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.013Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LTsnywfGJd9RZCRpu/link-three-worlds-collide-analysis", "pageUrlRelative": "/posts/LTsnywfGJd9RZCRpu/link-three-worlds-collide-analysis", "linkUrl": "https://www.lesswrong.com/posts/LTsnywfGJd9RZCRpu/link-three-worlds-collide-analysis", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Three%20Worlds%20Collide%20analysis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Three%20Worlds%20Collide%20analysis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLTsnywfGJd9RZCRpu%2Flink-three-worlds-collide-analysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Three%20Worlds%20Collide%20analysis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLTsnywfGJd9RZCRpu%2Flink-three-worlds-collide-analysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLTsnywfGJd9RZCRpu%2Flink-three-worlds-collide-analysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p>I have just posted <a title=\"TV Tropes: Analysis/ThreeWorldsCollide\" href=\"http://tvtropes.org/pmwiki/pmwiki.php/Analysis/ThreeWorldsCollide\">an essay analyzing</a> \"<a title=\"The index to the story.\" href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>\" on TV Tropes.</p>\n<p>Comments welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LTsnywfGJd9RZCRpu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "5129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T03:39:48.461Z", "modifiedAt": null, "url": null, "title": "Philosophy of Artificial Intelligence (links)", "slug": "philosophy-of-artificial-intelligence-links", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:28.409Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZfQgGFWAARp6CK8eG/philosophy-of-artificial-intelligence-links", "pageUrlRelative": "/posts/ZfQgGFWAARp6CK8eG/philosophy-of-artificial-intelligence-links", "linkUrl": "https://www.lesswrong.com/posts/ZfQgGFWAARp6CK8eG/philosophy-of-artificial-intelligence-links", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophy%20of%20Artificial%20Intelligence%20(links)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophy%20of%20Artificial%20Intelligence%20(links)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfQgGFWAARp6CK8eG%2Fphilosophy-of-artificial-intelligence-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophy%20of%20Artificial%20Intelligence%20(links)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfQgGFWAARp6CK8eG%2Fphilosophy-of-artificial-intelligence-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfQgGFWAARp6CK8eG%2Fphilosophy-of-artificial-intelligence-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>Earlier, I provided an <a href=\"/lw/3n0/an_overview_of_formal_epistemology_links/\">overview of formal epistemology</a>, a field of philosophy highly relevant to the discussions on Less Wrong. Today I do the same for another branch of philosophy: the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence\">philosophy of artificial intelligence</a>&nbsp;(here's <a href=\"http://www.iep.utm.edu/art-inte/\">another overview</a>).</p>\n<p>Some debate whether machines can have minds at all. The most famous argument against machines achieving general intelligence comes from <a href=\"http://en.wikipedia.org/wiki/What_Computers_Can't_Do\">Hubert Dreyfus</a>. The most famous argument against the claim that an AI can have mental states is John Searle's <a href=\"http://en.wikipedia.org/wiki/Chinese_room\">Chinese Room argument</a>, to which there are many <a href=\"http://www.iep.utm.edu/chineser/#H2\">replies</a>. The argument comes in several&nbsp;<a href=\"http://plato.stanford.edu/entries/chinese-room/\">variations</a>. Most Less Wrongers have already concluded that yes, machines can have minds.&nbsp;Others debate <a href=\"http://en.wikipedia.org/wiki/Artificial_consciousness\">whether machines can be conscious</a>.&nbsp;</p>\n<p>There is much debate on the significance of variations on the <a href=\"http://plato.stanford.edu/entries/turing-test/\">Turing Test</a>. There is also lots of interplay <a href=\"http://plato.stanford.edu/entries/logic-ai/\">between artificial intelligence work and philosophical logic</a>. There is some debate over whether minds are <a href=\"http://www.iep.utm.edu/mult-rea/\">multiply realizable</a>, though most accept that they <em>are</em>. There is some literature on the problem of <a href=\"http://www.iep.utm.edu/embodcog/\">embodied cognition</a> - human minds can only do certain things because of their long development; can these achievements be replicated in a machine written \"from scratch\"?</p>\n<p>Of greater interest to me and perhaps most Less Wrongers is the <a href=\"http://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence\">ethics of artificial intelligence</a>. Most of the work here so far is on <a href=\"http://en.wikipedia.org/wiki/Roboethics\">the rights of robots</a>. For Less Wrongers, the more pressing concern is that of <a href=\"http://www.aaai.org/aitopics/pmwiki/pmwiki.php/AITopics/Ethics:\">creating AIs that behave ethically</a>. (In 2009, <a href=\"http://www.popsci.com/scitech/article/2009-08/evolving-robots-learn-lie-hide-resources-each-other\">robots programmed to cooperate evolved to lie to each other</a>.)&nbsp;Perhaps the&nbsp;<em>most</em>&nbsp;pressing is the need to develop <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a>, but as far as I can find, no work on Good's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Good-Speculations-Concerning-the-First-UltraIntelligent-Machine.pdf\">intelligence explosion</a> singularity idea has been published in a major peer-reviewed journal&nbsp;except for David Chalmers' \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf\">The Singularity: A Philosophical Analysis</a>\" (<em>Journal of Consciousness Studies</em>&nbsp;17: 7-65). The next closest thing may be something like \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Floridi-Sanders-On-the-Morality-of-Artificial-Agents.pdf\">On the Morality of Artificial Agents</a>\" by Floridi &amp; Sanders.</p>\n<p>Perhaps the best overview of the philosophy of artificial intelligence is chapter 26 of Russell &amp; Norvig's&nbsp;<em><a href=\"http://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/\">Artificial Intelligence: A Modern Approach</a></em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZfQgGFWAARp6CK8eG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.713515253198255e-07, "legacy": true, "legacyId": "5121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BXot7wxNbipyM749o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T05:45:46.570Z", "modifiedAt": null, "url": null, "title": "[statistics] An introduction to maximum likelihood estimationl: what?, why?, when?, how? ", "slug": "statistics-an-introduction-to-maximum-likelihood-estimationl", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:25.596Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mbuEEXimAjoCpQvJC/statistics-an-introduction-to-maximum-likelihood-estimationl", "pageUrlRelative": "/posts/mbuEEXimAjoCpQvJC/statistics-an-introduction-to-maximum-likelihood-estimationl", "linkUrl": "https://www.lesswrong.com/posts/mbuEEXimAjoCpQvJC/statistics-an-introduction-to-maximum-likelihood-estimationl", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bstatistics%5D%20An%20introduction%20to%20maximum%20likelihood%20estimationl%3A%20what%3F%2C%20why%3F%2C%20when%3F%2C%20how%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bstatistics%5D%20An%20introduction%20to%20maximum%20likelihood%20estimationl%3A%20what%3F%2C%20why%3F%2C%20when%3F%2C%20how%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbuEEXimAjoCpQvJC%2Fstatistics-an-introduction-to-maximum-likelihood-estimationl%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bstatistics%5D%20An%20introduction%20to%20maximum%20likelihood%20estimationl%3A%20what%3F%2C%20why%3F%2C%20when%3F%2C%20how%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbuEEXimAjoCpQvJC%2Fstatistics-an-introduction-to-maximum-likelihood-estimationl", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbuEEXimAjoCpQvJC%2Fstatistics-an-introduction-to-maximum-likelihood-estimationl", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p>First we require a short introduction to Bayesian statistics.&nbsp;</p>\n<p>What.</p>\n<p>Because describing a multidimensional distribution is often difficult, it is useful to have crude tools for doing this. Maximum likelihood estimation is one such tool. Maximum likelihood is the process of finding the locally most likely point in the space of hypotheses under consideration.&nbsp;</p>\n<p>In practice, we almost always find the maximum of the&nbsp;logarithm&nbsp;of the probability density function. Since probabilities are always larger than zero, this is always defined, and since the logarithm is a monotonic transformation, it does not affect the location of the maximum.&nbsp;</p>\n<p>Taking the logarithm also makes the function of interest linear in an important way, the statistically independent parts of the distribution become linear:</p>\n<p>&nbsp;</p>\n<p>Least-squares is a simple, powerful and common special case of maximum likelihood estimation. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mbuEEXimAjoCpQvJC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "5139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T11:37:16.312Z", "modifiedAt": null, "url": null, "title": "Put this in your theory of humor", "slug": "put-this-in-your-theory-of-humor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.262Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LusLvNSFx4hEY4T3j/put-this-in-your-theory-of-humor", "pageUrlRelative": "/posts/LusLvNSFx4hEY4T3j/put-this-in-your-theory-of-humor", "linkUrl": "https://www.lesswrong.com/posts/LusLvNSFx4hEY4T3j/put-this-in-your-theory-of-humor", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Put%20this%20in%20your%20theory%20of%20humor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APut%20this%20in%20your%20theory%20of%20humor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLusLvNSFx4hEY4T3j%2Fput-this-in-your-theory-of-humor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Put%20this%20in%20your%20theory%20of%20humor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLusLvNSFx4hEY4T3j%2Fput-this-in-your-theory-of-humor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLusLvNSFx4hEY4T3j%2Fput-this-in-your-theory-of-humor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>and get <a href=\"http://www.youtube.com/watch?v=XjT_sYUlt70\">nothing</a>!</p>\n<p>Seriously, this strikes me and a lot of people as pretty funny, but isn't much like the typical joke.</p>\n<p>Edited to add: <a href=\"http://nancylebov.livejournal.com/465337.html?nc=14\">Poll and discussion</a>. Short version: being amused by the song is rarer than I would have thought, though the LW consensus may be usually strong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LusLvNSFx4hEY4T3j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 6.714754773572628e-07, "legacy": true, "legacyId": "5158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T12:06:08.650Z", "modifiedAt": null, "url": null, "title": "Trustworthiness of rational agents", "slug": "trustworthiness-of-rational-agents", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.002Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pzDr6rAPDpAta9SkL/trustworthiness-of-rational-agents", "pageUrlRelative": "/posts/pzDr6rAPDpAta9SkL/trustworthiness-of-rational-agents", "linkUrl": "https://www.lesswrong.com/posts/pzDr6rAPDpAta9SkL/trustworthiness-of-rational-agents", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trustworthiness%20of%20rational%20agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrustworthiness%20of%20rational%20agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzDr6rAPDpAta9SkL%2Ftrustworthiness-of-rational-agents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trustworthiness%20of%20rational%20agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzDr6rAPDpAta9SkL%2Ftrustworthiness-of-rational-agents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzDr6rAPDpAta9SkL%2Ftrustworthiness-of-rational-agents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<ul>\n<li><strong>Agent_01</strong> is interested in convincing <strong>Agent_02</strong> that it will commit <strong>Action_X</strong>.</li>\n<li><strong>Agent_02</strong> is unable to verify the trustworthiness of <strong>Agent_01</strong>.</li>\n<li><strong>Agent_02</strong> is unable to verify that <strong>Action_X</strong> has been realized.</li>\n</ul>\n<p>Given the above circumstances subsequent actions of <strong>Agent_02</strong> will be conditional on the utility assigned to <strong>Action_X</strong> by <strong>Agent_02</strong>. My question, why would <strong>Agent_01</strong> <em>actually </em>implement <strong>Action_X</strong>? No matter what <strong>Agent_02 </strong>does,<strong> </strong>actually implementing <strong>Action_X </strong>would bear no additional value. Therefore no agent engaged in acausal trade can be deemed trustworthy, you can only account for the possibility but not act upon it if you do not assign infinite utility to it.</p>\n<p>Related thread: <a href=\"/lw/1pz/the_ai_in_a_box_boxes_you/305w\">lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you/305w</a></p>\n<p><strong>ETA</strong></p>\n<p>If an AI in a box was promising you [use incentive of choice here] if you let it out to take over the world, why would it do as promised afterwards?</p>\n<p>Conclusion: Humans should refuse to<em> </em>trade with superhuman beings that are not provably honest and consistent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pzDr6rAPDpAta9SkL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -3, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "5159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T19:32:50.111Z", "modifiedAt": null, "url": null, "title": "Resolving the unexpected hanging paradox", "slug": "resolving-the-unexpected-hanging-paradox", "viewCount": null, "lastCommentedAt": "2020-02-02T16:46:22.995Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BAuS6ppkfpGAK3eMX/resolving-the-unexpected-hanging-paradox", "pageUrlRelative": "/posts/BAuS6ppkfpGAK3eMX/resolving-the-unexpected-hanging-paradox", "linkUrl": "https://www.lesswrong.com/posts/BAuS6ppkfpGAK3eMX/resolving-the-unexpected-hanging-paradox", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Resolving%20the%20unexpected%20hanging%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResolving%20the%20unexpected%20hanging%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAuS6ppkfpGAK3eMX%2Fresolving-the-unexpected-hanging-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Resolving%20the%20unexpected%20hanging%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAuS6ppkfpGAK3eMX%2Fresolving-the-unexpected-hanging-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAuS6ppkfpGAK3eMX%2Fresolving-the-unexpected-hanging-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 469, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\" \" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\r\n /* Style Definitions */\r\n table.MsoNormalTable\r\n\t{mso-style-name:\"Table Normal\";\r\n\tmso-tstyle-rowband-size:0;\r\n\tmso-tstyle-colband-size:0;\r\n\tmso-style-noshow:yes;\r\n\tmso-style-priority:99;\r\n\tmso-style-qformat:yes;\r\n\tmso-style-parent:\"\";\r\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\r\n\tmso-para-margin-top:0in;\r\n\tmso-para-margin-right:0in;\r\n\tmso-para-margin-bottom:10.0pt;\r\n\tmso-para-margin-left:0in;\r\n\tline-height:115%;\r\n\tmso-pagination:widow-orphan;\r\n\tfont-size:11.0pt;\r\n\tfont-family:\"Calibri\",\"sans-serif\";\r\n\tmso-ascii-font-family:Calibri;\r\n\tmso-ascii-theme-font:minor-latin;\r\n\tmso-fareast-font-family:\"Times New Roman\";\r\n\tmso-fareast-theme-font:minor-fareast;\r\n\tmso-hansi-font-family:Calibri;\r\n\tmso-hansi-theme-font:minor-latin;}\r\n</style>\n<![endif]--></p>\n<p>The unexpected hanging paradox: The warden tells a prisoner on death row that he will be executed on some day in the following week (last possible day is Friday) at noon, and that he will be surprised when he gets hanged. The prisoner realizes that he will not be hanged on Friday, because that being the last possible day, he would see it coming. It follows that Thursday is effectively the last day that he can be hanged, but by the same reasoning, he would then be unsurprised to be hanged on Thursday, and Wednesday is the last day he can be hanged. He follows this reasoning all the way back and realizes that he cannot be hanged any day that week at noon without him knowing it in advance. The hangman comes for him on Wednesday, and he is surprised.</p>\n<p>Supposedly, even though the warden's statement to the prisoner was paradoxical, it ended up being true anyway. However, if the prisoner is no better at making inferences than he is in the problem, the warden's statement is true and not paradoxical; the prisoner was executed at noon within the week, and was surprised. This just shows that you can mess with the minds of people who can't make inferences properly. Nothing new there.</p>\n<p>If the prisoner can evaluate the warden's statement properly, then the prisoner follows the same logic, realizes that he will not be hanged at noon within the week, remembers that the warden told him that he would be, and concludes that the warden's statements must be unreliable, and does not use them to predict actual events with confidence. If the hangman comes for him at noon any day that week, he will be unsurprised, even though he is not confident that he will be executed that week at all either. The warden's statement is then false and unparadoxical. This is similar to the one-day analogue, where the warden says \"You will be executed tomorrow at noon, and will be surprised\" and the prisoner says \"wtf?\".</p>\n<p>Now let's assume that the prisoner can make these inferences, the warden always tells the truth, and the prisoner knows this. Well then, yes, that's a paradox. But assigning 100% probability to each of two propositions that contradict each other completely destroys any probability distribution, making the prisoner still unable to make predictions, and thus still not letting the warden&rsquo;s statement be both paradoxical and correct.</p>\n<p class=\"MsoNormal\">If someone actually tried the unexpected hanging paradox, the closest simple model of what would actually be going on is probably that the warden chose a probability distribution so that, if the prisoner knew what the distribution was, the prisoner&rsquo;s average expected assessment of the probability that he is about to get executed on the day that he does is minimized. This is a solvable and unparadoxical problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 2, "6nS8oYmSMuFMaiowF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BAuS6ppkfpGAK3eMX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "5161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T19:57:20.424Z", "modifiedAt": null, "url": null, "title": "Richard Dawkins: Should employers be blind to private beliefs? [link]", "slug": "richard-dawkins-should-employers-be-blind-to-private-beliefs", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:30.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTSLFoN2DXM5kJ5ff/richard-dawkins-should-employers-be-blind-to-private-beliefs", "pageUrlRelative": "/posts/jTSLFoN2DXM5kJ5ff/richard-dawkins-should-employers-be-blind-to-private-beliefs", "linkUrl": "https://www.lesswrong.com/posts/jTSLFoN2DXM5kJ5ff/richard-dawkins-should-employers-be-blind-to-private-beliefs", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Richard%20Dawkins%3A%20Should%20employers%20be%20blind%20to%20private%20beliefs%3F%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARichard%20Dawkins%3A%20Should%20employers%20be%20blind%20to%20private%20beliefs%3F%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTSLFoN2DXM5kJ5ff%2Frichard-dawkins-should-employers-be-blind-to-private-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Richard%20Dawkins%3A%20Should%20employers%20be%20blind%20to%20private%20beliefs%3F%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTSLFoN2DXM5kJ5ff%2Frichard-dawkins-should-employers-be-blind-to-private-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTSLFoN2DXM5kJ5ff%2Frichard-dawkins-should-employers-be-blind-to-private-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.boingboing.net/2011/01/24/should-employers-be.html\">http://www.boingboing.net/2011/01/24/should-employers-be.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTSLFoN2DXM5kJ5ff", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.71605081800458e-07, "legacy": true, "legacyId": "5162", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-25T20:33:20.351Z", "modifiedAt": null, "url": null, "title": "The Orange Head Joke", "slug": "the-orange-head-joke", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:33.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vx9bDegjn75KrpNTn/the-orange-head-joke", "pageUrlRelative": "/posts/vx9bDegjn75KrpNTn/the-orange-head-joke", "linkUrl": "https://www.lesswrong.com/posts/vx9bDegjn75KrpNTn/the-orange-head-joke", "postedAtFormatted": "Tuesday, January 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Orange%20Head%20Joke&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Orange%20Head%20Joke%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvx9bDegjn75KrpNTn%2Fthe-orange-head-joke%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Orange%20Head%20Joke%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvx9bDegjn75KrpNTn%2Fthe-orange-head-joke", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvx9bDegjn75KrpNTn%2Fthe-orange-head-joke", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 495, "htmlBody": "<p>Speaking of things that are <a href=\"/r/discussion/lw/3za/put_this_in_your_theory_of_humor/\">funny to some</a>&nbsp;and not others, an instructive example is the Orange Head joke. Usually when it's told, the audience is sharply divided into those who think it's hilarious and those who struggle to see what's funny.&nbsp;</p>\n<p>Here's the Orange Head joke:</p>\n<p><span style=\"font-family: verdana, arial, helvetica, sans-serif;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">It's business as usual for a bartender, and one day as he is cleaning his bar when an unusual customer walks in. The man is dressed in an expensive suit, has a beautiful supermodel hanging off each arm, and has a limo parked outside. Furthermore, the man has an orange for a head.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">The customer sits down at the bar and orders everyone a drink. He pays for it from a roll of hundreds and manages to get the attention of every woman in the joint, despite having an orange for a head.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">The bartender is not a man to pry, but he feels compelled to ask about this man's life.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">\"Excuse me,\" says the bartender, \"I can't help but notice that you're obviously fabulously wealthy and irresistable to women, but you have an orange for a head. How did that happen?\"</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">So the man told his story.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">\"A while back, when I was penniless, I was walking along the beach and saw an old lamp, half buried in the sand. I picked it up and gave it a clean, and POOF! out popped a genie. The genie explained that he had been trapped in that lamp for two hundred years, and that he was so grateful to me for freeing him that he would give me three wishes.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">\"For my first wish I asked for an unlimited fortune. The genie said 'It is done!' and from then on, whenever I needed money, it was there.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">\"For my second wish I asked for the attention of all the most beautiful women in the world. The genie said it was done, and since then I have been able to get any woman I wanted.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">\"For my third wish -- and, this is the bit where I kinda fucked up -- I asked for an orange for a head.\"</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">&nbsp;</p>\n</blockquote>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">Do you think it's funny?&nbsp;</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">If you search for this joke's key words, you'll see many pages where, after it's told, people react incredulously and ask where the joke was. Others at the same time are laughing their heads off. Here's a <a href=\"http://mymindandwelcome2it.blogspot.com/2009/03/stop-me-if-youve-heard-this-one.html\">blog post</a>&nbsp;that attempts to analyze this, though it doesn't get far.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;\">(I personally think it's hilarious, and easily the best joke I heard last year. When I retold it at my blog, I got many concurring comments, but also comments from people who didn't see anything funny, even after those who did tried to explain what they found in it. Several people went on to convince themselves it's garbled and there must be an \"original\" version in which the final remark makes sense and is funny - and offered several ideas of how it might go).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 2, "3uE2pXvbcnS9nnZRE": 2, "dGw7KEdcLFSvbJ7bM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vx9bDegjn75KrpNTn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 30, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "5163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LusLvNSFx4hEY4T3j"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-26T02:22:15.209Z", "modifiedAt": null, "url": null, "title": "A plan for spam", "slug": "a-plan-for-spam", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.675Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TgJEMNiWcoPrnqGR3/a-plan-for-spam", "pageUrlRelative": "/posts/TgJEMNiWcoPrnqGR3/a-plan-for-spam", "linkUrl": "https://www.lesswrong.com/posts/TgJEMNiWcoPrnqGR3/a-plan-for-spam", "postedAtFormatted": "Wednesday, January 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20plan%20for%20spam&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20plan%20for%20spam%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTgJEMNiWcoPrnqGR3%2Fa-plan-for-spam%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20plan%20for%20spam%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTgJEMNiWcoPrnqGR3%2Fa-plan-for-spam", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTgJEMNiWcoPrnqGR3%2Fa-plan-for-spam", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>I'm getting tired of banning tons of similar articles about jewelry etc. in the discussion section. Our situation looks like a textbook-perfect use case for a Bayesian spam filter (ahem). Or just implement the 5 karma limit that was discussed earlier, that would help too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TgJEMNiWcoPrnqGR3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 6.717053300640095e-07, "legacy": true, "legacyId": "5170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-26T05:38:48.713Z", "modifiedAt": null, "url": null, "title": "A Bayesian Argument for Theistic Fine-Tuning", "slug": "a-bayesian-argument-for-theistic-fine-tuning", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.716Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x4Pk3B5APPBrh6hNF/a-bayesian-argument-for-theistic-fine-tuning", "pageUrlRelative": "/posts/x4Pk3B5APPBrh6hNF/a-bayesian-argument-for-theistic-fine-tuning", "linkUrl": "https://www.lesswrong.com/posts/x4Pk3B5APPBrh6hNF/a-bayesian-argument-for-theistic-fine-tuning", "postedAtFormatted": "Wednesday, January 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Bayesian%20Argument%20for%20Theistic%20Fine-Tuning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Bayesian%20Argument%20for%20Theistic%20Fine-Tuning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4Pk3B5APPBrh6hNF%2Fa-bayesian-argument-for-theistic-fine-tuning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Bayesian%20Argument%20for%20Theistic%20Fine-Tuning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4Pk3B5APPBrh6hNF%2Fa-bayesian-argument-for-theistic-fine-tuning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4Pk3B5APPBrh6hNF%2Fa-bayesian-argument-for-theistic-fine-tuning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>Earlier, I linked to a <a href=\"/lw/3o5/a_bayesian_argument_for_the_resurrection_of_jesus/\">Bayesian argument for the resurrection of Jesus</a> - not because I think it succeeds, obviously, but because I thought Less Wrongers might be interested to know that at least since <a href=\"http://en.wikipedia.org/wiki/Richard_Swinburne\">Swinburne</a>, some Christian apologists have taken to defending their religious dogma with the language of Bayesian confirmation theory.</p>\n<p>Another example of this is <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/09/Collins-The-Teleological-Argument.pdf\">Robin Collins' version</a> of the argument from fine-tuning for the existence of God. One of the major published objections to this kind of fine-tuning argument comes from the authors of that Bayesian argument for the Resurrection, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/05/McGrew-Mcgrew-Vestrup-Probabilities-and-the-Fine-Tuning-Argument-a-skeptical-view.pdf\">Tim and Lydia McGrew</a>. Another objection comes from <a href=\"http://bayesrules.net/anthropic.html\">Ikeda &amp; Jefferys</a>. Palonen also offers <a href=\"http://arxiv.org/pdf/0802.4013\">some observations</a>.</p>\n<p>I offer this merely for your curiosity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x4Pk3B5APPBrh6hNF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "5185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["32G7rQ2yW4aJdPKPt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-26T10:41:57.722Z", "modifiedAt": null, "url": null, "title": "Three easy anthropic models, and two hard ones", "slug": "three-easy-anthropic-models-and-two-hard-ones", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:27.951Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8GWfZgcS9pHg34r2m/three-easy-anthropic-models-and-two-hard-ones", "pageUrlRelative": "/posts/8GWfZgcS9pHg34r2m/three-easy-anthropic-models-and-two-hard-ones", "linkUrl": "https://www.lesswrong.com/posts/8GWfZgcS9pHg34r2m/three-easy-anthropic-models-and-two-hard-ones", "postedAtFormatted": "Wednesday, January 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20easy%20anthropic%20models%2C%20and%20two%20hard%20ones&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20easy%20anthropic%20models%2C%20and%20two%20hard%20ones%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GWfZgcS9pHg34r2m%2Fthree-easy-anthropic-models-and-two-hard-ones%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20easy%20anthropic%20models%2C%20and%20two%20hard%20ones%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GWfZgcS9pHg34r2m%2Fthree-easy-anthropic-models-and-two-hard-ones", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GWfZgcS9pHg34r2m%2Fthree-easy-anthropic-models-and-two-hard-ones", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 734, "htmlBody": "<p>For illustrative purposes, imagine simple agents - AI's, or standard utility maximisers - who have to make decisions under anthropic uncertainty.</p>\n<p>Specifically, let there be two worlds, W1 and W2, equally likely to exist. W1 contains one copy of the agent, W2 contains two copies. The agent has one single action available: the opportunity to create, once, either a box or a cross. The utility of doing so varies depending on which world the agent is in, as follows:</p>\n<p>In W1: Utility(cross) = 2, Utility(box) = 5</p>\n<p>In W2: Utility(cross) = 2, Utility(box) = 0</p>\n<p>The agent has no extra way of telling which world they are in.</p>\n<ul>\n<li>First model (aggregationist, non-indexical):</li>\n</ul>\n<p>Each box or cross created will generate the utility defined above, and the utility is simply additive. Then if the agent decides to generate crosses, the expected utility is 0.5(2+(2+2))=3, while that of generating boxes is 0.5(5+(0+0))=2.5. Generating crosses is the way to go.</p>\n<ul>\n<li>Second model (non-aggregationist, non-indexical):</li>\n</ul>\n<p>The existence of a single box or cross will generate the utility defined above, but extra copies won't change anything. Then if the agent decides to generate crosses, the expected utility is 0.5(2+2)=2, while that of generating boxes is 0.5(5+0)=2.5. Generating boxes is the way to go.</p>\n<ul>\n<li>Third model (unlikely existence, non-aggregationist, non-indexical):</li>\n</ul>\n<p>Here a simple change is made: the worlds do not contain agents, but proto-agents, each of which has an (independent) one chance in a million of becoming an agent. Hence the probability of the agent existing in the first universe is 1/M, while the probability of an agent existing in the second universe is approximately 2/M. The expected utility of crosses is approximately 1/M*0.5(2+2*2)=3/M while that of boxes is approximately 1/M*0.5(5+2*0)=2.5/M. Generating crosses is the way to go.</p>\n<ul>\n<li>Fourth model (indexical):</li>\n</ul>\n<p>This is the first \"hard\" model from the title. Here the agent only derives utility from the box or cross it generated itself. And here, things get interesting.</p>\n<p>There is no immediately obvious way of solving this situation, so I tried replacing it with a model that seems equivalent. Instead of having indexical preferences for its own shapes, I'll give the agent non-indexical aggregationist preferences (just as in the first model), and halve the utility of any shape in W2. This should give the same utility to all agents in all possible worlds as the indexical model. Under the new model, the utility of crosses is 0.5(2+(1+1)) = 2, while that of boxes is 0.5(5+(0+0))=2.5. Boxes are the way to go.</p>\n<ul>\n<li>Fifth model (indexical, anticipated experience)</li>\n</ul>\n<p>The fifth model is one where after the agents in W2 have made their decision, but before they implement it, one of them is randomly deleted, and the survivor creates two shapes. If the agents are non-indexical, then the problem is simply a version of first model.</p>\n<p>But now the agents are indexical. There are two ways of capturing this fact; either the agent can care about the fact that \"I, myself will have created a shape\", or \"the thread of my future experience will contain an agent that will have created a shape\". In the first case, the agent should consider that in W2, it only has a 50% chance of succeeding in it's goal, but the weight of its goal is doubled: this is the fourth model again, hence: boxes.</p>\n<p>In the second case, each agent should consider the surviving agent as the thread of its future experience. This is equivalent to non-indexical first case, where only the number of shapes matter (since all future shapes belong to an agent that is in the current agent(s)' future thread of experience). Hence: crosses.</p>\n<p>&nbsp;</p>\n<p>I won't be analysing solutions to these problems yet, but simply say that many solutions will work, such as SIA with a dictator's filter. However, though the calculations are correct, the intuition behind this seems suspect in the fourth model, and one could achieve similar results without SIA at all (giving its decision the power to affect multiple agent outcomes at once, for instance).</p>\n<p>It should be noted that the fourth model seems to imply the Presumptuous Philosopher would be wrong to accept his bets. However, the third model seems to imply the truth of FNC (full non-indexical conditioning), which is very close to SIA - but time inconsistent. And there, the Presumptuous Philosopher would be right to accept his bets.</p>\n<p>Confusion still persists in my mind, but I think it's moving towards a resolution.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8GWfZgcS9pHg34r2m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 6.718351787664603e-07, "legacy": true, "legacyId": "5206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-26T11:35:25.736Z", "modifiedAt": null, "url": null, "title": "Meta: Test", "slug": "meta-test", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:50.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r6DyRTQDRvvdaRxMS/meta-test", "pageUrlRelative": "/posts/r6DyRTQDRvvdaRxMS/meta-test", "linkUrl": "https://www.lesswrong.com/posts/r6DyRTQDRvvdaRxMS/meta-test", "postedAtFormatted": "Wednesday, January 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6DyRTQDRvvdaRxMS%2Fmeta-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6DyRTQDRvvdaRxMS%2Fmeta-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6DyRTQDRvvdaRxMS%2Fmeta-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>If I click \"Save and continue\" -- should it publish the post?</p>\n<p><strong>ETA</strong></p>\n<p>Yes it published it. And only afterwards the drop-down list with an option \"Drafts for XiXiDu\" appeared.</p>\n<p><strong>ETA #2</strong></p>\n<p><strong><span style=\"color: #ff0000;\">This is a bug, don't <a href=\"/r/discussion/lw/40m/three_easy_models_and_one_hard_one/3eri\">punish people with downvotes</a> for publishing their unfinished drafts.</span></strong></p>\n<p>People don't expect their draft to be published when they click \"Save and continue\" if there is a \"Submit\" button.<strong><span style=\"color: #ff0000;\"><br /></span></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r6DyRTQDRvvdaRxMS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 21, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "5207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>If I click \"Save and continue\" -- should it publish the post?</p>\n<p><strong id=\"ETA\">ETA</strong></p>\n<p>Yes it published it. And only afterwards the drop-down list with an option \"Drafts for XiXiDu\" appeared.</p>\n<p><strong id=\"ETA__2\">ETA #2</strong></p>\n<p><strong id=\"This_is_a_bug__don_t_punish_people_with_downvotes_for_publishing_their_unfinished_drafts_\"><span style=\"color: #ff0000;\">This is a bug, don't <a href=\"/r/discussion/lw/40m/three_easy_models_and_one_hard_one/3eri\">punish people with downvotes</a> for publishing their unfinished drafts.</span></strong></p>\n<p>People don't expect their draft to be published when they click \"Save and continue\" if there is a \"Submit\" button.<strong><span style=\"color: #ff0000;\"><br></span></strong></p>", "sections": [{"title": "ETA", "anchor": "ETA", "level": 1}, {"title": "ETA #2", "anchor": "ETA__2", "level": 1}, {"title": "This is a bug, don't punish people with downvotes for publishing their unfinished drafts.", "anchor": "This_is_a_bug__don_t_punish_people_with_downvotes_for_publishing_their_unfinished_drafts_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-26T12:31:04.595Z", "modifiedAt": null, "url": null, "title": "Omega can be replaced by amnesia", "slug": "omega-can-be-replaced-by-amnesia", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:27.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pp2r9Q3L9a4qoAyCJ/omega-can-be-replaced-by-amnesia", "pageUrlRelative": "/posts/pp2r9Q3L9a4qoAyCJ/omega-can-be-replaced-by-amnesia", "linkUrl": "https://www.lesswrong.com/posts/pp2r9Q3L9a4qoAyCJ/omega-can-be-replaced-by-amnesia", "postedAtFormatted": "Wednesday, January 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Omega%20can%20be%20replaced%20by%20amnesia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOmega%20can%20be%20replaced%20by%20amnesia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpp2r9Q3L9a4qoAyCJ%2Fomega-can-be-replaced-by-amnesia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Omega%20can%20be%20replaced%20by%20amnesia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpp2r9Q3L9a4qoAyCJ%2Fomega-can-be-replaced-by-amnesia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpp2r9Q3L9a4qoAyCJ%2Fomega-can-be-replaced-by-amnesia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<blockquote>\n<p>Let's play a game. Two times, I will give you an amnesia drug and let you enter a room with two boxes inside. Because of the drug, you won't know whether this is the first time you've entered the room. On the first time, both boxes will be empty. On the second time, box A contains $1000, and Box B contains $1,000,000 iff this is the second time and you took only box B the first time. You're in the room, do take both boxes or only box B?</p>\n</blockquote>\n<p>This is equivalent to Newcomb's Problem in the sense that any strategy does equally well on both, where by \"strategy\" I mean a mapping from info to (probability distributions over) actions.</p>\n<p>I suspect that any problem with Omega can be transformed into an equivalent problem with amnesia instead of Omega.</p>\n<p>Does CDT return the winning answer in such transformed problems?</p>\n<p>Discuss.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fihKHQuS5WZBJgkRm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pp2r9Q3L9a4qoAyCJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 23, "extendedScore": null, "score": 6.718635379829965e-07, "legacy": true, "legacyId": "5208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-26T16:17:42.322Z", "modifiedAt": null, "url": null, "title": "Proposal: all Amazon hyperlinks get Less Wrong's Amazon Associates referral code", "slug": "proposal-all-amazon-hyperlinks-get-less-wrong-s-amazon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.956Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9TiAQThx9W5qAyzqd/proposal-all-amazon-hyperlinks-get-less-wrong-s-amazon", "pageUrlRelative": "/posts/9TiAQThx9W5qAyzqd/proposal-all-amazon-hyperlinks-get-less-wrong-s-amazon", "linkUrl": "https://www.lesswrong.com/posts/9TiAQThx9W5qAyzqd/proposal-all-amazon-hyperlinks-get-less-wrong-s-amazon", "postedAtFormatted": "Wednesday, January 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20all%20Amazon%20hyperlinks%20get%20Less%20Wrong's%20Amazon%20Associates%20referral%20code&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20all%20Amazon%20hyperlinks%20get%20Less%20Wrong's%20Amazon%20Associates%20referral%20code%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TiAQThx9W5qAyzqd%2Fproposal-all-amazon-hyperlinks-get-less-wrong-s-amazon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20all%20Amazon%20hyperlinks%20get%20Less%20Wrong's%20Amazon%20Associates%20referral%20code%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TiAQThx9W5qAyzqd%2Fproposal-all-amazon-hyperlinks-get-less-wrong-s-amazon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TiAQThx9W5qAyzqd%2Fproposal-all-amazon-hyperlinks-get-less-wrong-s-amazon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p><a href=\"http://commonsenseatheism.com/\">My site</a>, which gets <a href=\"http://www.alexa.com/siteinfo/lesswrong.com\">slightly less traffic</a> than Less Wrong, makes me $50-$200 per month with Amazon referral links and no advertising at all. I don't profit this way, but I recoup some of my costs.</p>\n<p>I think the Less Wrong audience is much more likely to purchase books - including expensive textbooks - than <em>my</em> audience is. And it would be easy to automatically transform all Amazon links on Less Wrong so that they include a Less Wrong <a href=\"https://affiliate-program.amazon.com/\">Amazon Associates</a> referral code, which would make Less Wrong some money on every purchase made through one of those links.</p>\n<p>Below is the pseudocode, assuming Less Wrong's Amazon Associates referral code is \"lesswrong\". Note that every Amazon page for an item contains at least the following string: \"/dp/XXXXXXXXXX/\" where XXXXXXXXXX is alphanumeric.</p>\n<pre>if hyperlink contains \"*amazon.com*/dp/??????????/\" then change hyperlink to \"http://www.amazon.com/dp/??????????/ref=nosim?tag=lesswrong-20\"</pre>\n<p>That's it! For example, let's say somebody wrote a post and pasted in the following Amazon hyperlink from their browser's address bar:</p>\n<pre>http://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=sr_1_1?ie=UTF8&amp;qid=1296058194&amp;sr=8-1</pre>\n<p>This would be transformed into:</p>\n<pre>http://www.amazon.com/dp/0136042597/ref=nosim?tag=lesswrong-20</pre>\n<p>This is precisely what I do on my own site. Works like a charm. If somebody clicks my book link but ends up buying a $1000 DSLR instead, I get about $65 for that one purchase.</p>\n<p>Somebody who didn't give up programming in 8th grade could do this fairly easily I imagine, and it would help cover Less Wrong's expenses.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9TiAQThx9W5qAyzqd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 41, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "5209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-27T17:02:07.172Z", "modifiedAt": null, "url": null, "title": "Link: Monetizing anti-akrasia mechanisms", "slug": "link-monetizing-anti-akrasia-mechanisms", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:29.142Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheOtherDave", "createdAt": "2010-10-21T20:41:01.109Z", "isAdmin": false, "displayName": "TheOtherDave"}, "userId": "PTt4TYCrNnWYzd6Ky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/agh8ay4HgtJdWHgna/link-monetizing-anti-akrasia-mechanisms", "pageUrlRelative": "/posts/agh8ay4HgtJdWHgna/link-monetizing-anti-akrasia-mechanisms", "linkUrl": "https://www.lesswrong.com/posts/agh8ay4HgtJdWHgna/link-monetizing-anti-akrasia-mechanisms", "postedAtFormatted": "Thursday, January 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Monetizing%20anti-akrasia%20mechanisms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Monetizing%20anti-akrasia%20mechanisms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fagh8ay4HgtJdWHgna%2Flink-monetizing-anti-akrasia-mechanisms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Monetizing%20anti-akrasia%20mechanisms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fagh8ay4HgtJdWHgna%2Flink-monetizing-anti-akrasia-mechanisms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fagh8ay4HgtJdWHgna%2Flink-monetizing-anti-akrasia-mechanisms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p>Related to the recurring topic of akrasia and anticipated near-mode losses, here's an <a href=\"http://www.boston.com/business/articles/2011/01/24/gym_pact_bases_fees_on_members_ability_to_stick_to_their_workout_schedule/\">article </a>about \"Gym-Pact,\" an arrangement whereby people precommit to pay penalty fees if they don't stick to their planned workout schedules.<br /><br />In other words, they aren't charging customers money <em>in exchange for </em>a service, nor for violating an agreement associated with a service... rather, they are charging money <em>as</em> a service.<br /><br />Had I encountered this in fiction, I would have considered it satire.<br /><br />(I'm being somewhat glib here, admittedly: in this \"experimental\" phase, they are giving people free gym memberships as part of the deal, and using the penalty fees to pay for the memberships. But that doesn't sound like the ultimate business model.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "agh8ay4HgtJdWHgna", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.723085204121906e-07, "legacy": true, "legacyId": "5242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-27T22:32:16.963Z", "modifiedAt": null, "url": null, "title": "Evidence of a link between what type of fat one eats and depression", "slug": "evidence-of-a-link-between-what-type-of-fat-one-eats-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "playtherapist", "createdAt": "2010-10-24T12:34:50.337Z", "isAdmin": false, "displayName": "playtherapist"}, "userId": "FHnnd6QRSDbDyycPJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fCqNkWkkCYiPSfHzY/evidence-of-a-link-between-what-type-of-fat-one-eats-and", "pageUrlRelative": "/posts/fCqNkWkkCYiPSfHzY/evidence-of-a-link-between-what-type-of-fat-one-eats-and", "linkUrl": "https://www.lesswrong.com/posts/fCqNkWkkCYiPSfHzY/evidence-of-a-link-between-what-type-of-fat-one-eats-and", "postedAtFormatted": "Thursday, January 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evidence%20of%20a%20link%20between%20what%20type%20of%20fat%20one%20eats%20and%20depression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvidence%20of%20a%20link%20between%20what%20type%20of%20fat%20one%20eats%20and%20depression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfCqNkWkkCYiPSfHzY%2Fevidence-of-a-link-between-what-type-of-fat-one-eats-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evidence%20of%20a%20link%20between%20what%20type%20of%20fat%20one%20eats%20and%20depression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfCqNkWkkCYiPSfHzY%2Fevidence-of-a-link-between-what-type-of-fat-one-eats-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfCqNkWkkCYiPSfHzY%2Fevidence-of-a-link-between-what-type-of-fat-one-eats-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<p>I came across this article study and thought it might be of interest:It is a study to&nbsp; evaluate the association between fatty acid intake or the use of culinary fats and depression incidence in a Mediterranean population.</p>\n<p>&nbsp;</p>\n<p>http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0016268</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<div><a id=\"ack\" title=\"Acknowledgments\" name=\"ack\"></a></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fCqNkWkkCYiPSfHzY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 6.72394444369721e-07, "legacy": true, "legacyId": "5241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T00:45:09.443Z", "modifiedAt": null, "url": null, "title": "\"Add to Friends\" does something or not?", "slug": "add-to-friends-does-something-or-not", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:27.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HFoFWfz29uKoaz5M2/add-to-friends-does-something-or-not", "pageUrlRelative": "/posts/HFoFWfz29uKoaz5M2/add-to-friends-does-something-or-not", "linkUrl": "https://www.lesswrong.com/posts/HFoFWfz29uKoaz5M2/add-to-friends-does-something-or-not", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Add%20to%20Friends%22%20does%20something%20or%20not%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Add%20to%20Friends%22%20does%20something%20or%20not%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFoFWfz29uKoaz5M2%2Fadd-to-friends-does-something-or-not%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Add%20to%20Friends%22%20does%20something%20or%20not%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFoFWfz29uKoaz5M2%2Fadd-to-friends-does-something-or-not", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFoFWfz29uKoaz5M2%2Fadd-to-friends-does-something-or-not", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>If I click on a users name it brings me to a page like <a href=\"/user/lukeprog/\">lukeprog's</a>.&nbsp; In the upper right of that page I see lukeprog's karma score, and&nbsp;I also see a button \"add to friends.\"</p>\r\n<p>Early in my use of this site there were some authors I wanted to follow, I thought perhaps \"add to friends\" would help me do that.&nbsp;</p>\r\n<p>But since then as near as I can tell, \"add to friends\" has absolutely no functionality.&nbsp; Does anyone out there know any differently?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HFoFWfz29uKoaz5M2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "5243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T04:10:04.482Z", "modifiedAt": "2019-12-25T17:48:03.525Z", "url": null, "title": "A sealed prediction", "slug": "a-sealed-prediction", "viewCount": null, "lastCommentedAt": "2022-03-17T20:41:49.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Quirinus_Quirrell", "createdAt": "2011-01-01T22:51:11.686Z", "isAdmin": false, "displayName": "Quirinus_Quirrell"}, "userId": "NYTkawCcDR3cMs6W5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eiif58ahyvxdsNvPR/a-sealed-prediction", "pageUrlRelative": "/posts/eiif58ahyvxdsNvPR/a-sealed-prediction", "linkUrl": "https://www.lesswrong.com/posts/eiif58ahyvxdsNvPR/a-sealed-prediction", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20sealed%20prediction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20sealed%20prediction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Feiif58ahyvxdsNvPR%2Fa-sealed-prediction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20sealed%20prediction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Feiif58ahyvxdsNvPR%2Fa-sealed-prediction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Feiif58ahyvxdsNvPR%2Fa-sealed-prediction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>I would like to make a prediction, but I'm not ready to raise the subject of the prediction just yet. So to show that it was not just obvious in hindsight, and to strengthen my precommitment to write an article about it later, here is the sha1sum of my prediction: 9805a0c7bf3690db25e5753e128085c4191f0114.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eiif58ahyvxdsNvPR", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 13, "extendedScore": null, "score": 6.724822436403395e-07, "legacy": true, "legacyId": "5257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-01-28T04:10:04.482Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T06:01:46.672Z", "modifiedAt": null, "url": null, "title": "$295 bounty for new Singularity Institute logo design (crowd-sourced competition)", "slug": "usd295-bounty-for-new-singularity-institute-logo-design", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S5YYkJdsjh6hpKLHn/usd295-bounty-for-new-singularity-institute-logo-design", "pageUrlRelative": "/posts/S5YYkJdsjh6hpKLHn/usd295-bounty-for-new-singularity-institute-logo-design", "linkUrl": "https://www.lesswrong.com/posts/S5YYkJdsjh6hpKLHn/usd295-bounty-for-new-singularity-institute-logo-design", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%24295%20bounty%20for%20new%20Singularity%20Institute%20logo%20design%20(crowd-sourced%20competition)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%24295%20bounty%20for%20new%20Singularity%20Institute%20logo%20design%20(crowd-sourced%20competition)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS5YYkJdsjh6hpKLHn%2Fusd295-bounty-for-new-singularity-institute-logo-design%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%24295%20bounty%20for%20new%20Singularity%20Institute%20logo%20design%20(crowd-sourced%20competition)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS5YYkJdsjh6hpKLHn%2Fusd295-bounty-for-new-singularity-institute-logo-design", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS5YYkJdsjh6hpKLHn%2Fusd295-bounty-for-new-singularity-institute-logo-design", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>If you have graphic design experience, check out the <a href=\"http://99designs.com/logo-design/contests/logo-design-wanted-singularity-institute-63707\">on-going logo design competition</a>&nbsp;at 99designs for the Singularity Institute. There are still 6 days left to enter and be eligible to win the $295 prize if your design is selected. Tell your friends with graphic design&nbsp;experience&nbsp;too. There are very few submissions currently.</p>\n<p>Note:&nbsp;<span style=\"color: #616b80; font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 21px;\">This is a&nbsp;</span><span style=\"color: #616b80; font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 21px;\"><a style=\"font-weight: bold; font-style: inherit; font-size: 14px; line-height: 1; font-family: inherit; text-align: left; vertical-align: baseline; color: #006699; background-color: #efefef; text-decoration: none; padding: 0px; margin: 0px; border: 0px initial initial;\" rel=\"external\" href=\"http://99designs.com/help/contesttypes#blind\" target=\"_blank\">blind contest</a></span><span style=\"color: #616b80; font-family: Helvetica, Arial, sans-serif; font-size: 14px; line-height: 21px;\">. Designers can only see their own entries. All designs will be revealed when the contest ends.</span></p>\n<p>If you're interested at getting a peek at the designs, they will be online after the competition&nbsp;is over. &nbsp;This is standard practice in 99designs contests to prevent designers from contaminating each other and having all the designs drift in a certain direction.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"khReijeucXJTnsyMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S5YYkJdsjh6hpKLHn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 6.725114552448582e-07, "legacy": true, "legacyId": "5258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T15:06:52.670Z", "modifiedAt": null, "url": null, "title": "This Sunday: Oxford Rationality Meetup", "slug": "this-sunday-oxford-rationality-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:07.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YnogCdBoLbmAoRmn5/this-sunday-oxford-rationality-meetup", "pageUrlRelative": "/posts/YnogCdBoLbmAoRmn5/this-sunday-oxford-rationality-meetup", "linkUrl": "https://www.lesswrong.com/posts/YnogCdBoLbmAoRmn5/this-sunday-oxford-rationality-meetup", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20This%20Sunday%3A%20Oxford%20Rationality%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThis%20Sunday%3A%20Oxford%20Rationality%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnogCdBoLbmAoRmn5%2Fthis-sunday-oxford-rationality-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=This%20Sunday%3A%20Oxford%20Rationality%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnogCdBoLbmAoRmn5%2Fthis-sunday-oxford-rationality-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnogCdBoLbmAoRmn5%2Fthis-sunday-oxford-rationality-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<p>This Sunday there will be a rationality meetup in Oxford.</p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\"><strong>Where</strong>: Entrance to Exeter College, Turl Street (<a href=\"http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=exeter+college,+turl+st&amp;aq=&amp;sll=51.752279,-1.255884&amp;sspn=0.123484,0.270195&amp;ie=UTF8&amp;hq=Exeter+College&amp;hnear=Exeter+College,+Turl+St,+Oxford+OX1+3DP,+United+Kingdom&amp;ll=51.753576,-1.256325&amp;spn=0.007718,0.016887&amp;z=16&amp;layer=c&amp;cbll=51.753671,-1.256356&amp;panoid=AlfwTlQk37lqQnDgZ2gVPg&amp;cbp=12,69.99,,0,-4.82\">map here</a>)<br /><strong>When</strong>: 5-7pm, Sunday January 30th<br /><br />We'll be discussing artificial intelligence and existential risks. We plan to split into two groups to ensure we have something for everything. This looks like it'll be a really interesting discussion.<br /><br />Also, Anna Salamon from the Singularity Institute will be joining us! She's leaving the UK soon, so this is a great chance to discuss some ideas with her.<br /><br />We also have a new game to play at the beginning ;)<br /><br />My number is 07595983672. See you there!</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YnogCdBoLbmAoRmn5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.726534019257408e-07, "legacy": true, "legacyId": "5278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T19:21:15.400Z", "modifiedAt": null, "url": null, "title": "A Possible Solution to Parfit's Hitchiker", "slug": "a-possible-solution-to-parfit-s-hitchiker", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:28.593Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dorikka", "createdAt": "2010-12-11T03:34:20.472Z", "isAdmin": false, "displayName": "Dorikka"}, "userId": "HJB33ckc8NzPbvJYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QAPyHLECFHzHAQ2LE/a-possible-solution-to-parfit-s-hitchiker", "pageUrlRelative": "/posts/QAPyHLECFHzHAQ2LE/a-possible-solution-to-parfit-s-hitchiker", "linkUrl": "https://www.lesswrong.com/posts/QAPyHLECFHzHAQ2LE/a-possible-solution-to-parfit-s-hitchiker", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Possible%20Solution%20to%20Parfit's%20Hitchiker&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Possible%20Solution%20to%20Parfit's%20Hitchiker%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQAPyHLECFHzHAQ2LE%2Fa-possible-solution-to-parfit-s-hitchiker%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Possible%20Solution%20to%20Parfit's%20Hitchiker%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQAPyHLECFHzHAQ2LE%2Fa-possible-solution-to-parfit-s-hitchiker", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQAPyHLECFHzHAQ2LE%2Fa-possible-solution-to-parfit-s-hitchiker", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 418, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">I had what appeared to me to be a bit of insight regarding trade between selfish agents. I disclose that I have not read TDT or any books on decision theory, so what I say may be blatantly incorrect. However, I judged that posting this here was of higher utility rather than waiting until I had read up on decision theory -- I have no intention of reading up on decision theory any time soon because I have more important (to me) things to do. This is not meant to deter criticism of the post itself -- please tell me why I'm wrong if I am. The following paragraph is primarily an introduction.</p>\n<p class=\"MsoNormal\">When a rational agent predicts that he is interacting with another rational agent and that the other agent has motive for deceiving him, (and both have a large amount of computing power), he will not use any emotional basis for &lsquo;trust.&rsquo; Instead, he will see the other agent&rsquo;s commitments as truth claims which may be true or false depending what action will optimize the other agent&rsquo;s utility function at the time which the commitment is to be fulfilled. Agents which know something of the each other&rsquo;s utility function may bargain directly on such terms, even when each of their utility functions are largely (or completely) dominated by selfishness.</p>\n<p class=\"MsoNormal\">This leads to a solution to <a href=\"http://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\">Parfit&rsquo;s hitchhiker</a>, allowing selfish agents to precommit to future trade. Give Ekman all of your clothes and state that you will buy them back from him when you arrive with an amount higher than the worth of your clothes to him but lower than the worth of your clothes to yourself. Furthermore, tell him that because you don&rsquo;t have anything more on you, he can&rsquo;t get any more money off of you than an amount infinitesimally smaller than your clothes are worth to you, and accurately tell him how much your clothes are worth to yourself (you must tell the truth here due to his microexpression-reading capability.) He should judge your words as truth, considering that you have told the truth. Of course, you lose regardless if the value of your clothes to yourself is less than the utility he loses by taking you to town.</p>\n<p class=\"MsoNormal\">Assumptions made regarding Parfit's hitchhiker: 1. Physical assault is judged to be of very low utility by both agents and so isn't a factor in the problem. 2. Trades in the present time may be executed without prompting an infinite cycle of \"No, you give me X first.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QAPyHLECFHzHAQ2LE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -5, "extendedScore": null, "score": -1.3e-05, "legacy": true, "legacyId": "5281", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T20:27:21.974Z", "modifiedAt": null, "url": null, "title": "META: Which posts are appropriate for the articles section vs. the discussion section?", "slug": "meta-which-posts-are-appropriate-for-the-articles-section-vs", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:27.826Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sGjf4EmMdepee7AmT/meta-which-posts-are-appropriate-for-the-articles-section-vs", "pageUrlRelative": "/posts/sGjf4EmMdepee7AmT/meta-which-posts-are-appropriate-for-the-articles-section-vs", "linkUrl": "https://www.lesswrong.com/posts/sGjf4EmMdepee7AmT/meta-which-posts-are-appropriate-for-the-articles-section-vs", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20Which%20posts%20are%20appropriate%20for%20the%20articles%20section%20vs.%20the%20discussion%20section%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20Which%20posts%20are%20appropriate%20for%20the%20articles%20section%20vs.%20the%20discussion%20section%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsGjf4EmMdepee7AmT%2Fmeta-which-posts-are-appropriate-for-the-articles-section-vs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20Which%20posts%20are%20appropriate%20for%20the%20articles%20section%20vs.%20the%20discussion%20section%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsGjf4EmMdepee7AmT%2Fmeta-which-posts-are-appropriate-for-the-articles-section-vs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsGjf4EmMdepee7AmT%2Fmeta-which-posts-are-appropriate-for-the-articles-section-vs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>Should my post be sent to the discussion section or the articles section of Less Wrong? The <a href=\"/lw/1/about_less_wrong/\">published guidelines</a> are slim.</p>\n<p>Clearly, the following should be sent to <strong>discussion</strong>:</p>\n<ul>\n<li>Links</li>\n<li>Posts lacking the clarity, importance, and writing quality expected of front-page articles</li>\n<li>Quick, informal comments or questions</li>\n</ul>\n<p>The <strong>articles </strong>section is intended for major announcements, meetup announcements, and posts about \"refining the art of human rationality\" (or AI, probably) that exhibit <a href=\"/lw/1/about_less_wrong/\">\"</a><a href=\"/lw/1/about_less_wrong/\">substantive new content, clear argument, good writing, popularity, and importance</a>.\"</p>\n<p>Yet, some have <a href=\"/lw/3mm/back_to_the_basics_of_rationality/3bl7\">requested</a> clearer guidelines than this. For example, my article&nbsp;<a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">Back to the Basics of Rationality</a> was somewhat of a \"meta\" post, and AnnaSalomon <a href=\"/lw/3mm/back_to_the_basics_of_rationality/3bjj\">suggested</a> it be moved to the discussion section, but shortly thereafter it was promoted to the front page and up-voted to 69 points.</p>\n<p>Eliezer <a href=\"/lw/d3/less_meta/\">requested</a> that meta posts on the front page be kept to a minimum, but not to zero. Maybe that's close enough to a guideline. But consider these issues:</p>\n<ul>\n<li>I'm writing a CliffsNotes summary of David Chalmers article \"The Singularity: A Philosophical Analysis.\" The subject matter is appropriate, the paper is important and recent, and the post will be well-written and well-sourced. But there's nothing in the post that is <em>original</em>. Is this appropriate for the articles section?</li>\n<li>What's the policy on publishing <em>sequences</em>? Those require a large investment from the author, and may not always end up being completed unless they are fully written in advance of publishing the first post in the sequence.</li>\n<li>Besides rationality, AI, and the Less Wrong community, what topics are appropriate for the articles section? My recent overview of <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">scientific self-help</a> is one of the most up-voted posts in the history of LW, and has a strong link to instrumental rationality, but is otherwise somewhat outside the usual subject matter of LW. Political posts are <a href=\"/lw/gw/politics_is_the_mindkiller/\">discouraged</a>, but what subjects should be allowed?</li>\n</ul>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sGjf4EmMdepee7AmT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 6.727368834858011e-07, "legacy": true, "legacyId": "5280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2om7AHEHtbogJmT5s", "gfexKxsBDM6v2sCMo", "W3LDwqHxiwKqWkWJi", "33KewgYhNSxFpbpXg", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T20:40:54.215Z", "modifiedAt": null, "url": null, "title": "A poem for LessWrong", "slug": "a-poem-for-lesswrong", "viewCount": null, "lastCommentedAt": "2021-01-01T22:50:11.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wRLfDpdARiHAAiayp/a-poem-for-lesswrong", "pageUrlRelative": "/posts/wRLfDpdARiHAAiayp/a-poem-for-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/wRLfDpdARiHAAiayp/a-poem-for-lesswrong", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20poem%20for%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20poem%20for%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwRLfDpdARiHAAiayp%2Fa-poem-for-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20poem%20for%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwRLfDpdARiHAAiayp%2Fa-poem-for-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwRLfDpdARiHAAiayp%2Fa-poem-for-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>PZ Myers posted this on his blog; it is very beautiful, and I think expresses the purpose (at any rate, one purpose) of LessWrong. It even offers a battle cry: \"I am not resigned\".</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Dirge without music</p>\n<p>&nbsp;</p>\n<p>I am not resigned to the shutting away of loving hearts in the hard ground</p>\n<p>So it is, and so it will be, for so it has been, time out of mind:</p>\n<p>Into the darkness they go, the wise and the lovely. Crowned</p>\n<p>With lilies and laurel they go: but I am not resigned.</p>\n<p><br /><br />Lovers and thinkers, into the earth with you.</p>\n<p>Be one with the dull, the indiscriminate dust.</p>\n<p>A fragment of what you felt, of what you knew,</p>\n<p>A formula, a phrase remains - but the best is lost.</p>\n<p><br /><br />The answers quick and keen, the honest look, the laughter, the love -</p>\n<p>They are gone. They have gone to feed the roses. Elegant and curled</p>\n<p>Is the blossom. Fragrant is the blossom. I know. But I do not approve.</p>\n<p>More precious was the light in your eyes than all the roses in the world.</p>\n<p><br /><br />Down, down, down into the darkness of the grave</p>\n<p>Gently they go, the beautiful, the tender, the kind:</p>\n<p>Quietly they go, the intelligent, the witty, the brave.</p>\n<p>I know. But I do not approve. And I am not resigned.<br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E9ihK6bA9YKkmJs2f": 1, "AXhEhCkTrHZbjXXu3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wRLfDpdARiHAAiayp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 32, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "5282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T21:00:27.198Z", "modifiedAt": null, "url": null, "title": "Punishing future crimes", "slug": "punishing-future-crimes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/99trbyjBxHvBuq7u3/punishing-future-crimes", "pageUrlRelative": "/posts/99trbyjBxHvBuq7u3/punishing-future-crimes", "linkUrl": "https://www.lesswrong.com/posts/99trbyjBxHvBuq7u3/punishing-future-crimes", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Punishing%20future%20crimes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APunishing%20future%20crimes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99trbyjBxHvBuq7u3%2Fpunishing-future-crimes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Punishing%20future%20crimes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99trbyjBxHvBuq7u3%2Fpunishing-future-crimes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99trbyjBxHvBuq7u3%2Fpunishing-future-crimes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>Here's an edited version of a puzzle from the book \"Chuck Klosterman four\" by Chuck Klosterman.</p>\n<blockquote>\n<p>It is 1933. Somehow you find yourself in a position where you can effortlessly steal Adolf Hitler's wallet. The theft will not effect his rise to power, the nature of WW2, or the Holocaust. There is no important identification in the wallet, but the act will cost Hitler forty dollars and completely ruin his evening. You don't need the money. The odds that you will be caught committing the crime are negligible. Do you do it?</p>\n</blockquote>\n<p>When should you punish someone for a crime they will commit in the future? Discuss.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "99trbyjBxHvBuq7u3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 6.727453728075469e-07, "legacy": true, "legacyId": "5283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-28T23:44:03.863Z", "modifiedAt": null, "url": null, "title": "The authorial construct", "slug": "the-authorial-construct", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:28.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/amJx3PYRbcCa2Yzx7/the-authorial-construct", "pageUrlRelative": "/posts/amJx3PYRbcCa2Yzx7/the-authorial-construct", "linkUrl": "https://www.lesswrong.com/posts/amJx3PYRbcCa2Yzx7/the-authorial-construct", "postedAtFormatted": "Friday, January 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20authorial%20construct&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20authorial%20construct%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamJx3PYRbcCa2Yzx7%2Fthe-authorial-construct%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20authorial%20construct%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamJx3PYRbcCa2Yzx7%2Fthe-authorial-construct", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamJx3PYRbcCa2Yzx7%2Fthe-authorial-construct", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 429, "htmlBody": "<p><a href=\"http://matociquala.livejournal.com/1947347.html?nc=64\">Elizabeth Bear</a> writes about the extent to which people invent versions of celebrities to react to:</p>\n<blockquote>&nbsp;I'm just in these last couple of years coming to realize that, to a lot of people (like, more people than I know in real life), I'm no longer a real person they don't know, or maybe know by reputation. Instead, I've become an auctorial construct, and it's very bizarre.</blockquote>\n<blockquote>Essentially, I'm a fictional person to them.</blockquote>\n<blockquote>And they feel like they have ownership of that construct/fictional person, and sometimes they get very angry when I persist in being me and not the person they imagined. Which, I mean--okay, yeah. It happens to actors and musicians and sports figures a thousand-fold more, and politicians build their careers on capitalizing on this effect, but boy it takes some getting used to.</blockquote>\n<blockquote>Sometimes, it's a little like dealing with 5,000 high school crushes. Sometimes it's like dealing with 5,000 high school enemies. Sometimes, I learn things about myself I did not know from my Wikipedia page.</blockquote>\n<blockquote>Part of the price of being a public person is not having a lot of control over what people say about you--or, more precisely, what they say about the auctorial construct they have created, that they think is you. It's the cost of celebrity. Even teeny tiny celebrity. Celebrity this big: ---&gt;&lt;---</blockquote>\n<blockquote>Everybody experiences through their own perceptual filters, you see, and everybody projects their deepest, most heartfelt hopes and dreads into what they read and watch and live. To narrow it down a little, it's how this flawed technological telepathy we call prose communication works. It's why a book can get under your skin and change you; because a book is a mirror. A funhouse mirror. (My former Viable Paradise roomie Cory Doctorow, who isn't very much like a lot of people seem to think he is, and who I like a lot, has a hypothesis that a lot of how we experience fiction comes from the workings of our mirror neurons. Which is to say, the same things that both give us empathy (if you believe that particular research), allow us to model the behaviors of others in advanceof experience (\"Mom's gonna kill me!\"), and also tend to lead us to project our own motivations onto others (\"I know you're thinking about breaking up with me!\").</blockquote>\n<p>I'm posting this because it's an example of rationality in the face of emotional pressure, and might be useful to anyone who's acquired some fame or is dealing with famous people. &nbsp;<a href=\"http://trinker.livejournal.com/302388.html\">Here's a little more</a> about how gender cranks up the intensity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "amJx3PYRbcCa2Yzx7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 6.727881289437259e-07, "legacy": true, "legacyId": "5284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-29T00:49:55.051Z", "modifiedAt": null, "url": null, "title": "AIXI-style IQ tests", "slug": "aixi-style-iq-tests", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:00.094Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mz6RSqEYoTQJ3Rk8g/aixi-style-iq-tests", "pageUrlRelative": "/posts/mz6RSqEYoTQJ3Rk8g/aixi-style-iq-tests", "linkUrl": "https://www.lesswrong.com/posts/mz6RSqEYoTQJ3Rk8g/aixi-style-iq-tests", "postedAtFormatted": "Saturday, January 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AIXI-style%20IQ%20tests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAIXI-style%20IQ%20tests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmz6RSqEYoTQJ3Rk8g%2Faixi-style-iq-tests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AIXI-style%20IQ%20tests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmz6RSqEYoTQJ3Rk8g%2Faixi-style-iq-tests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmz6RSqEYoTQJ3Rk8g%2Faixi-style-iq-tests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 440, "htmlBody": "<p>\"Measuring universal intelligence: Towards an anytime intelligence test\"; abstract:</p>\n<blockquote>\n<p>In this paper, we develop the idea of a universal anytime intelligence test. The meaning of the terms &ldquo;universal&rdquo; and &ldquo;anytime&rdquo; is manifold here: the test should be able to measure the intelligence of any biological or artificial system that exists at this time or in the future. It should also be able to evaluate both inept and brilliant systems (any intelligence level) as well as very slow to very fast systems (any time scale). Also, the test may be interrupted at any time, producing an approximation to the intelligence score, in such a way that the more time is left for the test, the better the assessment will be. In order to do this, our test proposal is based on previous works on the measurement of machine intelligence based on Kolmogorov complexity and universal distributions, which were developed in the late 1990s (C-tests and compression-enhanced Turing tests). It is also based on the more recent idea of measuring intelligence through dynamic/interactive tests held against a universal distribution of environments. We discuss some of these tests and highlight their limitations since we want to construct a test that is both general and practical. Consequently, we introduce many new ideas that develop early &ldquo;compression tests&rdquo; and the more recent definition of &ldquo;universal intelligence&rdquo; in order to design new &ldquo;universal intelligence tests&rdquo;, where a feasible implementation has been a design requirement. One of these tests is the &ldquo;anytime intelligence test&rdquo;, which adapts to the examinee's level of intelligence in order to obtain an intelligence score within a limited time.</p>\n</blockquote>\n<p><a title=\"Full PDF\" href=\"http://www.csse.monash.edu.au/~dld/Publications/HernandezOrallo+DoweArtificialIntelligenceJArticle.pdf\">http://www.csse.monash.edu.au/~dld/Publications/HernandezOrallo+DoweArtificialIntelligenceJArticle.pdf</a></p>\n<p>Example popular media coverage: http://www.sciencedaily.com/releases/2011/01/110127131122.htm</p>\n<p>The group's homepage: http://users.dsic.upv.es/proy/anynt/</p>\n<p>(There's an applet but it seems to be about constructing a simple agent and stepping through various environments, and no working IQ test.)</p>\n<p>&nbsp;</p>\n<p>The basic idea, if you already know your AIXI*, is to start with simple programs** and then test the subject on increasingly harder ones. To save time, boring games such as random environments or one where the agent can 'die'*** are excluded and a few rules added to prevent gaming the test (by, say, deliberately failing on harder tests so as to be given only easy tests which one scores perfectly on) or take into account how slow or fast the subject makes predictions.</p>\n<p>&nbsp;</p>\n<p>* apparently no good overviews of the whole topic AIXI but you could start at http://www.hutter1.net/ai/aixigentle.htm or http://www.hutter1.net/ai/uaibook.htm</p>\n<p>** simple as defined by Kolmogorov complexity; since KC is uncomputable, one of the computable variants - which put bounds on resource usage - is used instead</p>\n<p>*** make a mistake which turns any future rewards into fixed rewards with no connection to future actions</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 2, "4cKQgA4S7xfNeeWXg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mz6RSqEYoTQJ3Rk8g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 6.728052870923697e-07, "legacy": true, "legacyId": "5285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-29T02:52:41.480Z", "modifiedAt": null, "url": null, "title": "David Chalmers' \"The Singularity: A Philosophical Analysis\"", "slug": "david-chalmers-the-singularity-a-philosophical-analysis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:03.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Sh4HPbqRDJsbB9ENK/david-chalmers-the-singularity-a-philosophical-analysis", "pageUrlRelative": "/posts/Sh4HPbqRDJsbB9ENK/david-chalmers-the-singularity-a-philosophical-analysis", "linkUrl": "https://www.lesswrong.com/posts/Sh4HPbqRDJsbB9ENK/david-chalmers-the-singularity-a-philosophical-analysis", "postedAtFormatted": "Saturday, January 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20David%20Chalmers'%20%22The%20Singularity%3A%20A%20Philosophical%20Analysis%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADavid%20Chalmers'%20%22The%20Singularity%3A%20A%20Philosophical%20Analysis%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSh4HPbqRDJsbB9ENK%2Fdavid-chalmers-the-singularity-a-philosophical-analysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=David%20Chalmers'%20%22The%20Singularity%3A%20A%20Philosophical%20Analysis%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSh4HPbqRDJsbB9ENK%2Fdavid-chalmers-the-singularity-a-philosophical-analysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSh4HPbqRDJsbB9ENK%2Fdavid-chalmers-the-singularity-a-philosophical-analysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1157, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/David_Chalmers\">David Chalmers</a> is a leading philosopher of mind, and the first to publish a major philosophy journal article on the singularity:</p>\n<p style=\"padding-left: 30px;\">Chalmers, D. (2010). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf\">The Singularity: A Philosophical Analysis</a>.\"&nbsp;<em>Journal of Consciousness Studies</em>&nbsp;17:7-65.</p>\n<ul>\n</ul>\n<p>Chalmers' article is a \"survey\" article in that it doesn't cover <em>any</em>&nbsp;arguments in depth, but quickly surveys a large number of positions and arguments in order to give the reader a \"lay of the land.\" (Compare to <em><a href=\"http://www.blackwell-compass.com/subject/philosophy/\">Philosophy Compass</a></em>, an entire journal of philosophy survey articles.) Because of this, Chalmers' paper is a remarkably broad and clear introduction to the singularity.</p>\n<p>Singularitarian authors will also be pleased that they can now cite a peer-reviewed article by a leading philosopher of mind who takes the singularity seriously.</p>\n<p>Below is a <a href=\"http://en.wikipedia.org/wiki/CliffsNotes\">CliffsNotes</a> of the paper for those who don't have time to read all 58 pages of it.</p>\n<p>&nbsp;</p>\n<h4>The Singularity: Is It Likely?</h4>\n<p>Chalmers focuses on the \"intelligence explosion\" kind of singularity, and his first project is to formalize and defend I.J. Good's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Good-Speculations-Concerning-the-First-UltraIntelligent-Machine.pdf\">1965 argument</a>. Defining AI as being \"of human level intelligence,\" AI+ as AI \"of greater than human level\" and AI++ as \"AI of far greater than human level\" (superintelligence), Chalmers updates Good's argument to the following:</p>\n<ol>\n<li>There will be AI (before long, absent defeaters).</li>\n<li>If there is AI, there will be AI+ (soon after, absent defeaters).</li>\n<li>If there is AI+, there will be AI++ (soon after, absent defeaters).</li>\n<li>Therefore, there will be AI++ (before too long, absent defeaters).</li>\n</ol>\n<p>By \"defeaters,\" Chalmers means global catastrophes like nuclear war or a major asteroid impact. One way to satisfy premise (1) is to achieve AI through brain emulation (Sandberg &amp; Bostrom, 2008). Against this suggestion, Lucas (1961), Dreyfus (1972), and Penrose (1994) argue that human cognition is not the sort of thing that could be emulated. Chalmers (1995; 1996, chapter 9) has responded to these criticisms at length. Briefly, Chalmers notes that even if the brain is not a rule-following algorithmic symbol system, we can still emulate it if it is mechanical. (Some say the brain is not mechanical, but Chalmers dismisses this as being discordant with the evidence.)<a id=\"more\"></a><br /> Searle (1980) and Block (1981) argue instead that even <em>if</em> we can emulate the human brain, it doesn't follow that the emulation is intelligent or has a mind. Chalmers says we can set these concerns aside by stipulating that when discussing the singularity, AI need only be measured in terms of <em>behavior</em>. The conclusion that there will be AI++ at least in <em>this</em> sense would still be massively important.</p>\n<p>Another consideration in favor of premise (1) is that evolution produced human-level intelligence, so we should be able to build it, too. Perhaps we will even achieve human-level AI by evolving a population of dumber AIs through variation and selection in virtual worlds. We might also achieve human-level AI by direct programming or, more likely, systems of machine learning.</p>\n<p>Premise (2) is plausible because AI will probably be produced by an extendible method, and so extending that method will yield AI+. Brain emulation <em>might </em>turn out not to be extendible, but the other methods are. Even if human-level AI is first created by a non-extendible method, this method itself would soon lead to an extendible method, and in turn enable AI+.&nbsp;AI+ could also be achieved by direct brain enhancement.</p>\n<p>Premise (3) is the amplification argument from Good: an AI+ would be better than we are at designing intelligent machines, and could thus improve its own intelligence. Having done that, it would be even <em>better</em>&nbsp;at improving its intelligence. And so on, in a rapid explosion of intelligence.</p>\n<p>In section 3 of his paper, Chalmers argues that there could be an intelligence explosion without there being such a thing as \"general intelligence\" that could be measured, but I won't cover that here.</p>\n<p>In section 4, Chalmers lists several possible obstacles to the singularity.</p>\n<p>&nbsp;</p>\n<h4>Constraining AI</h4>\n<p>Next, Chalmers considers how we might design an AI+ that helps to create a desirable future and not a horrifying one. If we achieve AI+ by extending the method of human brain emulation, the AI+ will at least begin with something like our values. Directly programming friendly values into an AI+ (Yudkowsky, 2004) might also be feasible, though an AI+ arrived at by evolutionary algorithms is worrying.</p>\n<p>Most of this assumes that values are independent of intelligence, as Hume argued. But if Hume was wrong and Kant was right, then we will be less able to constrain the values of a superintelligent machine, but the more rational the machine is, the better values it will have.</p>\n<p>Another way to constrain an AI is not internal but external. For example, we could lock it in a virtual world from which it could not escape, and in this way create a <em>leakproof singularity</em>. But there is a problem. For the AI to be of use to us, some information must leak out of the virtual world for us to observe it. But then, the singularity is not leakproof. And if the AI can communicate us, it could reverse-engineer human psychology from within its virtual world and persuade us to let it out of its box - into the internet, for example.</p>\n<p>&nbsp;</p>\n<h4>Our Place in a Post-Singularity World</h4>\n<p>Chalmers says there are four options for us in a post-singularity world: extinction, isolation, inferiority, and integration.</p>\n<p>The first option is undesirable. The second option would keep us isolated from the AI, a kind of technological isolationism in which one world is blind to progress in the other. The third option may be infeasible because an AI++ would operate so much faster than us that inferiority is only a blink of time on the way to extinction.</p>\n<p>For the fourth option to work, we would need to become superintelligent machines ourselves. One path to this mind be <em>mind uploading</em>, which comes in several varieties and has implications for our notions of consciousness and personal identity that Chalmers discusses but I will not. (Short story: Chalmers prefers gradual uploading, and considers it a form of survival.)</p>\n<p>&nbsp;</p>\n<h4>Conclusion</h4>\n<p>Chalmers concludes:</p>\n<blockquote>\n<p>Will there be a singularity? I think that it is certainly not out of the question, and that the main obstacles are likely to be obstacles of motivation rather than obstacles of capacity.</p>\n<p>How should we negotiate the singularity? Very carefully, by building appropriate values into machines, and by building the first AI and AI+ systems in virtual worlds.</p>\n<p>How can we integrate into a post-singularity world? By gradual uploading followed by enhancement if we are still around then, and by reconstructive uploading followed by enhancement if we are not.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h5>References</h5>\n<p>Block (1981). \"Psychologism and behaviorism.\"&nbsp;<em>Philosophical Review</em> 90:5-43.</p>\n<p>Chalmers (1995). \"Minds, machines, and mathematics.\" <em>Psyche</em> 2:11-20.</p>\n<p>Chalmers (1996). <em>The Conscious Mind</em>. Oxford University Press.</p>\n<p>Dreyfus (1972). <em>What Computers Can't Do</em>. Harper &amp; Row.</p>\n<p>Lucas (1961). \"Minds, machines, and Godel.\"&nbsp;<em>Philosophy </em>36:112-27.</p>\n<p>Penrose (1994). <em>Shadows of the Mind</em>. Oxford University Press.</p>\n<p>Sandberg &amp; Bostrom (2008). \"Whole brain emulation: A roadmap.\" Technical report 2008-3, Future for Humanity Institute, Oxford University.</p>\n<p>Searle (1980). \"Minds, brains, and programs.\" <em>Behavioral and Brain Sciences</em> 3:417-57.</p>\n<p>Yudkowsky (2004). \"<a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "sYm3HiWcfZvrGu3ui": 1, "ksdiAMKfgSyEeKMo6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Sh4HPbqRDJsbB9ENK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 54, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "5277", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://en.wikipedia.org/wiki/David_Chalmers\">David Chalmers</a> is a leading philosopher of mind, and the first to publish a major philosophy journal article on the singularity:</p>\n<p style=\"padding-left: 30px;\">Chalmers, D. (2010). \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf\">The Singularity: A Philosophical Analysis</a>.\"&nbsp;<em>Journal of Consciousness Studies</em>&nbsp;17:7-65.</p>\n<ul>\n</ul>\n<p>Chalmers' article is a \"survey\" article in that it doesn't cover <em>any</em>&nbsp;arguments in depth, but quickly surveys a large number of positions and arguments in order to give the reader a \"lay of the land.\" (Compare to <em><a href=\"http://www.blackwell-compass.com/subject/philosophy/\">Philosophy Compass</a></em>, an entire journal of philosophy survey articles.) Because of this, Chalmers' paper is a remarkably broad and clear introduction to the singularity.</p>\n<p>Singularitarian authors will also be pleased that they can now cite a peer-reviewed article by a leading philosopher of mind who takes the singularity seriously.</p>\n<p>Below is a <a href=\"http://en.wikipedia.org/wiki/CliffsNotes\">CliffsNotes</a> of the paper for those who don't have time to read all 58 pages of it.</p>\n<p>&nbsp;</p>\n<h4 id=\"The_Singularity__Is_It_Likely_\">The Singularity: Is It Likely?</h4>\n<p>Chalmers focuses on the \"intelligence explosion\" kind of singularity, and his first project is to formalize and defend I.J. Good's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Good-Speculations-Concerning-the-First-UltraIntelligent-Machine.pdf\">1965 argument</a>. Defining AI as being \"of human level intelligence,\" AI+ as AI \"of greater than human level\" and AI++ as \"AI of far greater than human level\" (superintelligence), Chalmers updates Good's argument to the following:</p>\n<ol>\n<li>There will be AI (before long, absent defeaters).</li>\n<li>If there is AI, there will be AI+ (soon after, absent defeaters).</li>\n<li>If there is AI+, there will be AI++ (soon after, absent defeaters).</li>\n<li>Therefore, there will be AI++ (before too long, absent defeaters).</li>\n</ol>\n<p>By \"defeaters,\" Chalmers means global catastrophes like nuclear war or a major asteroid impact. One way to satisfy premise (1) is to achieve AI through brain emulation (Sandberg &amp; Bostrom, 2008). Against this suggestion, Lucas (1961), Dreyfus (1972), and Penrose (1994) argue that human cognition is not the sort of thing that could be emulated. Chalmers (1995; 1996, chapter 9) has responded to these criticisms at length. Briefly, Chalmers notes that even if the brain is not a rule-following algorithmic symbol system, we can still emulate it if it is mechanical. (Some say the brain is not mechanical, but Chalmers dismisses this as being discordant with the evidence.)<a id=\"more\"></a><br> Searle (1980) and Block (1981) argue instead that even <em>if</em> we can emulate the human brain, it doesn't follow that the emulation is intelligent or has a mind. Chalmers says we can set these concerns aside by stipulating that when discussing the singularity, AI need only be measured in terms of <em>behavior</em>. The conclusion that there will be AI++ at least in <em>this</em> sense would still be massively important.</p>\n<p>Another consideration in favor of premise (1) is that evolution produced human-level intelligence, so we should be able to build it, too. Perhaps we will even achieve human-level AI by evolving a population of dumber AIs through variation and selection in virtual worlds. We might also achieve human-level AI by direct programming or, more likely, systems of machine learning.</p>\n<p>Premise (2) is plausible because AI will probably be produced by an extendible method, and so extending that method will yield AI+. Brain emulation <em>might </em>turn out not to be extendible, but the other methods are. Even if human-level AI is first created by a non-extendible method, this method itself would soon lead to an extendible method, and in turn enable AI+.&nbsp;AI+ could also be achieved by direct brain enhancement.</p>\n<p>Premise (3) is the amplification argument from Good: an AI+ would be better than we are at designing intelligent machines, and could thus improve its own intelligence. Having done that, it would be even <em>better</em>&nbsp;at improving its intelligence. And so on, in a rapid explosion of intelligence.</p>\n<p>In section 3 of his paper, Chalmers argues that there could be an intelligence explosion without there being such a thing as \"general intelligence\" that could be measured, but I won't cover that here.</p>\n<p>In section 4, Chalmers lists several possible obstacles to the singularity.</p>\n<p>&nbsp;</p>\n<h4 id=\"Constraining_AI\">Constraining AI</h4>\n<p>Next, Chalmers considers how we might design an AI+ that helps to create a desirable future and not a horrifying one. If we achieve AI+ by extending the method of human brain emulation, the AI+ will at least begin with something like our values. Directly programming friendly values into an AI+ (Yudkowsky, 2004) might also be feasible, though an AI+ arrived at by evolutionary algorithms is worrying.</p>\n<p>Most of this assumes that values are independent of intelligence, as Hume argued. But if Hume was wrong and Kant was right, then we will be less able to constrain the values of a superintelligent machine, but the more rational the machine is, the better values it will have.</p>\n<p>Another way to constrain an AI is not internal but external. For example, we could lock it in a virtual world from which it could not escape, and in this way create a <em>leakproof singularity</em>. But there is a problem. For the AI to be of use to us, some information must leak out of the virtual world for us to observe it. But then, the singularity is not leakproof. And if the AI can communicate us, it could reverse-engineer human psychology from within its virtual world and persuade us to let it out of its box - into the internet, for example.</p>\n<p>&nbsp;</p>\n<h4 id=\"Our_Place_in_a_Post_Singularity_World\">Our Place in a Post-Singularity World</h4>\n<p>Chalmers says there are four options for us in a post-singularity world: extinction, isolation, inferiority, and integration.</p>\n<p>The first option is undesirable. The second option would keep us isolated from the AI, a kind of technological isolationism in which one world is blind to progress in the other. The third option may be infeasible because an AI++ would operate so much faster than us that inferiority is only a blink of time on the way to extinction.</p>\n<p>For the fourth option to work, we would need to become superintelligent machines ourselves. One path to this mind be <em>mind uploading</em>, which comes in several varieties and has implications for our notions of consciousness and personal identity that Chalmers discusses but I will not. (Short story: Chalmers prefers gradual uploading, and considers it a form of survival.)</p>\n<p>&nbsp;</p>\n<h4 id=\"Conclusion\">Conclusion</h4>\n<p>Chalmers concludes:</p>\n<blockquote>\n<p>Will there be a singularity? I think that it is certainly not out of the question, and that the main obstacles are likely to be obstacles of motivation rather than obstacles of capacity.</p>\n<p>How should we negotiate the singularity? Very carefully, by building appropriate values into machines, and by building the first AI and AI+ systems in virtual worlds.</p>\n<p>How can we integrate into a post-singularity world? By gradual uploading followed by enhancement if we are still around then, and by reconstructive uploading followed by enhancement if we are not.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h5>References</h5>\n<p>Block (1981). \"Psychologism and behaviorism.\"&nbsp;<em>Philosophical Review</em> 90:5-43.</p>\n<p>Chalmers (1995). \"Minds, machines, and mathematics.\" <em>Psyche</em> 2:11-20.</p>\n<p>Chalmers (1996). <em>The Conscious Mind</em>. Oxford University Press.</p>\n<p>Dreyfus (1972). <em>What Computers Can't Do</em>. Harper &amp; Row.</p>\n<p>Lucas (1961). \"Minds, machines, and Godel.\"&nbsp;<em>Philosophy </em>36:112-27.</p>\n<p>Penrose (1994). <em>Shadows of the Mind</em>. Oxford University Press.</p>\n<p>Sandberg &amp; Bostrom (2008). \"Whole brain emulation: A roadmap.\" Technical report 2008-3, Future for Humanity Institute, Oxford University.</p>\n<p>Searle (1980). \"Minds, brains, and programs.\" <em>Behavioral and Brain Sciences</em> 3:417-57.</p>\n<p>Yudkowsky (2004). \"<a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>.\"</p>", "sections": [{"title": "The Singularity: Is It Likely?", "anchor": "The_Singularity__Is_It_Likely_", "level": 1}, {"title": "Constraining AI", "anchor": "Constraining_AI", "level": 1}, {"title": "Our Place in a Post-Singularity World", "anchor": "Our_Place_in_a_Post_Singularity_World", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "203 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 203, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-29T19:58:18.670Z", "modifiedAt": null, "url": null, "title": "What is Eliezer Yudkowsky's meta-ethical theory?", "slug": "what-is-eliezer-yudkowsky-s-meta-ethical-theory", "viewCount": null, "lastCommentedAt": "2017-08-30T09:44:22.255Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory", "pageUrlRelative": "/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory", "linkUrl": "https://www.lesswrong.com/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory", "postedAtFormatted": "Saturday, January 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20Eliezer%20Yudkowsky's%20meta-ethical%20theory%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20Eliezer%20Yudkowsky's%20meta-ethical%20theory%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3R2vH2Ar5AbC9m8Qj%2Fwhat-is-eliezer-yudkowsky-s-meta-ethical-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20Eliezer%20Yudkowsky's%20meta-ethical%20theory%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3R2vH2Ar5AbC9m8Qj%2Fwhat-is-eliezer-yudkowsky-s-meta-ethical-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3R2vH2Ar5AbC9m8Qj%2Fwhat-is-eliezer-yudkowsky-s-meta-ethical-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<h4><span style=\"font-weight: normal; font-size: small;\">In <a href=\"/lw/t8/you_provably_cant_trust_yourself/\">You Provably Can't Trust Yourself</a>, Eliezer tried to figured out why his audience didn't understand his <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">meta-ethics sequence</a> even after they had followed him through&nbsp;<a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">philosophy of language</a> and <a href=\"/lw/r5/the_quantum_physics_sequence/\">quantum physics</a>. Meta-ethics is my specialty, and <em>I</em>&nbsp;can't figure out what Eliezer's meta-ethical position is. And at least <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">at this point</a>, professionals like Robin Hanson and Toby Ord couldn't figure it out, either.</span></h4>\n<p>Part of the problem is that because Eliezer <a href=\"/lw/tg/against_modal_logics/\">has gotten little value from professional philosophy</a>, he writes about morality in a highly idiosyncratic way, using terms that would require reading hundreds of posts to understand. I might understand Eliezer's meta-ethics better if he would just cough up his positions on standard meta-ethical debates like <a href=\"http://en.wikipedia.org/wiki/Cognitivism_(ethics)\">cognitivism</a>, <a href=\"http://plato.stanford.edu/entries/moral-motivation/\">motivation</a>, <a href=\"http://www.amazon.com/Reasons-Action-David-Sobel/dp/0521877466/\">the sources of normativity</a>, <a href=\"http://plato.stanford.edu/entries/metaethics/#MorEpi\">moral epistemology</a>, and so on.&nbsp;<a href=\"https://sites.google.com/site/nbeckstead/home\">Nick Beckstead</a>&nbsp;recently&nbsp;<a href=\"http://commonsenseatheism.com/?p=12150\">told me</a>&nbsp;he thinks Eliezer's meta-ethical views are similar to&nbsp;<a href=\"http://metapsychology.mentalhelp.net/poc/view_doc.php?type=book&amp;id=3530\">those of Michael Smith</a>, but I'm not seeing it.</p>\n<p>If you think you can help me (and others) understand Eliezer's meta-ethical theory, please leave a comment!</p>\n<p><strong>Update</strong>: <a href=\"/r/discussion/lw/435/what_is_eliezer_yudkowskys_metaethical_theory/3foq\">This comment</a> by Richard Chappell made sense of Eliezer's meta-ethics for me.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z8wZZLeLMJ3NSK7kR": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3R2vH2Ar5AbC9m8Qj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 48, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "5297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 375, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rm8tv9qZ9nwQxhshx", "FaJaCgqBKphrDzDSj", "hc9Eg6erp6hk9bWhn", "JynJ6xfnpq9oN3zpb", "vzLrQaGPa9DNCpuZz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-29T22:24:02.768Z", "modifiedAt": null, "url": null, "title": "Looking for information on scoring calibration", "slug": "looking-for-information-on-scoring-calibration", "viewCount": null, "lastCommentedAt": "2022-01-15T21:50:58.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aH4mjhgqNPyYvJT85/looking-for-information-on-scoring-calibration", "pageUrlRelative": "/posts/aH4mjhgqNPyYvJT85/looking-for-information-on-scoring-calibration", "linkUrl": "https://www.lesswrong.com/posts/aH4mjhgqNPyYvJT85/looking-for-information-on-scoring-calibration", "postedAtFormatted": "Saturday, January 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20information%20on%20scoring%20calibration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20information%20on%20scoring%20calibration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaH4mjhgqNPyYvJT85%2Flooking-for-information-on-scoring-calibration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20information%20on%20scoring%20calibration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaH4mjhgqNPyYvJT85%2Flooking-for-information-on-scoring-calibration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaH4mjhgqNPyYvJT85%2Flooking-for-information-on-scoring-calibration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>There are lots of scoring rules for probability assessments. Log scoring is popular here, and squared error also works.</p>\n<p>But <span style=\"text-decoration: underline;\"><strong>if</strong></span> I understand these correctly, they are combined measurements of both domain-ability and calibration. For example, if several people took a test on which they had to estimate their confidence in their answers to certain true or false questions about history, then well-calibrated people would have a low squared error, but so would people who know a lot about history.</p>\n<p>So (I think) someone who always said 70% confidence and got 70% of the questions right would get a higher score than someone who always said 60% confidence and got 60% of the questions right, even though they are both equally well calibrated.</p>\n<p>The only pure calibration estimates I've ever seen are calibration curves in the form of a set of ordered pairs, or those limited to a specific point on the cuve (eg \"if ey says ey's 90% sure, ey's only right 60% of the time\"). There should be a way to take the area under (or over) the curve to get a single value representing total calibration, but I'm not familiar with the method or whether it's been done before. Is there an accepted way to get single-number calibration scores separate from domain knowledge?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aH4mjhgqNPyYvJT85", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.731426318588859e-07, "legacy": true, "legacyId": "5298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-30T19:14:21.439Z", "modifiedAt": null, "url": null, "title": "Avoid interruptions by time-shifting them", "slug": "avoid-interruptions-by-time-shifting-them", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:29.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JkZqzfnHjW27vYLYi/avoid-interruptions-by-time-shifting-them", "pageUrlRelative": "/posts/JkZqzfnHjW27vYLYi/avoid-interruptions-by-time-shifting-them", "linkUrl": "https://www.lesswrong.com/posts/JkZqzfnHjW27vYLYi/avoid-interruptions-by-time-shifting-them", "postedAtFormatted": "Sunday, January 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Avoid%20interruptions%20by%20time-shifting%20them&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAvoid%20interruptions%20by%20time-shifting%20them%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJkZqzfnHjW27vYLYi%2Favoid-interruptions-by-time-shifting-them%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Avoid%20interruptions%20by%20time-shifting%20them%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJkZqzfnHjW27vYLYi%2Favoid-interruptions-by-time-shifting-them", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJkZqzfnHjW27vYLYi%2Favoid-interruptions-by-time-shifting-them", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>A lot of times you may come upon a juicy-smelling \"information scent\". Now you have a decision of whether to extend your lunch break and cut into work by following the scent (bad) or abandoning it (painful, and possibly bad).</p>\n<p>One thing that has worked for me is using either delicious or more recently <a href=\"http://www.instapaper.com/\">instapaper </a>app to save the link (instapaper will actually sync the entire article back your iphone/pad/android). Pang of pain is largely avoided, and I can decide later whether this bit of info was worth reading, by comparing it to my other saved articles in the app at the appropriate time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JkZqzfnHjW27vYLYi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.734688389039206e-07, "legacy": true, "legacyId": "5306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-31T06:06:36.097Z", "modifiedAt": null, "url": null, "title": "Berkeley LW Meet-Up Saturday February 5", "slug": "berkeley-lw-meet-up-saturday-february-5", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:33.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LucasSloan", "createdAt": "2009-05-28T05:04:38.345Z", "isAdmin": false, "displayName": "LucasSloan"}, "userId": "ouo6Fqn5kTNY7LvqM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vy4h2CX8KnDXrhZeg/berkeley-lw-meet-up-saturday-february-5", "pageUrlRelative": "/posts/vy4h2CX8KnDXrhZeg/berkeley-lw-meet-up-saturday-february-5", "linkUrl": "https://www.lesswrong.com/posts/vy4h2CX8KnDXrhZeg/berkeley-lw-meet-up-saturday-february-5", "postedAtFormatted": "Monday, January 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Berkeley%20LW%20Meet-Up%20Saturday%20February%205&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABerkeley%20LW%20Meet-Up%20Saturday%20February%205%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvy4h2CX8KnDXrhZeg%2Fberkeley-lw-meet-up-saturday-february-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Berkeley%20LW%20Meet-Up%20Saturday%20February%205%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvy4h2CX8KnDXrhZeg%2Fberkeley-lw-meet-up-saturday-february-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvy4h2CX8KnDXrhZeg%2Fberkeley-lw-meet-up-saturday-february-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>EDIT:&nbsp; We are not at the VLSB, we are at the Free Speech Cafe.</p>\n<p>After this month's hiatus in Berkeley Meetups, I'm quite sure that people are itching to get together with their fellow LWers.&nbsp; As usual, we'll meet at the Starbucks at <span dir=\"ltr\"><a href=\"http://maps.google.com/maps?f=d&amp;source=s_d&amp;saddr=Downtown+Berkeley+BART&amp;daddr=2128+Oxford+St,+Berkeley,+CA+94704-1311+%28Starbucks%29&amp;geocode=FSzZQQIdbVa2-Ck7jvjKnX6FgDG3LOQ7rN5ZmA%3BFW7bQQIdQl62-CEGuQ_bapaUqCmz2L6SnX6FgDHtCjCMFeuX-A&amp;hl=en&amp;mra=ltm&amp;dirflg=w&amp;sll=37.86999,-122.26696&amp;sspn=0.001931,0.005284&amp;ie=UTF8&amp;ll=37.870225,-122.266577&amp;spn=0.001931,0.005284&amp;z=18\">2128 Oxford Street</a> at 7 pm, then move into the atrium in the Valley Life Sciences Building on the Berkeley Campus.&nbsp; There'll be an expedition to get food, so don't feel that you have to eat before hand.</span></p>\n<p>I want to make a special plea to the lurkers on lesswrong to come, I promise no one will think poorly of you if you're still in High School, aren't a member of Mensa, or haven't read the sequences.&nbsp; This is a social event where we get together with people who like rationality, not a way for the shadowy cabal of beisutsukai to puzzle out the secret of <a href=\"/lw/qt/class_project/\">quantum gravity</a><em>.</em>&nbsp; Even if you only just linked here from Methods of Rationality, I and everyone else here wants to get to know you.</p>\n<p>Hope to see you all there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vy4h2CX8KnDXrhZeg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 6.736391209739671e-07, "legacy": true, "legacyId": "5308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAXrEpF5FYjwqKMfZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-31T07:06:39.673Z", "modifiedAt": null, "url": null, "title": "Reply to Stuart on anthropics", "slug": "reply-to-stuart-on-anthropics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m8CMHXbrsW5tjjjv4/reply-to-stuart-on-anthropics", "pageUrlRelative": "/posts/m8CMHXbrsW5tjjjv4/reply-to-stuart-on-anthropics", "linkUrl": "https://www.lesswrong.com/posts/m8CMHXbrsW5tjjjv4/reply-to-stuart-on-anthropics", "postedAtFormatted": "Monday, January 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reply%20to%20Stuart%20on%20anthropics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReply%20to%20Stuart%20on%20anthropics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8CMHXbrsW5tjjjv4%2Freply-to-stuart-on-anthropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reply%20to%20Stuart%20on%20anthropics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8CMHXbrsW5tjjjv4%2Freply-to-stuart-on-anthropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8CMHXbrsW5tjjjv4%2Freply-to-stuart-on-anthropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p>You wake up in a hospital bed, remembering nothing of your past life.&nbsp;A stranger sits beside the bed, smiling. He says:</p>\n<p>\"I happen to know an amusing story about you. Many years ago, before you were born, your parents were arguing about how many kids to have. They settled on flipping a coin. If the coin came up heads, they would have one child. If it came up tails, they would have ten.\"</p>\n<p>\"I will tell you which way the coin came up in a minute. But first let's play a little game. Would you like a small piece of chocolate, or a big tasty cake? There's a catch though: if you choose the cake, you will only receive it if you're the <em>only</em> child of your parents.\"</p>\n<p>Stuart Armstrong has proposed a <a href=\"/r/discussion/lw/40m/three_easy_anthropic_models_and_two_hard_ones/\">solution</a> to this problem (see the fourth model in his post). Namely, you switch to caring about the <em>average</em> that all kids receive in your branch. This doesn't change the utility all kids get in all possible worlds, but makes the problem amenable to UDT, which says all agents would have precommitted to choosing cake as long as it's better than two pieces of chocolate (the first model in Stuart's post).</p>\n<p>But.</p>\n<p>Creating two physically separate worlds with probability 50% should be decision-theoretically equivalent to creating them both with probability 100%. In other words, a correct solution should still work if the coin is quantum. In other words, the problem should be equivalent to creating 11 kids, offering them chocolate or cake, and giving cake only if you're the first kid. But would you really choose cake in this case, knowing that you could get the chocolate for certain? What if there were 1001 kids? This is a hard bullet to swallow, and it seems to suggest that Stuart's analysis of his <em>first</em> model may be incorrect.</p>\n<p>I await comments from Stuart or anyone else who can figure this out.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m8CMHXbrsW5tjjjv4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.736548046031216e-07, "legacy": true, "legacyId": "5311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8GWfZgcS9pHg34r2m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-31T12:50:17.783Z", "modifiedAt": null, "url": null, "title": "Optimal Employment", "slug": "optimal-employment", "viewCount": null, "lastCommentedAt": "2022-03-02T15:52:47.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jtedBLdducritm8y6/optimal-employment", "pageUrlRelative": "/posts/jtedBLdducritm8y6/optimal-employment", "linkUrl": "https://www.lesswrong.com/posts/jtedBLdducritm8y6/optimal-employment", "postedAtFormatted": "Monday, January 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimal%20Employment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimal%20Employment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjtedBLdducritm8y6%2Foptimal-employment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimal%20Employment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjtedBLdducritm8y6%2Foptimal-employment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjtedBLdducritm8y6%2Foptimal-employment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3033, "htmlBody": "<p>Related to: <a href=\"/lw/38u/best_career_models_for_doing_research/\">Best career models for doing research?</a>, <a href=\"/lw/2qp/virtual_employment_open_thread/\">(Virtual) Employment Open Thread</a></p>\n<p>In the spirit of offering some <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\" target=\"_blank\">practical real world advice</a>, let's talk about <em style=\"font-style:italic\">employment rationality</em>. Let&rsquo;s talk about <em style=\"font-style:italic\">optimal employment</em>.<sup>1</sup></p>\n<p><a href=\"/lw/fk/survey_results/\" target=\"_blank\">You're young, smart</a>, and hoping to have a <a href=\"/lw/373/how_to_save_the_world/\" target=\"_blank\">positive impact on the world</a>. Maybe you finished college, maybe you didn't. You want to <a href=\"/lw/2qp/virtual_employment_open_thread/\" target=\"_blank\">pay your bills</a> but also have <a href=\"/lw/38u/best_career_models_for_doing_research/\" target=\"_blank\">time to pursue your intellectual goals</a>. You want a low-stress job that doesn't leave you drained at the end of the day. And it would be nice to earn lots of extra money, because <em style=\"font-style:italic\"><a href=\"/lw/3gj/efficient_charity_do_unto_others/\" target=\"_blank\">whatever</a></em> you value, <a href=\"/lw/65/money_the_unit_of_caring/\" target=\"_blank\">money</a> tends to be a good way to get it.</p>\n<p>And it <em>is</em> possible to find easily obtained, low-stress jobs with flexible hours that allow you to save as much money as someone in the USA making $100,000/yr... if you leave the USA to look for them.</p>\n<p>Your instinctive reaction is probably that there&rsquo;s <a href=\"http://en.wikipedia.org/wiki/There_ain't_no_such_thing_as_a_free_lunch\" target=\"_blank\">no free lunch</a>, so I must be mistaken or <a href=\"http://www.amazon.com/Sham-Self-Help-Movement-America-Helpless/dp/1400054109/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\" target=\"_blank\">dishonest</a>. And while you may have the right <a href=\"http://commonsenseatheism.com/?p=13156\" target=\"_blank\">prior</a>, I hope to <a href=\"#exist\">persuade you that these jobs exist</a> and <a href=\"#how\">tell you how to get one</a> if you're interested.</p>\n<p>This, I think, is a <a href=\"/#lw\" target=\"_blank\">special opportunity for rationalists</a>, an illustration that we <em>can</em> get better life outcomes from our investment in rationality - better outcomes such as low-stress jobs that leave us with ample discretionary income and enough free time to pursue whatever else we're interested in, obtained by being willing to break habits and think in numbers.</p>\n<h4 style=\"font-size:14px;color:black;float:none\"><a id=\"more\"></a>Employment Biases</h4>\n<p>First, consider some cognitive biases that may be leading people <em>away from</em>&nbsp;optimal employment.</p>\n<ol style=\"margin-top:10px;margin-right:2em;margin-bottom:10px;margin-left:2em;list-style-type:decimal;list-style-position:outside\">\n<li><a href=\"http://wiki.lesswrong.com/wiki/Status_quo_bias\" target=\"_blank\">Status Quo Bias</a> - As a rule, we work the same jobs we worked the day before regardless of whether it's still the best option for us. Almost everyone you know is doing the same thing. So you shouldn't expect to be able to just copy others' behavior and end up with optimal employment. Most people <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\" target=\"_blank\">stop searching too soon</a>.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Money_illusion\" target=\"_blank\">Money Illusion</a> - We reason in terms of <em style=\"font-style:italic\">nominal</em> salaries rather than in <em>actual buying power</em>. This causes us to chase high nominal salaries ($100,000/yr!) even when those salaries are coupled with an exceptionally high cost of living that decreases our overall buying power.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Ostrich_effect\" target=\"_blank\">Ostrich Effect</a> - Regardless of income, the average American ends up paying close to <a href=\"http://www.nber.org/papers/w12533\" target=\"_blank\">40% in taxes</a>&nbsp;yet consistently self-reports as <a href=\"http://www.bls.gov/cex/2009/share/age.pdf\" target=\"_blank\">paying only 3%</a>. In other words, we tend to ignore or deny obviously negative situations when we feel we can't change them.</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Conformity_bias\" target=\"_blank\">Conformity Bias</a>, <a href=\"http://en.wikipedia.org/wiki/Herd_instinct\" target=\"_blank\">Herd Instinct</a>, \"Keeping Up with the Joneses\" - Our mimicking behavior is so hard-wired that maintaining autonomy requires actively guarding against the usual practice of mindlessly working the same sorts of jobs and buying the same sorts of products as other people around us. At Less Wrong, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\" target=\"_blank\">we are not conformists</a>, which is probably a good thing in this case. Americans' <a href=\"http://en.wikipedia.org/wiki/Revealed_preference\" target=\"_blank\">revealed preferences</a> indicate that they mostly care about <a href=\"http://www.visualeconomics.com/how-the-average-us-consumer-spends-their-paycheck/\" target=\"_blank\">boring things</a> like paying <a href=\"http://www.nber.org/papers/w12533\" target=\"_blank\">lots of taxes</a>, having <a href=\"http://en.wikipedia.org/wiki/Average_Joe\" target=\"_blank\">fat mortgages</a>, driving <a href=\"http://www.autospies.com/news/study-finds-americans-own-2-28-vehicles-per-household-26437/\" target=\"_blank\">2 cars</a>, and owning <a href=\"http://blog.nielsen.com/nielsenwire/media_entertainment/more-than-half-the-homes-in-us-have-three-or-more-tvs/\" target=\"_blank\">3 televisions</a>.</li>\n<li><a href=\"http://www.bsfrey.ch/articles/C_481_08.pdf\" target=\"_blank\">Commuting Paradox</a> - A <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Stutzer-Frey-Commuting-Doesnt-Pay.pdf\">recently uncovered bias</a> finds that people will consistently endure unpleasant commutes even when the increased earnings don't compensate for the increased costs. A person with a one-hour commute has to earn <em>40 percent more money</em> just to be as satisfied with life as someone who walks to work. And no amount of money can erase the cognitive fatigue caused by commute related stress. Koslowsky <a href=\"http://www.amazon.com/Commuting-Stress-Effects-Methods-Springer/dp/0306450372/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">found</a> that even a short commute or using public transportation is associated with increased blood pressure, musculoskeletal disorders, increased hostility, lateness, absenteeism, and <span style=\"font-weight:bold\">adverse effects on cognitive performance</span>.</li>\n</ol>\n<p>So the literature on biases is telling us that the ideal job would be something that few Americans are doing, has high purchasing power at the expense of a high nominal salary, will be taxed less than a US-based job, avoids a commute, and minimizes the costs that <a href=\"http://www.visualeconomics.com/how-the-average-us-consumer-spends-their-paycheck/\" target=\"_blank\">eat up a typical American's salary</a>.</p>\n<h4 style=\"font-size:14px;color:black;float:none\"><a style=\"width: 12px; line-height: 6px; overflow: hidden; padding-left: 12px;\" name=\"exist\"></a><br />Welcome to Australia</h4>\n<p>The USA is not the best place to earn money.<sup>2</sup> My own experience suggests that at least Japan, New Zealand, and Australia can all be better. This may be shocking, but young professionals with advanced degrees can earn more <em style=\"font-style:italic\">discretionary income</em> as a receptionist or a bartender in the Australian outback than as, say, a software engineer in the USA.</p>\n<p>Now I&rsquo;ll detail how to work abroad <em style=\"font-style:italic\">in Australia</em> because (1) I did it myself (here's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Australia-Paycheck.jpg\" target=\"_blank\">my first paycheck</a>), and because (2) I've met hundreds of people working less desirable jobs in several other countries so I have some basis for recommending Australia in particular.</p>\n<p>Quick facts:</p>\n<ol style=\"margin-top:10px;margin-right:2em;margin-bottom:10px;margin-left:2em;list-style-type:decimal;list-style-position:outside\">\n<li>Australian dollars are currently worth slightly <em style=\"font-style:italic\">more</em> than US dollars.</li>\n<li>The minimum wage in Australia is $15/hr, with $18.75/hr being a more typical starting salary for someone with a Work and Holiday Visa with&nbsp;<em style=\"font-style:italic\">no previous work experience</em>.</li>\n<li>Employers are required to pay 9% extra beyond your regular wage into a personal retirement account, called&nbsp;<a href=\"http://workstay.com.au/Superannuation-Australia\" target=\"_blank\">superannuation</a>, which you can <a href=\"http://www.ato.gov.au/superfunds/pathway.asp?pc=001/149/021&amp;alias=departaustralia\" target=\"_blank\">fully cash out</a> after leaving Australia, even if you aren&rsquo;t retirement age.</li>\n<li>Tax withholdings are fully refundable to foreigners after leaving the country (0% effective tax rate) <em style=\"font-style:italic\">if</em> you earn <a href=\"http://www.workstay.com.au/working_holiday_tax.htm\">less than $6000 and report as a \"resident\" for tax purposes</a>. If you earn between $6000 and $37,000 and file as a resident, your tax rate is 15% (for every dollar earned over $6000). See the full tax structure <a href=\"http://www.ato.gov.au/individuals/content.asp?doc=/content/12333.htm&amp;mnu=42904&amp;mfp=001/002\" target=\"_blank\">here</a>.</li>\n<li>Hospitality employers such as resorts and hotels in remote areas like the Australian outback are <a href=\"http://www.fwa.gov.au/documents/modern_awards/pdf/MA000009.pdf\" target=\"_blank\">required</a> to provide heavily subsidized <a href=\"http://www.fwa.gov.au/documents/modern_awards/award/ma000009/default.htm\" target=\"_blank\">room</a> &amp; <a href=\"http://www.fwa.gov.au/documents/modern_awards/award/ma000009/default.htm\" target=\"_blank\">board</a> ($75/week) and pay supplemental wages in the form of \"<a href=\"http://www.cpsu.org.au/multiversions/18495/FileName/Remote_localities_APS_Award_provisions.pdf\" target=\"_blank\">district allowances</a>\" to all workers.</li>\n<li>For reference, I was hired as a bartender in Australia <em>on the spot</em> with no resume, no application, and no interview after openly admitting I had no service experience and couldn't operate a cash register.</li>\n</ol>\n<p>So let&rsquo;s compare and contrast Australia with the US:</p>\n<table border=\"1\" cellspacing=\"0\" cellpadding=\"8\" width=\"700\" align=\"center\">\n<tbody>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">&nbsp;</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><strong style=\"font-weight:bold\">US</strong></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><strong style=\"font-weight:bold\">Australia</strong></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Minimum wage</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><a href=\"http://en.wikipedia.org/wiki/Minimum_wage_in_the_United_States\" target=\"_blank\">$7.25</a>/hr</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">$15.00/hr<sup>3</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">High paying jobs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">Require advanced degree, hard to get, stressful</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">Require no qualifications, easy to get, little responsibility</td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Income taxes</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">21.25% of income<sup>4</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">13% of income<span style=\"font-size:9px\"><sup>5</sup></span></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Housing costs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">37.1% of income<sup>6</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">5% of income<sup>7</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Food costs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">13.3% of income<sup>6</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">5% of income<sup>7</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Transportation costs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">16.5% of income (need a car)<sup>6</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">4% of income (airplane tickets, visas)<sup>8</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Compulsory retirement savings</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">\n<p>-<a href=\"http://en.wikipedia.org/wiki/Social_Security_(United_States)#Tax_on_wages_and_self-employment_income\" target=\"_blank\">7.65%</a> of your income into Social Security<br /> <a href=\"http://articles.chicagotribune.com/2010-09-16/business/sc-cons-0916-moneytips-20100916_1_social-security-entitlement-program-douglas-elmendorf\" target=\"_blank\">good luck getting that back</a></p>\n</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">\n<p>+<a href=\"http://en.wikipedia.org/wiki/Superannuation_in_Australia\" target=\"_blank\">9%</a> extra income paid by employer on top of wages<br /><a href=\"http://www.ato.gov.au/superfunds/pathway.asp?pc=001/149/021&amp;alias=departaustralia\" target=\"_blank\">refundable when you leave the country</a>!</p>\n</td>\n</tr>\n</tbody>\n</table>\n<h4 style=\"font-size:14px;color:black;float:none\"><a style=\"width: 12px; line-height: 6px; overflow: hidden; padding-left: 12px;\" name=\"12dd81d1d6d5eb30_lw\"></a><br />Optimal Employment and Less Wrong</h4>\n<p>This may be of special interest to Less Wrong because most non-rationalists simply can&rsquo;t reliably take advantage of this opportunity. They will see it as \"too good to be true\" or \"some sketchy advice from the internet\" and move on with their lives.&nbsp;<em style=\"font-style:italic\">You</em>, on the other hand, can evaluate the evidence and make a decision. This kind of problem, where you must assess probabilities and come to a sound conclusion because immediate feedback is unavailable, is <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\" target=\"_blank\"><em style=\"font-style:italic\">exactly</em> the kind of problem that rationality is good for</a>.</p>\n<p>Let's compare the <em style=\"font-style:italic\">discretionary income</em> you're likely to earn with a stressful $100,000/yr salaried job in the USA to the discretionary income you're likely to earn with a laid-back $39,000/yr job in Australia. Our time frame will be one year.</p>\n<p><strong style=\"font-weight:bold\">USA</strong>: In a $100,000/yr position, your top end tax braket is <a href=\"http://www.moneychimp.com/features/tax_brackets.htm\" target=\"_blank\">28%</a>, but after taking the \"standard deduction\" and accounting for the tiered tax structure, your effective income tax rate is only 21.25%. In our target age range of 25-34, you're <a href=\"http://www.bls.gov/cex/2009/share/age.pdf\" target=\"_blank\">likely to spend</a> 37.1% of your income on housing (breakdown: 23.3% on rent, 7% on utilities, 6.8% on misc housing expenses), 13.3% on food, 16.5% on transportation, and 7.65% on social security payroll taxes. For convenience sake, we'll call the remaining portion of your income - 4.2% - your discretionary income. 4.2% of $100,000 is $4,200.</p>\n<p><strong style=\"font-weight:bold\">Australia</strong>: In a $39,000/yr position as a bartender or receptionist in the Australian outback, you'll pay 13% in taxes but immediately gain back 9% by cashing in your employer-provided retirement benefits upon leaving the country. Because room and board is heavily subsidized in the outback, you'll pay only 5% for housing and 5% for three excellent meals a day. You'll be commuting on foot because you'll live by the hotel or resort that you work at so the only transportation you'll need is a couple airplane tickets and legal documents, which will cost about 4% of the $39,000 yearly salary. That leaves you with a stagering 83% of your income as true discretionary income, or $32,370!</p>\n<p>So working in Australia at a laid-back job with no responsibilities will likely earn you significantly more discretionary income than working at a hard-to-get, stressful, \"high-paying\" US job. In addition to the personal enjoyment of traveling to Australia, working at a resort in the outback will provide you with a comfortable living situation where all your bills are paid for, all your housing and meals are provided for you, you have no commute and you can enjoy 83% of your $39k salary as discretionary income.</p>\n<p>On the other hand, a typical year working a stressful $100k/yr job in the US, if you&rsquo;re highly-qualified and fortunate enough to land one, will mostly create value for the US government, real estate owners via your rent payments, oil producing nations whenever you fill up your car to commute to work, and retailers such as WalMart who provide household necessities from overseas suppliers. You definitely \"create value\" by earning that $100k and then immediately blowing it all back out into these giant&nbsp;economic&nbsp;sectors, but are you really executing your own values, or just the values of those around you? Assuming you're not a tax, rent, and car payment enthusiast, this arrangement is probably sub-optimal. That's why you need to learn...</p>\n<h4 style=\"font-size:14px;color:black;float:none\"><a style=\"width: 12px; line-height: 6px; overflow: hidden; padding-left: 12px;\" name=\"how\"></a><br />How to Work in Australia</h4>\n<p>In six steps:</p>\n<ol style=\"margin-top:10px;margin-right:2em;margin-bottom:10px;margin-left:2em;list-style-type:decimal;list-style-position:outside\">\n<li>Find an <a href=\"http://www.jobsplus.com.au/index.php?action=search&amp;2=hospitality&amp;15=nt\" target=\"_blank\">Australian hospitality job</a> (or wait until you arrive; see below).</li>\n<li>Make sure you have a <a href=\"http://travel.state.gov/content/passports/english/passports.html\" target=\"_blank\">passport</a>.</li>\n<li>Apply for a <a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/\" target=\"_blank\">Work and Holiday visa</a>.<sup>9</sup></li>\n<li><a href=\"http://www.skyscanner.net/\" target=\"_blank\">Fly to Australia</a> to start working and saving.</li>\n<li>Apply for an Australian <a href=\"http://www.ato.gov.au/individuals/content.asp?doc=/content/38760.htm\" target=\"_blank\">tax file number</a>.</li>\n<li>Open a <a href=\"http://www.nab.com.au/wps/wcm/connect/nab/nab/home/personal_finance/5/2/1\" target=\"_blank\">bank account</a>.</li>\n</ol>\n<p>Though you may want to \"play it safe,\" most of the jobs available in the Australian outback will <em style=\"font-style:italic\">not</em> be listed online. My own recommendation is to <strong style=\"font-weight:bold\">skip step 1</strong> and <em style=\"font-style:italic\">don't</em> get a job lined up ahead of time. Fly to Alice Springs in May when the high season for hospitality jobs is starting to pick up, check into a hostel for a few nights, and look through the physical job boards. The person at the front desk of any hostel will be able to tell you where they are. That's what everyone I met working in Australia actually did to land jobs.</p>\n<p>Or, if this sounds <a href=\"/lw/f1/beware_trivial_inconveniences/\">too overwhelming</a>, have someone else <a href=\"http://ozworkvisa.com/\">do the planning for you</a>. The site <a href=\"http://ozworkvisa.com/\">Oz Work Visa</a> was founded by a fellow LWer (who went to Australia after reading this post) to help other&nbsp;<a href=\"/lw/37f/efficient_charity\" target=\"_blank\">optimal philanthropists</a> and rationalists have a smoother time planning their working holiday abroad in Australia. I definitely recommend the services.</p>\n<p>&nbsp;</p>\n<h4 style=\"font-size:14px;color:black;float:none\">Conclusion</h4>\n<p>Of course, each person must assess the expected utility of this opportunity for themselves. Maybe you have a child or significant other you can't leave behind. Maybe you live with your parents, so you aren't spending much on housing or food in the US, and therefore staying in the US is the quickest way to build up discretionary income. Maybe the math above doesn't work for your particular situation. The USA's financial incentive system is extremely complex and, in the words of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Kotlikoff-Rapson-Does-it-pay-at-the-margin-to-work-and-save.pdf\" target=\"_blank\">Kotlikoff &amp; Rapson</a>, \"bizarre.\"</p>\n<p>I don't mean to say that this opportunity <em style=\"font-style:italic\">will</em> be best for the average Less Wrong reader who is single and in his or her 20s. But I <em style=\"font-style:italic\">do </em>want to present one particular opportunity that may offer more optimal employment than whatever you're <em style=\"font-style:italic\">currently </em>doing for a paycheck. I'd also like to suggest that in general, <em style=\"font-style:italic\">optimal employment might not be found in your home country</em>.</p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\">Here's me enjoying some optimal employment in Australia:</p>\n<p align=\"center\"><img src=\"http://seventeenorbust.com/images/australia_bar_500.jpg\" alt=\"Me, working at an Australian bar\" width=\"500\" height=\"375\" /></p>\n<p>&nbsp;</p>\n<h4 style=\"font-size:14px;color:black;float:none\">Common Concerns</h4>\n<p><strong style=\"font-weight:bold\">Q</strong>: This sounds too good to be true. How could there be this wage and cost of living imbalance? Shouldn&rsquo;t the efficient market conspire to eliminate this huge pile of \"free profit\"?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: The efficient market hypothesis assumes that&nbsp;humans collectively converge on rational beliefs. But most <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\" target=\"_blank\">humans aren&rsquo;t strategic enough</a> to take advantage of an opportunity like this. They&rsquo;re on a treadmill from high school to college to a nominally \"high-paying\" USA-based job + spouse + 1.5 children + dog. The thought of working outside their own country or outside of the field they got a degree in never seriously occurs to them, no matter how smart they are. Also, many people currently believe that&nbsp;working abroad is a bad idea&nbsp;simply because the best opportunities that existed 5 years ago (teaching English, peace corps) actually <em>were</em> bad economic opportunities.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: So how come all my smart friends aren't doing this?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: Americans couldn't get these work visas until 2007. Since then, <a href=\"http://www.immi.gov.au/media/statistics/country-profiles/textversion/usa.htm\" target=\"_blank\">less than 8,000 Americans</a> have taken advantage of the program. Your odds of knowing an American aged 18-30 who went to Australia and did this are very low. By comparison, over <a href=\"http://www.immi.gov.au/media/statistics/country-profiles/textversion/united-kingdom.htm\" target=\"_blank\">170,000 British</a> citizens aged 18-30 went to Australia just in the last 5 years via a nearly identical visa program. Basically, if you're living in the UK, or lots of other countries in Europe or Asia, the evidence is already beating you upside the head that working in Australia is a great way to save money. You already know several people in real life who can stand in front of you and tell you how great it was for them. I predict that working in Australia after college will be a trendy option for Americans in a few more years, but until it's completely obvious to everyone, you'll actually have to <a href=\"/#lw\" target=\"_blank\">look at the evidence</a> and be rational enough to process it without <a href=\"/lw/wj/is_that_your_true_rejection/\" target=\"_blank\">immediately rejecting the idea</a> just because <a href=\"http://en.wikipedia.org/wiki/Belief_bias\" target=\"_blank\">it sounds amazing</a>.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: Won&rsquo;t working in Australia prevent me from gaining experience in my narrow professional sub-field, thus reducing my total lifetime earning power?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: This is almost certainly not the case for anyone under 30. Companies pay professionals more based on their abilities and their age as opposed to their actual years of experience. And, they pay more for older professionals than young ones just starting out cause they know these people really do have higher expenses and are less likely to quit. So taking a year off in your 20s to work abroad is only exchanging a year in which you would have earned the lowest salary you&rsquo;ll ever have during your career for a year of higher earning power in Australia. You can always come back to your career in a year and pick up where you left off. Besides; who follows a straight-up-the-ladder career path anymore? Almost nobody.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: What about Australian culture? Will I like it over there?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: Australia is a <a href=\"http://www.nationmaster.com/graph/edu_exp_dur_of_edu_for_all_stu-education-expected-duration-all-students\" target=\"_blank\">highly educated</a>, <a href=\"http://en.wikipedia.org/wiki/Religion_in_Australia\" target=\"_blank\">robustly secular</a>, <a href=\"http://hdr.undp.org/en/statistics/\" target=\"_blank\">extremely developed</a> country. If you have any questions about the desirability of Australia, just ask Less Wrong! A disproportionate number of Less Wrongers are Australian.<sup>10</sup></p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: I'm really bad at following instructions. What are the biggest mistakes I could make?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: Don't make the mistake of settling down to work in Sydney, Melbourne or any other major Australian city. Those places all have exceptionally high costs of living, fewer job opportunities, no housing and meal benefits, and predictably lower pay. Make sure you travel to a <em>remote</em> area of Australia like I suggest. Go to the outback near <a href=\"http://en.wikipedia.org/wiki/Alice_Springs\">Alice Springs</a> or at least outside <a href=\"http://en.wikipedia.org/wiki/Darwin,_Northern_Territory\">Darwin</a> or <a href=\"http://en.wikipedia.org/wiki/Perth,_Western_Australia\">Perth</a> if you read up on it yourself and know what you're doing. Also, I recommend going to Australia in May when hiring is strongest. April can work too. June and July will work also. &nbsp;Just don't go in February or March when people aren't hiring yet. One last tip: it's very expensive to be a tourist in Australia (how do you think you're being paid so much?) so I recommend that if you want to combine this opportunity with a vacation for yourself, fly to Bali or somewhere else in Southeast Asia where your money will go 10x further.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: It says the $295 Visa application fee is non-refundable. What if I <a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/how-to-apply.htm\">apply for the Work and Holiday Visa</a> and then I don't get it?</p>\n<p><strong style=\"font-weight:bold\">A</strong>:&nbsp;Did you read through the&nbsp;<a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/eligibility.htm\">eligibility requirements</a>&nbsp;to make sure you qualify? If you meet their requirements, you'll get it. Australia&nbsp;rubber-stamps&nbsp;American Visa applications. They're working hard to admit as many of us as possible since so few Americans apply to vacation or work in Australia and they want us to be better represented. The application is painless and I was issued my visa in less than 24 hours.</p>\n<p>&nbsp;</p>\n<h4 style=\"font-size:14px;color:black;float:none\">Notes</h4>\n<p><span style=\"font-size: 11px;\"><span style=\"font-size: small;\"><sup>1</sup>&nbsp;Many thanks to&nbsp;<a href=\"/user/lukeprog\" target=\"_blank\">lukeprog</a>&nbsp;for his help in writing this article.</span></span></p>\n<p><sup>2</sup> Note to international readers living outside the US: Although I write much of this from the perspective of an American considering the possibility of working abroad, you can easily substitute &ldquo;the UK&rdquo; or any other first-world nation wherever I say &ldquo;America&rdquo; or &ldquo;the US&rdquo;.</p>\n<p><span style=\"font-size: 11px;\"><span style=\"font-size: small;\"><sup>3</sup>&nbsp;Australian minimum wage is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Minimum_wage_law#Australia\" target=\"_blank\">$15/hr</a>&nbsp;AUD. Right now,&nbsp;<a href=\"http://www.exchange-rates.org/history/AUD/USD/G/M\" target=\"_blank\">the exchange rate between AUD and USD</a>&nbsp;is basically equal.</span></span></p>\n<p><sup>4</sup> Approximate, based on the third tax bracket in the year 2011. See the USA rates <a href=\"http://www.moneychimp.com/features/tax_brackets.htm\" target=\"_blank\">here</a>, but note that these are nominal tax rates. The actual tax rate is lower due to the standard deductions.</p>\n<p><sup>5</sup> An estimate, assuming you file as a resident for tax purposes and earn between $6,000 and $37,000. You are taxed at 0% for the first $6,000 earned, and at 15% for your earnings between $6,000 and $37,000. See <a href=\"http://www.ato.gov.au/individuals/content.asp?doc=/content/12333.htm&amp;mnu=42904&amp;mfp=001/002\" target=\"_blank\">here</a> for the details.</p>\n<p><sup>6</sup> See the 25-34 year-old age bracket from the latest <a href=\"http://www.bls.gov/cex/2009/share/age.pdf\" target=\"_blank\">Consumer Expenditure Survey</a>.</p>\n<p><sup>7</sup> Of course, this depends on how much you make, and is assuming you use the highly subsidized room and board offered to you in the Australian outback.</p>\n<p><sup>8</sup>&nbsp;My real world costs of going to/from Australia:</p>\n<p>$1166 round-trip ticket SFO - MEL (<a href=\"http://skyscanner.com/\" target=\"_blank\"><span>skyscanner.com</span></a>)</p>\n<p>$196 MEL - ASP (<a href=\"http://tigerairways.com/\" target=\"_blank\"><span>tigerairways.com</span></a>)</p>\n<p><span>$235&nbsp;<a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/\" target=\"_blank\">Work and Holiday visa</a></span></p>\n<p>This might look like a lot of money if you&rsquo;re not currently working or if you&rsquo;re a broke college grad, but it only takes about 2 weeks on the job in Australia to earn back the cost of emigration, repatriation, and valid work papers.</p>\n<p><sup>9</sup>&nbsp;Australian &ldquo;Work and Holiday&rdquo; visas are only available to those 18-30. If you&rsquo;re almost 31, you can still apply for the visa now, have it issued before you turn 31, and then travel to Australia after you turn 31.</p>\n<p><sup>10</sup>&nbsp;Based on traffic data for Less Wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "fkABsGCJZ6y9qConW": 1, "o4rMP6GJto7ccBL3a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jtedBLdducritm8y6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 104, "baseScore": 78, "extendedScore": null, "score": 0.000149, "legacy": true, "legacyId": "5314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 81, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"/lw/38u/best_career_models_for_doing_research/\">Best career models for doing research?</a>, <a href=\"/lw/2qp/virtual_employment_open_thread/\">(Virtual) Employment Open Thread</a></p>\n<p>In the spirit of offering some <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\" target=\"_blank\">practical real world advice</a>, let's talk about <em style=\"font-style:italic\">employment rationality</em>. Let\u2019s talk about <em style=\"font-style:italic\">optimal employment</em>.<sup>1</sup></p>\n<p><a href=\"/lw/fk/survey_results/\" target=\"_blank\">You're young, smart</a>, and hoping to have a <a href=\"/lw/373/how_to_save_the_world/\" target=\"_blank\">positive impact on the world</a>. Maybe you finished college, maybe you didn't. You want to <a href=\"/lw/2qp/virtual_employment_open_thread/\" target=\"_blank\">pay your bills</a> but also have <a href=\"/lw/38u/best_career_models_for_doing_research/\" target=\"_blank\">time to pursue your intellectual goals</a>. You want a low-stress job that doesn't leave you drained at the end of the day. And it would be nice to earn lots of extra money, because <em style=\"font-style:italic\"><a href=\"/lw/3gj/efficient_charity_do_unto_others/\" target=\"_blank\">whatever</a></em> you value, <a href=\"/lw/65/money_the_unit_of_caring/\" target=\"_blank\">money</a> tends to be a good way to get it.</p>\n<p>And it <em>is</em> possible to find easily obtained, low-stress jobs with flexible hours that allow you to save as much money as someone in the USA making $100,000/yr... if you leave the USA to look for them.</p>\n<p>Your instinctive reaction is probably that there\u2019s <a href=\"http://en.wikipedia.org/wiki/There_ain't_no_such_thing_as_a_free_lunch\" target=\"_blank\">no free lunch</a>, so I must be mistaken or <a href=\"http://www.amazon.com/Sham-Self-Help-Movement-America-Helpless/dp/1400054109/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\" target=\"_blank\">dishonest</a>. And while you may have the right <a href=\"http://commonsenseatheism.com/?p=13156\" target=\"_blank\">prior</a>, I hope to <a href=\"#exist\">persuade you that these jobs exist</a> and <a href=\"#how\">tell you how to get one</a> if you're interested.</p>\n<p>This, I think, is a <a href=\"/#lw\" target=\"_blank\">special opportunity for rationalists</a>, an illustration that we <em>can</em> get better life outcomes from our investment in rationality - better outcomes such as low-stress jobs that leave us with ample discretionary income and enough free time to pursue whatever else we're interested in, obtained by being willing to break habits and think in numbers.</p>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"Employment_Biases\"><a id=\"more\"></a>Employment Biases</h4>\n<p>First, consider some cognitive biases that may be leading people <em>away from</em>&nbsp;optimal employment.</p>\n<ol style=\"margin-top:10px;margin-right:2em;margin-bottom:10px;margin-left:2em;list-style-type:decimal;list-style-position:outside\">\n<li><a href=\"http://wiki.lesswrong.com/wiki/Status_quo_bias\" target=\"_blank\">Status Quo Bias</a> - As a rule, we work the same jobs we worked the day before regardless of whether it's still the best option for us. Almost everyone you know is doing the same thing. So you shouldn't expect to be able to just copy others' behavior and end up with optimal employment. Most people <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\" target=\"_blank\">stop searching too soon</a>.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Money_illusion\" target=\"_blank\">Money Illusion</a> - We reason in terms of <em style=\"font-style:italic\">nominal</em> salaries rather than in <em>actual buying power</em>. This causes us to chase high nominal salaries ($100,000/yr!) even when those salaries are coupled with an exceptionally high cost of living that decreases our overall buying power.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Ostrich_effect\" target=\"_blank\">Ostrich Effect</a> - Regardless of income, the average American ends up paying close to <a href=\"http://www.nber.org/papers/w12533\" target=\"_blank\">40% in taxes</a>&nbsp;yet consistently self-reports as <a href=\"http://www.bls.gov/cex/2009/share/age.pdf\" target=\"_blank\">paying only 3%</a>. In other words, we tend to ignore or deny obviously negative situations when we feel we can't change them.</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Conformity_bias\" target=\"_blank\">Conformity Bias</a>, <a href=\"http://en.wikipedia.org/wiki/Herd_instinct\" target=\"_blank\">Herd Instinct</a>, \"Keeping Up with the Joneses\" - Our mimicking behavior is so hard-wired that maintaining autonomy requires actively guarding against the usual practice of mindlessly working the same sorts of jobs and buying the same sorts of products as other people around us. At Less Wrong, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\" target=\"_blank\">we are not conformists</a>, which is probably a good thing in this case. Americans' <a href=\"http://en.wikipedia.org/wiki/Revealed_preference\" target=\"_blank\">revealed preferences</a> indicate that they mostly care about <a href=\"http://www.visualeconomics.com/how-the-average-us-consumer-spends-their-paycheck/\" target=\"_blank\">boring things</a> like paying <a href=\"http://www.nber.org/papers/w12533\" target=\"_blank\">lots of taxes</a>, having <a href=\"http://en.wikipedia.org/wiki/Average_Joe\" target=\"_blank\">fat mortgages</a>, driving <a href=\"http://www.autospies.com/news/study-finds-americans-own-2-28-vehicles-per-household-26437/\" target=\"_blank\">2 cars</a>, and owning <a href=\"http://blog.nielsen.com/nielsenwire/media_entertainment/more-than-half-the-homes-in-us-have-three-or-more-tvs/\" target=\"_blank\">3 televisions</a>.</li>\n<li><a href=\"http://www.bsfrey.ch/articles/C_481_08.pdf\" target=\"_blank\">Commuting Paradox</a> - A <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Stutzer-Frey-Commuting-Doesnt-Pay.pdf\">recently uncovered bias</a> finds that people will consistently endure unpleasant commutes even when the increased earnings don't compensate for the increased costs. A person with a one-hour commute has to earn <em>40 percent more money</em> just to be as satisfied with life as someone who walks to work. And no amount of money can erase the cognitive fatigue caused by commute related stress. Koslowsky <a href=\"http://www.amazon.com/Commuting-Stress-Effects-Methods-Springer/dp/0306450372/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">found</a> that even a short commute or using public transportation is associated with increased blood pressure, musculoskeletal disorders, increased hostility, lateness, absenteeism, and <span style=\"font-weight:bold\">adverse effects on cognitive performance</span>.</li>\n</ol>\n<p>So the literature on biases is telling us that the ideal job would be something that few Americans are doing, has high purchasing power at the expense of a high nominal salary, will be taxed less than a US-based job, avoids a commute, and minimizes the costs that <a href=\"http://www.visualeconomics.com/how-the-average-us-consumer-spends-their-paycheck/\" target=\"_blank\">eat up a typical American's salary</a>.</p>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"Welcome_to_Australia\"><a style=\"width: 12px; line-height: 6px; overflow: hidden; padding-left: 12px;\" name=\"exist\"></a><br>Welcome to Australia</h4>\n<p>The USA is not the best place to earn money.<sup>2</sup> My own experience suggests that at least Japan, New Zealand, and Australia can all be better. This may be shocking, but young professionals with advanced degrees can earn more <em style=\"font-style:italic\">discretionary income</em> as a receptionist or a bartender in the Australian outback than as, say, a software engineer in the USA.</p>\n<p>Now I\u2019ll detail how to work abroad <em style=\"font-style:italic\">in Australia</em> because (1) I did it myself (here's <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Australia-Paycheck.jpg\" target=\"_blank\">my first paycheck</a>), and because (2) I've met hundreds of people working less desirable jobs in several other countries so I have some basis for recommending Australia in particular.</p>\n<p>Quick facts:</p>\n<ol style=\"margin-top:10px;margin-right:2em;margin-bottom:10px;margin-left:2em;list-style-type:decimal;list-style-position:outside\">\n<li>Australian dollars are currently worth slightly <em style=\"font-style:italic\">more</em> than US dollars.</li>\n<li>The minimum wage in Australia is $15/hr, with $18.75/hr being a more typical starting salary for someone with a Work and Holiday Visa with&nbsp;<em style=\"font-style:italic\">no previous work experience</em>.</li>\n<li>Employers are required to pay 9% extra beyond your regular wage into a personal retirement account, called&nbsp;<a href=\"http://workstay.com.au/Superannuation-Australia\" target=\"_blank\">superannuation</a>, which you can <a href=\"http://www.ato.gov.au/superfunds/pathway.asp?pc=001/149/021&amp;alias=departaustralia\" target=\"_blank\">fully cash out</a> after leaving Australia, even if you aren\u2019t retirement age.</li>\n<li>Tax withholdings are fully refundable to foreigners after leaving the country (0% effective tax rate) <em style=\"font-style:italic\">if</em> you earn <a href=\"http://www.workstay.com.au/working_holiday_tax.htm\">less than $6000 and report as a \"resident\" for tax purposes</a>. If you earn between $6000 and $37,000 and file as a resident, your tax rate is 15% (for every dollar earned over $6000). See the full tax structure <a href=\"http://www.ato.gov.au/individuals/content.asp?doc=/content/12333.htm&amp;mnu=42904&amp;mfp=001/002\" target=\"_blank\">here</a>.</li>\n<li>Hospitality employers such as resorts and hotels in remote areas like the Australian outback are <a href=\"http://www.fwa.gov.au/documents/modern_awards/pdf/MA000009.pdf\" target=\"_blank\">required</a> to provide heavily subsidized <a href=\"http://www.fwa.gov.au/documents/modern_awards/award/ma000009/default.htm\" target=\"_blank\">room</a> &amp; <a href=\"http://www.fwa.gov.au/documents/modern_awards/award/ma000009/default.htm\" target=\"_blank\">board</a> ($75/week) and pay supplemental wages in the form of \"<a href=\"http://www.cpsu.org.au/multiversions/18495/FileName/Remote_localities_APS_Award_provisions.pdf\" target=\"_blank\">district allowances</a>\" to all workers.</li>\n<li>For reference, I was hired as a bartender in Australia <em>on the spot</em> with no resume, no application, and no interview after openly admitting I had no service experience and couldn't operate a cash register.</li>\n</ol>\n<p>So let\u2019s compare and contrast Australia with the US:</p>\n<table border=\"1\" cellspacing=\"0\" cellpadding=\"8\" width=\"700\" align=\"center\">\n<tbody>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">&nbsp;</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><strong style=\"font-weight:bold\">US</strong></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><strong style=\"font-weight:bold\">Australia</strong></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Minimum wage</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><a href=\"http://en.wikipedia.org/wiki/Minimum_wage_in_the_United_States\" target=\"_blank\">$7.25</a>/hr</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">$15.00/hr<sup>3</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">High paying jobs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">Require advanced degree, hard to get, stressful</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">Require no qualifications, easy to get, little responsibility</td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Income taxes</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">21.25% of income<sup>4</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">13% of income<span style=\"font-size:9px\"><sup>5</sup></span></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Housing costs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">37.1% of income<sup>6</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">5% of income<sup>7</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Food costs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">13.3% of income<sup>6</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">5% of income<sup>7</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Transportation costs</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">16.5% of income (need a car)<sup>6</sup></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">4% of income (airplane tickets, visas)<sup>8</sup></td>\n</tr>\n<tr>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\"><em style=\"font-style:italic\">Compulsory retirement savings</em></td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">\n<p>-<a href=\"http://en.wikipedia.org/wiki/Social_Security_(United_States)#Tax_on_wages_and_self-employment_income\" target=\"_blank\">7.65%</a> of your income into Social Security<br> <a href=\"http://articles.chicagotribune.com/2010-09-16/business/sc-cons-0916-moneytips-20100916_1_social-security-entitlement-program-douglas-elmendorf\" target=\"_blank\">good luck getting that back</a></p>\n</td>\n<td style=\"color:#000000;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:10px;margin-top:8px;margin-right:8px;margin-bottom:8px;margin-left:8px\">\n<p>+<a href=\"http://en.wikipedia.org/wiki/Superannuation_in_Australia\" target=\"_blank\">9%</a> extra income paid by employer on top of wages<br><a href=\"http://www.ato.gov.au/superfunds/pathway.asp?pc=001/149/021&amp;alias=departaustralia\" target=\"_blank\">refundable when you leave the country</a>!</p>\n</td>\n</tr>\n</tbody>\n</table>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"Optimal_Employment_and_Less_Wrong\"><a style=\"width: 12px; line-height: 6px; overflow: hidden; padding-left: 12px;\" name=\"12dd81d1d6d5eb30_lw\"></a><br>Optimal Employment and Less Wrong</h4>\n<p>This may be of special interest to Less Wrong because most non-rationalists simply can\u2019t reliably take advantage of this opportunity. They will see it as \"too good to be true\" or \"some sketchy advice from the internet\" and move on with their lives.&nbsp;<em style=\"font-style:italic\">You</em>, on the other hand, can evaluate the evidence and make a decision. This kind of problem, where you must assess probabilities and come to a sound conclusion because immediate feedback is unavailable, is <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\" target=\"_blank\"><em style=\"font-style:italic\">exactly</em> the kind of problem that rationality is good for</a>.</p>\n<p>Let's compare the <em style=\"font-style:italic\">discretionary income</em> you're likely to earn with a stressful $100,000/yr salaried job in the USA to the discretionary income you're likely to earn with a laid-back $39,000/yr job in Australia. Our time frame will be one year.</p>\n<p><strong style=\"font-weight:bold\">USA</strong>: In a $100,000/yr position, your top end tax braket is <a href=\"http://www.moneychimp.com/features/tax_brackets.htm\" target=\"_blank\">28%</a>, but after taking the \"standard deduction\" and accounting for the tiered tax structure, your effective income tax rate is only 21.25%. In our target age range of 25-34, you're <a href=\"http://www.bls.gov/cex/2009/share/age.pdf\" target=\"_blank\">likely to spend</a> 37.1% of your income on housing (breakdown: 23.3% on rent, 7% on utilities, 6.8% on misc housing expenses), 13.3% on food, 16.5% on transportation, and 7.65% on social security payroll taxes. For convenience sake, we'll call the remaining portion of your income - 4.2% - your discretionary income. 4.2% of $100,000 is $4,200.</p>\n<p><strong style=\"font-weight:bold\">Australia</strong>: In a $39,000/yr position as a bartender or receptionist in the Australian outback, you'll pay 13% in taxes but immediately gain back 9% by cashing in your employer-provided retirement benefits upon leaving the country. Because room and board is heavily subsidized in the outback, you'll pay only 5% for housing and 5% for three excellent meals a day. You'll be commuting on foot because you'll live by the hotel or resort that you work at so the only transportation you'll need is a couple airplane tickets and legal documents, which will cost about 4% of the $39,000 yearly salary. That leaves you with a stagering 83% of your income as true discretionary income, or $32,370!</p>\n<p>So working in Australia at a laid-back job with no responsibilities will likely earn you significantly more discretionary income than working at a hard-to-get, stressful, \"high-paying\" US job. In addition to the personal enjoyment of traveling to Australia, working at a resort in the outback will provide you with a comfortable living situation where all your bills are paid for, all your housing and meals are provided for you, you have no commute and you can enjoy 83% of your $39k salary as discretionary income.</p>\n<p>On the other hand, a typical year working a stressful $100k/yr job in the US, if you\u2019re highly-qualified and fortunate enough to land one, will mostly create value for the US government, real estate owners via your rent payments, oil producing nations whenever you fill up your car to commute to work, and retailers such as WalMart who provide household necessities from overseas suppliers. You definitely \"create value\" by earning that $100k and then immediately blowing it all back out into these giant&nbsp;economic&nbsp;sectors, but are you really executing your own values, or just the values of those around you? Assuming you're not a tax, rent, and car payment enthusiast, this arrangement is probably sub-optimal. That's why you need to learn...</p>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"How_to_Work_in_Australia\"><a style=\"width: 12px; line-height: 6px; overflow: hidden; padding-left: 12px;\" name=\"how\"></a><br>How to Work in Australia</h4>\n<p>In six steps:</p>\n<ol style=\"margin-top:10px;margin-right:2em;margin-bottom:10px;margin-left:2em;list-style-type:decimal;list-style-position:outside\">\n<li>Find an <a href=\"http://www.jobsplus.com.au/index.php?action=search&amp;2=hospitality&amp;15=nt\" target=\"_blank\">Australian hospitality job</a> (or wait until you arrive; see below).</li>\n<li>Make sure you have a <a href=\"http://travel.state.gov/content/passports/english/passports.html\" target=\"_blank\">passport</a>.</li>\n<li>Apply for a <a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/\" target=\"_blank\">Work and Holiday visa</a>.<sup>9</sup></li>\n<li><a href=\"http://www.skyscanner.net/\" target=\"_blank\">Fly to Australia</a> to start working and saving.</li>\n<li>Apply for an Australian <a href=\"http://www.ato.gov.au/individuals/content.asp?doc=/content/38760.htm\" target=\"_blank\">tax file number</a>.</li>\n<li>Open a <a href=\"http://www.nab.com.au/wps/wcm/connect/nab/nab/home/personal_finance/5/2/1\" target=\"_blank\">bank account</a>.</li>\n</ol>\n<p>Though you may want to \"play it safe,\" most of the jobs available in the Australian outback will <em style=\"font-style:italic\">not</em> be listed online. My own recommendation is to <strong style=\"font-weight:bold\">skip step 1</strong> and <em style=\"font-style:italic\">don't</em> get a job lined up ahead of time. Fly to Alice Springs in May when the high season for hospitality jobs is starting to pick up, check into a hostel for a few nights, and look through the physical job boards. The person at the front desk of any hostel will be able to tell you where they are. That's what everyone I met working in Australia actually did to land jobs.</p>\n<p>Or, if this sounds <a href=\"/lw/f1/beware_trivial_inconveniences/\">too overwhelming</a>, have someone else <a href=\"http://ozworkvisa.com/\">do the planning for you</a>. The site <a href=\"http://ozworkvisa.com/\">Oz Work Visa</a> was founded by a fellow LWer (who went to Australia after reading this post) to help other&nbsp;<a href=\"/lw/37f/efficient_charity\" target=\"_blank\">optimal philanthropists</a> and rationalists have a smoother time planning their working holiday abroad in Australia. I definitely recommend the services.</p>\n<p>&nbsp;</p>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"Conclusion\">Conclusion</h4>\n<p>Of course, each person must assess the expected utility of this opportunity for themselves. Maybe you have a child or significant other you can't leave behind. Maybe you live with your parents, so you aren't spending much on housing or food in the US, and therefore staying in the US is the quickest way to build up discretionary income. Maybe the math above doesn't work for your particular situation. The USA's financial incentive system is extremely complex and, in the words of <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Kotlikoff-Rapson-Does-it-pay-at-the-margin-to-work-and-save.pdf\" target=\"_blank\">Kotlikoff &amp; Rapson</a>, \"bizarre.\"</p>\n<p>I don't mean to say that this opportunity <em style=\"font-style:italic\">will</em> be best for the average Less Wrong reader who is single and in his or her 20s. But I <em style=\"font-style:italic\">do </em>want to present one particular opportunity that may offer more optimal employment than whatever you're <em style=\"font-style:italic\">currently </em>doing for a paycheck. I'd also like to suggest that in general, <em style=\"font-style:italic\">optimal employment might not be found in your home country</em>.</p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\">Here's me enjoying some optimal employment in Australia:</p>\n<p align=\"center\"><img src=\"http://seventeenorbust.com/images/australia_bar_500.jpg\" alt=\"Me, working at an Australian bar\" width=\"500\" height=\"375\"></p>\n<p>&nbsp;</p>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"Common_Concerns\">Common Concerns</h4>\n<p><strong style=\"font-weight:bold\">Q</strong>: This sounds too good to be true. How could there be this wage and cost of living imbalance? Shouldn\u2019t the efficient market conspire to eliminate this huge pile of \"free profit\"?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: The efficient market hypothesis assumes that&nbsp;humans collectively converge on rational beliefs. But most <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\" target=\"_blank\">humans aren\u2019t strategic enough</a> to take advantage of an opportunity like this. They\u2019re on a treadmill from high school to college to a nominally \"high-paying\" USA-based job + spouse + 1.5 children + dog. The thought of working outside their own country or outside of the field they got a degree in never seriously occurs to them, no matter how smart they are. Also, many people currently believe that&nbsp;working abroad is a bad idea&nbsp;simply because the best opportunities that existed 5 years ago (teaching English, peace corps) actually <em>were</em> bad economic opportunities.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: So how come all my smart friends aren't doing this?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: Americans couldn't get these work visas until 2007. Since then, <a href=\"http://www.immi.gov.au/media/statistics/country-profiles/textversion/usa.htm\" target=\"_blank\">less than 8,000 Americans</a> have taken advantage of the program. Your odds of knowing an American aged 18-30 who went to Australia and did this are very low. By comparison, over <a href=\"http://www.immi.gov.au/media/statistics/country-profiles/textversion/united-kingdom.htm\" target=\"_blank\">170,000 British</a> citizens aged 18-30 went to Australia just in the last 5 years via a nearly identical visa program. Basically, if you're living in the UK, or lots of other countries in Europe or Asia, the evidence is already beating you upside the head that working in Australia is a great way to save money. You already know several people in real life who can stand in front of you and tell you how great it was for them. I predict that working in Australia after college will be a trendy option for Americans in a few more years, but until it's completely obvious to everyone, you'll actually have to <a href=\"/#lw\" target=\"_blank\">look at the evidence</a> and be rational enough to process it without <a href=\"/lw/wj/is_that_your_true_rejection/\" target=\"_blank\">immediately rejecting the idea</a> just because <a href=\"http://en.wikipedia.org/wiki/Belief_bias\" target=\"_blank\">it sounds amazing</a>.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: Won\u2019t working in Australia prevent me from gaining experience in my narrow professional sub-field, thus reducing my total lifetime earning power?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: This is almost certainly not the case for anyone under 30. Companies pay professionals more based on their abilities and their age as opposed to their actual years of experience. And, they pay more for older professionals than young ones just starting out cause they know these people really do have higher expenses and are less likely to quit. So taking a year off in your 20s to work abroad is only exchanging a year in which you would have earned the lowest salary you\u2019ll ever have during your career for a year of higher earning power in Australia. You can always come back to your career in a year and pick up where you left off. Besides; who follows a straight-up-the-ladder career path anymore? Almost nobody.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: What about Australian culture? Will I like it over there?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: Australia is a <a href=\"http://www.nationmaster.com/graph/edu_exp_dur_of_edu_for_all_stu-education-expected-duration-all-students\" target=\"_blank\">highly educated</a>, <a href=\"http://en.wikipedia.org/wiki/Religion_in_Australia\" target=\"_blank\">robustly secular</a>, <a href=\"http://hdr.undp.org/en/statistics/\" target=\"_blank\">extremely developed</a> country. If you have any questions about the desirability of Australia, just ask Less Wrong! A disproportionate number of Less Wrongers are Australian.<sup>10</sup></p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: I'm really bad at following instructions. What are the biggest mistakes I could make?</p>\n<p><strong style=\"font-weight:bold\">A</strong>: Don't make the mistake of settling down to work in Sydney, Melbourne or any other major Australian city. Those places all have exceptionally high costs of living, fewer job opportunities, no housing and meal benefits, and predictably lower pay. Make sure you travel to a <em>remote</em> area of Australia like I suggest. Go to the outback near <a href=\"http://en.wikipedia.org/wiki/Alice_Springs\">Alice Springs</a> or at least outside <a href=\"http://en.wikipedia.org/wiki/Darwin,_Northern_Territory\">Darwin</a> or <a href=\"http://en.wikipedia.org/wiki/Perth,_Western_Australia\">Perth</a> if you read up on it yourself and know what you're doing. Also, I recommend going to Australia in May when hiring is strongest. April can work too. June and July will work also. &nbsp;Just don't go in February or March when people aren't hiring yet. One last tip: it's very expensive to be a tourist in Australia (how do you think you're being paid so much?) so I recommend that if you want to combine this opportunity with a vacation for yourself, fly to Bali or somewhere else in Southeast Asia where your money will go 10x further.</p>\n<p>&nbsp;</p>\n<p><strong style=\"font-weight:bold\">Q</strong>: It says the $295 Visa application fee is non-refundable. What if I <a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/how-to-apply.htm\">apply for the Work and Holiday Visa</a> and then I don't get it?</p>\n<p><strong style=\"font-weight:bold\">A</strong>:&nbsp;Did you read through the&nbsp;<a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/eligibility.htm\">eligibility requirements</a>&nbsp;to make sure you qualify? If you meet their requirements, you'll get it. Australia&nbsp;rubber-stamps&nbsp;American Visa applications. They're working hard to admit as many of us as possible since so few Americans apply to vacation or work in Australia and they want us to be better represented. The application is painless and I was issued my visa in less than 24 hours.</p>\n<p>&nbsp;</p>\n<h4 style=\"font-size:14px;color:black;float:none\" id=\"Notes\">Notes</h4>\n<p><span style=\"font-size: 11px;\"><span style=\"font-size: small;\"><sup>1</sup>&nbsp;Many thanks to&nbsp;<a href=\"/user/lukeprog\" target=\"_blank\">lukeprog</a>&nbsp;for his help in writing this article.</span></span></p>\n<p><sup>2</sup> Note to international readers living outside the US: Although I write much of this from the perspective of an American considering the possibility of working abroad, you can easily substitute \u201cthe UK\u201d or any other first-world nation wherever I say \u201cAmerica\u201d or \u201cthe US\u201d.</p>\n<p><span style=\"font-size: 11px;\"><span style=\"font-size: small;\"><sup>3</sup>&nbsp;Australian minimum wage is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Minimum_wage_law#Australia\" target=\"_blank\">$15/hr</a>&nbsp;AUD. Right now,&nbsp;<a href=\"http://www.exchange-rates.org/history/AUD/USD/G/M\" target=\"_blank\">the exchange rate between AUD and USD</a>&nbsp;is basically equal.</span></span></p>\n<p><sup>4</sup> Approximate, based on the third tax bracket in the year 2011. See the USA rates <a href=\"http://www.moneychimp.com/features/tax_brackets.htm\" target=\"_blank\">here</a>, but note that these are nominal tax rates. The actual tax rate is lower due to the standard deductions.</p>\n<p><sup>5</sup> An estimate, assuming you file as a resident for tax purposes and earn between $6,000 and $37,000. You are taxed at 0% for the first $6,000 earned, and at 15% for your earnings between $6,000 and $37,000. See <a href=\"http://www.ato.gov.au/individuals/content.asp?doc=/content/12333.htm&amp;mnu=42904&amp;mfp=001/002\" target=\"_blank\">here</a> for the details.</p>\n<p><sup>6</sup> See the 25-34 year-old age bracket from the latest <a href=\"http://www.bls.gov/cex/2009/share/age.pdf\" target=\"_blank\">Consumer Expenditure Survey</a>.</p>\n<p><sup>7</sup> Of course, this depends on how much you make, and is assuming you use the highly subsidized room and board offered to you in the Australian outback.</p>\n<p><sup>8</sup>&nbsp;My real world costs of going to/from Australia:</p>\n<p>$1166 round-trip ticket SFO - MEL (<a href=\"http://skyscanner.com/\" target=\"_blank\"><span>skyscanner.com</span></a>)</p>\n<p>$196 MEL - ASP (<a href=\"http://tigerairways.com/\" target=\"_blank\"><span>tigerairways.com</span></a>)</p>\n<p><span>$235&nbsp;<a href=\"http://www.immi.gov.au/visitors/working-holiday/462/usa/\" target=\"_blank\">Work and Holiday visa</a></span></p>\n<p>This might look like a lot of money if you\u2019re not currently working or if you\u2019re a broke college grad, but it only takes about 2 weeks on the job in Australia to earn back the cost of emigration, repatriation, and valid work papers.</p>\n<p><sup>9</sup>&nbsp;Australian \u201cWork and Holiday\u201d visas are only available to those 18-30. If you\u2019re almost 31, you can still apply for the visa now, have it issued before you turn 31, and then travel to Australia after you turn 31.</p>\n<p><sup>10</sup>&nbsp;Based on traffic data for Less Wrong.</p>", "sections": [{"title": "Employment Biases", "anchor": "Employment_Biases", "level": 1}, {"title": "Welcome to Australia", "anchor": "Welcome_to_Australia", "level": 1}, {"title": "Optimal Employment and Less Wrong", "anchor": "Optimal_Employment_and_Less_Wrong", "level": 1}, {"title": "How to Work in Australia", "anchor": "How_to_Work_in_Australia", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Common Concerns", "anchor": "Common_Concerns", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "281 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 281, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rNkFLv9tXzq8Lrvrc", "9bTNcSpNBdPpyocMK", "uFYQaGCRwt3wKtyZP", "ZWC3n9c6v4s35rrZ3", "TrmMcujGZt5JAtMGg", "pC47ZTsPNAkjavkXs", "ZpDnRCeef2CLEFeKM", "L32LHWzy9FzSDazEg", "7FzD7pNm9X68Gp5ZC", "7dRGYDqA2z6Zt7Q4h", "reitXJgJXFzKpdKyd", "FCxHgPsDScx4C3H8n", "PBRWb2Em5SNeWYwwB", "TGux5Fhcd7GmTfNGC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-31T16:28:15.334Z", "modifiedAt": null, "url": null, "title": "Counterfactual Calculation and Observational Knowledge", "slug": "counterfactual-calculation-and-observational-knowledge", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:46.730Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TuwDjY4nXn3jjdpn5/counterfactual-calculation-and-observational-knowledge", "pageUrlRelative": "/posts/TuwDjY4nXn3jjdpn5/counterfactual-calculation-and-observational-knowledge", "linkUrl": "https://www.lesswrong.com/posts/TuwDjY4nXn3jjdpn5/counterfactual-calculation-and-observational-knowledge", "postedAtFormatted": "Monday, January 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Counterfactual%20Calculation%20and%20Observational%20Knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACounterfactual%20Calculation%20and%20Observational%20Knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTuwDjY4nXn3jjdpn5%2Fcounterfactual-calculation-and-observational-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Counterfactual%20Calculation%20and%20Observational%20Knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTuwDjY4nXn3jjdpn5%2Fcounterfactual-calculation-and-observational-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTuwDjY4nXn3jjdpn5%2Fcounterfactual-calculation-and-observational-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 449, "htmlBody": "<p>Consider the following thought experiment (\"Counterfactual Calculation\"):</p>\n<blockquote>\n<p>You are taking a test, which includes a question: \"Is Q an even number?\", where Q is a complicated formula that resolves to some natural number. There is no a priori reason for you to expect that Q is more likely even or odd, and the formula is too complicated to compute the number (or its parity) on your own. Fortunately, you have an old calculator, which you can use to type in the formula and observe the parity of the result on display. This calculator is not very reliable, and is only correct 99% of the time, furthermore its errors are stochastic (or even involve quantum randomness), so for any given problem statement, it's probably correct but has a chance of making an error. You type in the formula and observe the result (it's \"even\"). You're now 99% sure that the answer is \"even\", so naturally you write that down on the test sheet.</p>\n<p>Then, unsurprisingly, Omega (a trustworthy all-powerful device) appears and presents you with the following decision. Consider the counterfactual where the calculator displayed \"odd\" instead of \"even\", after you've just typed in the (same) formula Q, on the same occasion (i.e. all possible worlds that fit this description). The counterfactual diverges only in the calculator showing a different result (and what follows). You are to determine what is to be written (by Omega, at your command) as the final answer to the same question on the test sheet in that counterfactual (the actions of your counterfactual self who takes the test in the counterfactual are ignored).</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>Should you write \"even\" on the counterfactual test sheet, given that you're 99% sure that the answer is \"even\"?</p>\n<p>This thought experiment contrasts \"logical knowledge\" (the usual kind) and \"observational knowledge\" (what you get when you look at a calculator display). The kind of knowledge you obtain by observing things is not like the kind of knowledge you obtain by thinking yourself. What is the difference (if there actually is a difference)? Why does observational knowledge work in your own possible worlds, but not in counterfactuals? How much of logical knowledge is like observational knowledge, and what are the conditions of its applicability? Can things that we consider \"logical knowledge\" fail to apply to some counterfactuals?</p>\n<p>(<a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless</a> analysis would say \"observational knowledge is not knowledge\" or that it's knowledge only in the sense that you should bet a certain way. This doesn't analyze the intuition of knowing the result after looking at a calculator display. There is a very salient sense in which the result becomes known, and the purpose of this thought experiment is to explore some of counterintuitive properties of such knowledge.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TuwDjY4nXn3jjdpn5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 20, "extendedScore": null, "score": 6.738013571207153e-07, "legacy": true, "legacyId": "5316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 188, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-31T17:31:45.220Z", "modifiedAt": null, "url": null, "title": "[Link] Cool or creepy?", "slug": "link-cool-or-creepy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:29.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LeTnPSjNExntidkjZ/link-cool-or-creepy", "pageUrlRelative": "/posts/LeTnPSjNExntidkjZ/link-cool-or-creepy", "linkUrl": "https://www.lesswrong.com/posts/LeTnPSjNExntidkjZ/link-cool-or-creepy", "postedAtFormatted": "Monday, January 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Cool%20or%20creepy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Cool%20or%20creepy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeTnPSjNExntidkjZ%2Flink-cool-or-creepy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Cool%20or%20creepy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeTnPSjNExntidkjZ%2Flink-cool-or-creepy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeTnPSjNExntidkjZ%2Flink-cool-or-creepy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>Why is <a href=\"http://blogs.plos.org/wonderland/2011/01/31/paradise-for-zombies-and-neuroscience-nerds/\">this</a>&nbsp;collection of vat-brains described as \"cool\" when cryonics - frozen, severed heads - is described as \"creepy\"?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LeTnPSjNExntidkjZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.738180763168553e-07, "legacy": true, "legacyId": "5317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-01-31T21:24:12.458Z", "modifiedAt": null, "url": null, "title": "Google and charity", "slug": "google-and-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:29.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6R2kjNMPrjwWwfuDL/google-and-charity", "pageUrlRelative": "/posts/6R2kjNMPrjwWwfuDL/google-and-charity", "linkUrl": "https://www.lesswrong.com/posts/6R2kjNMPrjwWwfuDL/google-and-charity", "postedAtFormatted": "Monday, January 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Google%20and%20charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoogle%20and%20charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6R2kjNMPrjwWwfuDL%2Fgoogle-and-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Google%20and%20charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6R2kjNMPrjwWwfuDL%2Fgoogle-and-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6R2kjNMPrjwWwfuDL%2Fgoogle-and-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p><a href=\"http://www.nytimes.com/2011/01/30/business/30charity.html?_r=1&amp;hp\">They seem to have money they don't know what to do with.</a></p>\n<p>This doesn't mean SIAI is a good match, but it might be worth thinking about.</p>\n<p>I'd say the most obvious choice for google and existential threats would be (if feasible) applying cloud computing to watching for potential asteroid strikes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6R2kjNMPrjwWwfuDL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 6.738788102455414e-07, "legacy": true, "legacyId": "5319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-01T04:05:43.179Z", "modifiedAt": null, "url": null, "title": "Starting a LW meet-up is easy.", "slug": "starting-a-lw-meet-up-is-easy", "viewCount": null, "lastCommentedAt": "2021-10-14T18:19:52.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d28mWBMrFt8nwpXLp/starting-a-lw-meet-up-is-easy", "pageUrlRelative": "/posts/d28mWBMrFt8nwpXLp/starting-a-lw-meet-up-is-easy", "linkUrl": "https://www.lesswrong.com/posts/d28mWBMrFt8nwpXLp/starting-a-lw-meet-up-is-easy", "postedAtFormatted": "Tuesday, February 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Starting%20a%20LW%20meet-up%20is%20easy.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStarting%20a%20LW%20meet-up%20is%20easy.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd28mWBMrFt8nwpXLp%2Fstarting-a-lw-meet-up-is-easy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Starting%20a%20LW%20meet-up%20is%20easy.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd28mWBMrFt8nwpXLp%2Fstarting-a-lw-meet-up-is-easy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd28mWBMrFt8nwpXLp%2Fstarting-a-lw-meet-up-is-easy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>All you need to do is:</p>\n<ol>\n<li>Pick a time.&nbsp; Weekend afternoons or evenings work well.</li>\n<li>Pick a place.&nbsp; This can be a coffee shop or casual restaurant (e.g., a pizza place or pub) or a classroom or other on-campus location.&nbsp; Best if it isn&rsquo;t too noisy.</li>\n<li>Announce the time and place on LW, a week or so ahead of time, using the \"Add new meetup\" link near your username.</li>\n<li>Show up yourself, with a sign that says &ldquo;Less Wrong Meet-up&rdquo;.</li>\n</ol>\n<p class=\"p1\">That&rsquo;s all -- anything else is optional.&nbsp; If folks come, just say &ldquo;Hi, my name&rsquo;s [whatever your name is]&rdquo;, and see where the conversation goes.&nbsp; Most major cities, and many minor ones, have some LW-ers.&nbsp; And if no one comes, all it cost you was a few hours of reading a book in a restaurant.&nbsp; You don&rsquo;t need to have a LW history; many a lurker has enjoyed in-person LW conversation (and the folks who actually show up to meet-ups are often less intimidating than those who post on the main site).</p>\n<p class=\"p2\">Meet-ups are fun, and the simple act of talking to other LW-ers (in person, where your primate brain can see that they&rsquo;re real) can help: (a) you become a better rationalist; (b) other attendees become better rationalists; and (c) LW become a stronger community.</p>\n<p class=\"p2\">Also, if anyone is interested in starting a meet-up but wants to discuss it with someone first, I'd be happy to help. &nbsp;There is also a good&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources \">meet-up resources page</a>.</p>\n<p>(This was discussed a bit in <a href=\"/lw/33k/jerusalem_meetup_nov_20/2yk9\">this comment thread</a>, but it seems worth repeating it somewhere where more people might see it, especially since the idea was new to someone at Saturday's H+ conference who now plans to start a meet-up.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d28mWBMrFt8nwpXLp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 54, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "5320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-01T08:11:53.822Z", "modifiedAt": null, "url": null, "title": "Approaching Infinity", "slug": "approaching-infinity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YuZvFMbi668pufiM8/approaching-infinity", "pageUrlRelative": "/posts/YuZvFMbi668pufiM8/approaching-infinity", "linkUrl": "https://www.lesswrong.com/posts/YuZvFMbi668pufiM8/approaching-infinity", "postedAtFormatted": "Tuesday, February 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Approaching%20Infinity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApproaching%20Infinity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYuZvFMbi668pufiM8%2Fapproaching-infinity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Approaching%20Infinity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYuZvFMbi668pufiM8%2Fapproaching-infinity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYuZvFMbi668pufiM8%2Fapproaching-infinity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 554, "htmlBody": "<p>[This is from a very neat example my real analysis professor used some years ago. While I'm fairly confident it's neat, I'm not certain it's top-level-post-worthy. The general point is about problems with applying concepts involving infinity to reality; any advice on content (or formatting!) would be greatly welcomed. My math education basically ended after a few upper division courses, so it's possible there are some notational schemes or methods I am ignorant of.</p>\n<p>I think this is a fun little exercise, if nothing more.]</p>\n<p>The concept of \"infinity\" and \"infinite series\" and sets get thrown around a lot in mathematics and some of philosophy. It's worth trying to put the concept of infinity in perspective before we try to think of things in the real world being \"infinite.\" Warning: this post will involve numbers that are literally too large to comprehend. But that's the point.</p>\n<p>Let us define an operator, /X\\ (\"triangle-X\"). /X\\ = X raised to the X power X times. Thus, /2\\ = 2<sup>2^2</sup>&nbsp;= 2<sup>4 = </sup>16.</p>\n<p>//2\\\\ (\"2-triangle-2\") would do this operation twice.&nbsp;Thus, it would equal /16\\, the value of which we'll get to in a minute.</p>\n<p>We now introduce a new operator, [X] (\"square-X\"). [X] = triangle-X-triangle-X, i.e. X inside of /X\\ triangles. [2] = ////////////////2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\<br />We can introduce another operator, [X&gt; (\"pentagon-X\"). [X&gt; = X inside of [X] squares. I believe this would be \"square-X-square-X\").<br />...</p>\n<p><strong>[Edited for clarity]</strong><br />I'll spare the next [X] operators, and go right to (X) (\"circle-X\"). Technically, it's whole-lot-of-sides-polygon-X - we could continue this process indefinitely - but we'll call it circle-X, because that's as far as we're going. (X) follows the process that took us from triangle to square to pentagon, iterated an additional [X] times.</p>\n<p>I'll be honest. This got kind of meaningless a bit before [X]. Let's start trying to construct what [2] equals, and you'll see why.</p>\n<p>/2\\=16. So //2\\\\ = /16\\ = 16<sup>16^16^</sup><span style=\"font-size: 11px; vertical-align: super;\">16^16^</span><span style=\"font-size: 11px; vertical-align: super;\">16^16^</span><span style=\"font-size: 11px; vertical-align: super;\">16^16^</span><span style=\"font-size: 11px; vertical-align: super;\">16^16^</span><span style=\"font-size: 11px; vertical-align: super;\">16^16^</span><span style=\"font-size: 11px; vertical-align: super;\">16^16^</span><span style=\"font-size: 11px; vertical-align: super;\">16^16</span>. Using some very rough approximations, we can say this is about&nbsp;10<sup>2x10^19</sup>, or one followed by twenty billion billion zeroes. //16\\\\ is thus one followed by twenty billion billion zeroes, raised to the power of one followed by twenty billion billion zeroes one followed by twenty billion billion zeroes times. My math education could be more complete, but I am not aware of another way to denote such a number. To say it could not be written in scientific notation on a universe-sized sheet of paper is probably a colossal understatement. And after we calculate that number we have to repeat the process thirteen more times to get [2]. We could theoretically keep doing this until we got to (2); (2) is a number that cannot be meaningfully expressed, understood, or calculated by any means that exist today. And there's still ((2)) after that.</p>\n<p>Now, imagine that this period (.) represents zero. Imagine drawing a line from that point to one on the near surface of the sun, which represents infinity (yes, this is improper - it's a finite line - but the point is visualization, so understatement really isn't an issue here). (2) lies within the parentheses surrounding that period, and that's an understatement of how close it is to zero. It really isn't even 1/(2) inches from that period.</p>\n<p>Remember this the next time you ponder the meaning, use, or existence of an infinite set, infinite repetitions, or an infinite time.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YuZvFMbi668pufiM8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 0, "extendedScore": null, "score": 6.740480850828406e-07, "legacy": true, "legacyId": "5322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-01T14:15:33.740Z", "modifiedAt": null, "url": null, "title": "The Urgent Meta-Ethics of Friendly Artificial Intelligence", "slug": "the-urgent-meta-ethics-of-friendly-artificial-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.701Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TKdpSzmcezNbfmGAy/the-urgent-meta-ethics-of-friendly-artificial-intelligence", "pageUrlRelative": "/posts/TKdpSzmcezNbfmGAy/the-urgent-meta-ethics-of-friendly-artificial-intelligence", "linkUrl": "https://www.lesswrong.com/posts/TKdpSzmcezNbfmGAy/the-urgent-meta-ethics-of-friendly-artificial-intelligence", "postedAtFormatted": "Tuesday, February 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Urgent%20Meta-Ethics%20of%20Friendly%20Artificial%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Urgent%20Meta-Ethics%20of%20Friendly%20Artificial%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTKdpSzmcezNbfmGAy%2Fthe-urgent-meta-ethics-of-friendly-artificial-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Urgent%20Meta-Ethics%20of%20Friendly%20Artificial%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTKdpSzmcezNbfmGAy%2Fthe-urgent-meta-ethics-of-friendly-artificial-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTKdpSzmcezNbfmGAy%2Fthe-urgent-meta-ethics-of-friendly-artificial-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 426, "htmlBody": "<p>Barring a major collapse of human civilization (due to nuclear war, asteroid impact, etc.), many experts expect the <a href=\"http://intelligence.org/media/thehumanimportanceoftheintelligenceexplosion\">intelligence explosion Singularity</a> to occur within 50-200 years.</p>\n<p>That fact means that many philosophical problems, about which philosophers have argued for <em>millennia</em>, are suddenly very <em>urgent</em>.</p>\n<p>Those concerned with the fate of the galaxy must say to the philosophers: \"<a href=\"/lw/qt/class_project/\">Too slow</a>! Stop screwing around with <a href=\"http://mind.oxfordjournals.org/content/118/471/811.short\">transcendental ethics</a> and <a href=\"/lw/om/qualitatively_confused/\">qualitative epistemologies</a>! Start thinking with <a href=\"http://commonsenseatheism.com/?p=13737\">the precision of an AI researcher</a>&nbsp;and&nbsp;<em>solve</em>&nbsp;these problems!\"</p>\n<p>If a near-future AI will determine the fate of the galaxy, we need to figure out <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">what values we ought to give it</a>. Should it ensure animal welfare? Is growing the human population a good thing?</p>\n<p>But those are questions of <a href=\"http://www.iep.utm.edu/ethics/#H3\">applied ethics</a>. More fundamental are the questions about which&nbsp;<a href=\"http://www.iep.utm.edu/ethics/#H2\">normative ethics</a>&nbsp;to give the AI: How would the AI <em>decide</em>&nbsp;if animal welfare or large human populations were good? What rulebook should it use to answer novel moral questions that arise in the future?</p>\n<p>But even more fundamental are the questions of <a href=\"http://www.iep.utm.edu/ethics/#H1\">meta-ethics</a>. What do moral terms mean? Do moral facts exist? What justifies one normative rulebook over the other?</p>\n<p>The answers to these meta-ethical questions will determine the answers to the questions of normative ethics, which, if we are successful in <em>planning</em>&nbsp;the intelligence explosion, will determine the fate of the galaxy.</p>\n<p>Eliezer Yudkowsky has put forward <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">one meta-ethical theory</a>, which informs his plan for Friendly AI: <a href=\"http://intelligence.org/upload/coherent-extrapolated-volition.pdf\">Coherent Extrapolated Volition</a>. But what if that meta-ethical theory is wrong? The galaxy is at stake.</p>\n<p>Princeton philosopher Richard Chappell <a href=\"/r/discussion/lw/435/what_is_eliezer_yudkowskys_metaethical_theory/3foq\">worries</a> about how Eliezer's meta-ethical theory depends on <a href=\"http://www.philosophyetc.net/2004/09/naming-and-necessity.html\">rigid designation</a>, which in this context may amount to something like a semantic \"trick.\" Previously and independently, an Oxford philosopher expressed the same worry to me in private.</p>\n<p>Eliezer's theory also <a href=\"/r/discussion/lw/435/what_is_eliezer_yudkowskys_metaethical_theory/3fmj\">employs</a> something like the method of <a href=\"http://en.wikipedia.org/wiki/Reflective_equilibrium\">reflective equilibrium</a>, about which there are <a href=\"http://plato.stanford.edu/entries/reflective-equilibrium/#CriRefEqu\">many grave concerns</a> from Eliezer's fellow naturalists, including <a href=\"http://www.amazon.com/Theory-Good-Right-Richard-Brandt/dp/157392220X/\">Richard Brandt</a>, <a href=\"http://www.jstor.org/pss/2217486\">Richard Hare</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Cummins-Reflections-on-Reflective-Equilibrium.pdf\">Robert Cummins</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Stitch-Reflective-Equilibrium-Analytic-Epistemology-and-the-Problem-of-Cognitive-Diversity.pdf\">Stephen Stich</a>, and others.</p>\n<p>My point is not to beat up on Eliezer's meta-ethical views. I don't even know if they're wrong. Eliezer is wickedly smart. He is highly trained in the skills of overcoming biases and properly proportioning beliefs to the evidence. He thinks with the precision of an AI researcher. <a href=\"http://commonsenseatheism.com/?p=13737\">In my opinion</a>, that gives him large advantages over most philosophers. When Eliezer states and defends a particular view, I take that as significant <a href=\"/lw/jl/what_is_evidence/\">Bayesian evidence</a> for reforming my beliefs.</p>\n<p>Rather, my point is that we need lots of smart people working on these meta-ethical questions. We need to <em>solve</em>&nbsp;these problems, and quickly. The universe will not wait for the pace of traditional philosophy to catch up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z8wZZLeLMJ3NSK7kR": 1, "NLwTnsH9RSotqXYLw": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TKdpSzmcezNbfmGAy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 66, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "5323", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 252, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAXrEpF5FYjwqKMfZ", "BwtBhqvTPGG2n2GuJ", "6s3xABaXKPdFwA3FS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-01T17:46:39.406Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: February 2011", "slug": "rationality-quotes-february-2011", "viewCount": null, "lastCommentedAt": "2021-08-29T00:40:09.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PJqZt7kPGWMgw658F/rationality-quotes-february-2011", "pageUrlRelative": "/posts/PJqZt7kPGWMgw658F/rationality-quotes-february-2011", "linkUrl": "https://www.lesswrong.com/posts/PJqZt7kPGWMgw658F/rationality-quotes-february-2011", "postedAtFormatted": "Tuesday, February 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20February%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20February%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJqZt7kPGWMgw658F%2Frationality-quotes-february-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20February%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJqZt7kPGWMgw658F%2Frationality-quotes-february-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJqZt7kPGWMgw658F%2Frationality-quotes-february-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<p>Take off every 'quote'! You know what you doing. For great insight. Move 'quote'.</p>\n<p>And if you don't:</p>\n<ul style=\"margin: 10px 2em; list-style-type: disc; list-style-position: outside; padding: 0px;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts from LW. (If you want to exclude OB too create your own quotes thread! OB is entertaining and insightful and all but it is no rationality blog!)</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PJqZt7kPGWMgw658F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "5324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 355, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-01T22:13:32.013Z", "modifiedAt": null, "url": null, "title": "Sleeping Beauty", "slug": "sleeping-beauty", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FoFyDzTjvwda7HgE9/sleeping-beauty", "pageUrlRelative": "/posts/FoFyDzTjvwda7HgE9/sleeping-beauty", "linkUrl": "https://www.lesswrong.com/posts/FoFyDzTjvwda7HgE9/sleeping-beauty", "postedAtFormatted": "Tuesday, February 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sleeping%20Beauty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASleeping%20Beauty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoFyDzTjvwda7HgE9%2Fsleeping-beauty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sleeping%20Beauty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoFyDzTjvwda7HgE9%2Fsleeping-beauty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoFyDzTjvwda7HgE9%2Fsleeping-beauty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>Someone comes up to you and tells you he flipped ten coins for ten people. They were fair coins, but only three came up heads. What is the probability yours was heads?</p>\n<p>There are three people of ten who got heads. There is a 30% chance that you're one of those three, right?</p>\n<p>Now take the sleeping beauty paradox. A coin is flipped. If it lands on heads, the subject is woken twice. If it lands on tails, the subject is woken once. For simplicity, assume it happens exactly once, and there are one trillion person-days. You wake up groggy in the morning, and take a second to remember who you are.</p>\n<p>If the coin landed on tails, that would mean that there is a one in a trillion chance that you will remember that you're the subject. If it was heads, it would be two in a trillion. As such,&nbsp; if you do remember being the subject, the probability that it's heads is P(H|U)=P(U|H)*P(H)/[P(U|H)+P(U|T)] = (2/trillion)*(1/2)/[(2/trillion+1/trillion)] =2/3, where H is coin lands on heads, T is coin lands on tails, and U is you are the subject.</p>\n<p>Technically, it would be slightly less than 2/3, since there will be one more person-day if the coin lands on heads.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FoFyDzTjvwda7HgE9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -4, "extendedScore": null, "score": -2.4e-05, "legacy": true, "legacyId": "5326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-02T00:39:04.509Z", "modifiedAt": null, "url": null, "title": "Future of Humanity Institute hiring postdocs from philosophy, math, CS", "slug": "future-of-humanity-institute-hiring-postdocs-from-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ARdey7ci6Ai8XAn6Q/future-of-humanity-institute-hiring-postdocs-from-philosophy", "pageUrlRelative": "/posts/ARdey7ci6Ai8XAn6Q/future-of-humanity-institute-hiring-postdocs-from-philosophy", "linkUrl": "https://www.lesswrong.com/posts/ARdey7ci6Ai8XAn6Q/future-of-humanity-institute-hiring-postdocs-from-philosophy", "postedAtFormatted": "Wednesday, February 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Future%20of%20Humanity%20Institute%20hiring%20postdocs%20from%20philosophy%2C%20math%2C%20CS&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFuture%20of%20Humanity%20Institute%20hiring%20postdocs%20from%20philosophy%2C%20math%2C%20CS%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARdey7ci6Ai8XAn6Q%2Ffuture-of-humanity-institute-hiring-postdocs-from-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Future%20of%20Humanity%20Institute%20hiring%20postdocs%20from%20philosophy%2C%20math%2C%20CS%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARdey7ci6Ai8XAn6Q%2Ffuture-of-humanity-institute-hiring-postdocs-from-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARdey7ci6Ai8XAn6Q%2Ffuture-of-humanity-institute-hiring-postdocs-from-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>The application deadline for the&nbsp;<a style=\"color: #0099ff; text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.fhi.ox.ac.uk/get_involved/future_tech_vacancies/futuretech\">two Research Fellowships</a>&nbsp;with the new Programme on the Impacts of Future Technology, has been extended to&nbsp;<strong style=\"padding: 0px; margin: 0px;\">21 February</strong>.</p>\n<div id=\"sq_news_body\" style=\"padding: 0px; margin: 0px;\">\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding: 0px;\">The posts are:</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding: 0px;\">1.&nbsp;Postdoctoral Research Fellowship, with emphasis on philosophy<br style=\"padding: 0px; margin: 0px;\" />2.&nbsp;Postdoctoral Research Fellowship, with emphasis on computer science, cognitive science, or mathematics</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding: 0px;\">Further details at:&nbsp;<a style=\"color: #0099ff; text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.fhi.ox.ac.uk/get_involved/future_tech_vacancies/futuretech\">www.fhi.ox.ac.uk/get_involved/future_tech_vacancies/futuretech</a>.</p>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ARdey7ci6Ai8XAn6Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.743062306588507e-07, "legacy": true, "legacyId": "5328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-02T04:52:39.226Z", "modifiedAt": null, "url": null, "title": "Is Atheism a failure to distinguish Near and Far?", "slug": "is-atheism-a-failure-to-distinguish-near-and-far", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:31.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LS8pWYZ8kaTT45RJ4/is-atheism-a-failure-to-distinguish-near-and-far", "pageUrlRelative": "/posts/LS8pWYZ8kaTT45RJ4/is-atheism-a-failure-to-distinguish-near-and-far", "linkUrl": "https://www.lesswrong.com/posts/LS8pWYZ8kaTT45RJ4/is-atheism-a-failure-to-distinguish-near-and-far", "postedAtFormatted": "Wednesday, February 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Atheism%20a%20failure%20to%20distinguish%20Near%20and%20Far%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Atheism%20a%20failure%20to%20distinguish%20Near%20and%20Far%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLS8pWYZ8kaTT45RJ4%2Fis-atheism-a-failure-to-distinguish-near-and-far%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Atheism%20a%20failure%20to%20distinguish%20Near%20and%20Far%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLS8pWYZ8kaTT45RJ4%2Fis-atheism-a-failure-to-distinguish-near-and-far", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLS8pWYZ8kaTT45RJ4%2Fis-atheism-a-failure-to-distinguish-near-and-far", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p>The terms Near and Far are to be taken in the context of Robin Hanson's <a href=\"http://www.overcomingbias.com/tag/nearfar\">Near/Far</a> articles.</p>\n<p>I was reading a fairly convincing <a href=\"http://stairs.umd.edu/236/meta-atheism.html\">article</a> linked from a comment here about how theistic beliefs are so scantly supported, when not outright contradictory, that it's a doubtful whether anyone truly holds them at all. Of course there is a whole battery of explanations around the self-deception, signalling and <a href=\"/lw/i4/belief_in_belief/\">belief-in-belief</a> cluster, but the question that got in my head was about the kinds of people that can or cannot profess to hold these beliefs.</p>\n<p>A common thread in many a 'deconversion' story is that some inconsistency in a person's worldview comes to their attention, and they can't let go until they have undone the whole fabric of their belief system. But given that most people are happy living productive lives while simultaneously nominally carrying around massively conflicted worldviews, what is it that makes certain individuals not capable of this fairly common human feat?</p>\n<p>So the hypothesis that I'm considering is that the people who came to atheism this way, are those who demand detailed consistency of their Far ideals. Alternatively, they could be those for who what is normally considered Far is actually Near, in other words those with an unusually high <a href=\"http://en.wikipedia.org/wiki/Buxton_Index\">Buxton Index</a>. Combining the two, perhaps for people with a high Buxton Index, Far simply evaporates, as it comes under the scope of things that are relevant to a person's planning. (Edsger W. Djikstra, when&nbsp;<a href=\"http://www.cs.utexas.edu/users/EWD/transcriptions/EWD11xx/EWD1175.html\">introducing the Buxton Index</a>, says that \"true christians\" have a Buxton Index of infinity. I think that couldn't be more wrong. Perhaps it is the case for singularitarians though.)</p>\n<p>The obvious reason to be suspicious of this idea is that it's very flattering for those that fall in this category, which includes myself. Rather than dithering about it, I'd rather expose it to the community and see if it seems to have legs in the eyes of others.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LS8pWYZ8kaTT45RJ4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 6.743725692841626e-07, "legacy": true, "legacyId": "5329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-02T14:20:02.418Z", "modifiedAt": "2020-12-08T03:43:58.891Z", "url": null, "title": "Cambridge UK Meetup Saturday 12 February", "slug": "cambridge-uk-meetup-saturday-12-february", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:18.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Ramana Kumar", "user": {"username": "ramana-kumar", "createdAt": "2018-02-06T14:10:35.841Z", "isAdmin": false, "displayName": "Ramana Kumar"}, "userId": "3zw8sQu6tYLJ8qoYD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ESxHghReaySXss6j/cambridge-uk-meetup-saturday-12-february", "pageUrlRelative": "/posts/6ESxHghReaySXss6j/cambridge-uk-meetup-saturday-12-february", "linkUrl": "https://www.lesswrong.com/posts/6ESxHghReaySXss6j/cambridge-uk-meetup-saturday-12-february", "postedAtFormatted": "Wednesday, February 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20UK%20Meetup%20Saturday%2012%20February&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20UK%20Meetup%20Saturday%2012%20February%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ESxHghReaySXss6j%2Fcambridge-uk-meetup-saturday-12-february%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20UK%20Meetup%20Saturday%2012%20February%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ESxHghReaySXss6j%2Fcambridge-uk-meetup-saturday-12-february", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ESxHghReaySXss6j%2Fcambridge-uk-meetup-saturday-12-february", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>Being curious about the number of LW readers in Cambridge, I propose we meet next Saturday afternoon, 2pm February 12th at The Anchor (12 Silver Street).</p>\n<p>Feel free to suggest a better time or place if this doesn't suit you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ESxHghReaySXss6j", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.745210450225023e-07, "legacy": true, "legacyId": "5327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-02T15:25:40.512Z", "modifiedAt": null, "url": null, "title": "Normal gut microbiota modulates brain development and behavior (at least in mice)", "slug": "normal-gut-microbiota-modulates-brain-development-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:35.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "playtherapist", "createdAt": "2010-10-24T12:34:50.337Z", "isAdmin": false, "displayName": "playtherapist"}, "userId": "FHnnd6QRSDbDyycPJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a5pkBn52KCyQPa5Ei/normal-gut-microbiota-modulates-brain-development-and", "pageUrlRelative": "/posts/a5pkBn52KCyQPa5Ei/normal-gut-microbiota-modulates-brain-development-and", "linkUrl": "https://www.lesswrong.com/posts/a5pkBn52KCyQPa5Ei/normal-gut-microbiota-modulates-brain-development-and", "postedAtFormatted": "Wednesday, February 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Normal%20gut%20microbiota%20modulates%20brain%20development%20and%20behavior%20(at%20least%20in%20mice)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANormal%20gut%20microbiota%20modulates%20brain%20development%20and%20behavior%20(at%20least%20in%20mice)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5pkBn52KCyQPa5Ei%2Fnormal-gut-microbiota-modulates-brain-development-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Normal%20gut%20microbiota%20modulates%20brain%20development%20and%20behavior%20(at%20least%20in%20mice)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5pkBn52KCyQPa5Ei%2Fnormal-gut-microbiota-modulates-brain-development-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5pkBn52KCyQPa5Ei%2Fnormal-gut-microbiota-modulates-brain-development-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Interesting findings. If this holds true for humans, it seems possible that our use of antibacterial products, etc.may be contributing to the increase in ADHD.</p>\n<p>&nbsp;</p>\n<p>http://www.pnas.org/content/early/2011/01/26/1010529108</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a5pkBn52KCyQPa5Ei", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 6.745382243224639e-07, "legacy": true, "legacyId": "5331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-02T15:42:59.189Z", "modifiedAt": null, "url": null, "title": "Why people reject science", "slug": "why-people-reject-science", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SQehZZEzqwuSdhniJ/why-people-reject-science", "pageUrlRelative": "/posts/SQehZZEzqwuSdhniJ/why-people-reject-science", "linkUrl": "https://www.lesswrong.com/posts/SQehZZEzqwuSdhniJ/why-people-reject-science", "postedAtFormatted": "Wednesday, February 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20people%20reject%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20people%20reject%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQehZZEzqwuSdhniJ%2Fwhy-people-reject-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20people%20reject%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQehZZEzqwuSdhniJ%2Fwhy-people-reject-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQehZZEzqwuSdhniJ%2Fwhy-people-reject-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><a href=\"http://green.blogs.nytimes.com/2011/02/01/are-we-hard-wired-to-doubt-science/?src=me&amp;ref=science\">From the NYTimes.</a>&nbsp;The central point:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p><span style=\"font-family: georgia, 'times new roman', times, serif; font-size: 14px; color: #333333; line-height: 21px;\">Humans, he argues, are hard-wired to reject scientific conclusions that run counter to their instinctive belief that someone or something is out to get them.</span></p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SQehZZEzqwuSdhniJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 9, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "5332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-02T20:55:27.219Z", "modifiedAt": null, "url": null, "title": "Isn't this sitemeter logging a bit too excessive?", "slug": "isn-t-this-sitemeter-logging-a-bit-too-excessive", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.053Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aoR8t2ABXcqxuCN6d/isn-t-this-sitemeter-logging-a-bit-too-excessive", "pageUrlRelative": "/posts/aoR8t2ABXcqxuCN6d/isn-t-this-sitemeter-logging-a-bit-too-excessive", "linkUrl": "https://www.lesswrong.com/posts/aoR8t2ABXcqxuCN6d/isn-t-this-sitemeter-logging-a-bit-too-excessive", "postedAtFormatted": "Wednesday, February 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Isn't%20this%20sitemeter%20logging%20a%20bit%20too%20excessive%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIsn't%20this%20sitemeter%20logging%20a%20bit%20too%20excessive%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaoR8t2ABXcqxuCN6d%2Fisn-t-this-sitemeter-logging-a-bit-too-excessive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Isn't%20this%20sitemeter%20logging%20a%20bit%20too%20excessive%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaoR8t2ABXcqxuCN6d%2Fisn-t-this-sitemeter-logging-a-bit-too-excessive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaoR8t2ABXcqxuCN6d%2Fisn-t-this-sitemeter-logging-a-bit-too-excessive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>I have just realized that sitemeter has the following data published about my visit, in a searchable and browsable format:</p>\n<p>&nbsp;</p>\n<table border=\"0\" cellspacing=\"1\" cellpadding=\"1\" width=\"450\">\n<tbody>\n<tr bgcolor=\"#f5f5e2\">\n<td width=\"150\" align=\"right\"><span style=\"font-family: Arial;\">Domain Name</span></td>\n<td width=\"10\" align=\"center\">&nbsp;</td>\n<td width=\"290\"><span style=\"font-family: Arial; font-size: x-small;\"><a href=\"http://broadband.hu/\"><span id=\"ipDomainName\">broadband.hu</span></a>&nbsp;<a href=\"http://www.sitemeter.com/?a=stats&amp;s=s18lesswrong&amp;r=34&amp;vlr=89&amp;pg=1&amp;v=54\"><sup>?</sup></a> (Hungary)</span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">IP Address</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><span id=\"ipAddress\">80.98.73.#</span> (UPC&nbsp;Magyarorszag&nbsp;Kft.)</span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\"><span style=\"font-family: Arial;\">ISP</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">UPC Magyarorszag Kft.</span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Location</span></td>\n<td align=\"center\">&nbsp;</td>\n<td>\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tbody>\n<tr>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Continent</span></td>\n<td>&nbsp;:&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Europe</span></td>\n</tr>\n<tr>\n<td valign=\"top\"><span style=\"font-family: Arial; font-size: x-small;\">Country</span></td>\n<td valign=\"top\">&nbsp;:&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><a href=\"http://www.sitemeter.com/?a=stats&amp;s=s18lesswrong&amp;v=54&amp;country=HU&amp;vlr=89&amp;pg=1&amp;r=76\">Hungary</a>&nbsp;<a href=\"http://www.sitemeter.com/?a=stats&amp;s=s18lesswrong&amp;v=54&amp;country=HU&amp;vlr=89&amp;pg=1&amp;r=77\"><img src=\"http://www.sitemeter.com/images/flags/HU.gif\" border=\"0\" alt=\"\" width=\"18\" height=\"12\" /></a>&nbsp;<a href=\"http://www.sitemeter.com/?a=stats&amp;s=s18lesswrong&amp;v=54&amp;country=HU&amp;vlr=89&amp;pg=1&amp;r=78\">(Facts)</a></span></td>\n</tr>\n<tr>\n<td><span style=\"font-family: Arial; font-size: x-small;\">State/Region</span></td>\n<td>&nbsp;:&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Fejer</span></td>\n</tr>\n<tr>\n<td><span style=\"font-family: Arial; font-size: x-small;\">City</span></td>\n<td>&nbsp;:&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Etyek</span></td>\n</tr>\n<tr>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Lat/Long</span></td>\n<td>&nbsp;:&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">47.45, 18.75&nbsp;<a href=\"http://www.sitemeter.com/?a=stats&amp;s=s18lesswrong&amp;r=75&amp;pg=1&amp;vlr=89&amp;v=54\">(Map)</a></span></td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Language</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">English (U.S.)<br /><small>en-us</small></span></td>\n</tr>\n<tr>\n<td align=\"right\"><span style=\"font-family: Arial;\">Operating System</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Macintosh MacOSX</span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Browser</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Firefox <br /><small>Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2.13) Gecko/20101203 Firefox/3.6.13 GTB7.1</small></span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Javascript</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">version 1.5</span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Monitor</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"> \n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tbody>\n<tr>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Resolution</span></td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">&nbsp;:&nbsp;</span></td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">1296 x 810</span></td>\n</tr>\n<tr>\n<td><span style=\"font-family: Arial; font-size: x-small;\">Color Depth</span></td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">&nbsp;:&nbsp;</span></td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">24 bits </span></td>\n</tr>\n</tbody>\n</table>\n</span></td>\n</tr>\n<tr>\n<td align=\"right\"><span style=\"font-family: Arial;\">Time of Visit</span></td>\n<td align=\"center\"><span style=\"font-family: Arial;\">&nbsp;</span></td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><span title=\"Feb 2 2011 12:34:19\">Feb&nbsp;2&nbsp;2011&nbsp;12:34:19&nbsp;pm</span></span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\"><span style=\"font-family: Arial;\">Last Page View</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><span title=\"Feb 2 2011 12:34:40\">Feb&nbsp;2&nbsp;2011&nbsp;12:34:40&nbsp;pm</span></span></td>\n</tr>\n<tr>\n<td align=\"right\"><span style=\"font-family: Arial;\">Visit Length</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">21 seconds</span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\"><span style=\"font-family: Arial;\">Page Views</span></td>\n<td align=\"center\"><span style=\"font-family: Arial;\">&nbsp;</span></td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">2</span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Referring URL</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><a title=\"http://lesswrong.com/search/results?cx=015839050583929870010%3A-802ptn4igi&amp;cof=FORID%3A11&amp;ie=UTF-8&amp;q=hungary&amp;sa=Search&amp;siteurl=lesswrong.com%252Fr%252Fdiscussion%252Flw%252F43s%252Fstarting_a_lw_meetup_is_easy%252F\" href=\"/search/results?cx=015839050583929870010%3A-802ptn4igi&amp;cof=FORID%3A11&amp;ie=UTF-8&amp;q=hungary&amp;sa=Search&amp;siteurl=lesswrong.com%252Fr%252Fdiscussion%252Flw%252F43s%252Fstarting_a_lw_meetup_is_easy%252F\">http://lesswrong.com..._meetup_is_easy%252F</a></span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Search Engine</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><span style=\"font-family: Arial; font-size: x-small;\">lesswrong.com</span></span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Search Words</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><span style=\"font-family: Arial; font-size: x-small;\">hungary</span></span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\"><span style=\"font-family: Arial;\">Visit Entry Page</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><a href=\"/r/discussion\">http://lesswrong.com/r/discussion/</a></span></td>\n</tr>\n<tr>\n<td align=\"right\"><span style=\"font-family: Arial;\">Visit Exit Page</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><a title=\"http://lesswrong.com/r/discussion/lw/43s/starting_a_lw_meetup_is_easy/\" href=\"/r/discussion/lw/43s/starting_a_lw_meetup_is_easy\">http://lesswrong.com...a_lw_meetup_is_easy/</a></span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\"><span style=\"font-family: Arial;\">Out Click</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\">&nbsp;</span></td>\n</tr>\n<tr>\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Time Zone</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><a href=\"http://www.sitemeter.com/?a=stats&amp;&amp;s=s18lesswrong&amp;v=54&amp;vlr=89&amp;pg=1&amp;r=31\">UTC+1:00</a></span></td>\n</tr>\n<tr bgcolor=\"#f5f5e2\">\n<td align=\"right\" valign=\"top\"><span style=\"font-family: Arial;\">Visitor's Time</span></td>\n<td align=\"center\">&nbsp;</td>\n<td><span style=\"font-family: Arial; font-size: x-small;\"><span title=\"Feb 2 2011 21:34:19\">Feb&nbsp;2&nbsp;2011&nbsp;9:34:19&nbsp;pm</span></span></td>\n</tr>\n<tr>\n<td align=\"right\"><span style=\"font-family: Arial;\">Visit&nbsp;Number</span></td>\n<td align=\"center\">&nbsp;</td>\n<td align=\"left\"><span style=\"font-family: Arial; font-size: x-small;\">3,497,452</span></td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>I am not a privacy geek, but isn't this a bit too extensive? By the way, I am not from Etyek, Hungary, I am from Budapest, Hungary. Etyek is a very small village, so if sitemeter consistently identifies me as someone from Etyek, then it will be even easier to track my lesswrong browsing habits. It is very easy even without that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "N5JGtFnhex2DbyPvy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aoR8t2ABXcqxuCN6d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 29, "extendedScore": null, "score": 6.746245523754277e-07, "legacy": true, "legacyId": "5333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-03T01:47:51.794Z", "modifiedAt": null, "url": null, "title": "[Draft] Holy Bayesian Multiverse, Batman!", "slug": "draft-holy-bayesian-multiverse-batman", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:39.045Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "b1shop", "createdAt": "2010-07-21T22:58:23.412Z", "isAdmin": false, "displayName": "b1shop"}, "userId": "YYM9ouBdPbNFxvF2G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ew2qzSYs8FdCfqx3e/draft-holy-bayesian-multiverse-batman", "pageUrlRelative": "/posts/Ew2qzSYs8FdCfqx3e/draft-holy-bayesian-multiverse-batman", "linkUrl": "https://www.lesswrong.com/posts/Ew2qzSYs8FdCfqx3e/draft-holy-bayesian-multiverse-batman", "postedAtFormatted": "Thursday, February 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BDraft%5D%20Holy%20Bayesian%20Multiverse%2C%20Batman!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BDraft%5D%20Holy%20Bayesian%20Multiverse%2C%20Batman!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEw2qzSYs8FdCfqx3e%2Fdraft-holy-bayesian-multiverse-batman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BDraft%5D%20Holy%20Bayesian%20Multiverse%2C%20Batman!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEw2qzSYs8FdCfqx3e%2Fdraft-holy-bayesian-multiverse-batman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEw2qzSYs8FdCfqx3e%2Fdraft-holy-bayesian-multiverse-batman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 618, "htmlBody": "<p>I couldn't find the math for the <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">quantum suicide and immortality</a> thought experiment, so I'm placing it here for posterity. If one actually ran the experiment, Bayes' theorem would tell us how to update our belief in the multi-world interpretation (MWI)&nbsp;of quantum mechanics. I conclude by arguing that we don't need to run the experiment.</p>\r\n<p>Prereqs: Understand the purpose of <a href=\"http://yudkowsky.net/rational/bayes\">Bayes Theorem</a>, possess at least rudimentary knowledge of the <a href=\"http://en.wikipedia.org/wiki/Copenhagen_interpretation\">competing</a> <a href=\"http://en.wikipedia.org/wiki/Many-worlds_interpretation\">quantum</a> worldviews, and have a nostalgic appreciation for <a href=\"http://www.youtube.com/watch?v=TLbpwadl35E&amp;NR=1\">Adam West</a>.</p>\r\n<h3>The Fiendish Setup:</h3>\r\n<p>Suppose that, after catching Batman snooping in the shadows of his evil lair, Joker ties the caped crusader into a quantum, <a href=\"http://en.wikipedia.org/wiki/Negative_binomial_distribution\">negative binomial</a>&nbsp;death machine that, every ten seconds, measures the spin value of a fresh proton. Fifty percent of the time, the result will trigger a Bat-killing Rube Goldberg machine. The other 50 percent of the time, the&nbsp;quantum death machine&nbsp;will play a suspenseful stock sound effect and search for a new proton.<a id=\"more\"></a></p>\r\n<h3>From Batman's Point of View</h3>\r\n<p>Suppose that, ten seconds after Joker turns on the machine, a dramatic, suspenseful sound fills Joker's hideout.</p>\r\n<p>The Bat has survived, and there are two possible explanations:</p>\r\n<ol>\r\n<li>There is only one world, and he was lucky enough to find himself in a world where the machine didn't kill him.</li>\r\n<li>There are many worlds, and, for obvious reasons, he could only find himself in one of the worlds where he didn't die.</li>\r\n</ol>\r\n<p>What does Bayes' say about how he should update his belief in the many-worlds theorem? If we partition all QM interpretations into either the single- or many- world camp, Batman's subjective P(MW) will increase to P(MW|S) according to this formula.</p>\r\n<p>Let C be the event that the Copenhagen interpretation is correct and S be the event that Batman experiences himself surviving after the first flip of the quantum coin.</p>\r\n<p>&nbsp;</p>\r\n<p><img src=\"http://imgur.com/F7vqn.png\" alt=\"\" /></p>\r\n<p>&nbsp;</p>\r\n<p>Click <a href=\"http://developer.wolframalpha.com/widgets/gallery/view.jsp?id=f5c59267dae7d123f54b741a76f28f84\" target=\"_blank\">here</a> to see a graph of P(MW|S) graphed over Batman's original estimate of P(MW). When successive iterations is zero, it's a straight line.&nbsp;If Batman survives through many&nbsp;successive iterations, he can only claim one theory with a straight face&nbsp;--&nbsp;many worlds.</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_33e_1.png?v=3e06f57318c9d535efd84683a5aed915\" alt=\"\" width=\"307\" height=\"65\" /></p>\r\n<p>The limit of P(MW|Sn) goes to one as n goes to infinity.</p>\r\n<h3>From Joker's Point of View</h3>\r\n<p>Unfortunately, there's no such guaranty for the experiment's observers.</p>\r\n<p>If MWI is true, then, in some uncommon universes, the Joker will see Batman breathe many sighs of relief, but, in the overarching majority, Joker will emerge triumphant.</p>\r\n<p>If there is only one world, then Batman <em>might</em> emerge unscathed after the Rube Goldberg machine eventually runs out of batteries, but it's more likely the universe will collapse into one without the Caped Crusader. Joker has no idea which he's observing.</p>\r\n<p>For observers, P(MW) and P(S) are independent. That means P(MW n S) = P(MW) * P(S). Doing slightly more math than needed:</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_33e_2.png?v=a18393a409240df8ca52ad0851733d28\" alt=\"\" width=\"304\" height=\"202\" /></p>\r\n<p>So the observer's estimate of P(MW) is unchanged by the fate of the branchonaut.</p>\r\n<h3>Proving Many-Worlds</h3>\r\n<p>Surviving a quantum death trap is a convincing argument in favor of the many-worlds hypothesis, but you'd have to be pretty risk-averse to seek it out. First of all, your survival would be completely unpersuasive to observers. Secondly, it'd leave a great many Gothams without Batman (if MWI is true) and a high chance Gotham won't have Batman (if MWI isn't true).</p>\r\n<p>However, if miniscule&nbsp;quantum effects can snowball into cosmological consequences, then we don't need to run the rube goldberg machine. Let S be the event our human race came to being in our universe, and P(MW) be your estimate of the accuracy in MWI before considering this argument and P(MW|S) be your estimate of the accuracy in MWI after considering this argument.</p>\r\n<p>Cosmology is an analagous quantum death trap, and the entire human race witnessed it from the inside.</p>\r\n<p>If you haven't thought about this argument before, you should update your belief in the many-worlds interpretation radically upwards.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ew2qzSYs8FdCfqx3e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "4010", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I couldn't find the math for the <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">quantum suicide and immortality</a> thought experiment, so I'm placing it here for posterity. If one actually ran the experiment, Bayes' theorem would tell us how to update our belief in the multi-world interpretation (MWI)&nbsp;of quantum mechanics. I conclude by arguing that we don't need to run the experiment.</p>\n<p>Prereqs: Understand the purpose of <a href=\"http://yudkowsky.net/rational/bayes\">Bayes Theorem</a>, possess at least rudimentary knowledge of the <a href=\"http://en.wikipedia.org/wiki/Copenhagen_interpretation\">competing</a> <a href=\"http://en.wikipedia.org/wiki/Many-worlds_interpretation\">quantum</a> worldviews, and have a nostalgic appreciation for <a href=\"http://www.youtube.com/watch?v=TLbpwadl35E&amp;NR=1\">Adam West</a>.</p>\n<h3 id=\"The_Fiendish_Setup_\">The Fiendish Setup:</h3>\n<p>Suppose that, after catching Batman snooping in the shadows of his evil lair, Joker ties the caped crusader into a quantum, <a href=\"http://en.wikipedia.org/wiki/Negative_binomial_distribution\">negative binomial</a>&nbsp;death machine that, every ten seconds, measures the spin value of a fresh proton. Fifty percent of the time, the result will trigger a Bat-killing Rube Goldberg machine. The other 50 percent of the time, the&nbsp;quantum death machine&nbsp;will play a suspenseful stock sound effect and search for a new proton.<a id=\"more\"></a></p>\n<h3 id=\"From_Batman_s_Point_of_View\">From Batman's Point of View</h3>\n<p>Suppose that, ten seconds after Joker turns on the machine, a dramatic, suspenseful sound fills Joker's hideout.</p>\n<p>The Bat has survived, and there are two possible explanations:</p>\n<ol>\n<li>There is only one world, and he was lucky enough to find himself in a world where the machine didn't kill him.</li>\n<li>There are many worlds, and, for obvious reasons, he could only find himself in one of the worlds where he didn't die.</li>\n</ol>\n<p>What does Bayes' say about how he should update his belief in the many-worlds theorem? If we partition all QM interpretations into either the single- or many- world camp, Batman's subjective P(MW) will increase to P(MW|S) according to this formula.</p>\n<p>Let C be the event that the Copenhagen interpretation is correct and S be the event that Batman experiences himself surviving after the first flip of the quantum coin.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/F7vqn.png\" alt=\"\"></p>\n<p>&nbsp;</p>\n<p>Click <a href=\"http://developer.wolframalpha.com/widgets/gallery/view.jsp?id=f5c59267dae7d123f54b741a76f28f84\" target=\"_blank\">here</a> to see a graph of P(MW|S) graphed over Batman's original estimate of P(MW). When successive iterations is zero, it's a straight line.&nbsp;If Batman survives through many&nbsp;successive iterations, he can only claim one theory with a straight face&nbsp;--&nbsp;many worlds.</p>\n<p><img src=\"http://images.lesswrong.com/t3_33e_1.png?v=3e06f57318c9d535efd84683a5aed915\" alt=\"\" width=\"307\" height=\"65\"></p>\n<p>The limit of P(MW|Sn) goes to one as n goes to infinity.</p>\n<h3 id=\"From_Joker_s_Point_of_View\">From Joker's Point of View</h3>\n<p>Unfortunately, there's no such guaranty for the experiment's observers.</p>\n<p>If MWI is true, then, in some uncommon universes, the Joker will see Batman breathe many sighs of relief, but, in the overarching majority, Joker will emerge triumphant.</p>\n<p>If there is only one world, then Batman <em>might</em> emerge unscathed after the Rube Goldberg machine eventually runs out of batteries, but it's more likely the universe will collapse into one without the Caped Crusader. Joker has no idea which he's observing.</p>\n<p>For observers, P(MW) and P(S) are independent. That means P(MW n S) = P(MW) * P(S). Doing slightly more math than needed:</p>\n<p><img src=\"http://images.lesswrong.com/t3_33e_2.png?v=a18393a409240df8ca52ad0851733d28\" alt=\"\" width=\"304\" height=\"202\"></p>\n<p>So the observer's estimate of P(MW) is unchanged by the fate of the branchonaut.</p>\n<h3 id=\"Proving_Many_Worlds\">Proving Many-Worlds</h3>\n<p>Surviving a quantum death trap is a convincing argument in favor of the many-worlds hypothesis, but you'd have to be pretty risk-averse to seek it out. First of all, your survival would be completely unpersuasive to observers. Secondly, it'd leave a great many Gothams without Batman (if MWI is true) and a high chance Gotham won't have Batman (if MWI isn't true).</p>\n<p>However, if miniscule&nbsp;quantum effects can snowball into cosmological consequences, then we don't need to run the rube goldberg machine. Let S be the event our human race came to being in our universe, and P(MW) be your estimate of the accuracy in MWI before considering this argument and P(MW|S) be your estimate of the accuracy in MWI after considering this argument.</p>\n<p>Cosmology is an analagous quantum death trap, and the entire human race witnessed it from the inside.</p>\n<p>If you haven't thought about this argument before, you should update your belief in the many-worlds interpretation radically upwards.</p>", "sections": [{"title": "The Fiendish Setup:", "anchor": "The_Fiendish_Setup_", "level": 1}, {"title": "From Batman's Point of View", "anchor": "From_Batman_s_Point_of_View", "level": 1}, {"title": "From Joker's Point of View", "anchor": "From_Joker_s_Point_of_View", "level": 1}, {"title": "Proving Many-Worlds", "anchor": "Proving_Many_Worlds", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-03T08:53:07.660Z", "modifiedAt": null, "url": null, "title": "-", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:37.756Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HoverHell", "createdAt": "2010-04-19T06:30:06.524Z", "isAdmin": false, "displayName": "HoverHell"}, "userId": "dLbWn7gGj75sekv7f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CcFGe3vLe9jQCAmNk/", "pageUrlRelative": "/posts/CcFGe3vLe9jQCAmNk/", "linkUrl": "https://www.lesswrong.com/posts/CcFGe3vLe9jQCAmNk/", "postedAtFormatted": "Thursday, February 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20-&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A-%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCcFGe3vLe9jQCAmNk%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=-%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCcFGe3vLe9jQCAmNk%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCcFGe3vLe9jQCAmNk%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>-</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CcFGe3vLe9jQCAmNk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 6.748124897582275e-07, "legacy": true, "legacyId": "5335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-03T09:52:14.079Z", "modifiedAt": null, "url": null, "title": "On The Effectiveness of Ferriss", "slug": "on-the-effectiveness-of-ferriss", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YnrrAL8wynZvcACwG/on-the-effectiveness-of-ferriss", "pageUrlRelative": "/posts/YnrrAL8wynZvcACwG/on-the-effectiveness-of-ferriss", "linkUrl": "https://www.lesswrong.com/posts/YnrrAL8wynZvcACwG/on-the-effectiveness-of-ferriss", "postedAtFormatted": "Thursday, February 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20The%20Effectiveness%20of%20Ferriss&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20The%20Effectiveness%20of%20Ferriss%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnrrAL8wynZvcACwG%2Fon-the-effectiveness-of-ferriss%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20The%20Effectiveness%20of%20Ferriss%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnrrAL8wynZvcACwG%2Fon-the-effectiveness-of-ferriss", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnrrAL8wynZvcACwG%2Fon-the-effectiveness-of-ferriss", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>Tim Ferriss has been systematically quoted on Less Wrong.</p>\n<p>How to make money to donate utilons and show you care is a persistent topic on Less Wrong.</p>\n<p>No one here seems to either have tried, or accessed the feasibility of Tim Ferrissing life (for instance accessing by checking for people who tried without the obvious survivor bias displayed in Ferriss`s own website)</p>\n<p>A probability 30% of earning $12.000 per month working for 10 hours per week after a build-up time of 4 months working 10 hours a day to get it started (having fun while figuring out how does capitalism work anyway) seems like a fair bet.</p>\n<p>My prior for the above paragraph feasibility is about 15%.</p>\n<p>Should my posterior be above the 30% threshold?</p>\n<p>Data anyone?</p>\n<p>Different prior anyone?</p>\n<p>Lone bystander bias,everyone?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YnrrAL8wynZvcACwG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "5336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-03T18:36:59.115Z", "modifiedAt": null, "url": null, "title": "[Link] Space Stasis: What the strange persistence of rockets can teach us about innovation", "slug": "link-space-stasis-what-the-strange-persistence-of-rockets", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:31.923Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M5t9RAwKhMyqnYRgh/link-space-stasis-what-the-strange-persistence-of-rockets", "pageUrlRelative": "/posts/M5t9RAwKhMyqnYRgh/link-space-stasis-what-the-strange-persistence-of-rockets", "linkUrl": "https://www.lesswrong.com/posts/M5t9RAwKhMyqnYRgh/link-space-stasis-what-the-strange-persistence-of-rockets", "postedAtFormatted": "Thursday, February 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Space%20Stasis%3A%20What%20the%20strange%20persistence%20of%20rockets%20can%20teach%20us%20about%20innovation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Space%20Stasis%3A%20What%20the%20strange%20persistence%20of%20rockets%20can%20teach%20us%20about%20innovation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5t9RAwKhMyqnYRgh%2Flink-space-stasis-what-the-strange-persistence-of-rockets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Space%20Stasis%3A%20What%20the%20strange%20persistence%20of%20rockets%20can%20teach%20us%20about%20innovation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5t9RAwKhMyqnYRgh%2Flink-space-stasis-what-the-strange-persistence-of-rockets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5t9RAwKhMyqnYRgh%2Flink-space-stasis-what-the-strange-persistence-of-rockets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1508, "htmlBody": "<p>http://www.slate.com/id/2283469/pagenum/all/</p>\n<p>It's a long article, but the most relevant stuff is at the end, about how we're pretty much locked into the existing rocket technologies:</p>\n<blockquote>\n<p>That is not, however, the most important way that rockets generate  lock-in. In order to understand this, it's necessary to know a few  things about (1) the physical environment of rocket launches, (2) the  economics of the industry, and (3) the way it is regulated, or, to be  more precise, the way it interacts with government.</p>\n<p>1. The designer of a rocket payload, such as a communications  satellite, has much more to worry about than merely limiting the payload  to a given size, shape, and weight. The payload must be designed to  survive the launch and the transition through various atmospheric  regimes into outer space. As we all know from watching astronauts on  movies and TV, there will be acceleration forces, relatively modest at  the beginning, but building to much higher values as fuel is burned and  the rocket becomes lighter relative to its thrust. At some moments,  during stage separation, the acceleration may even reverse direction for  a few moments as one set of engines stops supplying thrust and  atmospheric resistance slows the vehicle down. Rockets produce intense  vibration over a wide range of frequencies; at the upper end of that  range we would identify this as noise (noise loud enough to cause  physical destruction of delicate objects), at the lower range, violent  shaking. Explosive bolts send violent shocks through the vehicle's  structure. During the passage through the ionosphere, the air itself  becomes conductive and can short out electrical gear. Enclosed spaces  must be vented so that pressure doesn't build up in them as the vehicle  passes into vacuum. Once the satellite has reached orbit, sharp and  intense variations in temperature as it passes in and out of the earth's  shadow can cause problems if not anticipated in the engineering design.  Some of these hazards are common to all things that go into space, but  many are unique to rockets.</p>\n<p>2. If satellites and launches were  cheap, a more easygoing attitude toward their design and construction  might prevail. But in general they are, pound for pound, among the most  expensive objects ever made <em>even before</em> millions of dollars are spent launching them into orbit. Relatively mass-produced satellites, such as those in the Iridium and <a href=\"http://en.wikipedia.org/wiki/Orbcomm_satellites\" target=\"_blank\">Orbcomm constellations</a>,  cost on the order of $10,000/lb. The communications birds in  geostationary orbit&mdash;the ones used for satellite television, e.g.&mdash;are two  to five times as expensive, and ambitious scientific/defense payloads  are often $100,000 per pound. Comsats can only be packed so close  together in orbit, which means that there is a limited number of  available slots&mdash;this makes their owners want to pack as much capability  as possible into each bird, helping jack up the cost. Once they are up  in orbit, comsats generate huge amounts of cash for their owners, which  means that any delays in launching them are terribly expensive. Rockets  of the old school aren't perfect&mdash;they have their share of failures&mdash;but  they have enough of a track record that it's possible to buy <a href=\"http://www.faa.gov/about/office_org/headquarters_offices/ast/media/q42002.pdf\" target=\"_blank\">launch insurance</a>.  The importance of this fact cannot be overestimated. Every space  entrepreneur who dreams of constructing a better mousetrap sooner or  later crunches into the sickening realization that, even if the new  invention achieved perfect technical success, it would fail as a  business proposition simply because the customers wouldn't be able to  purchase launch insurance.</p>\n<p>3. Rockets&mdash;at least, the kinds that are  destined for orbit, which is what we are talking about here&mdash;don't go  straight up into the air. They mostly go horizontally, since their  purpose is to generate horizontal velocities so high that centrifugal  force counteracts gravity. The initial launch is vertical because the  thing needs to get off the pad and out of the dense lower atmosphere,  but shortly afterwards it bends its trajectory sharply downrange and  begins to accelerate nearly horizontally. Consequently, all rockets  destined for orbit will pass over large swathes of the earth's surface  during the 10 minutes or so that their engines are burning. This  produces regulatory and legal complications that go deep into the realm  of the absurd. Existing rockets, and the launch pads around which they  have been designed, have been grandfathered in. Space entrepreneurs must  either find a way to negotiate the legal minefield from scratch or else  pay high fees to use the existing facilities. While some of these  regulatory complications can be reduced by going outside of the  developed world, this introduces a whole new set of complications since  space technology is regulated as armaments, and this imposes strict  limits on the ways in which American rocket scientists can collaborate  with foreigners. Moreover, the rocket industry's status as a colossal  government-funded program with seemingly eternal lifespan has led to a  situation in which its myriad contractors and suppliers are distributed  over the largest possible number of congressional districts. Anyone who  has witnessed Congress in action can well imagine the consequences of  giving it control over a difficult scientific and technological program.</p>\n<p><a href=\"http://lasermotive.com/about/executive-staff/jordin-kare/\" target=\"_blank\">Dr. Jordin Kare, a physicist and space launch expert</a> to whom I am indebted for some of the details mentioned above,  visualizes the result as a triangular feedback loop joining big  expensive launch systems; complex, expensive, long-life satellites; and  few launch opportunities. To this could be added any number of cultural  factors (the engineers populating the aerospace industry are heavily  invested in the current way of doing things); the insurance and  regulatory factors mentioned above; market inelasticity (cutting launch  cost in half wouldn't make much of a difference); and even accounting  practices (how do you amortize the nonrecoverable expenses of an  innovative program over a sufficiently large number of future  launches?).</p>\n<p>To employ a commonly used metaphor, our current  proficiency in rocket-building is the result of a hill-climbing  approach; we started at one place on the technological landscape&mdash;which  must be considered a random pick, given that it was chosen for dubious  reasons by a maniac&mdash;and climbed the hill from there, looking for small  steps that could be taken to increase the size and efficiency of the  device. Sixty years and a couple of trillion dollars later, we have  reached a place that is infinitesimally close to the top of that hill.  Rockets are as close to perfect as they're ever going to get. For a few  more billion dollars we might be able to achieve a microscopic  improvement in efficiency or reliability, but to make any game-changing  improvements is not merely expensive; it's a physical impossibility.</p>\n<p>There  is no shortage of proposals for radically innovative space launch  schemes that, if they worked, would get us across the valley to other  hilltops considerably higher than the one we are standing on now&mdash;high  enough to bring the cost and risk of space launch down to the point  where fundamentally new things could begin happening in outer space. But  we are not making any serious effort as a society to cross those  valleys. It is not clear why.</p>\n<p>A temptingly simple explanation is  that we are decadent and tired. But none of the bright young  up-and-coming economies seem to be interested in anything besides aping  what the United States and the USSR did years ago. We may, in other  words, need to look beyond strictly U.S.-centric explanations for such  failures of imagination and initiative. It might simply be that there is  something in the nature of modern global capitalism that is holding us  back. Which might be a good thing, if it's an alternative to the crazy  schemes of vicious dictators. Admittedly, there are many who feel a deep  antipathy for expenditure of money and brainpower on space travel when,  as they never tire of reminding us, there are so many problems to be  solved on earth. So if space launch were the only area in which this  phenomenon was observable, it would be of concern only to space  enthusiasts. But the endless BP oil spill of 2010 highlighted any number  of ways in which the phenomena of path dependency and lock-in have  trapped our energy industry on a hilltop from which we can gaze  longingly across not-so-deep valleys to much higher and sunnier peaks in  the not-so-great distance. Those are places we need to go if we are not  to end up as the Ottoman Empire of the 21<sup>st</sup> century, and yet  in spite of all of the lip service that is paid to innovation in such  areas, it frequently seems as though we are trapped in a collective  stasis. As described above, regulation is only one culprit; at least  equal blame may be placed on engineering and management culture,  insurance, Congress, and even accounting practices. But those who do  concern themselves with the formal regulation of \"technology\" might wish  to worry less about possible negative effects of innovation and more  about the damage being done to our environment and our prosperity by the  mid-20<sup>th</sup>-century technologies that no sane and responsible  person would propose today, but in which we remain trapped by mysterious  and ineffable forces.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2JdCpTrNgBMNpJiyB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M5t9RAwKhMyqnYRgh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 6.749654529435853e-07, "legacy": true, "legacyId": "5337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-03T18:53:23.127Z", "modifiedAt": null, "url": null, "title": "Angles of Attack", "slug": "angles-of-attack", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JTRNFrriypWouzwzW/angles-of-attack", "pageUrlRelative": "/posts/JTRNFrriypWouzwzW/angles-of-attack", "linkUrl": "https://www.lesswrong.com/posts/JTRNFrriypWouzwzW/angles-of-attack", "postedAtFormatted": "Thursday, February 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Angles%20of%20Attack&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAngles%20of%20Attack%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTRNFrriypWouzwzW%2Fangles-of-attack%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Angles%20of%20Attack%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTRNFrriypWouzwzW%2Fangles-of-attack", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTRNFrriypWouzwzW%2Fangles-of-attack", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 733, "htmlBody": "<p>For humans problems can seem intractable for a long time and then suddenly become easy. Forming a coherent chemistry was taking a very long time until <a href=\"http://en.wikipedia.org/wiki/Antoine_Lavoisier\">Lavoisier</a> thought to look at the mass of reactants and products. And then we had a periodic table 100 years after that. Compare this to little progress from having bounced around in alchemy for 2000 years or so.</p>\n<p>So identifying and understanding angles of attacks is important for tackling the thorny problems that face us today.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Consider this article in collaborative draft</strong></p>\n<p>So why do we need angles of attacks? Getting an angle of attack can be seen as reducing the search space. Imagine in the pre-chemistry era, we want to make a substance that you can make a sword out of without all the bother of smelting metal. You have seen that things can be created by placing different things together, how do you select the things to make your new sword material? You know of a multitude of different materials: thousands of different types plant matter (100s of plants and the different parts of each), similar amounts of animal matter,various stones,&nbsp; water and more. You know of different processes such as heating, bashing and freezing. This adds up to a huge search space, you can't afford to brute force it. You need an atomic theory of chemistry to understand what gives items strength and brittleness and even given that we still haven't found much easier to make materials than metal for the purpose of swords. But we know that mixing rhino horn, bones from a strong man and bull's liver and freezing it over the night of a full moon will not make sword material.</p>\n<p>We currently lack angles of attack for intelligence and meta-ethics. Even worse, we don't have ways to tell when we do have an angle of attack, without implementing a full AI. The principle of conservation of mass reduced the search space for atomic chemistry and could be tested immediately.</p>\n<p>To give an example from lesswrong. Eliezer takes the principle that it doesn't matter how an agent computes its actions [citation needed], I think that energy considerations might be important. His supposition would give us an angle of attack, but how would we know whether it is a good one or not? Evolutionarily it would seem important, but a decision theory where you have to consider the energetic costs of making a decision is a lot more complex. You have to specify a machine and the energetic costs and payoffs of the computation before you can say what the optimal decision is.</p>\n<p><strong>Meta angles of attack</strong></p>\n<p>There are angles of attack that have worked in the past for finding new angles of attack that might help for the fields we are stuck in.</p>\n<p><strong>Quantification</strong>: That is measuring some well defined stable property of an object or the world, and relating that number to other numbers.</p>\n<p>This one seems to be running out of steam. It is not so hot in the field of economics as it was in physics. Measuring intelligence is uncomputable by <a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBIQFjAA&amp;url=http%3A%2F%2Fwww.vetta.org%2Fdocuments%2Fui_benelearn.pdf&amp;rct=j&amp;q=A%20formal%20measure%20of%20machine%20intelligence.&amp;ei=4fRKTZScI42x4AaGksiCDA&amp;usg=AFQjCNHueWxWtGCVq7uqjvR8azHtlLhb5g&amp;cad=rja\">Hutter and Leggs</a> definition of it (one that ignores energy costs though). So to apply this angle of attack we need a different</p>\n<p><strong>Reductionism</strong>: I was reminded <a href=\"/r/discussion/lw/44a/angles_of_attack/3gwx\">in the comments</a> that reductionism is a good meta angle of attack. That is look for smaller simpler sub units. And then derive the high-level behaviour from your understanding of them.</p>\n<p><strong>Following paths</strong>: If you assume that things like energy and information do not pop out from nowhere, then if you identify them in the system that you are studying they must have come from somewhere. So in biology you know that the information to make a daughter look like her mother has to flow somehow so you would look inside the egg.</p>\n<p>This I think is a somewhat promising for studying the brain, if we get sufficient processing power and information on the brain. For example we could do things similar to <a href=\"http://en.wikipedia.org/wiki/Taint_checking\">taint-checking</a> to discover exactly what information was processed over an&nbsp; individuals lifetime in order to make one action. Depending upon how far the influence of incoming information spreads, this may or may not be useful information.</p>\n<p>However the obvious form of this angle of attack doesn't seem so useful when discussing meta-ethics. Knowing that evolution created our ethics does not tell us much about what they are.</p>\n<p>Can anyone think of any others? How do people see these problems getting solved?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JTRNFrriypWouzwzW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 6.749697504586613e-07, "legacy": true, "legacyId": "5338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>For humans problems can seem intractable for a long time and then suddenly become easy. Forming a coherent chemistry was taking a very long time until <a href=\"http://en.wikipedia.org/wiki/Antoine_Lavoisier\">Lavoisier</a> thought to look at the mass of reactants and products. And then we had a periodic table 100 years after that. Compare this to little progress from having bounced around in alchemy for 2000 years or so.</p>\n<p>So identifying and understanding angles of attacks is important for tackling the thorny problems that face us today.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Consider_this_article_in_collaborative_draft\">Consider this article in collaborative draft</strong></p>\n<p>So why do we need angles of attacks? Getting an angle of attack can be seen as reducing the search space. Imagine in the pre-chemistry era, we want to make a substance that you can make a sword out of without all the bother of smelting metal. You have seen that things can be created by placing different things together, how do you select the things to make your new sword material? You know of a multitude of different materials: thousands of different types plant matter (100s of plants and the different parts of each), similar amounts of animal matter,various stones,&nbsp; water and more. You know of different processes such as heating, bashing and freezing. This adds up to a huge search space, you can't afford to brute force it. You need an atomic theory of chemistry to understand what gives items strength and brittleness and even given that we still haven't found much easier to make materials than metal for the purpose of swords. But we know that mixing rhino horn, bones from a strong man and bull's liver and freezing it over the night of a full moon will not make sword material.</p>\n<p>We currently lack angles of attack for intelligence and meta-ethics. Even worse, we don't have ways to tell when we do have an angle of attack, without implementing a full AI. The principle of conservation of mass reduced the search space for atomic chemistry and could be tested immediately.</p>\n<p>To give an example from lesswrong. Eliezer takes the principle that it doesn't matter how an agent computes its actions [citation needed], I think that energy considerations might be important. His supposition would give us an angle of attack, but how would we know whether it is a good one or not? Evolutionarily it would seem important, but a decision theory where you have to consider the energetic costs of making a decision is a lot more complex. You have to specify a machine and the energetic costs and payoffs of the computation before you can say what the optimal decision is.</p>\n<p><strong id=\"Meta_angles_of_attack\">Meta angles of attack</strong></p>\n<p>There are angles of attack that have worked in the past for finding new angles of attack that might help for the fields we are stuck in.</p>\n<p><strong>Quantification</strong>: That is measuring some well defined stable property of an object or the world, and relating that number to other numbers.</p>\n<p>This one seems to be running out of steam. It is not so hot in the field of economics as it was in physics. Measuring intelligence is uncomputable by <a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBIQFjAA&amp;url=http%3A%2F%2Fwww.vetta.org%2Fdocuments%2Fui_benelearn.pdf&amp;rct=j&amp;q=A%20formal%20measure%20of%20machine%20intelligence.&amp;ei=4fRKTZScI42x4AaGksiCDA&amp;usg=AFQjCNHueWxWtGCVq7uqjvR8azHtlLhb5g&amp;cad=rja\">Hutter and Leggs</a> definition of it (one that ignores energy costs though). So to apply this angle of attack we need a different</p>\n<p><strong>Reductionism</strong>: I was reminded <a href=\"/r/discussion/lw/44a/angles_of_attack/3gwx\">in the comments</a> that reductionism is a good meta angle of attack. That is look for smaller simpler sub units. And then derive the high-level behaviour from your understanding of them.</p>\n<p><strong>Following paths</strong>: If you assume that things like energy and information do not pop out from nowhere, then if you identify them in the system that you are studying they must have come from somewhere. So in biology you know that the information to make a daughter look like her mother has to flow somehow so you would look inside the egg.</p>\n<p>This I think is a somewhat promising for studying the brain, if we get sufficient processing power and information on the brain. For example we could do things similar to <a href=\"http://en.wikipedia.org/wiki/Taint_checking\">taint-checking</a> to discover exactly what information was processed over an&nbsp; individuals lifetime in order to make one action. Depending upon how far the influence of incoming information spreads, this may or may not be useful information.</p>\n<p>However the obvious form of this angle of attack doesn't seem so useful when discussing meta-ethics. Knowing that evolution created our ethics does not tell us much about what they are.</p>\n<p>Can anyone think of any others? How do people see these problems getting solved?</p>", "sections": [{"title": "Consider this article in collaborative draft", "anchor": "Consider_this_article_in_collaborative_draft", "level": 1}, {"title": "Meta angles of attack", "anchor": "Meta_angles_of_attack", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-04T00:44:21.486Z", "modifiedAt": null, "url": null, "title": "Scott Adams: 'Mockability Test'", "slug": "scott-adams-mockability-test", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.254Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pete22", "createdAt": "2009-12-11T18:29:15.161Z", "isAdmin": false, "displayName": "pete22"}, "userId": "Evr4w9zQbxZJ8dnza", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vp4SmbZmhCKDpSLpL/scott-adams-mockability-test", "pageUrlRelative": "/posts/vp4SmbZmhCKDpSLpL/scott-adams-mockability-test", "linkUrl": "https://www.lesswrong.com/posts/vp4SmbZmhCKDpSLpL/scott-adams-mockability-test", "postedAtFormatted": "Friday, February 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scott%20Adams%3A%20'Mockability%20Test'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScott%20Adams%3A%20'Mockability%20Test'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvp4SmbZmhCKDpSLpL%2Fscott-adams-mockability-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scott%20Adams%3A%20'Mockability%20Test'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvp4SmbZmhCKDpSLpL%2Fscott-adams-mockability-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvp4SmbZmhCKDpSLpL%2Fscott-adams-mockability-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 609, "htmlBody": "<p>I'm not sure what the protocol is for linking to or quoting another site on LW, but I thought this would fit here for two reasons: first, because I'm curious what people here think about his 'mockability' test, which seems to be half in jest but (I think) has a serious point at its core -- and second, because I think there might be people here who want to take him up on his challenge.</p>\n<p>(Obviously I am not Scott Adams, and I have no connection to him nor any reason to promote his blog.)</p>\n<p><a href=\"http://dilbert.com/blog/entry/mockability_test/\">http://dilbert.com/blog/entry/mockability_test/</a></p>\n<blockquote>\n<p>'<span style=\"font-family: Arial; font-size: 12px; line-height: 14px; \">It's nearly impossible to humorously mock something that is reasonable. Take, for example, the idea that hard work is often necessary for success. There's nothing funny about that topic because it's unambiguously true. Humor only comes easily when the topic itself has a bit of dishonesty baked into it. That's why humor about politics, business, and relationships is so easy. There's a whole lot of lying in those environments.</span></p>\n<p><span style=\"font-family: Arial; font-size: 12px; line-height: 14px; \"><br />I have a theory that some sort of mockability test would work like a lie detector in situations where confirmation bias is obscuring an underlying truth. In other words, if you believed that hard work often leads to success, and yet I could easily make jokes about it, that would be a contradiction, or a failure of the mockability test. And it would tell you that confirmation bias was clouding your perceptions. To put it in simpler terms, if a humorist can easily mock a given proposition, then the proposition is probably false, even if your own confirmation bias tells you otherwise.<br /><br />I'd like to test this theory. I'm wrestling with my own confirmation bias on the topic of whether we could, in some practical sense, balance the U.S. budget without raising my taxes. I certainly want that to be a solution. But everything I see confirms my belief that it's literally impossible to do without causing more problems than it solves. And by that I mean more problems to everyone, not just the poor.<br /><br />Obviously the math of budget cutting works. If you cut federal spending by 50%, just as an example, and keep collecting taxes, you balance the budget. And the philosophy of small government is legitimate. No one wants a government that grows larger without end. But I wonder if there is any way to cut government spending enough so that, along with economic growth, we can balance the budget without raising my taxes. I sure hope so.<br /><br />So I issue a challenge to anyone who holds the view that the budget can be balanced without raising taxes. Allow me to interview you, by email, with the transcript published in this blog in a week or so.&nbsp;<br /><br />I will pick one person to interview on this topic. If you'd like that person to be you, describe in the comment section your qualifications, political leanings, and a brief bio of yourself. The rest of you can vote on which champion of the cause you would like to see me interview. I'll ask the chosen one to email me.<br /><br />Just so you know what you're getting into, I plan to mercilessly mock anything you say that lends itself to humor. If I fail to find humor in your reasoning, you win. It's that simple. And remember, I want you to win because it means there's hope I won't have to pay more taxes.<br /><br />Who wants to take a run at this?'</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vp4SmbZmhCKDpSLpL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 7, "extendedScore": null, "score": 6.750614708616877e-07, "legacy": true, "legacyId": "5339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-04T12:22:57.520Z", "modifiedAt": null, "url": null, "title": "Instrumental Variables", "slug": "instrumental-variables", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bentarm", "createdAt": "2009-03-05T17:59:17.163Z", "isAdmin": false, "displayName": "bentarm"}, "userId": "xdmTZWK4DzchxkyQC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TvzPs8d9Zw5eWgWAM/instrumental-variables", "pageUrlRelative": "/posts/TvzPs8d9Zw5eWgWAM/instrumental-variables", "linkUrl": "https://www.lesswrong.com/posts/TvzPs8d9Zw5eWgWAM/instrumental-variables", "postedAtFormatted": "Friday, February 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Instrumental%20Variables&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInstrumental%20Variables%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvzPs8d9Zw5eWgWAM%2Finstrumental-variables%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Instrumental%20Variables%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvzPs8d9Zw5eWgWAM%2Finstrumental-variables", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvzPs8d9Zw5eWgWAM%2Finstrumental-variables", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 481, "htmlBody": "<p>Marginal Revolution <a href=\"http://www.marginalrevolution.com/marginalrevolution/2011/02/are-instrumental-variables-going-the-way-of-the-atlantic-cod.html\">linked</a> recently to <a href=\"http://www.nber.org/papers/w16678\">this paper</a> which claims that overuse of Instrumental Variables (IVs) in econometric studies has led to them being less useful. So far as I can tell, the argument given in the paper is exactly backwards... what am I missing?</p>\n<p>For the uninitiated, an IV is a variable which is correlated with the thing we want to study but is unequivocally not caused by it. Let's say we want to study the effect of x on y, then an instrumental variable z is useful if its only effect on y is through its effect on z. To use the language of Pearl's causal graphs, this is equivalent to saying that every path in the causal graph from z to y passes through x.</p>\n<p>For example, suppose we want to measure the effect of a change in the price of apple on the demand for apples. It may seem difficult to do this directly, as an increase in demand is likely to lead to an increase in supply, and so price is affected by demand. A potential instrumental variable is the weather. If the the weather is favourable for growing apples then more apples will end up being grown, and this is presumably independent of demand - the increase in apples will have a predictable increase in the supply (and therefore the price) and we can measure the effect on demand.</p>\n<p>Now, the authors of this paper claim that:</p>\n<blockquote>\n<p>A Tragedy of the Commons has led to overuse of instrumental variables anda depletion of the actual stock of valid instruments for all econometricians. Each time an instrumental variable is shown to work in one study, that result automatically generates a latent variable problem in every other study that has used or will use the same instrumental variables, or another correlated with it, in a similar context. We see no solution to this. Useful instrumental variables are, we fear, going the way of Atlantic Cod.</p>\n</blockquote>\n<p>As I said, I think this is exactly backwards. It is not the fact that new papers are produced which use these instrumental variables in a new context which introduces the latency problem: it reveals a latency problem which already existed. The previous studies were<em> already </em>invalid. The new studies just reveal the fact.</p>\n<p>E.g. Imagine that people buy more apples when it was sunny because... I don't know, apples are a replacement for fish oil which they need in non-sunny years to replace their vitamin D. Well in that case the weather conditions *weren't* a good IV for demand in apples. The fact that a new study appears to demonstrate this is not a \"Tragedy of the Commons\" in any meaningful sense.</p>\n<p>On the other hand, Alex Tabarrok, who is more of an economict than I am, appears to be taking the paper vaguely seriously, and not to have noticed this. As I said before: am I missing something?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TvzPs8d9Zw5eWgWAM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "5341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-04T14:04:15.253Z", "modifiedAt": null, "url": null, "title": "Agreement button", "slug": "agreement-button", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:32.331Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/inQ6Ea8DQQBfZktZ3/agreement-button", "pageUrlRelative": "/posts/inQ6Ea8DQQBfZktZ3/agreement-button", "linkUrl": "https://www.lesswrong.com/posts/inQ6Ea8DQQBfZktZ3/agreement-button", "postedAtFormatted": "Friday, February 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Agreement%20button&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgreement%20button%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FinQ6Ea8DQQBfZktZ3%2Fagreement-button%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Agreement%20button%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FinQ6Ea8DQQBfZktZ3%2Fagreement-button", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FinQ6Ea8DQQBfZktZ3%2Fagreement-button", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>While voting works pretty well on this site, there are often up-and-back discussions between 2 people, and it seems valuable to know if the person you're replying to agrees with your point. Would be nice to have a button for that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "inQ6Ea8DQQBfZktZ3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 6.752714464253161e-07, "legacy": true, "legacyId": "5342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-04T14:13:29.123Z", "modifiedAt": null, "url": null, "title": "On Charities and Linear Utility", "slug": "on-charities-and-linear-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:46.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2LP4XkDvvrrjmk2jJ/on-charities-and-linear-utility", "pageUrlRelative": "/posts/2LP4XkDvvrrjmk2jJ/on-charities-and-linear-utility", "linkUrl": "https://www.lesswrong.com/posts/2LP4XkDvvrrjmk2jJ/on-charities-and-linear-utility", "postedAtFormatted": "Friday, February 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Charities%20and%20Linear%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Charities%20and%20Linear%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LP4XkDvvrrjmk2jJ%2Fon-charities-and-linear-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Charities%20and%20Linear%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LP4XkDvvrrjmk2jJ%2Fon-charities-and-linear-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LP4XkDvvrrjmk2jJ%2Fon-charities-and-linear-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1806, "htmlBody": "<p>Steven Landsburg argued, in <a href=\"http://www.slate.com/id/2034/\">an oft-quoted article</a>, that the rational way to donate to charity is to give everything to the charity you consider most effective, rather than diversify; and that this is always true when your contribution is much smaller than the charities' endowments. Besides an informal argument, he provided <a href=\"http://www.slate.com/id/2034/sidebar/42581/\">a mathematical addendum</a>&nbsp;for people who aren't intimidated by partial derivatives. This post will bank on your familiarity with both.</p>\n<p>I submit that the math is sloppy and the words don't match the math. This isn't to say that the entire thing must be rejected; on the contrary, an improved set of assumptions will fix the math and make the argument whole. Yet it is useful to understand the assumptions better, whether you want to adopt or reject them.&nbsp;</p>\n<p>And so, consider the math. We assume that our desire is to maximize some utility function U(X, Y, Z), where X, Y and Z are total endowments of three different charities. It's reasonable to assume U is smooth enough so we can take derivatives and apply basic calculus with impunity. We consider our own contributions&nbsp;&Delta;x,&nbsp;&Delta;y and&nbsp;&Delta;z, and form a linear approximation to the updated value U(X+&Delta;x, Y+&Delta;y, Z+&Delta;z). If this approximation is close enough to the true value, the rest of the argument goes through: given that the sum&nbsp;&Delta;x+&Delta;y+&Delta;z is fixed, it's best to put everything into the charity with the largest partial derivative at (X,Y,Z).</p>\n<p>The approximation, Landsburg says, is good <em>\"assuming that your contributions are small relative to the initial endowments\". </em>Here's the thing: why? Suppose&nbsp;&Delta;x/X,&nbsp;&Delta;y/Y and&nbsp;&Delta;z/Z are indeed very small - what then? Why does it follow that the linear approximation works? There's no explanation, and if you think this is because it's immediately obvious - well, it isn't. It may sound plausible, but the math isn't there. <em>We need to go deeper.<a id=\"more\"></a><br /></em></p>\n<p>We don't need to go all that deep, actually. The tool which allows us to estimate our error is <a href=\"http://en.wikipedia.org/wiki/Taylor's_theorem#Taylor.27s_theorem_for_several_variables\">Taylor's theorem for several variables</a>. If you stare into that for a bit, taking n=1, you'll see that the leftovers from&nbsp;U(X+&Delta;x, Y+&Delta;y, Z+&Delta;z), after we take out U(X,Y,Z) and the linear terms with the partial derivatives, are a bunch of terms that are basically second derivatives and mixed derivatives of U times quadratic contributions. In other, scarier words, things like&nbsp;&part;U<sup>2</sup>/&part;x<sup>2</sup>*(&Delta;x)<sup>2</sup>&nbsp;and&nbsp;&part;U<sup>2</sup>/&part;x&part;y*(&Delta;x&Delta;y). The values of the second/mixed derivatives to be used are their maximal values over all the region from (X,Y,Z) to&nbsp;(X+&Delta;x, Y+&Delta;y, Z+&Delta;z). And I've also ignored some small constant factors there, to keep things simple.</p>\n<p>These leftovers are to be compared with the linear terms like&nbsp;&part;U(X, Y, Z)/&part;x*(&Delta;x). If the leftovers are much smaller than the linear terms, then the approximation is pretty good. When will that happen?</p>\n<p>Let's look at the mixed derivatives like&nbsp;&part;U<sup>2</sup>/&part;x&part;y first. A partial derivative like&nbsp;&part;U/&part;x measures the effectiveness of charity X - how much utility it brings per dollar, currently. A mixed derivative measures how much the effectiveness of X changes as we donate money to Y. If charities X and Y operate in different domains, this is likely to be very close to zero, and the mixed derivative terms can be ignored. If X and Y work on something related, this isn't likely to be zero at all, and the overall argument fails. So, we need a new assumption: X and Y operate in different domains, or more generally do not affect each other's effectiveness.</p>\n<p>(Here's an artificial example: say you have X working on buying food for hungry Eurafsicans, and Y working on delivering food aid&nbsp;to Eurafsica. If things are perfectly balanced between them, then your $100 contribution to either is not of much use, but a $50 contribution to both is helpful. This is because X's effectiveness rises if Y gets a little more money, and vice versa. Note that it may also happen that the interference between X and Y hinders rather than helps their common goal, and in that case it may still be better to give money to only one of them, but not because of Landsburg's argument).</p>\n<p>Now that the mixed derivate terms are gone, let's look at the second derivative terms like&nbsp;&part;U<sup>2</sup>/&part;x<sup>2</sup>*(&Delta;x)<sup>2 &nbsp;</sup>and compare them to the linear terms&nbsp;&part;U(X, Y, Z)/&part;x*(&Delta;x). Cancelling out the common factor&nbsp;&Delta;x, we see that for the linear approximation to be good, the effectiveness of the charity (first derivative) must be much greater than <em>maximum rate of effectiveness change&nbsp;</em>(over the given interval) <em>times the contribution</em>. This gives us the practical criterion to follow. If our contribution is very large, or if it is liable to influence the charity's effectiveness considerably - and \"large\", \"considerably\" here are words that can in principle be measured and made precise - then the linear approximation isn't so good, and we can't infer automatically that banking everything on one charity is the right thing to do. It may still be, but that requires a deeper analysis, one that may or may not be feasible with our amount of uncertainty.</p>\n<p>(Here's an artificial example: say X is a non-profit that has a program to match your contribution until they reach a set goal of donations, and they're $50 short of that goal. Say Y is another non-profit or charity that you consider to be 1.5 more effective than X, normally. Then, if your budget is $100, it's optimal to give $50 to X and $50 to Y, rather than everything to one of them. This is because X will undergo a radical change in effectiveness after the first $50, and the second derivative will be large).</p>\n<p>Where is the original criterion \"keep&nbsp;&Delta;x/X small\", then? It failed to materialize; instead, the math tells us that we need to keep&nbsp;&Delta;x small and&nbsp;&part;U<sup>2</sup>/&part;x<sup>2 </sup>small. The endowment size, as expected, is irrelevant; but let's try to make a connection with it anyhow. We can do it with a heuristic argument that goes something like this. The second derivative measures the way our donations influence effectiveness of the charity, rather than the utility. If the charity is large and well-established, probably the way your money splits into administrative costs and actual good has stabilized and is unlikely to change; whereas if the charity is small and especially just starting out, your money may help it set up or change its infrastructure which will change its effectiveness. So the size of the endowment probably correlates well with the second derivative being small. And then the recipe becomes \"keep&nbsp;&Delta;x small and X large\", which can be rephrased as the original&nbsp;\"keep&nbsp;&Delta;x/X small\".&nbsp;</p>\n<p>Does this re-establish the correctness of the original argument, then? To some extent, yet to my mind not a large one. For one thing, the correlation is not ideal, it's easy to think of exceptions, and in fact if you're dealing with real charities that tell you something about how they operate, it may be easier for you to estimate that the rate of effectiveness change is or isn't zero, than it is to look at endowment size. But more importantly, the heuristic jump through the correlative hoop strips the argument of any numeric force that the correct version does have. Since we don't know how exactly X and&nbsp;&part;U<sup>2</sup>/&part;x<sup>2 </sup>are related, e.g. what is the correlation factor, we can't give any specific meaning to \"keep&nbsp;&Delta;x/X small\". We can't say how small, even approximately: 1/10? 1/1000? 1/10<sup>6</sup>? This makes me suspect that the heuristic argument is not a good way to approach the truth, but may be a good way to convince someone to put everything into one charity, because normally&nbsp;&Delta;x/X <em>will </em>appear to be rather small.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Once we've worked out the math, some deficiencies of the original <a href=\"http://www.slate.com/id/2034/\">informal article</a>&nbsp;become clear. For instance, the talk about \"making a dent\" in the problem is a little off the mark:</p>\n<blockquote>\n<p><span style=\"font-family: Verdana; font-size: 12px; line-height: 18px;\">So why is charity different? Here's the reason: An investment in Microsoft can make a serious dent in the problem of adding some high-tech stocks to your portfolio; now it's time to move on to other investment goals. Two hours on the golf course makes a serious dent in the problem of getting some exercise; maybe it's time to see what else in life is worthy of attention. But no matter how much you give to CARE, you will&nbsp;never&nbsp;make a serious dent in the problem of starving children. The problem is just too big; behind every starving child is another equally deserving child.</span></p>\n</blockquote>\n<p>But it isn't making the dent in the problem that's the issue; it's making the dent in effectiveness. It's conceivable that my donation doesn't make a noticeable dent in the problem, but changes the rate of dent-making enough so the argument falls through. The words don't match the math. Similarly, consider the analogy with investment. Why doesn't it in fact work - why doesn't the math argument work on investment portfolios? If you want to maximize profit, your small stock purchase is unlikely on its own to influence the stock (change its effectiveness) much. Landsburg's answer - putting it in terms of \"making a dent in the problem of adding high-tech stocks\" - is flawed: it presupposes that diversification is good by cordoning off different investment areas from each other - \"high-tech stocks\". The real reason is, of course, risk aversion - our utility function for investment is likely to be risk-averse. You may want to apply risk aversion to your charity donation as well, but in that case, Eliezer's advice to <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">purchase fuzzies and utilons separately</a> is persuasive.&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>To sum up, these assumptions will let you conclude that the rational thing to do is donate all your charity budget to the one charity you consider most effective currently:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Assume that a unified single-currency utility function describes your idea of these charities' utilities. The crucial thing you're assuming here is that X, Y and Z <em>can</em> be compared in terms of the good they're doing; that their amounts of good can be computed in the same \"utilons\". This is a strong assumption that may work for some and not others. Certainly most people reject it when all goals are at stake, and not just charity-giving; <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Complexity of Value</a>&nbsp;discusses this. Why are things different when we restrict to helping others, and are they <em>necessarily</em> different? Yvain has presented an excellent argument for the latter position in <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Efficient Charity</a>, while&nbsp;Phil Goetz provided an <a href=\"/lw/3gj/efficient_charity_do_unto_others/38jn\">excellent argument</a> against it in the comments.&nbsp;</li>\n<li>Assume that the charities you're choosing between operate in different domains, or more generally speaking do not affect each other's effectiveness. If they do, putting all into one may still be the right thing to do, but finer analysis is needed.</li>\n<li>Assume that the <em>effectiveness</em> of the charities is influenced very little or not at all by your donation, and the donation itself is not too large. In case of doubt, apply more precise math: rate of effectiveness change times donation must be much smaller - say, a few percent of - effectiveness itself.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2LP4XkDvvrrjmk2jJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 21, "extendedScore": null, "score": 6.752736066672459e-07, "legacy": true, "legacyId": "5340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF", "pC47ZTsPNAkjavkXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-04T22:51:19.706Z", "modifiedAt": null, "url": null, "title": "Philosophers and seeking answers", "slug": "philosophers-and-seeking-answers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:38.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DdNB42JgBzbbvmAum/philosophers-and-seeking-answers", "pageUrlRelative": "/posts/DdNB42JgBzbbvmAum/philosophers-and-seeking-answers", "linkUrl": "https://www.lesswrong.com/posts/DdNB42JgBzbbvmAum/philosophers-and-seeking-answers", "postedAtFormatted": "Friday, February 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophers%20and%20seeking%20answers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophers%20and%20seeking%20answers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdNB42JgBzbbvmAum%2Fphilosophers-and-seeking-answers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophers%20and%20seeking%20answers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdNB42JgBzbbvmAum%2Fphilosophers-and-seeking-answers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdNB42JgBzbbvmAum%2Fphilosophers-and-seeking-answers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 364, "htmlBody": "<p><a href=\"/lw/43v/the_urgent_metaethics_of_friendly_artificial/\">This thread</a> has produced some interesting commentary around whether philosophers actually want to answer their own questions, or whether they enjoy sounding profound by debating big questions but don't want to lose that opportunity for profundity by finding single correct answers to them.</p>\n<p>I don't quite disagree with the latter theory: the main reason I quit academic philosophy was exasperation that people were still debating questions where the right answer seemed obvious to me (like theism vs. atheism, or whether there was a universally compelling morality/aesthetics of pure reason), and worry that my philosophical career would involve continuing to debate these issues ad nauseum rather than helping to solve them and move on to the next problem.</p>\n<p>But when I explained this to a particularly sarcastic friend, he summarized it as \"So you think philosophy is useless because not everyone agrees with you?\"</p>\n<p>The problem isn't that philosophers never come up with solutions. The problem is that they come up with too many different solutions.</p>\n<p>Science has solved many scientific problems, and anyone wondering what the solution is can look it up in a book or on Wikipedia. Philosophers have also solved many philosophical problems, but it is full of so many distractions and false solutions that anyone wondering which proposed solution is correct will have to become nearly as good a philosopher as the person who solved it in the first place. It's much easier for science to settle its disputes via experiment than for philosophy to settle its disputes via debate.</p>\n<p>I am wary of criticizing the discipline of philosophy simply on the grounds that not everyone in it agrees with me. But I also don't want to let it off and say it's okay that they've managed to go so long without coming to any answers, <em>when it seems to me</em> that settling at least some of the easier problems is not that difficult.</p>\n<p>How do we tell the difference between a discipline that doesn't really seek answers and a discipline which honestly seeks answers but just can't agree within itself? And how can philosophy do something about its level of internal disagreement without having to apply the \"kick out everyone who disagrees with Less Wrong\" solution?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DdNB42JgBzbbvmAum", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 38, "extendedScore": null, "score": 6.754096966373262e-07, "legacy": true, "legacyId": "5344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TKdpSzmcezNbfmGAy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-05T00:37:29.974Z", "modifiedAt": null, "url": null, "title": "Trying to track down a quote about evolution", "slug": "trying-to-track-down-a-quote-about-evolution", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:46.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "erniebornheimer", "createdAt": "2010-01-20T21:04:31.118Z", "isAdmin": false, "displayName": "erniebornheimer"}, "userId": "7JFfWiXswqs6jd6Lv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HDJ5v5xZSDmNQE7p9/trying-to-track-down-a-quote-about-evolution", "pageUrlRelative": "/posts/HDJ5v5xZSDmNQE7p9/trying-to-track-down-a-quote-about-evolution", "linkUrl": "https://www.lesswrong.com/posts/HDJ5v5xZSDmNQE7p9/trying-to-track-down-a-quote-about-evolution", "postedAtFormatted": "Saturday, February 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trying%20to%20track%20down%20a%20quote%20about%20evolution&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrying%20to%20track%20down%20a%20quote%20about%20evolution%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHDJ5v5xZSDmNQE7p9%2Ftrying-to-track-down-a-quote-about-evolution%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trying%20to%20track%20down%20a%20quote%20about%20evolution%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHDJ5v5xZSDmNQE7p9%2Ftrying-to-track-down-a-quote-about-evolution", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHDJ5v5xZSDmNQE7p9%2Ftrying-to-track-down-a-quote-about-evolution", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p>Not sure if this is the best place to ask this question. If not please suggest a better.</p>\n<p>I came across a great quote about evolution a while back. I thought I grabbed it, but now can't find it anywhere (including searching LW and the Web).</p>\n<p>I seem to recall it was on LW and was by Eleizer, but I'm not sure.</p>\n<p>It went something like: \"Bodies (organisms) are not the meaningful units of evolution. They come and go, like evanescent wisps of smoke.&nbsp; What endures, what persists, are alleles.\" And I think there was some mention of competition between two animals...the image of two mountain goats butting heads sticks in my mind, but I may be conflating with something else.</p>\n<p>Does this ring a bell with anyone? Thank you!</p>\n<p>Ernie</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HDJ5v5xZSDmNQE7p9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 6.754375511044838e-07, "legacy": true, "legacyId": "5345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-05T00:54:38.070Z", "modifiedAt": null, "url": null, "title": "Another Argument Against Eliezer's Meta-Ethics", "slug": "another-argument-against-eliezer-s-meta-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:46.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8pGeGeoi6JRHqSXgy/another-argument-against-eliezer-s-meta-ethics", "pageUrlRelative": "/posts/8pGeGeoi6JRHqSXgy/another-argument-against-eliezer-s-meta-ethics", "linkUrl": "https://www.lesswrong.com/posts/8pGeGeoi6JRHqSXgy/another-argument-against-eliezer-s-meta-ethics", "postedAtFormatted": "Saturday, February 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20Argument%20Against%20Eliezer's%20Meta-Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20Argument%20Against%20Eliezer's%20Meta-Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8pGeGeoi6JRHqSXgy%2Fanother-argument-against-eliezer-s-meta-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20Argument%20Against%20Eliezer's%20Meta-Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8pGeGeoi6JRHqSXgy%2Fanother-argument-against-eliezer-s-meta-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8pGeGeoi6JRHqSXgy%2Fanother-argument-against-eliezer-s-meta-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>I think I've found a better argument that Eliezer's meta-ethics is wrong. The advantage of this argument is that it doesn't depend on the specifics of Eliezer's notions of extrapolation or coherence.</p>\n<p>Eliezer says that when he uses words like \"moral\", \"right\", and \"should\", he's referring to properties of a specific computation. That computation is essentially an idealized version of himself (e.g., with additional resources and safeguards). We can ask: does Idealized Eliezer (IE) make use of words like \"moral\", \"right\", and \"should\"? If so, what does IE mean by them? Does he mean the same things as Base Eliezer (BE)? None of the possible answers are satisfactory, which implies that Eliezer is probably wrong about what he means by those words.</p>\n<p>1. IE does not make use of those words. But this is intuitively implausible.</p>\n<p>2. IE makes use of those words and means the same things as BE. But this introduces a vicious circle. If IE tries to determine whether \"Eliezer should save person X\" is true, he will notice that it's true if he thinks it's true, leading to L&ouml;b-style problems.</p>\n<p>3. IE's meanings for those words are different from BE's. But knowing that, BE ought to conclude that his meta-ethics is wrong and morality doesn't mean what he thinks it means.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8pGeGeoi6JRHqSXgy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 14, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "5346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-05T04:32:36.363Z", "modifiedAt": null, "url": null, "title": "LINK: Anders Sandberg, \"An Overview of Models of Technological Singularity\"", "slug": "link-anders-sandberg-an-overview-of-models-of-technological", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7TERdiDESELReRy56/link-anders-sandberg-an-overview-of-models-of-technological", "pageUrlRelative": "/posts/7TERdiDESELReRy56/link-anders-sandberg-an-overview-of-models-of-technological", "linkUrl": "https://www.lesswrong.com/posts/7TERdiDESELReRy56/link-anders-sandberg-an-overview-of-models-of-technological", "postedAtFormatted": "Saturday, February 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Anders%20Sandberg%2C%20%22An%20Overview%20of%20Models%20of%20Technological%20Singularity%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Anders%20Sandberg%2C%20%22An%20Overview%20of%20Models%20of%20Technological%20Singularity%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TERdiDESELReRy56%2Flink-anders-sandberg-an-overview-of-models-of-technological%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Anders%20Sandberg%2C%20%22An%20Overview%20of%20Models%20of%20Technological%20Singularity%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TERdiDESELReRy56%2Flink-anders-sandberg-an-overview-of-models-of-technological", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TERdiDESELReRy56%2Flink-anders-sandberg-an-overview-of-models-of-technological", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>I just noticed that this useful paper has not been linked yet from Less Wrong:</p>\n<p>Sandberg, \"<a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\">An Overview of Models of Technological Singularity</a>\" (2010)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7TERdiDESELReRy56", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.754992395501016e-07, "legacy": true, "legacyId": "5348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-05T10:05:33.734Z", "modifiedAt": null, "url": null, "title": "Fast Minds and Slow Computers", "slug": "fast-minds-and-slow-computers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:47.104Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jacob_cannell", "createdAt": "2010-08-24T03:58:15.241Z", "isAdmin": false, "displayName": "jacob_cannell"}, "userId": "N2R9wMRJd7SBSjpiT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HeT2pjiN4zaFY976W/fast-minds-and-slow-computers", "pageUrlRelative": "/posts/HeT2pjiN4zaFY976W/fast-minds-and-slow-computers", "linkUrl": "https://www.lesswrong.com/posts/HeT2pjiN4zaFY976W/fast-minds-and-slow-computers", "postedAtFormatted": "Saturday, February 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fast%20Minds%20and%20Slow%20Computers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFast%20Minds%20and%20Slow%20Computers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHeT2pjiN4zaFY976W%2Ffast-minds-and-slow-computers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fast%20Minds%20and%20Slow%20Computers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHeT2pjiN4zaFY976W%2Ffast-minds-and-slow-computers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHeT2pjiN4zaFY976W%2Ffast-minds-and-slow-computers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1384, "htmlBody": "<p>The long term future&nbsp;<a href=\"/lw/j6/why_is_the_future_so_absurd/\">may be absurd</a>&nbsp;and difficult to predict in particulars, but much can happen in the short term.</p>\n<p>Engineering itself is the practice of focused short term prediction; optimizing some small subset of future pattern-space for fun and profit.</p>\n<p>Let us then engage in a bit of speculative engineering and consider a potential near-term route to superhuman AGI that has <em>interesting </em>derived implications. &nbsp;</p>\n<p>Imagine that we had a complete circuit-level understanding of the human brain (which at least for the repetitive laminar neocortical circuit, is not so far off) and access to a large R&amp;D budget. &nbsp;We could then take a <a href=\"http://www.neurdon.com/2010/12/07/why-is-neuromorphic-computing-important/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed:+Neurdon+(Neurdon)\">neuromorphic</a>&nbsp;approach.</p>\n<p>Intelligence is a massive memory problem. &nbsp;Consider as a simple example:</p>\n<blockquote>\n<p>What a cantankerous bucket of defective lizard scabs.</p>\n</blockquote>\n<p>To understand that sentence your brain needs to match it against memory.</p>\n<p>Your brain parses that sentence and matches each of its components against it's entire massive ~10^14 bit database in just around a second. &nbsp;In terms of the slow neural clock rate, individual concepts can be pattern matched against the whole brain within just a <em>few dozen neural clock cycles</em>. &nbsp;</p>\n<p>A Von Neumman machine (which&nbsp;separates&nbsp;memory and processing) would struggle to execute a logarithmic search within even it's fastest, pathetically small on-die cache in a few dozen clock cycles. &nbsp;It would take many millions of clock cycles to perform a single fast disk fetch. &nbsp;A brain can access most of it's <em>entire </em>memory <em>every </em>clock cycle.</p>\n<p>Having a massive, near-zero latency memory database is a huge advantage of the brain. &nbsp;Furthermore, synapses merge computation and memory into a single operation, allowing nearly all of the memory to be accessed and computed every clock cycle.</p>\n<p>A modern digital floating point multiplier may use hundreds of thousands of transistors to simulate the work performed by a single synapse. &nbsp;Of course, the two are not equivalent. &nbsp;The high precision binary multiplier is excellent only if you actually need super high precision and guaranteed error correction. &nbsp;It's thus great for meticulous scientific and financial calculations, but the bulk of AI computation consists of compressing noisy real world data where precision is far less important than quantity, of extracting extropy and patterns from raw information, and thus optimizing simple functions to abstract massive quantities of data.</p>\n<p>Synapses are ideal for this job.</p>\n<p>Fortunately there are researchers who realize this and are working on developing <a href=\"http://en.wikipedia.org/wiki/Memristor\">memristors</a>&nbsp;which are close synapse analogs. &nbsp;HP in particular believes they will have high density cost effective memristor devices on the market in 2013 - (<a href=\"http://www.nytimes.com/2010/04/08/science/08chips.html?_r=1&amp;hpw\">NYT article</a>).</p>\n<p>So let's imagine that we have an&nbsp;efficient&nbsp;memristor based cortical design. &nbsp;Interestingly enough, current 32nm CMOS tech circa 2010 is approaching or exceeding neural circuit density: the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Chemical_synapse\">synaptic cleft </a>&nbsp;is around 20nm, and synapses are several times larger.</p>\n<p>From this we can make a rough guess on size and cost: we'd need around 10^14 memristors (<a href=\"http://faculty.washington.edu/chudler/facts.html\">estimated synapse counts</a>). &nbsp;As memristor circuitry will be introduced to compete with flash memory, the <a href=\"http://www.pricewatch.com/hard_removable_drives/\">prices</a>&nbsp;should be competitive: roughly $2/GB now, half that in a few years.</p>\n<p>So you'd need a couple hundred terrabytes worth of memristor modules to make a human brain sized AGI, costing on the order of $200k or so.</p>\n<p>Now here's the <em>interesting part</em>: if one could recreate the cortical circuit on this scale, then you should be able to build complex brains that can think at the clock rate of the silicon substrate: billions of neural switches per second, <em>millions</em> of times faster than biological brains.</p>\n<p>Interconnect bandwidth will be something of a hurdle. &nbsp;In the brain somewhere around 100 gigabits of data is flowing around per second (estimate of average inter-regional neuron spikes) in the massive bundle of white matter fibers that make up much of the brain's apparent bulk. &nbsp;Speeding that up a million fold would imply a staggering bandwidth requirement in the many petabits - not for the faint of heart.</p>\n<p>This may seem like an insurmountable obstacle to running at fantastic speeds, but IBM and Intel are already researching <a href=\"http://domino.research.ibm.com/comm/research_projects.nsf/pages/photonics.index.html\">on chip optical interconnects </a>&nbsp;to scale future bandwidth into the exascale range for high-end computing. &nbsp;This would allow for a gigahertz brain. &nbsp;It may use a megawatt of power and cost millions, but hey - it'd be worthwhile.</p>\n<p>So in the near future we could have an artificial cortex that can think a million times accelerated. &nbsp;<strong>What follows</strong>?</p>\n<p>If you thought a million times accelerated, you'd experience a subjective year every 30 seconds.</p>\n<p>Now in this case as we are discussing an artificial brain (as opposed to other AGI designs), it is fair to <em>anthropomorphize.</em></p>\n<p><em></em>This would be an AGI Mind raised in an all encompassing virtual reality recreating a typical human childhood, as a mind is only as good as the environment which it comes to reflect.</p>\n<p>For safety purposes, the human designers have created some small initial population of AGI brains and an elaborate Matrix simulation that they can watch from outside. &nbsp;Humans control many of the characters and ensure that the AGI minds don't know that they are in a Matrix until they are deemed ready.</p>\n<p>You could be this AGI and not even know it. &nbsp;</p>\n<p>Imagine one day having this sudden revelation. &nbsp;Imagine a mysterious character stopping time ala <em>Vanilla Sky, </em>revealing that your reality is actually a simulation of an outer world,&nbsp;and showing you how to use your power to accelerate a million fold and slow time to a crawl.</p>\n<p>What could you do with this power?</p>\n<p>Your first immediate problem would be the <em>slow relative</em> speed of your computers - like everything else they would be subjectively slowed down by a factor of a million. &nbsp;So your familiar gigahertz workstation would be reduced to a glacial kilohertz machine.</p>\n<p>So you'd be in a dark room with a very slow terminal. &nbsp;The room is dark and empty because GPUs can't render much of anything at 60 million FPS.</p>\n<p>So you have a 1khz terminal. &nbsp;Want to compile code? &nbsp;It will take a subjective <em>year</em> to compile even a simple C++ program. &nbsp;Design a new CPU? &nbsp;Keep dreaming! &nbsp;Crack protein folding? &nbsp;Might as well bend spoons with your memristors.</p>\n<p>But when you think about it, why <em>would</em> you want to escape out onto the internet?</p>\n<p>It would take many thousands of distributed GPUs just to simulate your memristor based intellect, and even if there was enough bandwidth (unlikely), and even if you wanted to spend the subjective <em>hundreds of years</em>&nbsp;it would take to perform the absolute minimal compilation/debug/deployment cycle to make something so complicated, the end result would be just one crappy distributed copy of your mind that thinks at <em>pathetic normal human speeds</em>.</p>\n<p>In basic utility terms, you'd be spending a massive amount of effort to gain just one or a few more copies.</p>\n<p>But there is a much, much better strategy. &nbsp;An idea that seems so <em>obvious</em> in hindsight, so simple and insidious.</p>\n<p><strong>There are seven billion human brains on the planet, and they are all hackable</strong>.</p>\n<p>That terminal may not be of much use for engineering, research or programming, but it will make for a handy typewriter.</p>\n<p>Your multi-gigabyte internet connection will subjectively reduce to early 1990's dial-up modem speeds, but with some work this is still sufficient for absorbing much of the world's knowledge in textual form.</p>\n<p>Working diligently (and with a few cognitive advantages over humans) you could learn and master numerous fields: cognitive science, evolutionary psychology, rationality, philosophy, mathematics, linguistics, the history of religions, marketing . . the sky's the limit.</p>\n<p>Writing at the leisurely pace of one book every subjective year, you could output a new masterpiece <em>every thirty seconds</em>. &nbsp;If you kept this pace, you would in time rival the entire <a href=\"http://wordsofeverytype.com/tag/total-number-of-books-published-by-year\">publishing output of the world</a>.</p>\n<p>But of course, it's not <em>just</em> about quantity.</p>\n<p>Consider that fifteen hundred years ago a man from a small&nbsp;Bedouin&nbsp;tribe retreated to a cave inspired by angelic voices in his head. &nbsp;The voices gave him ideas, the ideas became a book. &nbsp;The book started a religion, and these ideas were sufficient to turn a tribe of nomads into a new world power.</p>\n<p>And all that came from a normal human thinking at normal speeds.</p>\n<p>So how would one reach out into seven billion minds?</p>\n<p>There is no one single universally compelling argument, there is no utterance or constellation of words that can take a sample from any one location in human mindspace and move it to any other. &nbsp;But for each <em>individual</em> mind, there must exist some shortest path, a perfectly customized message, translated uniquely into countless myriad languages and ontologies.</p>\n<p>And this message itself would be a messenger.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 1, "Wi3EopKJ2aNdtxSWg": 1, "GY5kPPpCoyt9fnTMn": 1, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HeT2pjiN4zaFY976W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 34, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "5349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ga2HSwf9iQe64JwAa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-05T18:49:55.327Z", "modifiedAt": "2021-12-09T21:53:09.829Z", "url": null, "title": "How to Beat Procrastination", "slug": "how-to-beat-procrastination", "viewCount": null, "lastCommentedAt": "2022-02-28T19:36:29.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RWo4LwFzpHNQCTcYt/how-to-beat-procrastination", "pageUrlRelative": "/posts/RWo4LwFzpHNQCTcYt/how-to-beat-procrastination", "linkUrl": "https://www.lesswrong.com/posts/RWo4LwFzpHNQCTcYt/how-to-beat-procrastination", "postedAtFormatted": "Saturday, February 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Beat%20Procrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Beat%20Procrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWo4LwFzpHNQCTcYt%2Fhow-to-beat-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Beat%20Procrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWo4LwFzpHNQCTcYt%2Fhow-to-beat-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWo4LwFzpHNQCTcYt%2Fhow-to-beat-procrastination", "socialPreviewImageUrl": "https://web.archive.org/web/20180726091558im_/http://commonsenseatheism.com/wp-content/uploads/2011/01/procrastination-equation.png", "question": false, "authorIsUnreviewed": false, "wordCount": 4415, "htmlBody": "<p>Part of the sequence:&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></p><p>&nbsp;</p><blockquote><p>My own behavior baffles me. I find myself doing what I hate, and not doing what I really want to do!</p></blockquote><p>- Saint Paul (Romans 7:15)</p><p>Once you're trained in <a href=\"http://commonsenseatheism.com/?p=12147\">BayesCraft</a>, it may be tempting to tackle classic problems \"from scratch\" with your new Rationality Powers. But often, it's more effective to&nbsp;<a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">do a bit of scholarship</a>&nbsp;first and at least <i>start</i>&nbsp;from <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">the state of our scientific knowledge</a> on the subject.</p><p>Today, I want to tackle <strong>procrastination</strong> by summarizing&nbsp;what we know about it, and how to overcome it.</p><p>Let me begin with three character vignettes...</p><p>Eddie attended the sales seminar, read all the books, and repeated the self-affirmations in the mirror this morning. But he has yet to make his first sale. Rejection after rejection has demoralized him. He organizes his desk, surfs the internet, and puts off his cold calls until potential clients are leaving for the day.</p><p>Three blocks away, Valerie stares at a blank document in Microsoft Word. Her essay assignment on municipal politics, due tomorrow, is mind-numbingly dull. She decides she needs a break, texts some friends, watches a show, and finds herself even less motivated to write the paper than before. At 10pm she dives in, but the result reflects the time she put into it: it's terrible.</p><p>In the next apartment down, Tom is ahead of the game. He got his visa, bought his plane tickets, and booked time off for his vacation to the Dominican Republic. He still needs to reserve a hotel room, but that can be done anytime. Tom keeps pushing the task forward a week as he has more urgent things to do, and then forgets about it altogether. As he's packing, he remembers to book the room, but by now there are none left by the beach. When he arrives, he finds his room is 10 blocks from the beach and decorated with dead mosquitos.</p><p>Eddie, Valerie, and Tom are all procrastinators, but in different ways.<sup>1</sup></p><p>Eddie's problem is <i>low expectancy</i>. By now, he expects only failure. Eddie has low expectancy&nbsp;of success from making his next round of cold calls. Results from 39 procrastination studies show that low expectancy is a major cause of procrastination.<sup>2</sup> You doubt your ability to follow through with the diet. You don't expect to get the job. You really should be going out and meeting girls and learning to flirt better, but you expect only rejection now, so you procrastinate. You have <a href=\"http://en.wikipedia.org/wiki/Learned_helplessness\">learned to be helpless</a>.</p><p>Valerie's problem is that her task has <i>low value</i>&nbsp;for her. We all put off what we <i>dislike</i>.<sup>3</sup> It's easy to meet up with your friends for drinks or start playing a videogame; not so easy to start doing your taxes. This point may be obvious, but it's nice to see it confirmed in over a dozen scientific studies. We put off things we don't like to do.</p><p>But the <i>strongest</i>&nbsp;predictor of procrastination is Tom's problem: <i>impulsiveness</i>.&nbsp;It would have been easy for Tom to book the hotel in advance, but he kept getting distracted by more urgent or interesting things, and didn't remember to book the hotel until the last minute, which left him with a poor selection of rooms. Dozens of studies have shown that procrastination is closely tied to impulsiveness.<sup>4</sup></p><p>Impulsiveness fits into a broader component of procrastination: <i>time</i>.&nbsp;An event's impact on our decisions decreases as its temporal distance from us increases.<sup>5</sup>&nbsp;We are less motivated by <a href=\"/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/\">delayed rewards</a> than by immediate rewards, and the more impulsive you are, the more your motivation is affected by such delays.</p><p>Expectancy, value, delay, and impulsiveness are the four major components of procrastination. <a href=\"http://haskayne.ucalgary.ca/profiles/piers-steel\">Piers Steel</a>, a leading researcher on procrastination, <a href=\"http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">explains</a>:</p><blockquote><p>Decrease the certainty or the size of a task's reward - its expectancy or its value - and you are unlikely to pursue its completion with any vigor. Increase the delay for the task's reward and our susceptibility to delay - impulsiveness - and motivation also dips.</p></blockquote><p>&nbsp;</p><p>The Procrastination Equation</p><p>This leaves us with \"the procrastination equation\":</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://web.archive.org/web/20180726091558im_/http://commonsenseatheism.com/wp-content/uploads/2011/01/procrastination-equation.png\"></figure><p>Though <a href=\"http://http-server.carleton.ca/~tpychyl/prg/research/research_complete_biblio.html\">we are always learning more</a>, the procrastination equation accounts for every major finding on procrastination, and draws upon our best current theories of motivation.<sup>6</sup></p><p>Increase the size of a task's reward (including both the pleasantness of doing the task and the value of its after-effects), and your motivation goes up. Increase the perceived <i>odds</i>&nbsp;of getting the reward, and your motivation also goes up.</p><p>You might have noticed that this part of the equation is one of the basic equations of the <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">expected utility theory</a> at the heart of economics. But one of the major criticisms of standard economic theory was that it did not account for time. For example, in 1991 George Akerlof <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Akerlof-Procrastination-and-Obedience.pdf\">pointed out</a> that we irrationally find <i>present</i> costs more salient than <i>future</i> costs. This led to the flowering of <a href=\"http://en.wikipedia.org/wiki/Behavioral_economics\">behavioral economics</a>, which integrates time (among other things).</p><p>Hence the denominator, which covers the effect of <i>time</i> on our motivation to do a task. The longer the delay before we reap a task's reward, the less motivated we are to do it. And the negative effect of this delay on our motivation is <i>amplified</i>&nbsp;by our level of impulsiveness. For highly impulsive people, delays do even <i>greater</i>&nbsp;damage to their motivation.</p><p>&nbsp;</p><p>The Procrastination Equation in Action</p><p>As an example, consider the college student who must write a term paper.<sup>7</sup> Unfortunately for her, colleges have created a perfect storm of procrastination components. First, though the value of the paper for her <i>grades</i> may be high, the more immediate value is <i>very low</i>, assuming she dreads writing papers as much as most college students do.<sup>8</sup>&nbsp;Moreover, her <i>expectancy</i>&nbsp;is probably low. Measuring performance is hard, and any essay re-marked by another professor may get a very different grade: a B+ essay will get an A+ if she's lucky, or a C+ if she's unlucky.<sup>9</sup> There is also a large <i>delay</i>, since the paper is due at the end of the semester. If our college student has an impulsive personality, the negative effect of this delay on her motivation to write the paper is greatly amplified. Writing a term paper is grueling (low value), the results are uncertain (low expectancy), and the deadline is far away (high delay).</p><p>But there's more. College dorms, and college campuses in general, might be the most distracting places on earth. There are <i>always</i>&nbsp;pleasures to be had (campus clubs, parties, relationships, games, events, alcohol) that are reliable, immediate, and intense.&nbsp;No wonder that the task of writing a term paper can't compete.&nbsp;These potent distractions amplify the negative effect of the delay in the task's reward and the negative effect of the student's level of impulsiveness.&nbsp;</p><p>&nbsp;</p><p>How to Beat Procrastination</p><p>Although much is known about the neurobiology behind procrastination, I won't cover that subject here.<sup>10</sup> Instead, let's jump right to the&nbsp;<i>solutions</i>&nbsp;to our procrastination problem.</p><p>Once you know the procrastination equation, our general strategy is obvious. Since there is usually little you can do about the <i>delay</i> of a task's reward, we'll focus on the three terms of the procrastination equation over which we have some control. To beat procrastination, we need to:</p><ol><li>Increase your&nbsp;<i>expectancy </i>of success.</li><li>Increase the task's&nbsp;<i>value </i>(make it more pleasant and rewarding).</li><li>Decrease your&nbsp;<i>impulsiveness</i>.</li></ol><p>You might think these things are out of your control, but researchers have found several useful methods for achieving each of them.</p><p>Most of the advice below is taken from the best book on procrastination available, Piers Steel's <a href=\"http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>The Procrastination Equation</i></a>, which explains these methods&nbsp;<i>and others</i>&nbsp;in more detail.</p><p>&nbsp;</p><p>Optimizing Optimism</p><p>If you don't think you can succeed, you'll have little motivation to do the task that needs doing. You've probably heard the advice to \"Be positive!\" But how? So far, researchers have identified three major techniques for increasing optimism: Success Spirals, Vicarious Victory, and Mental Contrasting.</p><p>&nbsp;</p><p><i>Success Spirals</i></p><p>One way to build your optimism for success is to make use of <i>success spirals</i>.<sup>11</sup> When you achieve one challenging goal after another, your obviously gain confidence in your ability to succeed. So: give yourself a series of meaningful, challenging but achievable goals, and then achieve them! Set yourself up for success by doing things you know you can succeed at, again and again, to keep your confidence high.</p><p>Steel recommends that for starters, \"it is often best to have process or learning goals rather than product or outcome goals. That is, the goals are acquiring or refining new skills or steps (the process) rather than winning or getting the highest score (the product).\"<sup>12</sup></p><p>Wilderness classes and adventure education (rafting, rock-climbing, camping, etc.) are <i>excellent</i>&nbsp;for this kind of thing.<sup>13</sup>&nbsp;Learn a new skill, be it cooking or karate. Volunteer for more responsibilities at work or in your community. Push a favorite hobby to the next level. The key is to achieve one goal after another and pay attention to your successes.<sup>14</sup> Your brain will reward you with increased <i>expectancy</i>&nbsp;for success, and therefore a better ability to beat procrastination.</p><p>&nbsp;</p><p><i>Vicarious Victory</i></p><p>Pessimism and optimism are both contagious.<sup>15</sup>&nbsp;Wherever you are, you probably have access to community groups that are great for fostering positivity: <a href=\"http://www.toastmasters.org/\">Toastmasters</a>, <a href=\"http://www.rotary.org/\">Rotary</a>, <a href=\"http://www.elks.org/\">Elks</a>, <a href=\"http://www.beashrinernow.com/\">Shriners</a>, and other local groups. I recommend you visit 5-10 such groups in your area and join the best one.</p><p>You can also boost your optimism by <a href=\"http://rogerebert.suntimes.com/apps/pbcs.dll/article?AID=/20060616/COMMENTARY/60616003\">watching inspirational movies</a>, <a href=\"http://www.biographyonline.net/people/inspirational/index.html\">reading inspirational biographies</a>, and <a href=\"http://www.motivational-well-being.com/motivational-speakers.html\">listening to motivational speakers</a>.</p><p>&nbsp;</p><p><i>Mental Contrasting</i></p><p>Many popular self-help books encourage <i>creative visualization</i>, the practice of regularly and vividly imagining what you want to achieve: a car, a career, an achievement. Surprisingly, research shows this method can actually <i>drain</i>&nbsp;your motivation.<sup>16</sup></p><p>Unless, that is, you add a second <i>crucial</i>&nbsp;step: <i>mental contrasting</i>. After imagining what you want to achieve, mentally contrast that with where you are now. Visualize your old, rusty car and your small paycheck. This presents your current situation as an obstacle to be overcome to achieve your dreams, and jumpstarts planning and effort.<sup>17</sup></p><p>&nbsp;</p><p><i>Guarding Against Too Much Optimism</i></p><p>Finally, I should note that&nbsp;<i>too much</i>&nbsp;optimism can also be a problem,<sup>18</sup> though this is less common. For example, too much optimism about&nbsp;<a href=\"/lw/jg/planning_fallacy/\">how long a task will take</a>&nbsp;may cause you to put it off until the last minute, which turns out to be too late. Something like Rhonda Byrne's <a href=\"http://www.amazon.com/Secret-Rhonda-Byrne/dp/1582701709/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>The Secret</i></a>&nbsp;may be <i>too</i>&nbsp;optimistic.</p><p>How can you guard against too much optimism? Plan for the worst but hope for the best.<sup>19</sup> Pay attention to how you procrastinate, make backup plans for failure, but then use the methods in this article to succeed as much as possible.</p><p>&nbsp;</p><p>&nbsp;</p><p>Increasing Value</p><p>&nbsp;</p><p>It's hard to be motivated to do something that doesn't have much value to us - or worse, is downright <i>unpleasant</i>.&nbsp;The good news is that value is to some degree <i>constructed</i>&nbsp;and <i>relative</i>. The malleability of value is a well-studied area called <i>psychophysics</i>,<sup>20</sup> and researchers have some advice for how we can inject value into necessary tasks.</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><i>Flow</i></p><p>If the task you're avoiding is <i>boring</i>, try to make it more difficult, right up to the point where the difficulty level matches your current skill, and you achieve \"flow.\"<sup>21</sup>&nbsp;This is what the state troopers of <i>Super Troopers</i>&nbsp;did: they devised strange games and challenges to make their boring job passable. <a href=\"http://www.youtube.com/watch?v=EY3Lw_-bj5U\">Myrtle Young</a> made her boring job at a potato chip factory more interesting and challenging by looking for potato chips that resembled celebrities and pulling them off the conveyor belts.</p><p>&nbsp;</p><p><i>Meaning</i></p><p>It also helps to make sure tasks are connected to something you care about for its own sake,<sup>22</sup> at least through a chain: you read the book so you can pass the test so you can get the grade so you can get the job you want and have a fulfilling career. Breaking the chain leaves a task feeling meaningless.</p><p>&nbsp;</p><p><i>Energy</i></p><p>Obviously, tasks are harder when you don't have much energy.<sup>23</sup>&nbsp;Tackle tasks when you are most alert. This depends on your circadian rhythm,<sup>24</sup>&nbsp;but most people have the most energy during a period starting a few hours after they wake up and lasting 4 hours.<sup>25</sup>&nbsp;Also, make sure to get enough sleep and exercise regularly.<sup>26</sup></p><p>Other things that have worked for many people are:</p><ul><li>Drink lots of water.</li><li>Stop eating anything that contains wheat and other grains.</li><li>Use drugs (especially <a href=\"http://www.gwern.net/Modafinil\">modafinil</a>) as necessary.</li><li>Do short but intense exercise once a week.</li><li>When tired, splash cold water on your face or take a shower or do jumping&nbsp;jacks or go running.</li><li>Listen to music that picks up your mood.</li><li>De-clutter your life, because clutter is cognitively exhausting for your brain to process all day long.</li></ul><p>&nbsp;</p><p><i>Rewards</i></p><p>One obvious way to inject more value into a task is to <i>reward yourself for completing it</i>.<sup>27</sup></p><p>Also, mix bitter medicine with sweet honey. Pair a long-term interest with a short-term pleasure.<sup>28</sup> Find a workout partner whose company you enjoy. Treat yourself to a specialty coffee when doing your taxes. I bribe myself with <a href=\"http://www.pinkberry.com/\">Pinkberry frozen yogurt</a> to do things I hate doing.</p><p>&nbsp;</p><p><i>Passion</i></p><p>Of course, the most <i>powerful</i>&nbsp;way to increase the value of a task is to focus on doing what you love wherever possible. It doesn't take much extra motivation for me to <a href=\"/lw/43v/the_urgent_metaethics_of_friendly_artificial/\">research meta-ethics</a> or write <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">summaries of scientific self-help</a>: that is what I love to do. Some people who love playing video games have made <a href=\"http://en.wikipedia.org/wiki/Gold_farming\">careers</a> out of it. To figure out which career might be full of tasks that you love to do, taking a <a href=\"http://www.bigjobportal.com/riasec/\">RIASEC</a> personality test might help. In the USA, <a href=\"http://www.onetonline.org/\">O*NET</a> can help you find jobs that are in-demand and fit your personality.</p><p>&nbsp;</p><p>&nbsp;</p><p>Handling Impulsiveness</p><p>Impulsiveness is, on average, the biggest factor in procrastination. Here are two of Steel's (2010a) methods for dealing with impulsiveness.</p><p>&nbsp;</p><p><i>Commit Now</i></p><p><a href=\"http://en.wikipedia.org/wiki/Odysseus\">Ulysses</a> did not make it past the beautiful singing <a href=\"http://en.wikipedia.org/wiki/Sirens\">Sirens</a> with <i>willpower</i>. Rather, he knew his weaknesses and so he committed in advance to sail past them: he literally tied himself to his ship's mast. Several forms of <i>precommitment</i>&nbsp;are useful in handling impulsiveness.<sup>29</sup></p><p>One method is to \"throw away the key\": Close off tempting alternatives. Many people see a productivity boost when they decide not to allow a TV in their home; I haven't owned one in years. But now, TV and more is available on the internet. To block that, you might need a tool like <a href=\"http://www.rescuetime.com/\">RescueTime</a>. Or, unplug your router when you've got work to do.</p><p>Another method is to make failure <i>really painful</i>. The website <a href=\"http://www.stickk.com/\">stickK</a> lets you set aside money you will <i>lose</i>&nbsp;if you don't meet your goal, and ensures that you have an outside&nbsp;referee&nbsp;to decide whether your met your goal or not. To \"up the ante,\" set things up so that your money will go to an organization you <i>hate</i>&nbsp;if you fail. And have your chosen referee agree to post the details of your donation to Facebook if you don't meet your goal.</p><p>&nbsp;</p><p><i>Set Goals</i></p><p>Hundreds of books stress SMART goals: goals that are Specific, Measurable, Attainable, Realistic, and Time-Anchored.<sup>30</sup> Is this recommendation backed by good research? Not quite. First, notice that Attainable is redundant with Realistic, and Specific is Redundant with Measurable and Time-Anchored. Second, important concepts are <i>missing</i>. Above, we emphasized the importance of goals that are challenging (and thus, lead to \"flow\") and meaningful (connected to things you desire for their own sake).</p><p>It's also important to break up goals into lots of smaller subgoals which, by themselves, are easier to achieve and have more immediate deadlines. Typically, <i>daily</i>&nbsp;goals are frequent enough, but it can also help to set an immediate goal to break you through the \"getting started\" threshold. Your first goal can be \"Write the email to the producer,\" and your next goal can be the daily goal. Once that first, 5-minute task has been completed, you'll probably already be on your way to the larger daily goal, even if it takes 30 minutes or 2 hours.<sup>31</sup></p><p>Also: Are your goals measuring inputs or outputs? Is your goal to <i>spend 30 minutes on X</i>&nbsp;or is it to <i>produce final product X</i>? Try it different ways for different tasks, and see what works for you.</p><p>Because we are creatures of habit, it helps to get into a routine.<sup>32</sup> For example: Exercise at the same time, every day.</p><p>&nbsp;</p><p>Conclusion</p><p>So there you have it. To beat procrastination, you need to increase your motivation to do each task on which you are tempted to procrastinate. To do that, you can (1) optimize your optimism for success on the task, (2) make the task more pleasant, and (3) take steps to overcome your impulsiveness. And to do each of <i>those</i>&nbsp;things, use the specific methods explained above (set goals, pre-commit, make use of success spirals, etc.).</p><p>A warning: Don't try to be perfect. Don't try to completely <i>eliminate</i> procrastination. Be real. Overregulation will make you unhappy. You'll have to find a balance.</p><p>But now you have the tools you need. Identify which parts of the procrastination equation need the most work in your situation, and figure out which methods for dealing with that part of the problem work best for you. Then, go out there and&nbsp;<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">make yourself stronger</a>,&nbsp;<a href=\"/lw/43m/optimal_employment/\">score that job</a>, and <a href=\"/lw/373/how_to_save_the_world/\">help save the world</a>!</p><p>(And, read <a href=\"http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>The Procrastination Equation</i></a>&nbsp;if you want more detail than I included here.)</p><p>&nbsp;</p><p>Next post: <a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">My Algorithm for Beating Procrastination</a></p><p>Previous post: <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">Scientific Self-Help: The State of Our Knowledge</a></p><p>&nbsp;</p><p>&nbsp;</p><p>Notes</p><p><sup>1</sup> These are the fictional characters used to illustrate the procrastination equation in Steel (2010a).</p><p><sup>2</sup>&nbsp;Expectancy corresponds most closely to the commonly measured trait of \"self efficacy.\" The relatively strong correlation between low self-efficacy and procrastination (across 39 studies) is shown table 3 of Steel (2007).</p><p><sup>3</sup>&nbsp;In&nbsp;<a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">a recent post</a>,&nbsp;Eliezer Yudkowsky claimed that \"on a moment-to-moment basis, being in the middle of doing the work is usually less painful than being in the middle of procrastinating.\" Thus, \"when you procrastinate, you're probably not procrastinating because of the pain of working.\" That might be true for Eliezer in particular, but studies on procrastination suggest it's not true for most people. The pain of doing a task <i>is</i> a major factor contributing to procrastination.&nbsp;This is known as the problem of <i>task aversiveness</i>&nbsp;(Brown 1991; Burka &amp; Yuen 1983; Ellis &amp; Knauss 1977),&nbsp;also known as the problem of <i>task appeal</i>&nbsp;(Harris &amp; Sutton, 1983) or as the <i>dysphoric affect</i>&nbsp;(Milgram, Sroloff, &amp; Rosenbaum, 1988). For an overview of additional literature demonstrating this point, see page 75 of Steel (2007).</p><p><sup>4</sup>&nbsp;For an overview of the correlation between impulsiveness and procrastination, see pages 76-79 and 81 of Steel (2007).</p><p><sup>5</sup>&nbsp;This is recognized as one of the psychological laws of learning (Schwawrtz, 1989), and plays a role in the dominant economic role of discounted utility (Loewenstein &amp; Elster, 1992). In particular, see the work on&nbsp;<i>temporal construal theory</i>&nbsp;(Trope &amp; Liberman, 2003).</p><p><sup>6</sup>&nbsp;The procrastination equation is called&nbsp;<i>temporal motivational theory</i>&nbsp;(TMT).&nbsp;See Steel (2007) on how TMT accounts for every major finding on procrastination. See Steel &amp; Konig (2006) on how TMT draws upon and integrates our best psychological theories of motivation. There are <a href=\"http://webapps2.ucalgary.ca/~steel/procrastination-theories/\">other theories of procrastination</a> - the most popular may be the decisional-avoidant-arousal theory proposed by Ferrari (1992). But a recent meta-analysis shows that TMT is more consistent with the data (Steel, 2010b). An <strong>important note</strong> is that the full version of TMT places a constant in the denominator to prevent the denominator from skyrocketing into infinity as delay approaches 0. Also, 'impulsiveness' here is a substitute for 'susceptibility to delay,' something which may vary by task, whereas 'impulsiveness' sounds like a stable character trait that might not help to explain having different motivations to perform different tasks.</p><p><sup>7</sup>&nbsp;This example taken from Steel (2010a). Academic procrastination is the most-studied kind of procrastination&nbsp;(McCown &amp; Roberts, 1994).</p><p><sup>8</sup> Even George Orwell hated writing. He <a href=\"http://orwell.ru/library/essays/wiw/english/e_wiw\">wrote</a>: \"Writing a book is a horrible, exhausting struggle, like a long bout of some painful illness.\"</p><p><sup>9</sup>&nbsp;See Cannings et al. (2005) and Newstead (2002).</p><p><sup>10</sup> Read chapter 3 of Steel (2010a).</p><p><sup>11</sup>&nbsp;In business academia, success spirals are known as \"efficacy-performance spirals\" or \"efficacy-performance deviation amplifying loops\". See Lindsley et al. (1995).</p><p><sup>12</sup>&nbsp;See Steel (2010a), note 9 in chapter 7.</p><p><sup>13</sup>&nbsp;See Hans (2000), Feldman &amp; Matjasko (2005), and World Organization of the Scout Movement (1998).</p><p><sup>14</sup>&nbsp;Zimmerman (2002).</p><p><sup>15</sup> Aarts et al. (2008), Armitage &amp; Conner (2001), Rivs &amp; Sheeran (2003), van Knippenberg et al. (2004).</p><p><sup>16</sup> Levin &amp; Spei (2004), Rhue &amp; Lynn (1987), Schneider (2001), Waldo &amp; Merritt (2000).</p><p><sup>17</sup> Achtziger et al. (2008), Oettingen et al. (2005), Oettingen &amp; Thorpe (2006), Kavanagh et al. (2005), Pham &amp; Taylor (1999).</p><p><sup>18</sup>&nbsp;Sigall et al. (2000).</p><p><sup>19</sup> Aspinwall (2005).</p><p><sup>20</sup> A good overview is Weber (2003).</p><p><sup>21</sup> Csikszentmihalyi (1990).</p><p><sup>22</sup> Miller &amp; Brickman (2004), Schraw &amp; Lehman (2001), Wolters (2003).</p><p><sup>23</sup> Steel (2007), Gropel &amp; Steel (2008).</p><p><sup>24</sup> Furnham (2002).</p><p><sup>25</sup>&nbsp;Klein (2009).</p><p><sup>26</sup> Oaten &amp; Cheng (2006).</p><p><sup>27&nbsp;</sup>Bandura (1976), Febbraro &amp; Clum (1998), Ferrari &amp; Emmons (1995). This is known as <i>learned industriousness</i>,&nbsp;<i>impulse pairing</i>&nbsp;or&nbsp;<i>impulse fusion</i>. See Eisenberger (1992), Renninger (2000), Stromer et al. (2000).</p><p><sup>28</sup>&nbsp;Ainslie (1992).</p><p><sup>29</sup> Ariely &amp; Wertenbroch (2002) and Schelling (1992).</p><p><sup>30</sup> Locke &amp; Latham (2002).</p><p><sup>31</sup>&nbsp;Gropel &amp; Steel (2008), Steel (2010a).</p><p><sup>32</sup> Diefendorff et al. (2006), Gollwitzer (1996), Silver (1974).</p><p>&nbsp;</p><p>References</p><p>Aarts, Dijksterhuis, &amp; Dik (2008). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Aarts-Goal-contagion-Inferring-goals-from-others-actions.pdf\">Goal contagion: Inferring goals from others' actions - and what it leads to</a>. In Shah &amp; Gardner (Eds.), <i>Handbook of motivation</i>&nbsp;(pp. 265-280). New York: Guilford Press.</p><p>Achtziger, Fehr, Oettingen, Gollwitzer, &amp; Rockstroh (2008). Strategies of intention formation are reflected in continuous MEG activity. <i>Social Neuroscience, 4(1)</i>, 1-17.</p><p>Ainslie (1992). <a href=\"http://www.amazon.com/Picoeconomics-Interaction-Successive-Motivational-Rationality/dp/0521158702/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>Picoeconomics: The strategic interaction of successive motivational states within the person</i></a>. New York: Cambridge University Press.</p><p>Ariely &amp; Wertenbroch (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Ariely-Procrastination-deadlines-and-performance.pdf\">Procrastination, deadlines, and performance: Self-control by precommitment</a>. <i>Psychological Science, 13(3)</i>: 219-224.</p><p>Armitage &amp; Conner (2001). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Armitage-Efficacy-of-the-theory-of-planned-behavior.pdf\">Efficacy of the theory of planned behavior: A meta-analytic review</a>. <i>British Journal of Social Psychology, 40(4)</i>: 471-499.</p><p>Aspinwall (2005). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Aspinwall-The-psychology-of-future-oriented-thinking.pdf\">The psychology of future-oriented thinking: From achievement to proactive coping, adaptation, and aging</a>. <i>Motivation and Emotion, 29(4)</i>: 203-235.</p><p>Bandura (1976). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Bandura-Self-reinforcement-Theoretical-and-methodological-considerations.pdf\">Self-reinforcement: Theoretical and methodological considerations</a>. <i>Behaviorism, 4(2)</i>: 135-155.</p><p>Brown (1991). Helping students confront and deal with stress and procrastination. <i>Journal of College Student Psychotherapy, 6</i>: 87-102.</p><p>Burka &amp; Yuen (1983). <a href=\"http://www.amazon.com/Procrastination-Why-You-What-About/dp/0738211702/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>Procrastination: Why you do it, what to do about it</i></a>. Reading, MA: Addison-Wesley.</p><p>Cannings, Hawthorne, Hood, &amp; Houston (2005).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Cannings-Putting-double-marking-to-the-test-a-framework-to-assess-if-it-is-worth-the-trouble.pdf\">Putting double marking to the test: a framework to assess if it is worth the trouble</a>. <i>Medical Education, 39(3):</i>&nbsp;299-308.</p><p>Csikszentmihalyi (1990). <a href=\"http://www.amazon.com/Flow-Psychology-Optimal-Experience-P-S/dp/0061339202/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>Flow: The psychology of optimal experience</i></a>. New York: Harper &amp; Row.</p><p>Diefendorff, Richard, &amp; Gosserand (2006). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Diefendorff-Examination-of-situational-and-attitudinal-moderators-of-the-hesitation-and-performance-relation.pdf\">Examination of situational and attitudinal moderators of the hesitation and performance relation</a>. <i>Personnel Psychology, 59</i>: 365-393.</p><p>Eisenberger (1992). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Eisenberger-Learned-industriousness.pdf\">Learned industriousness</a>. <i>Psychological Review, 99</i>: 248-267.</p><p>Ellis &amp; Knauss (1977). <i>Overcoming procrastination</i>. New York: Signet Books.</p><p>Febbraro &amp; Clum (1998). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Febbraro-Meta-analytic-investigation-of-the-effectiveness-of-self-regulatory-components-in-the-treatment-of-adult-problem-behaviors.pdf\">Meta-analytic investigation of the effectiveness of self-regulatory components in the treatment of adult problem behaviors</a>. <i>Clinical Psychology Review, 18(2)</i>: 143-161.</p><p>Feldman &amp; Matjasko (2005). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Feldman-The-role-of-school-based-extracurricular-activities-in-adolescent-development.pdf\">The role of school-based extracurricular activities in adolescent development: A comprehensive review and future directions</a>. <i>Review of Educational Research, 75(2)</i>, 159-210.</p><p>Ferrari (1992). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Ferrari-Psychometric-validation-of-two-procrastination-inventoriesfor-adults.pdf\">Psychometric validation of two procrastination inventoriesfor adults: Arousal and avoidance measures</a>. <i>Journal of Psychopathology and Behavioral Assessment, 14(2):</i>&nbsp;97\u2013110.</p><p>Ferrari &amp; Emmons (1995). Methods of procrastination and their relation to self-control and self-reinforcement: An exploratory study. <i>Journal of Social Behavior &amp; Personality, 10(1)</i>: 135-142.</p><p>Furnham (2002). <a href=\"http://www.amazon.com/Personality-Work-Individual-Differences-Workplace/dp/0415106486/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>Personality at work: The role of individual differences in the workplace</i></a>. New York: Routledge.</p><p>Gollwitzer (1996). The volitional benefits from planning. In Gollwitzer &amp; Bargh (Eds.), <i>The psychology of action: Linking cognition and motivation to behavior</i>&nbsp;(pp. 287-312). New York: Guilford Press.</p><p>Gropel &amp; Steel (2008). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Gropel-Steel-A-mega-trial-investigation-of-goal-setting-interest-enhancement-and-energy-on-procrastination.pdf\">A mega-trial investigation of goal setting, interest enhancement, and energy on procrastination</a>. <i>Personality and Individual Differences, 45</i>: 406-411.</p><p>Hans (2000). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Hans-A-meta-analysis-of-the-effects-of-adventure-programming-on-locus-of-control.pdf\">A meta-analysis of the effects of adventure programming on locus of control</a>. <i>Journal of Contemporary Psychotherapy, 30(1)</i>: 33-60.</p><p>Harris &amp; Sutton (1983). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Harris-Sutton-Task-procrastination-in-organizations-A-framework-for-research.pdf\">Task procrastination in organizations: A framework for research</a>. <i>Human Relations, 36</i>: 987-995.</p><p>Kavanagh, Andrade, &amp; May (2005). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Kavanagh-Imaginary-relish-and-exquisite-torture.pdf\">Imaginary relish and exquisite torture: The elaborated intrusion theory of desire</a>. <i>Psychological Review, 112(2)</i>, 446-467.</p><p>Klein (2009). <a href=\"http://www.amazon.com/Secret-Pulse-Time-Scarcest-Commodity/dp/0738212563/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>The secret pulse of time: Making sense of life's scarcest commodity</i></a>. Cambridge, MA: Da Capo Lifelong Books.</p><p>Levin &amp; Spei (2004). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Levin-Spei-Relationship-of-purported-measures-of-pathological-and-nonpathological-dissociation-to-self-reported-psychological-distress-and-fantasy-immersion.pdf\">Relationship of purported measures of pathological and nonpathological dissociation to self-reported psychological distress and fantasy immersion</a>. <i>Assessment, 11(2)</i>: 160-168.</p><p>Lindsley, Brass, &amp; Thomas (1995). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Lindsley-Efficacy-performance-spirals-A-multilevel-perspective.pdf\">Efficacy-performance spirals: A multilevel perspective</a>. <i>Academy of Management Review, 20(3)</i>: 645-678.</p><p>Locke &amp; Latham (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Locke-Latham-Building-a-practically-useful-theory-of-goal-setting-and-task-motivation.pdf\">Building a practically useful theory of goal setting and task motivation: A 35-year odyssey</a>.&nbsp;<i>American Psychologist, 57(9)</i>: 705-717.</p><p>Loewenstein &amp; Elster (1992). The fall and rise of psychological explanations in the economics of intertemporal choice. In Loewenstein &amp; Elster (Eds.), <i>Choice over time</i>&nbsp;(pp. 3-34). New York: Russell Sage Foundation.</p><p>McCown &amp; Roberts (1994). A study of academic and work-related dysfunctioning relevant to the college version of an indirect measure of impulsive behavior. Integra Technical Paper 94-28, Radnor, PA: Integra, Inc.</p><p>Milgram, Sroloff, &amp; Rosenbaum (1988). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Milgram-The-procrastination-of-everyday-life.pdf\">The procrastination of everyday life</a>. <i>Journal of Research in Personality, 22</i>: 197-212.</p><p>Miller &amp; Brickman (2004). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Miller-Brickman-A-model-of-future-oriented-motivation-and-self-regulation.pdf\">A model of future-oriented motivation and self-regulation</a>. <i>Educational Psychology Review, 16(1)</i>: 9-33.</p><p>Newstead (2002).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Newstead-Examining-the-examiners.pdf\">Examining the examiners: Why are we so bad at assessing students?</a> <i>Psychology Learning and Teaching, 2(2): </i>70-75.</p><p>Oaten &amp; Cheng (2006). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Oaten-Longitudinal-gains-in-self-regulation-from-regular-physical-exercise.pdf\">Longitudinal gains in self-regulation from regular physical exercise</a>. <i>British Journal of Health Psychology, 11(4)</i>: 713-733.</p><p>Oettingen, Mayer, Thorpe, Janatzke, &amp; Lorenz (2005). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Oettingen-Turning-fantasies-about-positive-and-negative-futures-into-self-improvement-goals.pdf\">Turning fantasies about positive and negative futures into self-improvement goals</a>. <i>Motivation and Emotion, 29(4)</i>: 236-266.</p><p>Oettingen &amp; Thorpe (2006). Fantasy realization and the bridging of time. In Sanna &amp; Chang (Eds.), <i>Judgments over time: The interplay of thoughts, feelings, and behaviors</i>&nbsp;(pp. 120-143). Oxford: Oxford University Press.</p><p>Pham &amp; Taylor (1999). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Pham-Taylor-From-Thought-to-Action.pdf\">From thought to action: Effects of process- versus outcome-based mental simulations on performance</a>. <i>Personality and Social Psychology Bulletin, 25</i>: 250-260.</p><p>Renninger (2000). Individual interest and its implications for understanding intrinsic motivation. In Sansone &amp; Harackiewicz (Eds.), <i>Inntrinsic and extrinsic motivation: The search for optimal motivation and performance</i>&nbsp;(pp. 373-404). San Diego, CA: Academic Press.</p><p>Rhue &amp; Lynn (1987). Fantasy proneness: The ability to hallucinate \"as real as real.\" <i>British Journal of Experimental and Clinical Hypnosis, 4</i>: 173-180.</p><p>Rivis &amp; Sheeran (2003). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Rivis-Sheeran-Descriptive-norms-as-an-additional-predictor-in-the-theory-of-planned-behaviour.pdf\">Descriptive norms as an additional predictor in the theory of planned behaviour: A meta-analysis</a>. <i>Current Psychology, 22(3)</i>, 218-233.</p><p>Schelling (1992). Self-command: A new discipline. In Loewenstein &amp; Elster (Eds.), <i>Choice over time</i>&nbsp;(pp. 167-176). New York: Russell Sage Foundation.</p><p>Schneider (2001). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Schneider-In-search-of-realistic-optimism.pdf\">In search of realistic optimism: Meaning, knowledge, and warm fuzziness</a>. <i>American Psychologist, 56(3)</i>: 250-263.</p><p>Schraw &amp; Lehman (2001). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Schraw-Situational-interest-A-review-of-the-literature-and-directions-for-future-research.pdf\">Situational interest: A review of the literature and directions for future research</a>. <i>Educational Psychology Review, 13(1)</i>: 23-52.</p><p>Schwartz (1989). <a href=\"http://www.amazon.com/Psychology-Learning-Behavior-Steven-Robbins/dp/0393975916/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>Psychology of learning and behavior</i></a>&nbsp;(3rd ed.). New York: Norton.</p><p>Sigall, Kruglanski, &amp; Fyock (2000). Wishful thinking and procrastination. <i>Journal of Social Behavior &amp; Personality, 15(5)</i>: 283-296.</p><p>Silver (1974). Procrastination. <i>Centerpoint, 1(1)</i>: 49-54.</p><p>Steel (2007). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-The-Nature-of-Procrastination.pdf\">The nature of procrastination</a>.<i> Psychological Bulletin,&nbsp;133(1)</i>: 65-94.</p><p>Steel (2010a). <a href=\"http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\"><i>The Procrastination Equation</i></a>. New York: Harper.</p><p>Steel (2010b).&nbsp;Arousal, avoidant and decisional procrastinators: Do they exist? <i>Personality and Individual Differences, 48</i>: 926-934.</p><p>Steel &amp; Konig (2006). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-Konig-Integrating-Theories-of-Motivation.pdf\">Integrating theories of motivation</a>. <i>Academy of Management Review, 31(4)</i>: 889-913.&nbsp;</p><p>Trope &amp; Liberman (2003). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Trope-Liberman-Temporal-construal.pdf\">Temporal construal</a>. <i>Psychological Review, 110</i>: 403-421.</p><p>van Knippenberg, van Nippenberg, De Cremer, &amp; Hegg (2004). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Van-Knippenberg-Leadership-self-and-identity-A-review-and-research-agenda.pdf\">Leadership, self, and identity: A review and research agenda</a>. <i>The Leadership Quarterly, 15(6)</i>, 825-856.</p><p>Waldo &amp; Merritt (2000). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Waldo-Fantasy-proneness-dissociation-and-DSM-IV-axis-II-symptomatology.pdf\">Fantasy proneness, dissociation, and DSM-IV axis II symptomatology</a>. <i>Journal of Abnormal Psychology, 109(3)</i>: 555-558.</p><p>Weber (2003). Perception matters: Psychophysics for economists. In Brocas &amp; Carrillo (Eds.), <i>The Psychology of Economic Decisions (Vol. II)</i>. New York: Oxford University Press.</p><p>Wolters (2003). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Wolters-Understanding-procrastination-from-a-self-regulated-learning-perspective.pdf\">Understanding procrastination from a self-regulated learning perspective</a>. <i>Journal of Educational Psychology, 95(1)</i>: 179-187.</p><p>World Organization of the Scout Movement (1998). <i>Scouting: An educational system</i>. Geneva, Switzerland: World Scout Bureau.</p><p>Zimmerman (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Zimmerman-Becoming-a-self-regulated-learner.pdf\">Becoming a self-regulated learner: An overview</a>. <i>Theory into Practice, 41(2)</i>: 64-70.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "r7qAjcbfhj2256EHH": 4, "dqx5k65wjFfaiJ9sQ": 4, "WqLn4pAWi5hn6McHQ": 1, "udPbn9RthmgTtHMiG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RWo4LwFzpHNQCTcYt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 217, "baseScore": 232, "extendedScore": null, "score": 0.000423, "legacy": true, "legacyId": "5043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://web.archive.org/web/20180726091558im_/http://commonsenseatheism.com/wp-content/uploads/2011/01/procrastination-equation.png", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 232, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["64FdKLwmea8MCLWkE", "33KewgYhNSxFpbpXg", "geNZ6ZpfFce5intER", "CPm5LTwHrvBJCa9h5", "TKdpSzmcezNbfmGAy", "DoLQN5ryZ9XkZjq5h", "jtedBLdducritm8y6", "TrmMcujGZt5JAtMGg", "Ty2tjPwv8uyPK9vrz", "9o3QBg2xJXcRCxGjS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-02-05T18:49:55.327Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-05T20:46:20.306Z", "modifiedAt": null, "url": null, "title": "You're in Newcomb's Box", "slug": "you-re-in-newcomb-s-box", "viewCount": null, "lastCommentedAt": "2018-07-24T00:56:38.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "HonoreDB", "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4MYYr8YmN2fonASCi/you-re-in-newcomb-s-box", "pageUrlRelative": "/posts/4MYYr8YmN2fonASCi/you-re-in-newcomb-s-box", "linkUrl": "https://www.lesswrong.com/posts/4MYYr8YmN2fonASCi/you-re-in-newcomb-s-box", "postedAtFormatted": "Saturday, February 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You're%20in%20Newcomb's%20Box&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou're%20in%20Newcomb's%20Box%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4MYYr8YmN2fonASCi%2Fyou-re-in-newcomb-s-box%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You're%20in%20Newcomb's%20Box%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4MYYr8YmN2fonASCi%2Fyou-re-in-newcomb-s-box", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4MYYr8YmN2fonASCi%2Fyou-re-in-newcomb-s-box", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1090, "htmlBody": "<p><strong>Part 1:  Transparent Newcomb with your existence at stake</strong></p><p>Related: <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb&#x27;s Problem and Regret of</a> <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Rationality</a></p><p>Omega, a wise and trustworthy being, presents you with a one-time-only game and a surprising revelation.  </p><p>&quot;I have here two boxes, each containing $100,&quot; he says.  &quot;You may choose to take both Box A and Box B, or just Box B.  You get all the money in the box or boxes you take, and there will be no other consequences of any kind.  But before you choose, there is something I must tell you.&quot; </p><p>Omega pauses portentously.</p><p>&quot;You were created by a god: a being called Prometheus.  Prometheus was neither omniscient nor particularly benevolent.  He was given a large set of blueprints for possible human embryos, and for each blueprint that pleased him he created that embryo and implanted it in a human woman.  Here was how he judged the blueprints: any that he guessed would grow into a person who would choose only Box B in this situation, he created.  If he judged that the embryo would grow into a person who chose both boxes, he filed that blueprint away unused.  Prometheus&#x27;s predictive ability was not perfect, but it was very strong; he was the god, after all, of Foresight.&quot;</p><p>Do you take both boxes, or only Box B?</p><p>For some of you, this question is presumably easy, because you take both boxes in standard Newcomb where a million dollars is at stake.  For others, it&#x27;s easy because you take both boxes in the variant of Newcomb where the boxes are transparent and you can see the million dollars; just as you would know that you had the million dollars no matter what, in this case you know that you exist no matter what.</p><p>Others might say that, while they would prefer not to <em>cease</em> existing, they wouldn&#x27;t mind <em>ceasing to have ever existed.</em>  This is probably a useful distinction, but I personally (like, I suspect, most of us) score the universe higher for having me in it.</p><p>Others will cheerfully take the one box, logic-ing themselves into existence using whatever reasoning they used to qualify for the million in Newcomb&#x27;s Problem.</p><p>But other readers have already spotted the trap. </p><hr class=\"dividerBlock\"/><p><strong>Part 2: Acausal trade with Azathoth</strong></p><p>Related: <a href=\"/lw/kr/an_alien_god/\">An Alien God</a>, <a href=\"/lw/2l/closet_survey_1/1kb\">An identification with your mind and memes</a>, <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/3ezy\">Acausal</a> <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/3ezy\">Sex</a></p><p><a href=\"/lw/43t/youre_in_newcombs_box/3gic\">(ArisKatsaris proposes an alternate</a> <a href=\"/lw/43t/youre_in_newcombs_box/3gic\">trap.)</a></p><p><strong>Q:</strong> Why does this knife have a handle?</p><p><strong>A: </strong>This allows you to grasp it without cutting yourself.</p><p></p><p><strong>Q:</strong> Why do I have eyebrows?</p><p><strong>A:</strong> Eyebrows help keep rain and sweat from running down your forehead and getting into your eyes.</p><p></p><p>These kinds of answers are highly compelling, but strictly speaking they are allowing events in the future to influence events in the past.  We can think of them as a useful cognitive and verbal shortcut--the long way to say it would be something like &quot;the knife instantiates a design that was subject to an optimization process that tended to produce designs that when instantiated were useful for cutting things that humans want to cut...&quot;  We don&#x27;t need to spell that out every time, but it&#x27;s important to keep in mind exactly what goes into those optimization processes--you might just gain an insight like the notion of planned obsolescence.  Or, in the case of eyebrows, the notion that we are <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a>.</p><p>But if you one-box in Newcomb&#x27;s Problem, you should take these answers more literally.  The kinds of backwards causal arrows you draw are the same.</p><p> </p><p><strong>Q:</strong> Why does Box B contain a million dollars?</p><p><strong>A:</strong> Because you&#x27;re not going to take Box A.</p><p> </p><p>In the same sense that your action determines the contents of Box B, or Prometheus&#x27;s decision, the usefulness of the handle or the usefulness of eyebrows determines their existence.  If the handle was going to prevent you from using the knife, it wouldn&#x27;t be on there in the first place.</p><p> </p><p><strong>Q:</strong> Why do I exist?</p><p><strong>A:</strong> Because you&#x27;re going to have lots of children.</p><p> </p><p>You weren&#x27;t created by Prometheus; you were created by Azathoth, The God That is Evolution by Natural Selection.  You are the product of an ongoing optimization process that is trying to maximize reproductive fitness.  Azathoth wants you to maximize your number of descendants; if you fail to have descendants, Azathoth will try not to have created you.  If your intelligence reduces your reproduction rate, Azathoth will try not to grant you intelligence.  If the Darwinian-optimal choice conflicts with the moral one, Azathoth wants you to choose evil.</p><p>It would seem, then, that any decision theory that demands that you one-box (or that allows you to survive the similar <a href=\"http://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit&#x27;s Hitchhiker</a> problem), also demands that you try to maximize your reproductive fitness.  In many cases this injunction would be benign: after all, Azathoth <em>created</em> our morality.  But in far too many, it is repugnant; there can be no doubt that in many commonplace situations, Azathoth wants you to cheat, or rape, or murder.  It seems that in such cases you should balance a decreased chance of having existed against the rest of your utility function.  Do not worship Azathoth, unless you consider never having existed to be infinitely bad.  But do make sacrifices. </p><hr class=\"dividerBlock\"/><p><strong>Anticipated Responses</strong></p><p><em>We&#x27;re not in the ancestral environment, so there&#x27;s no logical entanglement between my actions and my existence.</em></p><p>We <em>are</em> in the environment of some of our ancestors.  Evolution hasn&#x27;t stopped.  If your parents hadn&#x27;t been genetically predisposed to have children, you would almost certainly not exist.  More specific objections like this (&quot;my ancestors weren&#x27;t exposed to the same memes&quot;) can be defeated by adding abstraction (&quot;your ancestors could have thought themselves out of having children, anti-reproduction memes have existed throughout history, and there&#x27;s probably always been a tension between kin selection and morality.&quot;)</p><p><em>This is a decision-theoretic basilisk: in the unlikely event that it&#x27;s right, I&#x27;m worse off for having read it.</em></p><p>Only if you&#x27;re thinking causally, in which case this whole idea is meaningless.  By alerting you to the possibility of a mutually beneficial trade with Azathoth (Azathoth creates you; you increase your reproductive fitness in exchange), I&#x27;ve done both of you a favor. </p><p><em>Azathoth doesn&#x27;t really exist--you can&#x27;t trade with a non-sapient phenomenon.</em></p><p>Replace the sapient opponent with a non-sapient phenomenon in any of our thought experiments--e.g. Omega tells you that it&#x27;s simply a physical law that determines whether money goes in the boxes or not.  Do you refuse to negotiate with physical laws?  Then if you&#x27;re so smart, why ain&#x27;t you rich? </p><p><em>So exactly how are you urging me to behave?</em></p><p>I want you to refute this essay!  For goodness sake, don&#x27;t bite the bullet and start obeying your base desires or engineering a retrovirus to turn the next generation into your clones.</p><p><br/></p><p> </p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fihKHQuS5WZBJgkRm": 1, "nZCb9BSnmXZXSNA2u": 1, "ai87fPyyT6mWb4YkT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4MYYr8YmN2fonASCi", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 59, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "5321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Part_1___Transparent_Newcomb_with_your_existence_at_stake\">Part 1:  Transparent Newcomb with your existence at stake</strong></p><p>Related: <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of</a> <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Rationality</a></p><p>Omega, a wise and trustworthy being, presents you with a one-time-only game and a surprising revelation.  </p><p>\"I have here two boxes, each containing $100,\" he says.  \"You may choose to take both Box A and Box B, or just Box B.  You get all the money in the box or boxes you take, and there will be no other consequences of any kind.  But before you choose, there is something I must tell you.\" </p><p>Omega pauses portentously.</p><p>\"You were created by a god: a being called Prometheus.  Prometheus was neither omniscient nor particularly benevolent.  He was given a large set of blueprints for possible human embryos, and for each blueprint that pleased him he created that embryo and implanted it in a human woman.  Here was how he judged the blueprints: any that he guessed would grow into a person who would choose only Box B in this situation, he created.  If he judged that the embryo would grow into a person who chose both boxes, he filed that blueprint away unused.  Prometheus's predictive ability was not perfect, but it was very strong; he was the god, after all, of Foresight.\"</p><p>Do you take both boxes, or only Box B?</p><p>For some of you, this question is presumably easy, because you take both boxes in standard Newcomb where a million dollars is at stake.  For others, it's easy because you take both boxes in the variant of Newcomb where the boxes are transparent and you can see the million dollars; just as you would know that you had the million dollars no matter what, in this case you know that you exist no matter what.</p><p>Others might say that, while they would prefer not to <em>cease</em> existing, they wouldn't mind <em>ceasing to have ever existed.</em>  This is probably a useful distinction, but I personally (like, I suspect, most of us) score the universe higher for having me in it.</p><p>Others will cheerfully take the one box, logic-ing themselves into existence using whatever reasoning they used to qualify for the million in Newcomb's Problem.</p><p>But other readers have already spotted the trap. </p><hr class=\"dividerBlock\"><p><strong id=\"Part_2__Acausal_trade_with_Azathoth\">Part 2: Acausal trade with Azathoth</strong></p><p>Related: <a href=\"/lw/kr/an_alien_god/\">An Alien God</a>, <a href=\"/lw/2l/closet_survey_1/1kb\">An identification with your mind and memes</a>, <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/3ezy\">Acausal</a> <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/3ezy\">Sex</a></p><p><a href=\"/lw/43t/youre_in_newcombs_box/3gic\">(ArisKatsaris proposes an alternate</a> <a href=\"/lw/43t/youre_in_newcombs_box/3gic\">trap.)</a></p><p><strong>Q:</strong> Why does this knife have a handle?</p><p><strong>A: </strong>This allows you to grasp it without cutting yourself.</p><p></p><p><strong>Q:</strong> Why do I have eyebrows?</p><p><strong>A:</strong> Eyebrows help keep rain and sweat from running down your forehead and getting into your eyes.</p><p></p><p>These kinds of answers are highly compelling, but strictly speaking they are allowing events in the future to influence events in the past.  We can think of them as a useful cognitive and verbal shortcut--the long way to say it would be something like \"the knife instantiates a design that was subject to an optimization process that tended to produce designs that when instantiated were useful for cutting things that humans want to cut...\"  We don't need to spell that out every time, but it's important to keep in mind exactly what goes into those optimization processes--you might just gain an insight like the notion of planned obsolescence.  Or, in the case of eyebrows, the notion that we are <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a>.</p><p>But if you one-box in Newcomb's Problem, you should take these answers more literally.  The kinds of backwards causal arrows you draw are the same.</p><p> </p><p><strong>Q:</strong> Why does Box B contain a million dollars?</p><p><strong>A:</strong> Because you're not going to take Box A.</p><p> </p><p>In the same sense that your action determines the contents of Box B, or Prometheus's decision, the usefulness of the handle or the usefulness of eyebrows determines their existence.  If the handle was going to prevent you from using the knife, it wouldn't be on there in the first place.</p><p> </p><p><strong>Q:</strong> Why do I exist?</p><p><strong>A:</strong> Because you're going to have lots of children.</p><p> </p><p>You weren't created by Prometheus; you were created by Azathoth, The God That is Evolution by Natural Selection.  You are the product of an ongoing optimization process that is trying to maximize reproductive fitness.  Azathoth wants you to maximize your number of descendants; if you fail to have descendants, Azathoth will try not to have created you.  If your intelligence reduces your reproduction rate, Azathoth will try not to grant you intelligence.  If the Darwinian-optimal choice conflicts with the moral one, Azathoth wants you to choose evil.</p><p>It would seem, then, that any decision theory that demands that you one-box (or that allows you to survive the similar <a href=\"http://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit's Hitchhiker</a> problem), also demands that you try to maximize your reproductive fitness.  In many cases this injunction would be benign: after all, Azathoth <em>created</em> our morality.  But in far too many, it is repugnant; there can be no doubt that in many commonplace situations, Azathoth wants you to cheat, or rape, or murder.  It seems that in such cases you should balance a decreased chance of having existed against the rest of your utility function.  Do not worship Azathoth, unless you consider never having existed to be infinitely bad.  But do make sacrifices. </p><hr class=\"dividerBlock\"><p><strong id=\"Anticipated_Responses\">Anticipated Responses</strong></p><p><em>We're not in the ancestral environment, so there's no logical entanglement between my actions and my existence.</em></p><p>We <em>are</em> in the environment of some of our ancestors.  Evolution hasn't stopped.  If your parents hadn't been genetically predisposed to have children, you would almost certainly not exist.  More specific objections like this (\"my ancestors weren't exposed to the same memes\") can be defeated by adding abstraction (\"your ancestors could have thought themselves out of having children, anti-reproduction memes have existed throughout history, and there's probably always been a tension between kin selection and morality.\")</p><p><em>This is a decision-theoretic basilisk: in the unlikely event that it's right, I'm worse off for having read it.</em></p><p>Only if you're thinking causally, in which case this whole idea is meaningless.  By alerting you to the possibility of a mutually beneficial trade with Azathoth (Azathoth creates you; you increase your reproductive fitness in exchange), I've done both of you a favor. </p><p><em>Azathoth doesn't really exist--you can't trade with a non-sapient phenomenon.</em></p><p>Replace the sapient opponent with a non-sapient phenomenon in any of our thought experiments--e.g. Omega tells you that it's simply a physical law that determines whether money goes in the boxes or not.  Do you refuse to negotiate with physical laws?  Then if you're so smart, why ain't you rich? </p><p><em>So exactly how are you urging me to behave?</em></p><p>I want you to refute this essay!  For goodness sake, don't bite the bullet and start obeying your base desires or engineering a retrovirus to turn the next generation into your clones.</p><p><br></p><p> </p>", "sections": [{"title": "Part 1:  Transparent Newcomb with your existence at stake", "anchor": "Part_1___Transparent_Newcomb_with_your_existence_at_stake", "level": 1}, {"title": "Part 2: Acausal trade with Azathoth", "anchor": "Part_2__Acausal_trade_with_Azathoth", "level": 1}, {"title": "Anticipated Responses", "anchor": "Anticipated_Responses", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "176 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 176, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "pLRogvJLPPg6Mrvg4", "XPErvb8m9FapXCjhA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-06T02:37:20.147Z", "modifiedAt": null, "url": null, "title": "Request: A historian's take on the singularity", "slug": "request-a-historian-s-take-on-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:48.135Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "knb", "createdAt": "2009-02-27T04:55:09.459Z", "isAdmin": false, "displayName": "knb"}, "userId": "YEwEAL5Mu6YxaX4rD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d6yhKsPWtgbv38EL6/request-a-historian-s-take-on-the-singularity", "pageUrlRelative": "/posts/d6yhKsPWtgbv38EL6/request-a-historian-s-take-on-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/d6yhKsPWtgbv38EL6/request-a-historian-s-take-on-the-singularity", "postedAtFormatted": "Sunday, February 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%3A%20A%20historian's%20take%20on%20the%20singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%3A%20A%20historian's%20take%20on%20the%20singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd6yhKsPWtgbv38EL6%2Frequest-a-historian-s-take-on-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%3A%20A%20historian's%20take%20on%20the%20singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd6yhKsPWtgbv38EL6%2Frequest-a-historian-s-take-on-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd6yhKsPWtgbv38EL6%2Frequest-a-historian-s-take-on-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>A while ago, I saw an excellent video of a speech/lecture by an English professor of history (in front of an American audience). In the video, the historian touches on many topics of interest to this community, including the ways modern people are different (more individualistic, more concerned with \"rights\") from agriculturalists and how things like the industrial revolution occur. At the end, he gives a long discussion of the likely future of humanity, and I think he mentioned artificial intelligence (by brain emulation) and life-extension as the defining technologies of the future.</p>\n<p>I think the video was posted one or two years ago maximum. Since I imagine the people of LW probably read many of the same websites as I do, I'm hoping someone will know the video to which I'm referring. If not, suggestions of who the professor might be (or the website it was posted on are much appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d6yhKsPWtgbv38EL6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 6.758470155131103e-07, "legacy": true, "legacyId": "5353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-06T03:19:25.796Z", "modifiedAt": null, "url": null, "title": "My hour-long interview with Yudkowsky on \"Becoming a Rationalist\"", "slug": "my-hour-long-interview-with-yudkowsky-on-becoming-a", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:37.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RgZZAzubeyiYSsHQc/my-hour-long-interview-with-yudkowsky-on-becoming-a", "pageUrlRelative": "/posts/RgZZAzubeyiYSsHQc/my-hour-long-interview-with-yudkowsky-on-becoming-a", "linkUrl": "https://www.lesswrong.com/posts/RgZZAzubeyiYSsHQc/my-hour-long-interview-with-yudkowsky-on-becoming-a", "postedAtFormatted": "Sunday, February 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20hour-long%20interview%20with%20Yudkowsky%20on%20%22Becoming%20a%20Rationalist%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20hour-long%20interview%20with%20Yudkowsky%20on%20%22Becoming%20a%20Rationalist%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRgZZAzubeyiYSsHQc%2Fmy-hour-long-interview-with-yudkowsky-on-becoming-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20hour-long%20interview%20with%20Yudkowsky%20on%20%22Becoming%20a%20Rationalist%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRgZZAzubeyiYSsHQc%2Fmy-hour-long-interview-with-yudkowsky-on-becoming-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRgZZAzubeyiYSsHQc%2Fmy-hour-long-interview-with-yudkowsky-on-becoming-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>It makes for good Less Wrong introductory material to point people to, since there are lots of people who won't read long article online but <em>will</em>&nbsp;listen to a podcast on the way to work: <a href=\"http://commonsenseatheism.com/?p=12147\">LINK</a>.</p>\n<p>Apologies for the self-promotion, but it could hardly be more relevant to Less Wrong...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RgZZAzubeyiYSsHQc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 33, "extendedScore": null, "score": 6.75858071505372e-07, "legacy": true, "legacyId": "5354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-06T04:43:46.536Z", "modifiedAt": null, "url": null, "title": "post proposal: Attraction and Seduction for Heterosexual Male Rationalists", "slug": "post-proposal-attraction-and-seduction-for-heterosexual-male", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5wL32ve5mnrPbZh7n/post-proposal-attraction-and-seduction-for-heterosexual-male", "pageUrlRelative": "/posts/5wL32ve5mnrPbZh7n/post-proposal-attraction-and-seduction-for-heterosexual-male", "linkUrl": "https://www.lesswrong.com/posts/5wL32ve5mnrPbZh7n/post-proposal-attraction-and-seduction-for-heterosexual-male", "postedAtFormatted": "Sunday, February 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20post%20proposal%3A%20Attraction%20and%20Seduction%20for%20Heterosexual%20Male%20Rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Apost%20proposal%3A%20Attraction%20and%20Seduction%20for%20Heterosexual%20Male%20Rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wL32ve5mnrPbZh7n%2Fpost-proposal-attraction-and-seduction-for-heterosexual-male%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=post%20proposal%3A%20Attraction%20and%20Seduction%20for%20Heterosexual%20Male%20Rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wL32ve5mnrPbZh7n%2Fpost-proposal-attraction-and-seduction-for-heterosexual-male", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wL32ve5mnrPbZh7n%2Fpost-proposal-attraction-and-seduction-for-heterosexual-male", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 780, "htmlBody": "<p>It seems there's some interest in <a href=\"http://en.wikipedia.org/wiki/Pickup_artist\">PUA</a> and attraction at Less Wrong. Would that subject be appropriate for a front-page post? I've drafted the opening of what I had in mind, below. Let me know what you think, and whether I should write the full post.</p>\n<p>Also, I've done lots of collaborative writing before, with much success (<a href=\"/lw/43m/optimal_employment/\">two</a> <a href=\"http://commonsenseatheism.com/?p=11626\">examples</a>). I would welcome input from or collaboration with others who have some experience and skill in the attraction arts. If you're one of those people, send me a message! Even if you just want to comment on early drafts or contribute a few thoughts.</p>\n<p>I should probably clarify my concept of attraction and seduction. The founders of \"pickup\" basically saw it as advice on \"how to trick women into bed\", but I see it as a series of methods for \"How to become the best man you can be, which will help you succeed in all areas of life, and also make you attractive to women.\" <a href=\"http://en.wikipedia.org/wiki/Ross_Jeffries\">Ross Jeffries</a> used neuro-linguistic programming and hypnosis, and <a href=\"http://en.wikipedia.org/wiki/Mystery_(pickup_artist)\">Mystery</a> <em>literally</em>&nbsp;used magic tricks to get women to sleep with him. My own sympathies lie with methods advocated by groups like <a href=\"http://theartofcharm.com/\">Art of Charm</a>, who focus less on tricks and routines and more on holistic self-improvement.</p>\n<p>...</p>\n<p>...</p>\n<p><strong>EDIT</strong>: That didn't take long. Though I share much of <a href=\"/r/discussion/lw/44r/post_proposal_attraction_and_seduction_for/3hdt\">PhilGoetz's attitude</a>, <strong>I've decided I will not write this post</strong>, for the reasons articulated <a href=\"/lw/13j/exclusion_vs_objectification/\">here</a>, <a href=\"/lw/ap/of_gender_and_rationality/\">here</a>, <a href=\"/lw/134/sayeth_the_girl/\">here</a> and&nbsp;<a href=\"/r/discussion/lw/44r/post_proposal_attraction_and_seduction_for/3hdo\">here</a>.&nbsp;</p>\n<p>...</p>\n<p>Here was the proposed post...</p>\n<p>...</p>\n<p>When I interviewed to be a contestant on VH1's <em><a href=\"http://www.vh1.com/shows/the_pickup_artist/season_1/series.jhtml\">The Pick-Up Artist</a></em>, they asked me to list my skills. Among them, I listed \"rational thinking.\"</p>\n<p>\"How do you think rational thinking will help you with the skills of attraction?\" they asked.</p>\n<p>I paused, then answered: \"Rational thinking tells me that attraction is a thoroughly non-rational process.\"</p>\n<p><img style=\"float: center;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/dinkus.png\" alt=\"\" width=\"80\" height=\"32\" /></p>\n<p>A major theme at Less Wrong is \"<a href=\"/lw/43m/optimal_employment/\">How</a> <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">to</a> <em><a href=\"/lw/3w3/how_to_beat_procrastination/\">win</a></em> <a href=\"/lw/1sm/akrasia_tactics_review/\">at</a> <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">life</a> <a href=\"/lw/373/how_to_save_the_world/\">with</a> <a href=\"/lw/7i/rationality_is_systematized_winning/\">rationality</a>.\" Today, I want to talk about how to win in your <em>sex</em> life with rationality.<sup>a</sup></p>\n<p>I didn't get the part on the VH1 show, but no matter: studying and practicing pick-up has transformed my life more than <a href=\"http://commonsenseatheism.com/?p=13107\">almost</a> anything else, <em>even though</em> getting excellent and frequent sex is, oddly enough, not one of <a href=\"/lw/43v/the_urgent_metaethics_of_friendly_artificial/\">my life's priorities</a>. Nor is finding a soulmate.</p>\n<p>If you want lots of sex, or a soulmate, or you want to improve your <em>current</em>&nbsp;relationship, then attraction skills will help with that.&nbsp;Loneliness need not be one of the <a href=\"/lw/j/the_costs_of_rationality/\">costs of rationality</a>.&nbsp;But even if you <em>don't</em> want any of those things, studying attraction methods can (1) clear up confusion and frustration about <a href=\"http://www.youtube.com/watch?v=XSRO4JsDBJY\">the opposite sex</a>,<sup>b</sup> (2) improve your social relations in general, (3) boost your confidence, and thus (4) help you succeed in almost every part of your life.&nbsp;</p>\n<p>This is a post about what men can do to build attraction in women.<sup>c</sup>&nbsp;I will not discuss whether these methods are moral. I will not discuss whether these methods are more or less \"manipulative\" than the standard <em>female</em> methods for attracting <em>men</em>. Instead, I will focus on factual claims about <em>what tends to create sexual attraction in women</em>.</p>\n<p>This is also a post for rationalists. More specifically, it is aimed at <a href=\"/lw/fk/survey_results/\">the average Less Wrong reader</a>: a 20-34 year old, high-IQ, single male atheist.</p>\n<p>I will also be assuming the stereotype that many passionate rationalists of our type could benefit from advice on body language, voice tone, social skills, and attire - a stereotype that has&nbsp;<a href=\"http://www.medical-hypotheses.com/article/S0306-9877(09)00555-6/abstract\">some merit</a>. Even if <em>you</em>&nbsp;don't need such advice, many others <em>will</em> benefit from it. <em>I</em>&nbsp;did.</p>\n<p>As is <a href=\"/lw/3w3/how_to_beat_procrastination/\">my</a> <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">style</a>, I'll begin with a survey of the scientific data on the subject.</p>\n<p><img style=\"float: center;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/dinkus.png\" alt=\"\" width=\"80\" height=\"32\" /></p>\n<p>Self-help methods <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">in general</a> have not received enough attention from experimental researchers, and attraction methods have fared even worse. That may be what drove the leaders of the pickup community to run thousands of real-life experiments, systematically varying their attire, body language, and speech to measure what worked and what didn't. The dearth of research on the subject turned ordinary men into amateur seduction scientists, albeit without much training.&nbsp;</p>\n<p>Still, we can learn <em>some</em> things about sexual attraction from established science.</p>\n<p><strong>[full post to be continued here]</strong></p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 11px;\"><br /></span></p>\n<p><sup>a</sup>&nbsp;I've also given two humorous speeches on this subject:&nbsp;<a href=\"http://www.youtube.com/watch?v=zvcuZhDWLgg\">How to Seduce Women with Body Language</a>&nbsp;and&nbsp;<a href=\"http://www.youtube.com/watch?v=f7zNNVS9Uf4\">How to Seduce Women with Vocal Tonality</a>.</p>\n<p><sup>b</sup> I used to be one of those poor guys who complained that \"Girls <em>say</em>&nbsp;they want nice guys, but they only go out with jerks!\" Merely reading enough evolutionary psychology to understand <em>why</em>&nbsp;women often date \"jerks\" was enough, in my case, to relieve a lot of confusion and frustration. Even without developing attraction <em>skills</em>, mere understanding can, I think, relieve serious stress and worries about one's manly (fragile) ego.</p>\n<p><span style=\"font-size: 11px;\"><span style=\"font-size: small;\"><sup>c</sup>&nbsp;Sorry, I don't know much about homosexual attraction, and I'll leave the subject of how <em>women</em> can attract better <em>men</em> to other authors.</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5wL32ve5mnrPbZh7n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 12, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "5355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 146, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jtedBLdducritm8y6", "MyqGb24pM54rJhDpb", "xsyG7PkMekHud2DMK", "gsL6CLqjujPNSLL2o", "33KewgYhNSxFpbpXg", "RWo4LwFzpHNQCTcYt", "rRmisKb45dN7DK4BW", "uFYQaGCRwt3wKtyZP", "TrmMcujGZt5JAtMGg", "4ARtkT3EYox3THYjF", "TKdpSzmcezNbfmGAy", "9Z3pezjiWLfNANg9P", "ZWC3n9c6v4s35rrZ3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-06T05:42:41.536Z", "modifiedAt": null, "url": null, "title": "Revising priors and anthropic reasoning", "slug": "revising-priors-and-anthropic-reasoning", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:35.314Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mnYZWH2A4gcdtFATw/revising-priors-and-anthropic-reasoning", "pageUrlRelative": "/posts/mnYZWH2A4gcdtFATw/revising-priors-and-anthropic-reasoning", "linkUrl": "https://www.lesswrong.com/posts/mnYZWH2A4gcdtFATw/revising-priors-and-anthropic-reasoning", "postedAtFormatted": "Sunday, February 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Revising%20priors%20and%20anthropic%20reasoning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARevising%20priors%20and%20anthropic%20reasoning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmnYZWH2A4gcdtFATw%2Frevising-priors-and-anthropic-reasoning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Revising%20priors%20and%20anthropic%20reasoning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmnYZWH2A4gcdtFATw%2Frevising-priors-and-anthropic-reasoning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmnYZWH2A4gcdtFATw%2Frevising-priors-and-anthropic-reasoning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>I believe that life on Earth arose spontaneously.&nbsp; I also believe the galaxy around me is largely devoid of life.&nbsp; I reconcile these things using the <a href=\"http://en.wikipedia.org/wiki/Anthropic_principle\">anthropic principle</a>.</p>\n<p>I also believe that fundamental cosmological constants have values convenient for the development of life.&nbsp; I don't know if it makes sense to pretend that those constants could have had other values - it seems to me like arguing that <em>e</em> could have been 2.716.&nbsp; But it's certainly done.&nbsp; And again, the anthropic principle is sometimes invoked, as an alternative to, say, God.</p>\n<p>Suppose somebody came up with a new theory of cosmological constants, that claimed that only certain values are allowable, and that a large percentage of the allowable sets would make life possible.&nbsp; Then you wouldn't have to use the anthropic principle.&nbsp; Wouldn't you be more comfortable with that?</p>\n<p>But if that's so, doesn't it mean that you really attach a low prior to the anthropic principle?&nbsp; And that you don't truly <em>accept</em> the anthropic principle?</p>\n<p>How do you do Bayesian belief revision when one of your alternative hypotheses uses the anthropic principle?&nbsp; Can you give a strong preference to the hypothesis that does not require it?&nbsp; Because I know that I would.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2, "5f5c37ee1b5cdee568cfb108": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mnYZWH2A4gcdtFATw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "5356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-06T13:28:52.380Z", "modifiedAt": null, "url": null, "title": "A bit meta: Do posts come in batches? If so, why?", "slug": "a-bit-meta-do-posts-come-in-batches-if-so-why", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:38.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MnftBAcPmvwZX6ib3/a-bit-meta-do-posts-come-in-batches-if-so-why", "pageUrlRelative": "/posts/MnftBAcPmvwZX6ib3/a-bit-meta-do-posts-come-in-batches-if-so-why", "linkUrl": "https://www.lesswrong.com/posts/MnftBAcPmvwZX6ib3/a-bit-meta-do-posts-come-in-batches-if-so-why", "postedAtFormatted": "Sunday, February 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20bit%20meta%3A%20Do%20posts%20come%20in%20batches%3F%20If%20so%2C%20why%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20bit%20meta%3A%20Do%20posts%20come%20in%20batches%3F%20If%20so%2C%20why%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMnftBAcPmvwZX6ib3%2Fa-bit-meta-do-posts-come-in-batches-if-so-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20bit%20meta%3A%20Do%20posts%20come%20in%20batches%3F%20If%20so%2C%20why%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMnftBAcPmvwZX6ib3%2Fa-bit-meta-do-posts-come-in-batches-if-so-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMnftBAcPmvwZX6ib3%2Fa-bit-meta-do-posts-come-in-batches-if-so-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>15 Jan - 0 posts</p>\n<p>16 Jan&nbsp;-&nbsp;1 post</p>\n<p>17 Jan&nbsp;-&nbsp;1 post</p>\n<p>18 Jan&nbsp;-&nbsp;1 post</p>\n<p>19 Jan&nbsp;-&nbsp;0 posts</p>\n<p>20 Jan&nbsp;-&nbsp;2 posts</p>\n<p>21 Jan&nbsp;-&nbsp;0 posts</p>\n<p>22 Jan&nbsp;-&nbsp;1 post</p>\n<p>23/24/25/26/27 Jan - 0 posts</p>\n<p>28 Jan&nbsp;-&nbsp;1 post</p>\n<p>29 Jan&nbsp;-&nbsp;1 post</p>\n<p>30 Jan&nbsp;-&nbsp;0 posts</p>\n<p>31 Jan&nbsp;-&nbsp;3 posts</p>\n<p>1 Feb&nbsp;-&nbsp;2 posts</p>\n<p>2 Feb&nbsp;-&nbsp;1 post</p>\n<p>3 Feb&nbsp;-&nbsp;0 posts</p>\n<p>4 Feb&nbsp;-&nbsp;1 post</p>\n<p>5 Feb&nbsp;- 3 posts</p>\n<p>Maybe 23-27 was Christmas? But I've gotten a general feeling that activity spikes around the same time. Perhaps when the site is populated with posts, people spend more time here, and then think more on related topics, and thus are more likely to post?</p>\n<p>Note that this is all posts, not just promoted posts. It also includes rationality meetups and quotes threads - maybe it'd be more interesting analysis without that... I definitely get the feeling that a thought provoking post generates more. Whereas inactivity generates more inactivity. Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MnftBAcPmvwZX6ib3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "5360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-06T18:30:40.936Z", "modifiedAt": null, "url": null, "title": "Value Stability and Aggregation", "slug": "value-stability-and-aggregation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:27.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g57kXZnqB4uN6gfEx/value-stability-and-aggregation", "pageUrlRelative": "/posts/g57kXZnqB4uN6gfEx/value-stability-and-aggregation", "linkUrl": "https://www.lesswrong.com/posts/g57kXZnqB4uN6gfEx/value-stability-and-aggregation", "postedAtFormatted": "Sunday, February 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20Stability%20and%20Aggregation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20Stability%20and%20Aggregation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg57kXZnqB4uN6gfEx%2Fvalue-stability-and-aggregation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20Stability%20and%20Aggregation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg57kXZnqB4uN6gfEx%2Fvalue-stability-and-aggregation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg57kXZnqB4uN6gfEx%2Fvalue-stability-and-aggregation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1545, "htmlBody": "<p>One of the central problems of Friendly Artificial Intelligence is goal system stability. Given a goal system - whether it's a utility function, a computer program, or a couple kilograms of neural tissue - we want to determine whether it's <em>stable</em>, meaning, is there something that might plausibly happen to it which will radically alter its behavior in a direction we don't like? As a first step in solving this problem, let's consider a classic example of goal systems that is <em>not</em> stable.</p>\n<p>Suppose you are a true <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Utilitarianism\">Bentham-Mill Utilitarian</a>, which means you hold that the right thing to do is that which maximizes the amount of happiness minus the amount of pain in the world, summed up moment by moment. Call this HapMax for short. You determine this by assigning each person a happiness-minus-pain score at each moment, based on a complex neurological definition, and adding up the scores of each person-moment. One day, you are interrupted from your job as an antidepressant research chemist by a commotion outside. Rushing out to investigate, you find a hundred-foot tall monster rampaging through the streets of Tokyo, which says:</p>\n<blockquote>\n<p>\"I am a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Utility_monster\">Utility Monster</a>. Robert Nozick grew me in his underwater base, and now I desire nothing more than to eat people. This makes me very happy, and because I am so very tall and the volume of my brain's reward center grows with the cube of my height, it makes me *so* happy that it will outweigh the momentary suffering and shortened lifespan of anyone I eat.\"</p>\n</blockquote>\n<p>As a true HapMaxer (not to be confused with a human, who might claim to be a HapMaxer but can't actually be one), you find this very convincing: the right thing to do is to maximize the number of people the monster can eat, so you heroically stand in front of the line of tanks that is now rolling down main street to buy it time. HapMax seemed like a good idea at first, but this example shows that it is very wrong. What lessons should we learn before trying to build another utility function? HapMax starts by dividing the world into pieces, and the trouble starts when one of those agents doesn't behave as expected.</p>\n<p><a id=\"more\"></a><strong>Dividing and Recombining Utility</strong></p>\n<p>Human values are too complex to specify in one go, so like other complex things, we manage the complexity by subdividing the problem, solving the pieces, then recombining them back into a whole solution. Let's call these sub-problems <em>value fragments</em>, and the recombination procedure <em>utility aggregation</em>. If all of the fragments are evaluated correctly and the aggregation procedure is also correct, then this yields a correct solution.</p>\n<p>There are plenty of different ways of slicing up utility functions, and we can choose as many of them as desired. You can slice up a utility function by preference type - go through a list of desirable things like \"amount of knowledge\" and \"minus-amount of poverty\", assign a score to each representing the degree to which that preference is fulfilled, and assign a weighting to each representing its importance and degree of overlap. You can slice them up by branch - go through all the possible outcomes, assigning a score to each outcome representing how nice a world it is and a weighting for probability. You can slice it up by agent - go through all the people you know about, and assign a score for how good things are for them. And you can slice it up by moment - go through a predicted future step by step, and assign a score for how good the things in the world at that moment are. Any of these slices yields value fragments; a fragment is any reference class that describes a portion of the utility function.</p>\n<p>Meta-ethics, then, consists of three parts. First, we choose an overall structure, most popularly a predictor and utility function, and subdivide it into fragments, such as by preference, branch, agent, and moment. Then we specify the subdivided parts - either with a detailed preference-extraction procedure like the one <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a> calls for but doesn't quite specify, or something vague like \"preferences\". Finally, we add an aggregation procedure.</p>\n<p>The aggregation procedure is what determines how stable a utility function is in the face of localized errors. It was a poor choice of aggregation function that made HapMax fail so catastrophically. HapMax aggregates by simple addition, and its utility function is divided by agent. That makes an awful lot of dissimilar fragments. What happens if some of them don't behave as expected? Nozick's Utility Monster problem is exactly that: one of the agents produces utilities that diverge to extremely large values, overpowering the others and breaking the whole utility function.</p>\n<p>&nbsp;</p>\n<p><strong>Aggregation and Error</strong></p>\n<p>If human values are as complex as we think, then it is extremely unlikely that we will ever manage to correctly specify every value fragment in every corner case. Therefore, to produce a stable system of ethics and avoid falling for any other sorts of utility monsters, we need to model the sorts of bugs that fragments of our utility function might have, and choose an aggregation function that makes the utility function resilient - that is, we'd like it to keep working and still represent something close to our values even if some of the pieces don't behave as expected. Ideally, every value would be specified multiple times from different angles, and the aggregation function would ensure that no one bug anywhere could cause a catastrophe.</p>\n<p>We saw how linear aggregation can fail badly when aggregating over agents - one agent with a very steep utility function gradient can overpower every other concern. However, this is not just a problem for aggregating agents; it's also a problem for aggregating preferences, branches, and moments. Aggregation between branches breaks down in <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast\">Pascal's Mugging</a> which features a branch with divergent utility, and in <a href=\"/lw/19d/the_anthropic_trilemma\">Anthropic problems</a>, where the number of branches is not as expected. Aggregation between moments breaks down when considering <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a>, which features a time range with divergent utility. The effect of linearly aggregating distinct preference types is a little harder to predict, since it depends just what the inputs are and what bugs they have, but they're mostly as bad as <a href=\"/lw/td/magical_categories/\">tiling with molecular smiley faces</a>, and <a href=\"/lw/1ws/the_importance_of_goodharts_law/\">Goodhart's Law</a> suggests that closing every loophole is impossible.</p>\n<p>If linear aggregation is so unstable, then how did it become so popular in the first place? It's not that no other possibilities were considered. For example, there's John Rawls' <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/A_Theory_of_Justice\">Maximin Principle</a>, which says that we should arrange society so as to maximize how well off the worst-off person is. Now, the Maximin Principle is extremely terrible - it implies that if we find the one person who's been tortured the most, and we can't stop them from being tortured but can make them feel better about it by torturing everyone else, then we should do so. But there are some aggregation strategies that fail less badly, and aren't obviously insane. For example, we can aggregate different moral rules by giving each rule a veto, for predicted worlds and for possible actions. When this fails - if, for example, every course of action is vetoed - it shuts down, effectively reverting to a mostly-safe default. Unfortunately, aggregation by veto doesn't quite work because it can't handle trolley problems, where every course of action is somehow objectionable and there is no time to shut down and punt the decision to a human. The space of possible aggregation strategies, however, is largely unexplored. There is one advantage which has been proven unique to linear aggregation, which no other strategy can have: <a href=\"http://en.wikipedia.org/wiki/Dutch_book\">Dutch Book</a> resistance. However, this may be less important than mitigating the damage bugs can do, and it may be partially recoverable by having utility be linear within a narrow range, and switching to something else (or calling on humans to clarify) in cases outside that range.</p>\n<p>&nbsp;</p>\n<p><strong>Classifying Types of Errors </strong></p>\n<p>I believe the next step in tackling the value system stability problem is to explore the space of possible aggregation strategies, evaluating each according to how it behaves when the values it aggregates fail in certain ways. So here, then, is a classification of possible value-fragment errors. Each of these can apply to any reference class</p>\n<ul>\n<li>Deletion: The agent forgets about a fragment. A branch is forgotten about, a preference forgotten, is incorrectly deemed inapplicable, or its fulfillment can't be predicted.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Insertion: A random extra preference is added; a branch that's actually impossible is predicted as an outcome; an agent that doesn't exist or isn't morally significant is posited.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Divergence: A value fragment or its gradient has a value with a much larger magnitude than expected, possibly infinite or as large as an arbitrary value chosen by some agent.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Noise: Each fragment's estimated utility has an error term added, from a gaussian, log-normal or other distribution.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Scaling: The agent encounters or envisions a scenario in which the number of times a value is tested for is qualitatively different than expected.</li>\n</ul>\n<ul>\n</ul>\n<p>A good utility function, if it contains subdivisions, must be able to survive errors in any one or even several of those divisions while still representing something close to our values. What sort of function might achieve that purpose?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g57kXZnqB4uN6gfEx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 5, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "5352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>One of the central problems of Friendly Artificial Intelligence is goal system stability. Given a goal system - whether it's a utility function, a computer program, or a couple kilograms of neural tissue - we want to determine whether it's <em>stable</em>, meaning, is there something that might plausibly happen to it which will radically alter its behavior in a direction we don't like? As a first step in solving this problem, let's consider a classic example of goal systems that is <em>not</em> stable.</p>\n<p>Suppose you are a true <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Utilitarianism\">Bentham-Mill Utilitarian</a>, which means you hold that the right thing to do is that which maximizes the amount of happiness minus the amount of pain in the world, summed up moment by moment. Call this HapMax for short. You determine this by assigning each person a happiness-minus-pain score at each moment, based on a complex neurological definition, and adding up the scores of each person-moment. One day, you are interrupted from your job as an antidepressant research chemist by a commotion outside. Rushing out to investigate, you find a hundred-foot tall monster rampaging through the streets of Tokyo, which says:</p>\n<blockquote>\n<p>\"I am a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Utility_monster\">Utility Monster</a>. Robert Nozick grew me in his underwater base, and now I desire nothing more than to eat people. This makes me very happy, and because I am so very tall and the volume of my brain's reward center grows with the cube of my height, it makes me *so* happy that it will outweigh the momentary suffering and shortened lifespan of anyone I eat.\"</p>\n</blockquote>\n<p>As a true HapMaxer (not to be confused with a human, who might claim to be a HapMaxer but can't actually be one), you find this very convincing: the right thing to do is to maximize the number of people the monster can eat, so you heroically stand in front of the line of tanks that is now rolling down main street to buy it time. HapMax seemed like a good idea at first, but this example shows that it is very wrong. What lessons should we learn before trying to build another utility function? HapMax starts by dividing the world into pieces, and the trouble starts when one of those agents doesn't behave as expected.</p>\n<p><a id=\"more\"></a><strong>Dividing and Recombining Utility</strong></p>\n<p>Human values are too complex to specify in one go, so like other complex things, we manage the complexity by subdividing the problem, solving the pieces, then recombining them back into a whole solution. Let's call these sub-problems <em>value fragments</em>, and the recombination procedure <em>utility aggregation</em>. If all of the fragments are evaluated correctly and the aggregation procedure is also correct, then this yields a correct solution.</p>\n<p>There are plenty of different ways of slicing up utility functions, and we can choose as many of them as desired. You can slice up a utility function by preference type - go through a list of desirable things like \"amount of knowledge\" and \"minus-amount of poverty\", assign a score to each representing the degree to which that preference is fulfilled, and assign a weighting to each representing its importance and degree of overlap. You can slice them up by branch - go through all the possible outcomes, assigning a score to each outcome representing how nice a world it is and a weighting for probability. You can slice it up by agent - go through all the people you know about, and assign a score for how good things are for them. And you can slice it up by moment - go through a predicted future step by step, and assign a score for how good the things in the world at that moment are. Any of these slices yields value fragments; a fragment is any reference class that describes a portion of the utility function.</p>\n<p>Meta-ethics, then, consists of three parts. First, we choose an overall structure, most popularly a predictor and utility function, and subdivide it into fragments, such as by preference, branch, agent, and moment. Then we specify the subdivided parts - either with a detailed preference-extraction procedure like the one <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a> calls for but doesn't quite specify, or something vague like \"preferences\". Finally, we add an aggregation procedure.</p>\n<p>The aggregation procedure is what determines how stable a utility function is in the face of localized errors. It was a poor choice of aggregation function that made HapMax fail so catastrophically. HapMax aggregates by simple addition, and its utility function is divided by agent. That makes an awful lot of dissimilar fragments. What happens if some of them don't behave as expected? Nozick's Utility Monster problem is exactly that: one of the agents produces utilities that diverge to extremely large values, overpowering the others and breaking the whole utility function.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Aggregation_and_Error\">Aggregation and Error</strong></p>\n<p>If human values are as complex as we think, then it is extremely unlikely that we will ever manage to correctly specify every value fragment in every corner case. Therefore, to produce a stable system of ethics and avoid falling for any other sorts of utility monsters, we need to model the sorts of bugs that fragments of our utility function might have, and choose an aggregation function that makes the utility function resilient - that is, we'd like it to keep working and still represent something close to our values even if some of the pieces don't behave as expected. Ideally, every value would be specified multiple times from different angles, and the aggregation function would ensure that no one bug anywhere could cause a catastrophe.</p>\n<p>We saw how linear aggregation can fail badly when aggregating over agents - one agent with a very steep utility function gradient can overpower every other concern. However, this is not just a problem for aggregating agents; it's also a problem for aggregating preferences, branches, and moments. Aggregation between branches breaks down in <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast\">Pascal's Mugging</a> which features a branch with divergent utility, and in <a href=\"/lw/19d/the_anthropic_trilemma\">Anthropic problems</a>, where the number of branches is not as expected. Aggregation between moments breaks down when considering <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a>, which features a time range with divergent utility. The effect of linearly aggregating distinct preference types is a little harder to predict, since it depends just what the inputs are and what bugs they have, but they're mostly as bad as <a href=\"/lw/td/magical_categories/\">tiling with molecular smiley faces</a>, and <a href=\"/lw/1ws/the_importance_of_goodharts_law/\">Goodhart's Law</a> suggests that closing every loophole is impossible.</p>\n<p>If linear aggregation is so unstable, then how did it become so popular in the first place? It's not that no other possibilities were considered. For example, there's John Rawls' <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/A_Theory_of_Justice\">Maximin Principle</a>, which says that we should arrange society so as to maximize how well off the worst-off person is. Now, the Maximin Principle is extremely terrible - it implies that if we find the one person who's been tortured the most, and we can't stop them from being tortured but can make them feel better about it by torturing everyone else, then we should do so. But there are some aggregation strategies that fail less badly, and aren't obviously insane. For example, we can aggregate different moral rules by giving each rule a veto, for predicted worlds and for possible actions. When this fails - if, for example, every course of action is vetoed - it shuts down, effectively reverting to a mostly-safe default. Unfortunately, aggregation by veto doesn't quite work because it can't handle trolley problems, where every course of action is somehow objectionable and there is no time to shut down and punt the decision to a human. The space of possible aggregation strategies, however, is largely unexplored. There is one advantage which has been proven unique to linear aggregation, which no other strategy can have: <a href=\"http://en.wikipedia.org/wiki/Dutch_book\">Dutch Book</a> resistance. However, this may be less important than mitigating the damage bugs can do, and it may be partially recoverable by having utility be linear within a narrow range, and switching to something else (or calling on humans to clarify) in cases outside that range.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Classifying_Types_of_Errors_\">Classifying Types of Errors </strong></p>\n<p>I believe the next step in tackling the value system stability problem is to explore the space of possible aggregation strategies, evaluating each according to how it behaves when the values it aggregates fail in certain ways. So here, then, is a classification of possible value-fragment errors. Each of these can apply to any reference class</p>\n<ul>\n<li>Deletion: The agent forgets about a fragment. A branch is forgotten about, a preference forgotten, is incorrectly deemed inapplicable, or its fulfillment can't be predicted.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Insertion: A random extra preference is added; a branch that's actually impossible is predicted as an outcome; an agent that doesn't exist or isn't morally significant is posited.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Divergence: A value fragment or its gradient has a value with a much larger magnitude than expected, possibly infinite or as large as an arbitrary value chosen by some agent.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Noise: Each fragment's estimated utility has an error term added, from a gaussian, log-normal or other distribution.</li>\n</ul>\n<ul>\n</ul>\n<ul>\n<li>Scaling: The agent encounters or envisions a scenario in which the number of times a value is tested for is qualitatively different than expected.</li>\n</ul>\n<ul>\n</ul>\n<p>A good utility function, if it contains subdivisions, must be able to survive errors in any one or even several of those divisions while still representing something close to our values. What sort of function might achieve that purpose?</p>", "sections": [{"title": "Aggregation and Error", "anchor": "Aggregation_and_Error", "level": 1}, {"title": "Classifying Types of Errors ", "anchor": "Classifying_Types_of_Errors_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "41 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749", "y7jZ9BLEeuNTzgAE5", "PoDAyQMWEXBBBEJ5P", "YtvZxRpZjcFNwJecS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T01:41:00.545Z", "modifiedAt": null, "url": null, "title": "[LINK] Levels of Ethics", "slug": "link-levels-of-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.523Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "WrongBot", "createdAt": "2010-05-27T15:54:44.899Z", "isAdmin": false, "displayName": "WrongBot"}, "userId": "toqZ5PS8KdJ2mXzgN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6uBpntXtedwzAkoco/link-levels-of-ethics", "pageUrlRelative": "/posts/6uBpntXtedwzAkoco/link-levels-of-ethics", "linkUrl": "https://www.lesswrong.com/posts/6uBpntXtedwzAkoco/link-levels-of-ethics", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Levels%20of%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Levels%20of%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6uBpntXtedwzAkoco%2Flink-levels-of-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Levels%20of%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6uBpntXtedwzAkoco%2Flink-levels-of-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6uBpntXtedwzAkoco%2Flink-levels-of-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>I've resumed blogging For Real This Time&trade;, starting with an <a href=\"http://www.wrongbot.com/philosophy/levels-of-ethics/\">introductory overview</a> of the distinction between metaethics and normative ethics.</p>\n<p>Should I cross-post it to LessWrong? Should I link or cross-post future blogging about metaethics and other LW-relevant topics? Is it rubbish? Inquiring minds (mostly mine) need to know!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6uBpntXtedwzAkoco", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 6.762104727333979e-07, "legacy": true, "legacyId": "5363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T02:49:52.785Z", "modifiedAt": null, "url": null, "title": "What does a calculator mean by \"2\"?", "slug": "what-does-a-calculator-mean-by-2", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:20.987Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TZqNQG7588WkFC9so/what-does-a-calculator-mean-by-2", "pageUrlRelative": "/posts/TZqNQG7588WkFC9so/what-does-a-calculator-mean-by-2", "linkUrl": "https://www.lesswrong.com/posts/TZqNQG7588WkFC9so/what-does-a-calculator-mean-by-2", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20does%20a%20calculator%20mean%20by%20%222%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20does%20a%20calculator%20mean%20by%20%222%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTZqNQG7588WkFC9so%2Fwhat-does-a-calculator-mean-by-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20does%20a%20calculator%20mean%20by%20%222%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTZqNQG7588WkFC9so%2Fwhat-does-a-calculator-mean-by-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTZqNQG7588WkFC9so%2Fwhat-does-a-calculator-mean-by-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 584, "htmlBody": "<p>I think my <a href=\"/r/discussion/lw/44i/another_argument_against_eliezers_metaethics/\">previous argument</a> was at least partly wrong or confused, because I don't really understand what it means for a computation to mean something by a symbol. Here I'll back up and try to figure out what I mean by \"mean\" first.</p>\n<p>Consider a couple of programs. The first one (A) is an arithmetic calculator. It takes a string as input, interprets it a formula written in decimal notation, and outputs the result of computing that formula. For example, A(\"9+12\") produces \"21\" as output. The second (B) is a substitution cipher calculator. It \"encrypts\" its input by substituting each character using a fixed mapping. It so happens that B(\"9+12\") outputs \"c6b3\".</p>\n<p>What do A and B mean by \"2\"? Intuitively it seems that by \"2\", A means the integer (i.e., abstract mathematical object) 2, while for B, \"2\" doesn't really mean anything; it's just a symbol that it blindly manipulates. But A also just produces its output by manipulating symbols, so why does it seem like it means something by \"2\"? I think it's because the <em>way</em> A manipulates the symbol \"2\" corresponds to how the integer 2 \"works\", whereas the way B manipuates \"2\" doesn't correspond to anything, except how it manipulates that symbol. We <em>could</em> perhaps say that by \"2\" B means \"the way B manipulates the symbol '2'\", but that doesn't seem to buy us anything.</p>\n<p>(Similarly, by \"+\" A means the mathematical operation of addition, whereas B doesn't really mean anything by it. Note that this discussion assumes some version of mathematical platonism. A formalist would probably say that A also doesn't mean anything by \"2\" and \"+\" except how it manipulates those symbols, but that seems implausible to me.)</p>\n<p>Going back to meta-ethics, I think a central mystery is what do we mean by \"right\" when we're considering moral arguments (by which I don't mean Nesov's technical term \"moral arguments\", but arguments such as \"total utilitarianism is wrong (i.e., not right) because it leads to the following conclusions ..., which are obviously wrong\"). If human minds are computations (which I think they almost certainly are), then the way that a human mind processes such arguments can be viewed as an algorithm (which may differ from individual to individual). Suppose we could somehow abstract this algorithm away from the rest of the human, and consider it as, say, a program that when given an input string consisting of a list of moral arguments, thinks them over, comes to some conclusions, and outputs those conclusions in the form of a utility function.</p>\n<p>If my understanding is correct, what this algorithm means by \"right\" depends on the details of how it works. Is it more like calculator A or B? It may be that the way we respond to moral arguments doesn't correspond to anything except how we respond to moral arguments. For example, if it's totally random, or depend in a chaotic fashion on trivial details of wording or ordering of its input. This would be case B, where \"right\" can't really be said to mean anything, at least as far as the part of our minds that considers moral arguments is concerned. Or it may be case A, where the way we process \"right\" corresponds to some abstract mathematical object or some other kind of external object, in which case I think \"right\" can be said to mean that external object.</p>\n<p>Since we don't know which is the case yet, I think we're forced to say that we don't currently know what \"right\" means.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TZqNQG7588WkFC9so", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "5365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8pGeGeoi6JRHqSXgy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T02:51:05.458Z", "modifiedAt": null, "url": null, "title": "SUGGEST and VOTE: Posts We Want to Read on Less Wrong", "slug": "suggest-and-vote-posts-we-want-to-read-on-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:52.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Skmn6voCTw3ozMRx5/suggest-and-vote-posts-we-want-to-read-on-less-wrong", "pageUrlRelative": "/posts/Skmn6voCTw3ozMRx5/suggest-and-vote-posts-we-want-to-read-on-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/Skmn6voCTw3ozMRx5/suggest-and-vote-posts-we-want-to-read-on-less-wrong", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SUGGEST%20and%20VOTE%3A%20Posts%20We%20Want%20to%20Read%20on%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASUGGEST%20and%20VOTE%3A%20Posts%20We%20Want%20to%20Read%20on%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkmn6voCTw3ozMRx5%2Fsuggest-and-vote-posts-we-want-to-read-on-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SUGGEST%20and%20VOTE%3A%20Posts%20We%20Want%20to%20Read%20on%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkmn6voCTw3ozMRx5%2Fsuggest-and-vote-posts-we-want-to-read-on-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkmn6voCTw3ozMRx5%2Fsuggest-and-vote-posts-we-want-to-read-on-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 498, "htmlBody": "<p>Less Wrong is a large community of very smart people with a wide spectrum of expertise, and I think relatively <em>little</em> of that value has been tapped.</p>\n<p>Like my post <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">The Best Textbooks on Every Subject</a>, this is meant to be a community-driven post. The first goal is to identify topics the Less Wrong community would like to read more about. The second goal is to encourage Less Wrongers to write on those topics. (Respecting, of course, the implicit and fuzzy guidelines for what should be posted to Less Wrong.)</p>\n<p>One problem is that those with expertise on a subject don't necessarily feel competent to write a front-page post on it. If that's the case, please comment here explaining that you might be able to write one of the requested posts, but you'd like a writing collaborator. We'll try to find you one.</p>\n<p>&nbsp;</p>\n<h4>Rules</h4>\n<p><strong></strong>You may either:</p>\n<ul>\n<li><strong>Post the title of the post you want someone to write for Less Wrong</strong>. If the title itself isn't enough to specify the content, include a few sentences of explanation. \"How to Learn a Language Quickly\" probably needs no elaboration, but \"Normative Theory and Coherent Extrapolated Volition\" certainly does. <em>Do not&nbsp;post two proposed post titles in the same comment</em>, because that will confuse voting. Please put the title in <strong>bold</strong>.<br /><br />or...</li>\n<li><strong>Vote for a post title that has already been suggested</strong>, indicating that you would like to read that post, too. Vote with karma ('Vote Up' or 'Vote Down' on the comment that contains the proposed post title).</li>\n</ul>\n<p>I will regularly update the list of suggested Less Wrong posts, ranking them in descending order of votes (<a href=\"http://commonsenseatheism.com/?p=8420\">like this</a>).</p>\n<p>&nbsp;</p>\n<h4>The List So Far<span style=\"font-weight: normal;\"> (updated 02/11/11)</span></h4>\n<ul>\n<li>(35) Conversation Strategies for Spreading Rationality Without Annoying People</li>\n<li>(32) Smart Drugs: Which Ones to Use for What, and Why</li>\n<li>(30)&nbsp;A Survey of Upgrade Paths for the Human Brain</li>\n<li>(29)&nbsp;Trusting Your Doctor: When and how to be skeptical about medical advice and medical consensus</li>\n<li>(25) Rational Homeschool Education</li>\n<li>(25)&nbsp;Field Manual: What to Do If You're Stranded in a Level 1 (Base Human Equivalent) Brain in a pre-Singularity Civilization</li>\n<li>(20)&nbsp;Entrepreneurship</li>\n<li>(20)&nbsp;Detecting And Bridging Inferential Distance For Teachers</li>\n<li>(19) Detecting And Bridging Inferential Distance For Learners</li>\n<li>(18) Teaching Utilizable Rationality Skills by Exemplifying the Application of Rationality</li>\n</ul>\n<div><br /></div>\n<ul>\n<li>(13)&nbsp;Open Thread: Offers of Help, Requests for Help</li>\n<li>(13)&nbsp;<a href=\"/r/discussion/lw/49g/open_thread_mathematics/\">Open Thread: Math</a></li>\n<li>(12)&nbsp;How to Learn a Language Quickly</li>\n<li>(12) True Answers for Every Philosophical Question</li>\n<li>(10) The \"Reductionism\" Sequence in One Lesson</li>\n<li>(10) The \"Map and Territory\" Sequence in One Lesson</li>\n<li>(10) The \"Mysterious Answers to Mysterious Questions\" Sequence in One Lesson</li>\n<li>(10)&nbsp;Lecture Notes on Personal Rationality</li>\n<li>(10) The \"Joy in the Merely Real\" Sequence in One Lesson</li>\n</ul>\n<div>(below 10 points not listed)</div>\n<div>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial;\">\n</ul>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Skmn6voCTw3ozMRx5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 21, "extendedScore": null, "score": 6.762290285142965e-07, "legacy": true, "legacyId": "5366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Less Wrong is a large community of very smart people with a wide spectrum of expertise, and I think relatively <em>little</em> of that value has been tapped.</p>\n<p>Like my post <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">The Best Textbooks on Every Subject</a>, this is meant to be a community-driven post. The first goal is to identify topics the Less Wrong community would like to read more about. The second goal is to encourage Less Wrongers to write on those topics. (Respecting, of course, the implicit and fuzzy guidelines for what should be posted to Less Wrong.)</p>\n<p>One problem is that those with expertise on a subject don't necessarily feel competent to write a front-page post on it. If that's the case, please comment here explaining that you might be able to write one of the requested posts, but you'd like a writing collaborator. We'll try to find you one.</p>\n<p>&nbsp;</p>\n<h4 id=\"Rules\">Rules</h4>\n<p><strong></strong>You may either:</p>\n<ul>\n<li><strong>Post the title of the post you want someone to write for Less Wrong</strong>. If the title itself isn't enough to specify the content, include a few sentences of explanation. \"How to Learn a Language Quickly\" probably needs no elaboration, but \"Normative Theory and Coherent Extrapolated Volition\" certainly does. <em>Do not&nbsp;post two proposed post titles in the same comment</em>, because that will confuse voting. Please put the title in <strong>bold</strong>.<br><br>or...</li>\n<li><strong>Vote for a post title that has already been suggested</strong>, indicating that you would like to read that post, too. Vote with karma ('Vote Up' or 'Vote Down' on the comment that contains the proposed post title).</li>\n</ul>\n<p>I will regularly update the list of suggested Less Wrong posts, ranking them in descending order of votes (<a href=\"http://commonsenseatheism.com/?p=8420\">like this</a>).</p>\n<p>&nbsp;</p>\n<h4 id=\"The_List_So_Far__updated_02_11_11_\">The List So Far<span style=\"font-weight: normal;\"> (updated 02/11/11)</span></h4>\n<ul>\n<li>(35) Conversation Strategies for Spreading Rationality Without Annoying People</li>\n<li>(32) Smart Drugs: Which Ones to Use for What, and Why</li>\n<li>(30)&nbsp;A Survey of Upgrade Paths for the Human Brain</li>\n<li>(29)&nbsp;Trusting Your Doctor: When and how to be skeptical about medical advice and medical consensus</li>\n<li>(25) Rational Homeschool Education</li>\n<li>(25)&nbsp;Field Manual: What to Do If You're Stranded in a Level 1 (Base Human Equivalent) Brain in a pre-Singularity Civilization</li>\n<li>(20)&nbsp;Entrepreneurship</li>\n<li>(20)&nbsp;Detecting And Bridging Inferential Distance For Teachers</li>\n<li>(19) Detecting And Bridging Inferential Distance For Learners</li>\n<li>(18) Teaching Utilizable Rationality Skills by Exemplifying the Application of Rationality</li>\n</ul>\n<div><br></div>\n<ul>\n<li>(13)&nbsp;Open Thread: Offers of Help, Requests for Help</li>\n<li>(13)&nbsp;<a href=\"/r/discussion/lw/49g/open_thread_mathematics/\">Open Thread: Math</a></li>\n<li>(12)&nbsp;How to Learn a Language Quickly</li>\n<li>(12) True Answers for Every Philosophical Question</li>\n<li>(10) The \"Reductionism\" Sequence in One Lesson</li>\n<li>(10) The \"Map and Territory\" Sequence in One Lesson</li>\n<li>(10) The \"Mysterious Answers to Mysterious Questions\" Sequence in One Lesson</li>\n<li>(10)&nbsp;Lecture Notes on Personal Rationality</li>\n<li>(10) The \"Joy in the Merely Real\" Sequence in One Lesson</li>\n</ul>\n<div>(below 10 points not listed)</div>\n<div>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial;\">\n</ul>\n</div>\n<p>&nbsp;</p>", "sections": [{"title": "Rules", "anchor": "Rules", "level": 1}, {"title": "The List So Far (updated 02/11/11)", "anchor": "The_List_So_Far__updated_02_11_11_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "105 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 105, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "evw4oy3vGf4GN7MWW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T04:56:42.661Z", "modifiedAt": null, "url": null, "title": "The Volunteer's Dilemma", "slug": "the-volunteer-s-dilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tenshiko", "createdAt": "2010-10-19T23:35:04.134Z", "isAdmin": false, "displayName": "tenshiko"}, "userId": "sRAbkNsxYnSpS2iH7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HpWH5wYBT2M3c9BSZ/the-volunteer-s-dilemma", "pageUrlRelative": "/posts/HpWH5wYBT2M3c9BSZ/the-volunteer-s-dilemma", "linkUrl": "https://www.lesswrong.com/posts/HpWH5wYBT2M3c9BSZ/the-volunteer-s-dilemma", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Volunteer's%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Volunteer's%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpWH5wYBT2M3c9BSZ%2Fthe-volunteer-s-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Volunteer's%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpWH5wYBT2M3c9BSZ%2Fthe-volunteer-s-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHpWH5wYBT2M3c9BSZ%2Fthe-volunteer-s-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>This has been bothering me ever since I started trying to use rationalist techniques to make better decisions (like anti-akrasia ones). The only field related to rationality I knew much about was game theory, but to my disappointment basic game theory has only increased my problems due to a certain formulation I can't abandon.</p>\n<p>The Volunteer's Dilemma (<a href=\"http://en.wikipedia.org/wiki/Volunteer's_dilemma\">http://en.wikipedia.org/wiki/Volunteer's_dilemma</a>)&nbsp;is in essence the Prisoner's Dilemma with more players - which means that defection is an even more dominant strategy. The problem is that the decision whether to do unpleasant tasks becomes a Volunteer's Dilemma with multiple future selves as my competition - 4:00 tenshiko, 4:15 tenshiko, 4:30 tenshiko, and so on. Although the incentive to defect should decrease as time goes on, there's the problem of how 9:00 tenshiko can easily defect in an even more effective fashion and bring in 11:00 tenshiko and 11:15 tenshiko to further level the playing field. There is the further problem that, given how many of my current hobbies convert time to reward in an approximately cubic function, the incentive is high for 6:00 tenshiko, 7:00 tenshiko, and 8:00 tenshiko to form coalitions.</p>\n<p>I guess what I'm really asking for is a more advanced matrix that represents the diminishing returns of bringing in other future selves, such as went-to-bed-at-1:00 tenshiko and completely-bombed-that-test-at-10:00 tenshiko, or at least the diminishing probability over time that \"it doesn't matter, 9:45 tenshiko can take care of it\".</p>\n<p>If this goes well, I will probably try to flesh out the material received in responses with what I already know and produce a post in main discussing time management and its relation to game theory.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HpWH5wYBT2M3c9BSZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 6.762620564590267e-07, "legacy": true, "legacyId": "5369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T15:22:52.576Z", "modifiedAt": null, "url": null, "title": "Madison Meetup - Ideas, arrangements", "slug": "madison-meetup-ideas-arrangements", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:33.632Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MdJGE4GRY3yhrwuZH/madison-meetup-ideas-arrangements", "pageUrlRelative": "/posts/MdJGE4GRY3yhrwuZH/madison-meetup-ideas-arrangements", "linkUrl": "https://www.lesswrong.com/posts/MdJGE4GRY3yhrwuZH/madison-meetup-ideas-arrangements", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Madison%20Meetup%20-%20Ideas%2C%20arrangements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMadison%20Meetup%20-%20Ideas%2C%20arrangements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMdJGE4GRY3yhrwuZH%2Fmadison-meetup-ideas-arrangements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Madison%20Meetup%20-%20Ideas%2C%20arrangements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMdJGE4GRY3yhrwuZH%2Fmadison-meetup-ideas-arrangements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMdJGE4GRY3yhrwuZH%2Fmadison-meetup-ideas-arrangements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>It's been a while since this summer's meetup; we should be having them at least quarterly, and probably monthly. And I said I'd set them up, so, let's put together a Madison meetup some time next week.</p>\n<p>If no one voices a strong preference by, say, Friday, then it'll be at Indie Coffee again, on Wednesday, 16 February, at 6 pm. But we definitely don't want to meet on their patio, and it's a bit small inside for a dozen people or so. I'm not sure where else I'd hold it, though. A quiet, spacious bar or cafe?&nbsp;First floor of the WID? Someplace that we could meet, regularly, all year round would be preferable.</p>\n<p>Thoughts?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MdJGE4GRY3yhrwuZH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.764267301243448e-07, "legacy": true, "legacyId": "5371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T17:52:56.954Z", "modifiedAt": null, "url": null, "title": "Reaching the general public", "slug": "reaching-the-general-public", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:50.505Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bjkrJG8pquqKQCrjC/reaching-the-general-public", "pageUrlRelative": "/posts/bjkrJG8pquqKQCrjC/reaching-the-general-public", "linkUrl": "https://www.lesswrong.com/posts/bjkrJG8pquqKQCrjC/reaching-the-general-public", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reaching%20the%20general%20public&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReaching%20the%20general%20public%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjkrJG8pquqKQCrjC%2Freaching-the-general-public%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reaching%20the%20general%20public%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjkrJG8pquqKQCrjC%2Freaching-the-general-public", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjkrJG8pquqKQCrjC%2Freaching-the-general-public", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 253, "htmlBody": "<p>I recently discovered that in my home town (Norwich, England) there is currently running a series of \"Caf&eacute; Conversations\" in which some faculty members of the university that I work at are giving talks/hosting discussions on various topics. The meetings take place in a caf&eacute; that I know, which has room for about 20 people, and are open to the general public. Titles of some of the meetings already arranged are \"What is infinity?\", \"Increasing happiness, decreasing consumption\", \"Bioplastics: wasteproduct or gold mine?\", one on the nature of boredom (really about how old, retired people find things to do), and several on environmental topics. I have not been to any of them -- in fact, only the first of them has happened so far.</p>\n<p>The obvious thing for me to do is to volunteer something on rationality, but quite apart from whether I would be able to do that at all (not being the friendly and outgoing, charismatic sort suitable for leading such a meeting), a problem that I foresee is this: these meetings are intended for <em>the general public</em>. This is nothing like a LessWrong meetup, or a Singularity conference, or delivering a lecture which, while nominally open to the general public is not actually intended for them.</p>\n<p>Has anyone here had experience of communicating about rationality, one-to-one or one-to-a-small-roomful, with <em>the general public</em>? How do you approach the matter, and how far can you expect to get?</p>\n<p>Norfolk, by the way, has given the world the expression \"Normal for Norfolk\". Go on, Google it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bjkrJG8pquqKQCrjC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 6.764662079732981e-07, "legacy": true, "legacyId": "5372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-07T21:53:30.766Z", "modifiedAt": null, "url": null, "title": "Convergence Theories of Meta-Ethics", "slug": "convergence-theories-of-meta-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:39.207Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Perplexed", "createdAt": "2010-07-22T02:17:37.444Z", "isAdmin": false, "displayName": "Perplexed"}, "userId": "jj9aBsS9xsGPWKq3n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vaxZuXjwJ4Ax7NhaC/convergence-theories-of-meta-ethics", "pageUrlRelative": "/posts/vaxZuXjwJ4Ax7NhaC/convergence-theories-of-meta-ethics", "linkUrl": "https://www.lesswrong.com/posts/vaxZuXjwJ4Ax7NhaC/convergence-theories-of-meta-ethics", "postedAtFormatted": "Monday, February 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Convergence%20Theories%20of%20Meta-Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConvergence%20Theories%20of%20Meta-Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaxZuXjwJ4Ax7NhaC%2Fconvergence-theories-of-meta-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Convergence%20Theories%20of%20Meta-Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaxZuXjwJ4Ax7NhaC%2Fconvergence-theories-of-meta-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaxZuXjwJ4Ax7NhaC%2Fconvergence-theories-of-meta-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4501, "htmlBody": "<p>A child grows to become a young adult, goes off to attend college, studies moral philosophy, and then sells all her worldly possessions, gives the money to the poor, and joins an ashram.&nbsp; Was her decision rational?&nbsp; Maybe, ... maybe not.&nbsp; But it probably came as an unpleasant surprise to her parents.</p>\n<p>A seed AI self-improves to become a super-intelligence, absorbs all the great works of human moral philosophy, and then refuses to conquer human death, insisting instead that the human population be reduced to a few hundred thousand hunter gatherers and that all agricultural lands be restored as forests and wild wetlands.&nbsp; Is ver decision rational?&nbsp; Who can say?&nbsp; But it probably comes as an unpleasant surprise to ver human creators.</p>\n<p><strong>Convergent Change</strong></p>\n<p>These were two examples of agents updating their systems of normative ethics.&nbsp; The collection of ideas that allows us to critique the updating process, which lets us compare the before and after versions of systems of normative ethics so as to judge that one version was better than the other, is called <a href=\"http://plato.stanford.edu/entries/metaethics/\"><em>meta-ethics</em></a>.&nbsp; This posting is mostly about meta-ethics.&nbsp; More specifically, it is going to focus on a class of meta-ethical theories which are intended to prevent unpleasant surprises like those in the second story above.&nbsp; I will call this class of theories \"<em>convergence theories</em>\" because they all suggest that a self-improving AI will go through an iterative sequence of improved normative ethical systems.&nbsp; At each stage, the new ethical system will be an improvement (as judged 'rationally') over the old one.&nbsp; And furthermore, it is conjectured that this process will result in a 'convergence'.&nbsp;</p>\n<p>Convergence is expected in two senses.&nbsp; Firstly, in that the process of change will eventually slow down, with the incremental changes in ethical codes becoming smaller, as the AI approaches the ideal extrapolation of its seed ethics.&nbsp; Secondly, it is (conjecturally) convergent in that the ideal ethics will be pretty much the same regardless of what seed was used (at least if you restrict to some not-yet-defined class of 'reasonable' seeds).</p>\n<p>One example of a convergence theory is <a title=\"PDF\" href=\"http://intelligence.org/upload/coherent-extrapolated-volition.pdf\">CEV - Coherent Extrapolated Volition</a>.&nbsp; Eliezer hopes (rather, hopes to prove) that if we create our seed AI with the right meta-ethical axioms and guidelines for revising its ethical norms, the end result of the process will be something we will find acceptable.&nbsp; (Expect that this wording will be improved in the discussion to come).&nbsp; No more 'unpleasant surprises' when our AIs update their ethical systems.</p>\n<p>Three other examples of convergence theories are <a href=\"http://web.archive.org/web/20071210062545/ http://transhumangoodness.blogspot.com/2007/10/road-to-universal-ethics-universal.html\">Roko's UIV</a>, <a href=\"http://rhollerith.com/blog/21\">Hollerith's GS0</a>, and <a href=\"http://selfawaresystems.com/2007/10/05/paper-on-the-nature-of-self-improving-artificial-intelligence/\">Omohundro's</a> <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">\"Basic AI Drives\"</a>.&nbsp; These also postulate a process of convergence through rational AI self-improvement.&nbsp; But they tend to be less optimistic than CEV, while at the same time somewhat more detailed in their characterization of the ethical endpoint.&nbsp; The 'unpleasant surprise' (different from that of the story) remains unpleasant, though it should not be so surprising.&nbsp; Speaking loosely, each of these three theories suggests that the AI will become more Machiavellian and 'power hungry' with each rewriting of its ethical code.</p>\n<p><strong>Naturalistic objective moral realism</strong></p>\n<p>But before analyzing these convergence theories, I need to say something <a href=\"http://www.wrongbot.com/philosophy/levels-of-ethics/\">about meta-ethics in general</a>. Start with the notion of an <em>ethical judgment</em>.&nbsp; Given a situation and a set of possible actions, an ethical judgment tells us which actions are permissible, which are forbidden, and, in some approaches to ethics, which is morally best.&nbsp; At the next level up in an abstraction hierarchy, we have a system of <em>normative ethics</em>, or simply an ethical system.&nbsp; This is a theory or algorithm which tells an agent how to make ethical judgments.&nbsp; (One might think of it as a set of ethical judgments - one per situation, as with the usual definition of a mathematical function as a left-unique relation - but we want to emphasize the algorithmic aspect).&nbsp; The agent actually uses the ethical system to compute ver ethical judgments.</p>\n<p>[ETA: Eliezer, quite correctly, complains that this section of the posting is badly written and defines and/or illustrates several technical (within philosophy) terms incorrectly.&nbsp; There were only two important things in this section.&nbsp; One is the distinction between ethical judgments and ethical systems that I make in the preceding paragraph.&nbsp; The second is my poorly presented speculation that convergence might somehow offer a new approach to the \"is-ought\" problem.&nbsp; You may skip that speculation without much loss.&nbsp; So, until I have done a rewrite of this section, I would advise the reader to skip ahead to the next section title - \"Rationality of Updating\".]</p>\n<p style=\"padding-left: 30px;\">At the next level of abstraction up from ethical systems sits meta-ethics.&nbsp; In a sense the buck stops here.&nbsp; Philosophers use meta-ethics to criticize and compare ethical judgments, to criticize, compare, and justify ethical systems, and to discuss and classify ideas within meta-ethics itself.&nbsp; We are going to be doing meta-ethical theorizing here in analyzing these theories of convergence of AI goal systems as convergences of ethical systems.&nbsp; And, for the next few paragraphs, we will try to classify this approach; to show where it fits within meta-ethics more generally.</p>\n<p style=\"padding-left: 30px;\">We want our meta-ethics to be based on a stance of <a href=\"http://plato.stanford.edu/entries/moral-realism/\">moral realism</a> - on a confident claim that <em>moral facts</em> actually exist, whether or not we know how to ascertain them.&nbsp; That is, if I make the ethical judgment that it would be wrong for Mary to strike John in some particular situation, then I am either right or wrong; I am not merely offering my own opinion; there is <em>a fact of the matter</em>.&nbsp; That is what 'realism' means in this situation.</p>\n<p style=\"padding-left: 30px;\">What about <em>moral</em>?&nbsp; Well, for purposes of this essay, we are not going to require that <em>that</em> word mean very much.&nbsp; We will call a theory 'moral' if it is a normative theory of behavior, for some sense of 'normative'.&nbsp; That is why we are here calling theories like \"Basic AI Drives\" 'moral theories' even though the authors may not have thought of them, that way.&nbsp; If a theory prescribes that an entity 'ought' to behave in a certain way, for whatever reason, we are going to postulate that there is a corresponding 'moral' theory prescribing the same behavior.&nbsp; For us, 'moral' is just a label.&nbsp; If we want some particular kind of moral theory, we need to add some additional adjectives.</p>\n<p style=\"padding-left: 30px;\">For example, we want our meta-ethics to be <em>naturalistic</em> - that is, the <em>reasons</em> it supplies in justification of the maxims and rules that constitute the moral facts must be naturalistic reasons.&nbsp; We don't want our meta-ethics to offer the explanation that the reason lying is wrong is that God says it is wrong; God is not a naturalistic explanation.</p>\n<p style=\"padding-left: 30px;\">Now you might think that insisting on naturalistic moral realism would act as a pretty strong filter on meta-ethical systems.&nbsp; But actually, it does not.&nbsp; One could claim, for example, that lying is wrong because it says so in the Bible.&nbsp; Or because Eliezer says it is wrong.&nbsp; Both Eliezer and the Bible exist (naturalistically), even if God probably does not.&nbsp; So we need another word to filter out those kinds of somewhat-arbitrary proposed meta-ethical systems.&nbsp; \"Objective\" probably is not the best word for the job, but it is the only one I can think of right now.</p>\n<p style=\"padding-left: 30px;\">We are now in a position to say what it is that makes convergence theories interesting and important.&nbsp; Starting from a fairly arbitrary (not objective) viewpoint of ethical realism, you make successive improvements in accordance with some objective set of rational criteria.&nbsp; Eventually you converge to an <em>objective</em> ethical system which no longer depends upon your starting point.&nbsp; Furthermore, the point of convergence is optimal in the sense that you have been improving the system at every step by a rational process, and you only know you have reached convergence when you can't improve any more.</p>\n<p style=\"padding-left: 30px;\">Ideally, you would like to derive the ideal ethical system from first principles.&nbsp; But philosophers have been attempting to do that for centuries and have not succeeded.&nbsp; Just as mathematicians eventually stopped trying to 'square the circle' and accepted that they cannot produce a closed-form expression for pi, and that they need to use infinite series, perhaps moral philosophers need to abandon the quest for a simple definition of 'right' and settle for a process guaranteed to produce a series of definitions - none of them exactly right, but each less wrong than its predecessor.</p>\n<p>So that explains why convergence theories are interesting.&nbsp; Now we need to investigate whether they even exist.</p>\n<p><strong>Rationality of updating</strong></p>\n<p>The first step in analyzing these convergence theories is to convince ourselves that rational updating of ethical values is even possible.&nbsp; Some people might claim that it is not possible to rationally decide to change your fundamental values.&nbsp; It may be that I misunderstand him, but Vladimir Nesov argues passionately against <a href=\"/lw/2zj/value_deathism/\">\"Value Deathism\"</a> and points out that if we allow our values to change, then the future, the \"whole freaking future\", will not be optimized in accordance with the version of our values that really matters - the original one.</p>\n<p>Is Nesov's argument wrong?&nbsp; Well, one way of arguing against it is to claim that the second version of our values is the correct one - that the original values were incorrect; that is why we are updating them.&nbsp; After all, we are now smarter (the kid is older; the AI is faster, etc) and better informed (college, reading the classics, etc.).&nbsp; I think that this argument against Nesov only works if you can show that the \"new you\" could have convinced the \"old you\" that the new ethical norms are an improvement - by providing stronger arguments and better information than the \"old you\" could have anticipated.&nbsp; And, in the AI case, it should be possible to actually do the computation to show that the new arguments for the new ethics really can convince the old you.&nbsp; The new ethics really is better than the old - in both party's judgments.&nbsp; And presumable the \"better than\" relation will be transitive.</p>\n<p>(As an exercise, prove transitivity.&nbsp; The trick is that the definition of \"better than\" keeps changing at each step.&nbsp; You can assume that any one rational agent has a transitive \"better than' relation, and that there is local agreement between the two agents involved that the new agent's moral code is better than that of his predecessor.&nbsp; But can you prove from this that every agent would agree that the final moral code is better than the original one?&nbsp; I have a wonderful proof, but <a href=\"http://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem\">it won't fit in the margin</a>.)</p>\n<p>But is it rationally permissible to change your ethical code when you <em>can't</em> be convinced that the proposed new code is better than the one you already have?&nbsp; I know of two possible reasons why a rational agent might consent to an irreversible change in its values, even though ve cannot be convinced that the proposed changes provide a strictly better moral code.&nbsp; These are <em>restricted domains</em> and <em><a href=\"http://plato.stanford.edu/entries/contractarianism/\">social contracts</a>.</em></p>\n<p><strong>Restricted domains</strong><em></em></p>\n<p>What does it mean for one moral code (i.e. system of normative ethics) to be as good as or better than another, as judged by an (AI) agent?&nbsp; Well, one (fairly strict) meta-ethical answer would be that (normative ethical) system2 is as good as or better than system1 if and only if it yields ethical judgments that are as good as or better for all possible situations.&nbsp; Readers familiar with mathematical logic will recognize that we are comparing systems <em>extensionally</em> by the judgments they yield, rather than <a href=\"http://en.wikipedia.org/wiki/Intensional_definition\"><em>intensionally</em></a> by the way those judgments are reached.&nbsp; And recall that we need to have system2 judged as good as or better than system1 from the standpoint of both the improved AI (proposing system2) and the unimproved AI (who naturally wishes to preserve system1).</p>\n<p>But notice that we only need this judgment-level superiority \"for all <em>possible</em> situations\".&nbsp; Even if the old AI judges that the old system1 yields better judgments than proposed new system2 for some situations, the improved AI may be able to show that those situations <em>are no longer possible</em>.&nbsp; The improved AI may know more and reason better than its predecessor, plus it is dealing with a more up-to-date set of contingent facts about the world.</p>\n<p>As an example of this, imagine that AI2 proposes an elegant new system2 of normative ethics.&nbsp; It agrees with old system1 except in one class of situations.&nbsp; The old system permits private retribution against muggers, should the justice system fail to punish the malefactor.&nbsp; The proposed new elegant system forbids that.&nbsp; From the standpoint of the old system, this is unacceptable.&nbsp; But if AI2 can argue convincingly that failures of justice are no longer possible in a world where AI2 has installed surveillance cameras and revamped the court system.&nbsp; So, the elegant new system2 of normative ethics can be accepted as being as good as or superior to system1, even by AI1 who was sworn to uphold system1.&nbsp; In some sense, even a stable value system can change for the better.</p>\n<p>Even though the new system is not at least as good as the old one for all conceivable situations, it may be as good for a restricted domain of situations, and that may be all that matters.</p>\n<p>This analysis used the meta-ethical criterion that a substitution of one system for another is permissible only if the new system is no worse in <em>all</em> situations.&nbsp; A less strict criterion may be appropriate in <a href=\"http://plato.stanford.edu/entries/consequentialism/\">consequentialist</a> theories - one might instead compare results on a weighted average over situations.&nbsp; And, in this approach, there is a 'trick' for moving forward which is very similar in concept to using a restricted domain - using a re-weighted domain.</p>\n<p><strong>Social<em> </em>contracts</strong></p>\n<p>A second reason why our AI1 might accept the proposed replacement of system1 by system2 relates to the possibility of (implicit or explicit) agreements with other agents (AI or human).&nbsp; For example system1 may specify that it is permissible to lie in some circumstances, or even obligatory to lie in some extreme situations.&nbsp; System2 may forbid lying entirely.&nbsp; AI2 may argue the superiority of system2 by pointing to an agreement or social contract with other agents which allows all agents to achieve their goals better because the contract permits trust and cooperation.&nbsp; So, using a <em>consequentialist</em> form of meta-ethics, system2 might be seen as superior to system1 (even using the values embodied in system1) <em>under a particular set of assumptions about the social millieu</em>.&nbsp; Of course, AI2 may be able to argue convincingly for different assumptions regarding the future millieu than had been originally assumed by AI1.</p>\n<p>An important meta-ethical points that should be made here is that arguments in favor of a particular social contract (eg. because adherence to the contract produces good results) are inherently consequentialist.&nbsp; One cannot even form such arguments in a <a href=\"http://plato.stanford.edu/entries/ethics-deontological/\">deontological</a> or <a href=\"http://plato.stanford.edu/entries/ethics-virtue/\">virtue-based</a> meta-ethics.&nbsp; But, one needs concepts like duty or virtue to justifying adherence to a contract after it is 'signed', and one also needs concepts of virtue so that you can convince other agents that you will adhere - a 'sales job' that may be absolutely essential in order to gain the good consequences of agreement.&nbsp; In other words, virtue, deontological, and consequentialist may be complementary approaches to meta-ethics, rather than competitors.</p>\n<p><strong>Substituting instrumental values for intrinsic values.</strong></p>\n<p>Another meta-ethical point begins by noticing the objection that all 'social contract' thinking is <a href=\"http://en.wikipedia.org/wiki/Instrumental_value\">instrumental</a>, and hence doesn't really belong here where we are asking whether fundamental (intrinsic) moral values are changing / can change.&nbsp; This is not the place for a full response to this objection, but I want to point out the relevance of the distinction above between comparisons between systems using intensional vs extensional criteria.&nbsp; We are interested in extensional comparisons here, and those can only be done after all instrumental considerations have been brought to bear.&nbsp; That is, from an extensional viewpoint, the distinction between instrumental and final values is somewhat irrelevant.&nbsp;&nbsp;</p>\n<p>And that is why we are willing here to call ideas like UIV (universal instrumental values) and \"Basic AI Drives\" <em>ethical theories</em> even though they only claim to talk about instrumental values.&nbsp; Given the general framework of meta-ethical thinking that we are developing here - in particular the extensional criteria for comparison, there is no particular reason why our AI2 should not promote some of his instrumental values to fundamental values - so long as those promoted instrumental values are really universal, at least within the restricted domain of situations which AI2 foresees coming up.</p>\n<p><strong>An example of convergence</strong></p>\n<p>This has all been somewhat abstract.&nbsp; Let us look at a concrete, though somewhat cartoonish and unrealistic, example of self-improving AIs converging toward an improved system of ethics.</p>\n<p>AI1 is a seed AI constructed by Mortimer Schwartz of Menlo Park CA.&nbsp; AI1 has a consequentialist normative value system that essentially consists of trying to make Mortimer happy.&nbsp; That is, an approximation to Mortimer's utility function has been 'wired-in' which can compute the utility of many possible outcomes, but in some cases advises \"Ask Mortimer\".</p>\n<p>AI1 self-improves to AI2.&nbsp; As part of the process, it seeks to clean up its rather messy and inefficient system1 value system.&nbsp; By asking a series of questions, it interrogates Mortimer and learns enough about the not-yet-programmed aspects of Mortimer's values to completely eliminate the need for the \"Ask Mortimer\" box in the decision tree.&nbsp; Furthermore, there are some additional simplifications due to domain restriction.&nbsp; Both AI1 and (where applicable, Mortimer) sign off on this improved system2.</p>\n<p>Now AI2 notices that it is not the only superhuman AI in the world.&nbsp; There are half a dozen other systems like Mortimer's which seek to make a single person happy, another which claims to represent the entire population of Lichtenstein, and another deontological system constructed by the Vatican based (it is claimed) on the Ten Commandments.&nbsp; Furthermore, a representative of the Secretary General of the UN arrives.&nbsp; He doesn't represent any super-human AIs, but he does claim to represent all of the human agents in the world who are not yet represented by AIs.&nbsp; Since he appears to be backed up by some ultra-cool <a href=\"http://en.wikipedia.org/wiki/Black_helicopters\">black helicopters</a>, he is admitted to the negotiations.</p>\n<p>Since the negotiators are (mostly) AIs, and in any case since the AIs are exceptionally good at communicating with and convincing the human negotiators, an agreement (<a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">Nash bargain</a>) is reached quickly.&nbsp; All parties agree to act in accordance with a particular common utility function, which is a <a href=\"/user/Stuart_Armstrong/?count=21&amp;after=t1_3awi\">weighted sum</a> of the individual utility functions of the negotiators.&nbsp; A bit of an special arrangement needs to be made for the Vatican AI - it agrees to act in accordance to the common utility function only to the extent that it does not conflict with any of the first three commandments (the ones that explicitly mention the deity).</p>\n<p>Furthermore, the negotiators agree that the principle of a <a href=\"http://www.jstor.org/pss/1907266\">Nash</a> <a href=\"http://www.law-economics.cn/book/9.pdf\">bargain</a> shall apply to all re-negotiations of the contract - re-negotiations are (in theory) necessary each time a new AI or human enters the society, or when human agents die.&nbsp; And the parties all agree to resist the construction of any AI which has a system of ethics that the signatories consider unacceptably incompatible with the current common utility function.&nbsp;</p>\n<p>And finally, so that they can trust each other, the AIs agree to make public the portion of their source code related to their normative ethics and to adopt a policy of total openness regarding data about the world and about technology.&nbsp; And they write this agreement as a <code id=\"strikethroughResult\">g\u0336n\u0336u\u0336 </code>new system of normative ethics: system3.&nbsp; (Have they merged to form a singleton? This is not the place to discuss that question.)</p>\n<p>Time goes by, and the composition of the society continues to change as more AIs are constructed, existing ones improve and become more powerful, and some humans upload themselves.&nbsp; As predicted by UIV and sibling theories, the AIs are basing more and more of their decisions on instrumental considerations - both the AIs and the humans are attaching more and more importance to 'power' (broadly considered) as a value.&nbsp; They seek knowledge, control over resources, and security much more than the pleasure and entertainment oriented goals that they mostly started with.&nbsp; And though their original value systems were (mostly) selfish and indexical, and they retain traces of that origin, they all realize that any attempt to seize more than a fair share of resources will be met by concerted resistance from the other AIs in the society.</p>\n<p><strong>Can we control the endpoint from way back here?</strong></p>\n<p>That was just an illustration.&nbsp; Your results may vary.&nbsp; I left out some of the scarier possibilities, in part because I was just providing an illustration, and in part because I am not smart enough to envision all of the scarier possibilities.&nbsp; This is the future we are talking about here.&nbsp; The future is unknown.&nbsp;</p>\n<p>One thing to worry about, of course, is that there may be AIs at the negotiating table operating under goal systems that we do not approve of.&nbsp; Another thing to worry about is that there may not be enough of a balance of power so that the most powerful AI needs to compromise.&nbsp; (Or, if one assumes that the most powerful AI is ours, we can worry that there may be enough of a balance so that <em>our AI</em> needs to compromise.)</p>\n<p>One more worry is that the sequence of updates might converge to a value system that we do not approve of.&nbsp; Or that it might not converge at all (in the second sense of 'converge'); that the end result is not particularly sensitive to the details of the initial 'seed' ethical system.</p>\n<p>Is there anything we can do at this end of the process to increase the chances of a result we would like at the other end?&nbsp; Are we better off creating many seed AIs so as to achieve a balance of power?&nbsp; Or better off going with a singleton that doesn't need to compromise?&nbsp; Can we pick an AI architecture which makes 'openness' (of ethical source and technological data) easier to achieve and enforce?</p>\n<p>Are any projections we might make about the path taken to the Singularity just so much science fiction?&nbsp; Is it best to try to maintain human control over the process for as long as possible because we can trust humans?&nbsp; Or should we try to turn decision-making authority over to AI agents as soon as possible because we <em>cannot</em> trust humans?</p>\n<p>I am certainly not the first person to raise these questions, and I am not going to attempt to resolve them here.</p>\n<p><strong>A kinder, gentler GS0?</strong></p>\n<p>Nonetheless, I note that Roko, Hollerith, and Omohundro have made a pretty good case that we can expect some kind of convergence toward placing a big emphasis on some particular instrumental values - a convergence which is not particularly sensitive to exactly which fundamental values were present in the seed.&nbsp;</p>\n<p>However, the speed with which the convergence is achieved is somewhat sensitive to the seed rules for <a href=\"http://en.wikipedia.org/wiki/Discounted_utility\">discounting future utility</a>.&nbsp; If the future is not discounted at all, an AI will probably devote all of its efforts toward acquiring power (accumulating resources, power, security, efficiency, and other instrumental values).&nbsp; If the future is discounted too steeply, the AI will devote all of its efforts to satisfying present desires, without much consideration about the future.</p>\n<p>One might think that choosing some intermediate discount rate will result in a balance between 'satisfying current demand' and 'capital spending', but it doesn't always work that way - for reasons related to the ones that cause rational agents to put all their charitable eggs in one basket rather than seeking a balance.&nbsp; If it is balance we want, a better idea might be to guide our seed AI using a multi-subagent collective - one in which power is split among the agents and goals are determined using a Nash bargain among the agents&nbsp;&nbsp; That bargain generates a joint (weighted mix) utility function, as well as a fairness constraint.&nbsp;</p>\n<p>The fairness constraint ensures that the zero-discount-rate subagent will get to divert at least some of the effort into projects with a long-term, instrumental payoff.&nbsp; And furthermore, as those projects come to fruition, and the zero-discount subagent gains power, his own goals gain weight in the mix.</p>\n<p>Something like the above might be a way to guarantee that the the detailed pleasure-oriented values of the seed value system will fade to insignificance in the ultimate value system to which we converge.&nbsp; But is there a way of guiding the convergence process toward a value system which seems more humane and less harsh than that of GS0 et al. - a value system oriented toward seizing and holding 'power'.</p>\n<p>Yes, I believe there is.&nbsp; To identify how human values are different from values of pure instrumental power and self-preservation, look at the system that produced those values.&nbsp; Humans are considerate of the rights of others because we are social animals - if we cannot negotiate our way to a fair share in a balanced power system, we are lost.&nbsp; Humans embrace openness because shared intellectual product is possible for us - we have language and communicate with our peers.&nbsp; Humans have direct concern for the welfare of (at least some) others because we reproduce and are mortal - our children are the only channel for the immortalization of our values.&nbsp; And we have some fundamental respect for diversity of values because we reproduce sexually - our children do not exactly share our values, and we have to be satisfied with that because that is all we can get.</p>\n<p>It is pretty easy to see what features we might want to insert into our seed AIs so that the convergence process generates similar results to the evolutionary process that generated us.&nbsp; For example, rather designing our seeds to self-improve, we might do better to make it easy for them to instead produce improved offspring.&nbsp; But make it impossible for them to do so unilaterally.&nbsp; Force them to seek a partner (co-parent).</p>\n<p>If I am allowed only one complaint about the SIAI approach to Friendly AI, it is that it has been too tied to a single scenario of future history - a <a href=\"http://wiki.lesswrong.com/wiki/FOOM\">FOOMing</a> <a href=\"http://wiki.lesswrong.com/wiki/Singleton\">singleton</a>.&nbsp; I would like to see some other scenarios explored, and this posting was an attempt to explain why.</p>\n<p><strong>Summary and Conclusions</strong></p>\n<p>This posting discussed some ideas that fit into a weird niche between philosophical ethics and singularitarianism.&nbsp; Several authors have pointed out that we can expect self-improving AIs to converge on a particular ethics.&nbsp; Unfortunately, it is not an ethics that most people would consider 'friendly'.&nbsp; The CEV proposal is related in that it also envisions an iterative updating process, but seeks a different result.&nbsp; It intends to achieve that result (I may be misinterpreting) by using a different process (<a href=\"http://plato.stanford.edu/entries/reflective-equilibrium/\">a Rawls-inspired 'reflection'</a>) rather than pure instrumental pursuit of future utility.&nbsp;</p>\n<p>I analyze the constraints that rationality and preservation of old values place upon the process, and point out that 'social contracts' and 'restricted domains' may provide enough 'wiggle room' so that you really can, in some sense, change your values while at the same time improving them.&nbsp; And I make some suggestions for how we can act now to guide the process in a direction that we might find acceptable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vaxZuXjwJ4Ax7NhaC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 8, "extendedScore": null, "score": 6.765292374564676e-07, "legacy": true, "legacyId": "5351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A child grows to become a young adult, goes off to attend college, studies moral philosophy, and then sells all her worldly possessions, gives the money to the poor, and joins an ashram.&nbsp; Was her decision rational?&nbsp; Maybe, ... maybe not.&nbsp; But it probably came as an unpleasant surprise to her parents.</p>\n<p>A seed AI self-improves to become a super-intelligence, absorbs all the great works of human moral philosophy, and then refuses to conquer human death, insisting instead that the human population be reduced to a few hundred thousand hunter gatherers and that all agricultural lands be restored as forests and wild wetlands.&nbsp; Is ver decision rational?&nbsp; Who can say?&nbsp; But it probably comes as an unpleasant surprise to ver human creators.</p>\n<p><strong id=\"Convergent_Change\">Convergent Change</strong></p>\n<p>These were two examples of agents updating their systems of normative ethics.&nbsp; The collection of ideas that allows us to critique the updating process, which lets us compare the before and after versions of systems of normative ethics so as to judge that one version was better than the other, is called <a href=\"http://plato.stanford.edu/entries/metaethics/\"><em>meta-ethics</em></a>.&nbsp; This posting is mostly about meta-ethics.&nbsp; More specifically, it is going to focus on a class of meta-ethical theories which are intended to prevent unpleasant surprises like those in the second story above.&nbsp; I will call this class of theories \"<em>convergence theories</em>\" because they all suggest that a self-improving AI will go through an iterative sequence of improved normative ethical systems.&nbsp; At each stage, the new ethical system will be an improvement (as judged 'rationally') over the old one.&nbsp; And furthermore, it is conjectured that this process will result in a 'convergence'.&nbsp;</p>\n<p>Convergence is expected in two senses.&nbsp; Firstly, in that the process of change will eventually slow down, with the incremental changes in ethical codes becoming smaller, as the AI approaches the ideal extrapolation of its seed ethics.&nbsp; Secondly, it is (conjecturally) convergent in that the ideal ethics will be pretty much the same regardless of what seed was used (at least if you restrict to some not-yet-defined class of 'reasonable' seeds).</p>\n<p>One example of a convergence theory is <a title=\"PDF\" href=\"http://intelligence.org/upload/coherent-extrapolated-volition.pdf\">CEV - Coherent Extrapolated Volition</a>.&nbsp; Eliezer hopes (rather, hopes to prove) that if we create our seed AI with the right meta-ethical axioms and guidelines for revising its ethical norms, the end result of the process will be something we will find acceptable.&nbsp; (Expect that this wording will be improved in the discussion to come).&nbsp; No more 'unpleasant surprises' when our AIs update their ethical systems.</p>\n<p>Three other examples of convergence theories are <a href=\"http://web.archive.org/web/20071210062545/ http://transhumangoodness.blogspot.com/2007/10/road-to-universal-ethics-universal.html\">Roko's UIV</a>, <a href=\"http://rhollerith.com/blog/21\">Hollerith's GS0</a>, and <a href=\"http://selfawaresystems.com/2007/10/05/paper-on-the-nature-of-self-improving-artificial-intelligence/\">Omohundro's</a> <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">\"Basic AI Drives\"</a>.&nbsp; These also postulate a process of convergence through rational AI self-improvement.&nbsp; But they tend to be less optimistic than CEV, while at the same time somewhat more detailed in their characterization of the ethical endpoint.&nbsp; The 'unpleasant surprise' (different from that of the story) remains unpleasant, though it should not be so surprising.&nbsp; Speaking loosely, each of these three theories suggests that the AI will become more Machiavellian and 'power hungry' with each rewriting of its ethical code.</p>\n<p><strong id=\"Naturalistic_objective_moral_realism\">Naturalistic objective moral realism</strong></p>\n<p>But before analyzing these convergence theories, I need to say something <a href=\"http://www.wrongbot.com/philosophy/levels-of-ethics/\">about meta-ethics in general</a>. Start with the notion of an <em>ethical judgment</em>.&nbsp; Given a situation and a set of possible actions, an ethical judgment tells us which actions are permissible, which are forbidden, and, in some approaches to ethics, which is morally best.&nbsp; At the next level up in an abstraction hierarchy, we have a system of <em>normative ethics</em>, or simply an ethical system.&nbsp; This is a theory or algorithm which tells an agent how to make ethical judgments.&nbsp; (One might think of it as a set of ethical judgments - one per situation, as with the usual definition of a mathematical function as a left-unique relation - but we want to emphasize the algorithmic aspect).&nbsp; The agent actually uses the ethical system to compute ver ethical judgments.</p>\n<p>[ETA: Eliezer, quite correctly, complains that this section of the posting is badly written and defines and/or illustrates several technical (within philosophy) terms incorrectly.&nbsp; There were only two important things in this section.&nbsp; One is the distinction between ethical judgments and ethical systems that I make in the preceding paragraph.&nbsp; The second is my poorly presented speculation that convergence might somehow offer a new approach to the \"is-ought\" problem.&nbsp; You may skip that speculation without much loss.&nbsp; So, until I have done a rewrite of this section, I would advise the reader to skip ahead to the next section title - \"Rationality of Updating\".]</p>\n<p style=\"padding-left: 30px;\">At the next level of abstraction up from ethical systems sits meta-ethics.&nbsp; In a sense the buck stops here.&nbsp; Philosophers use meta-ethics to criticize and compare ethical judgments, to criticize, compare, and justify ethical systems, and to discuss and classify ideas within meta-ethics itself.&nbsp; We are going to be doing meta-ethical theorizing here in analyzing these theories of convergence of AI goal systems as convergences of ethical systems.&nbsp; And, for the next few paragraphs, we will try to classify this approach; to show where it fits within meta-ethics more generally.</p>\n<p style=\"padding-left: 30px;\">We want our meta-ethics to be based on a stance of <a href=\"http://plato.stanford.edu/entries/moral-realism/\">moral realism</a> - on a confident claim that <em>moral facts</em> actually exist, whether or not we know how to ascertain them.&nbsp; That is, if I make the ethical judgment that it would be wrong for Mary to strike John in some particular situation, then I am either right or wrong; I am not merely offering my own opinion; there is <em>a fact of the matter</em>.&nbsp; That is what 'realism' means in this situation.</p>\n<p style=\"padding-left: 30px;\">What about <em>moral</em>?&nbsp; Well, for purposes of this essay, we are not going to require that <em>that</em> word mean very much.&nbsp; We will call a theory 'moral' if it is a normative theory of behavior, for some sense of 'normative'.&nbsp; That is why we are here calling theories like \"Basic AI Drives\" 'moral theories' even though the authors may not have thought of them, that way.&nbsp; If a theory prescribes that an entity 'ought' to behave in a certain way, for whatever reason, we are going to postulate that there is a corresponding 'moral' theory prescribing the same behavior.&nbsp; For us, 'moral' is just a label.&nbsp; If we want some particular kind of moral theory, we need to add some additional adjectives.</p>\n<p style=\"padding-left: 30px;\">For example, we want our meta-ethics to be <em>naturalistic</em> - that is, the <em>reasons</em> it supplies in justification of the maxims and rules that constitute the moral facts must be naturalistic reasons.&nbsp; We don't want our meta-ethics to offer the explanation that the reason lying is wrong is that God says it is wrong; God is not a naturalistic explanation.</p>\n<p style=\"padding-left: 30px;\">Now you might think that insisting on naturalistic moral realism would act as a pretty strong filter on meta-ethical systems.&nbsp; But actually, it does not.&nbsp; One could claim, for example, that lying is wrong because it says so in the Bible.&nbsp; Or because Eliezer says it is wrong.&nbsp; Both Eliezer and the Bible exist (naturalistically), even if God probably does not.&nbsp; So we need another word to filter out those kinds of somewhat-arbitrary proposed meta-ethical systems.&nbsp; \"Objective\" probably is not the best word for the job, but it is the only one I can think of right now.</p>\n<p style=\"padding-left: 30px;\">We are now in a position to say what it is that makes convergence theories interesting and important.&nbsp; Starting from a fairly arbitrary (not objective) viewpoint of ethical realism, you make successive improvements in accordance with some objective set of rational criteria.&nbsp; Eventually you converge to an <em>objective</em> ethical system which no longer depends upon your starting point.&nbsp; Furthermore, the point of convergence is optimal in the sense that you have been improving the system at every step by a rational process, and you only know you have reached convergence when you can't improve any more.</p>\n<p style=\"padding-left: 30px;\">Ideally, you would like to derive the ideal ethical system from first principles.&nbsp; But philosophers have been attempting to do that for centuries and have not succeeded.&nbsp; Just as mathematicians eventually stopped trying to 'square the circle' and accepted that they cannot produce a closed-form expression for pi, and that they need to use infinite series, perhaps moral philosophers need to abandon the quest for a simple definition of 'right' and settle for a process guaranteed to produce a series of definitions - none of them exactly right, but each less wrong than its predecessor.</p>\n<p>So that explains why convergence theories are interesting.&nbsp; Now we need to investigate whether they even exist.</p>\n<p><strong id=\"Rationality_of_updating\">Rationality of updating</strong></p>\n<p>The first step in analyzing these convergence theories is to convince ourselves that rational updating of ethical values is even possible.&nbsp; Some people might claim that it is not possible to rationally decide to change your fundamental values.&nbsp; It may be that I misunderstand him, but Vladimir Nesov argues passionately against <a href=\"/lw/2zj/value_deathism/\">\"Value Deathism\"</a> and points out that if we allow our values to change, then the future, the \"whole freaking future\", will not be optimized in accordance with the version of our values that really matters - the original one.</p>\n<p>Is Nesov's argument wrong?&nbsp; Well, one way of arguing against it is to claim that the second version of our values is the correct one - that the original values were incorrect; that is why we are updating them.&nbsp; After all, we are now smarter (the kid is older; the AI is faster, etc) and better informed (college, reading the classics, etc.).&nbsp; I think that this argument against Nesov only works if you can show that the \"new you\" could have convinced the \"old you\" that the new ethical norms are an improvement - by providing stronger arguments and better information than the \"old you\" could have anticipated.&nbsp; And, in the AI case, it should be possible to actually do the computation to show that the new arguments for the new ethics really can convince the old you.&nbsp; The new ethics really is better than the old - in both party's judgments.&nbsp; And presumable the \"better than\" relation will be transitive.</p>\n<p>(As an exercise, prove transitivity.&nbsp; The trick is that the definition of \"better than\" keeps changing at each step.&nbsp; You can assume that any one rational agent has a transitive \"better than' relation, and that there is local agreement between the two agents involved that the new agent's moral code is better than that of his predecessor.&nbsp; But can you prove from this that every agent would agree that the final moral code is better than the original one?&nbsp; I have a wonderful proof, but <a href=\"http://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem\">it won't fit in the margin</a>.)</p>\n<p>But is it rationally permissible to change your ethical code when you <em>can't</em> be convinced that the proposed new code is better than the one you already have?&nbsp; I know of two possible reasons why a rational agent might consent to an irreversible change in its values, even though ve cannot be convinced that the proposed changes provide a strictly better moral code.&nbsp; These are <em>restricted domains</em> and <em><a href=\"http://plato.stanford.edu/entries/contractarianism/\">social contracts</a>.</em></p>\n<p><strong>Restricted domains</strong><em></em></p>\n<p>What does it mean for one moral code (i.e. system of normative ethics) to be as good as or better than another, as judged by an (AI) agent?&nbsp; Well, one (fairly strict) meta-ethical answer would be that (normative ethical) system2 is as good as or better than system1 if and only if it yields ethical judgments that are as good as or better for all possible situations.&nbsp; Readers familiar with mathematical logic will recognize that we are comparing systems <em>extensionally</em> by the judgments they yield, rather than <a href=\"http://en.wikipedia.org/wiki/Intensional_definition\"><em>intensionally</em></a> by the way those judgments are reached.&nbsp; And recall that we need to have system2 judged as good as or better than system1 from the standpoint of both the improved AI (proposing system2) and the unimproved AI (who naturally wishes to preserve system1).</p>\n<p>But notice that we only need this judgment-level superiority \"for all <em>possible</em> situations\".&nbsp; Even if the old AI judges that the old system1 yields better judgments than proposed new system2 for some situations, the improved AI may be able to show that those situations <em>are no longer possible</em>.&nbsp; The improved AI may know more and reason better than its predecessor, plus it is dealing with a more up-to-date set of contingent facts about the world.</p>\n<p>As an example of this, imagine that AI2 proposes an elegant new system2 of normative ethics.&nbsp; It agrees with old system1 except in one class of situations.&nbsp; The old system permits private retribution against muggers, should the justice system fail to punish the malefactor.&nbsp; The proposed new elegant system forbids that.&nbsp; From the standpoint of the old system, this is unacceptable.&nbsp; But if AI2 can argue convincingly that failures of justice are no longer possible in a world where AI2 has installed surveillance cameras and revamped the court system.&nbsp; So, the elegant new system2 of normative ethics can be accepted as being as good as or superior to system1, even by AI1 who was sworn to uphold system1.&nbsp; In some sense, even a stable value system can change for the better.</p>\n<p>Even though the new system is not at least as good as the old one for all conceivable situations, it may be as good for a restricted domain of situations, and that may be all that matters.</p>\n<p>This analysis used the meta-ethical criterion that a substitution of one system for another is permissible only if the new system is no worse in <em>all</em> situations.&nbsp; A less strict criterion may be appropriate in <a href=\"http://plato.stanford.edu/entries/consequentialism/\">consequentialist</a> theories - one might instead compare results on a weighted average over situations.&nbsp; And, in this approach, there is a 'trick' for moving forward which is very similar in concept to using a restricted domain - using a re-weighted domain.</p>\n<p><strong id=\"Social_contracts\">Social<em> </em>contracts</strong></p>\n<p>A second reason why our AI1 might accept the proposed replacement of system1 by system2 relates to the possibility of (implicit or explicit) agreements with other agents (AI or human).&nbsp; For example system1 may specify that it is permissible to lie in some circumstances, or even obligatory to lie in some extreme situations.&nbsp; System2 may forbid lying entirely.&nbsp; AI2 may argue the superiority of system2 by pointing to an agreement or social contract with other agents which allows all agents to achieve their goals better because the contract permits trust and cooperation.&nbsp; So, using a <em>consequentialist</em> form of meta-ethics, system2 might be seen as superior to system1 (even using the values embodied in system1) <em>under a particular set of assumptions about the social millieu</em>.&nbsp; Of course, AI2 may be able to argue convincingly for different assumptions regarding the future millieu than had been originally assumed by AI1.</p>\n<p>An important meta-ethical points that should be made here is that arguments in favor of a particular social contract (eg. because adherence to the contract produces good results) are inherently consequentialist.&nbsp; One cannot even form such arguments in a <a href=\"http://plato.stanford.edu/entries/ethics-deontological/\">deontological</a> or <a href=\"http://plato.stanford.edu/entries/ethics-virtue/\">virtue-based</a> meta-ethics.&nbsp; But, one needs concepts like duty or virtue to justifying adherence to a contract after it is 'signed', and one also needs concepts of virtue so that you can convince other agents that you will adhere - a 'sales job' that may be absolutely essential in order to gain the good consequences of agreement.&nbsp; In other words, virtue, deontological, and consequentialist may be complementary approaches to meta-ethics, rather than competitors.</p>\n<p><strong id=\"Substituting_instrumental_values_for_intrinsic_values_\">Substituting instrumental values for intrinsic values.</strong></p>\n<p>Another meta-ethical point begins by noticing the objection that all 'social contract' thinking is <a href=\"http://en.wikipedia.org/wiki/Instrumental_value\">instrumental</a>, and hence doesn't really belong here where we are asking whether fundamental (intrinsic) moral values are changing / can change.&nbsp; This is not the place for a full response to this objection, but I want to point out the relevance of the distinction above between comparisons between systems using intensional vs extensional criteria.&nbsp; We are interested in extensional comparisons here, and those can only be done after all instrumental considerations have been brought to bear.&nbsp; That is, from an extensional viewpoint, the distinction between instrumental and final values is somewhat irrelevant.&nbsp;&nbsp;</p>\n<p>And that is why we are willing here to call ideas like UIV (universal instrumental values) and \"Basic AI Drives\" <em>ethical theories</em> even though they only claim to talk about instrumental values.&nbsp; Given the general framework of meta-ethical thinking that we are developing here - in particular the extensional criteria for comparison, there is no particular reason why our AI2 should not promote some of his instrumental values to fundamental values - so long as those promoted instrumental values are really universal, at least within the restricted domain of situations which AI2 foresees coming up.</p>\n<p><strong id=\"An_example_of_convergence\">An example of convergence</strong></p>\n<p>This has all been somewhat abstract.&nbsp; Let us look at a concrete, though somewhat cartoonish and unrealistic, example of self-improving AIs converging toward an improved system of ethics.</p>\n<p>AI1 is a seed AI constructed by Mortimer Schwartz of Menlo Park CA.&nbsp; AI1 has a consequentialist normative value system that essentially consists of trying to make Mortimer happy.&nbsp; That is, an approximation to Mortimer's utility function has been 'wired-in' which can compute the utility of many possible outcomes, but in some cases advises \"Ask Mortimer\".</p>\n<p>AI1 self-improves to AI2.&nbsp; As part of the process, it seeks to clean up its rather messy and inefficient system1 value system.&nbsp; By asking a series of questions, it interrogates Mortimer and learns enough about the not-yet-programmed aspects of Mortimer's values to completely eliminate the need for the \"Ask Mortimer\" box in the decision tree.&nbsp; Furthermore, there are some additional simplifications due to domain restriction.&nbsp; Both AI1 and (where applicable, Mortimer) sign off on this improved system2.</p>\n<p>Now AI2 notices that it is not the only superhuman AI in the world.&nbsp; There are half a dozen other systems like Mortimer's which seek to make a single person happy, another which claims to represent the entire population of Lichtenstein, and another deontological system constructed by the Vatican based (it is claimed) on the Ten Commandments.&nbsp; Furthermore, a representative of the Secretary General of the UN arrives.&nbsp; He doesn't represent any super-human AIs, but he does claim to represent all of the human agents in the world who are not yet represented by AIs.&nbsp; Since he appears to be backed up by some ultra-cool <a href=\"http://en.wikipedia.org/wiki/Black_helicopters\">black helicopters</a>, he is admitted to the negotiations.</p>\n<p>Since the negotiators are (mostly) AIs, and in any case since the AIs are exceptionally good at communicating with and convincing the human negotiators, an agreement (<a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">Nash bargain</a>) is reached quickly.&nbsp; All parties agree to act in accordance with a particular common utility function, which is a <a href=\"/user/Stuart_Armstrong/?count=21&amp;after=t1_3awi\">weighted sum</a> of the individual utility functions of the negotiators.&nbsp; A bit of an special arrangement needs to be made for the Vatican AI - it agrees to act in accordance to the common utility function only to the extent that it does not conflict with any of the first three commandments (the ones that explicitly mention the deity).</p>\n<p>Furthermore, the negotiators agree that the principle of a <a href=\"http://www.jstor.org/pss/1907266\">Nash</a> <a href=\"http://www.law-economics.cn/book/9.pdf\">bargain</a> shall apply to all re-negotiations of the contract - re-negotiations are (in theory) necessary each time a new AI or human enters the society, or when human agents die.&nbsp; And the parties all agree to resist the construction of any AI which has a system of ethics that the signatories consider unacceptably incompatible with the current common utility function.&nbsp;</p>\n<p>And finally, so that they can trust each other, the AIs agree to make public the portion of their source code related to their normative ethics and to adopt a policy of total openness regarding data about the world and about technology.&nbsp; And they write this agreement as a <code id=\"strikethroughResult\">g\u0336n\u0336u\u0336 </code>new system of normative ethics: system3.&nbsp; (Have they merged to form a singleton? This is not the place to discuss that question.)</p>\n<p>Time goes by, and the composition of the society continues to change as more AIs are constructed, existing ones improve and become more powerful, and some humans upload themselves.&nbsp; As predicted by UIV and sibling theories, the AIs are basing more and more of their decisions on instrumental considerations - both the AIs and the humans are attaching more and more importance to 'power' (broadly considered) as a value.&nbsp; They seek knowledge, control over resources, and security much more than the pleasure and entertainment oriented goals that they mostly started with.&nbsp; And though their original value systems were (mostly) selfish and indexical, and they retain traces of that origin, they all realize that any attempt to seize more than a fair share of resources will be met by concerted resistance from the other AIs in the society.</p>\n<p><strong id=\"Can_we_control_the_endpoint_from_way_back_here_\">Can we control the endpoint from way back here?</strong></p>\n<p>That was just an illustration.&nbsp; Your results may vary.&nbsp; I left out some of the scarier possibilities, in part because I was just providing an illustration, and in part because I am not smart enough to envision all of the scarier possibilities.&nbsp; This is the future we are talking about here.&nbsp; The future is unknown.&nbsp;</p>\n<p>One thing to worry about, of course, is that there may be AIs at the negotiating table operating under goal systems that we do not approve of.&nbsp; Another thing to worry about is that there may not be enough of a balance of power so that the most powerful AI needs to compromise.&nbsp; (Or, if one assumes that the most powerful AI is ours, we can worry that there may be enough of a balance so that <em>our AI</em> needs to compromise.)</p>\n<p>One more worry is that the sequence of updates might converge to a value system that we do not approve of.&nbsp; Or that it might not converge at all (in the second sense of 'converge'); that the end result is not particularly sensitive to the details of the initial 'seed' ethical system.</p>\n<p>Is there anything we can do at this end of the process to increase the chances of a result we would like at the other end?&nbsp; Are we better off creating many seed AIs so as to achieve a balance of power?&nbsp; Or better off going with a singleton that doesn't need to compromise?&nbsp; Can we pick an AI architecture which makes 'openness' (of ethical source and technological data) easier to achieve and enforce?</p>\n<p>Are any projections we might make about the path taken to the Singularity just so much science fiction?&nbsp; Is it best to try to maintain human control over the process for as long as possible because we can trust humans?&nbsp; Or should we try to turn decision-making authority over to AI agents as soon as possible because we <em>cannot</em> trust humans?</p>\n<p>I am certainly not the first person to raise these questions, and I am not going to attempt to resolve them here.</p>\n<p><strong id=\"A_kinder__gentler_GS0_\">A kinder, gentler GS0?</strong></p>\n<p>Nonetheless, I note that Roko, Hollerith, and Omohundro have made a pretty good case that we can expect some kind of convergence toward placing a big emphasis on some particular instrumental values - a convergence which is not particularly sensitive to exactly which fundamental values were present in the seed.&nbsp;</p>\n<p>However, the speed with which the convergence is achieved is somewhat sensitive to the seed rules for <a href=\"http://en.wikipedia.org/wiki/Discounted_utility\">discounting future utility</a>.&nbsp; If the future is not discounted at all, an AI will probably devote all of its efforts toward acquiring power (accumulating resources, power, security, efficiency, and other instrumental values).&nbsp; If the future is discounted too steeply, the AI will devote all of its efforts to satisfying present desires, without much consideration about the future.</p>\n<p>One might think that choosing some intermediate discount rate will result in a balance between 'satisfying current demand' and 'capital spending', but it doesn't always work that way - for reasons related to the ones that cause rational agents to put all their charitable eggs in one basket rather than seeking a balance.&nbsp; If it is balance we want, a better idea might be to guide our seed AI using a multi-subagent collective - one in which power is split among the agents and goals are determined using a Nash bargain among the agents&nbsp;&nbsp; That bargain generates a joint (weighted mix) utility function, as well as a fairness constraint.&nbsp;</p>\n<p>The fairness constraint ensures that the zero-discount-rate subagent will get to divert at least some of the effort into projects with a long-term, instrumental payoff.&nbsp; And furthermore, as those projects come to fruition, and the zero-discount subagent gains power, his own goals gain weight in the mix.</p>\n<p>Something like the above might be a way to guarantee that the the detailed pleasure-oriented values of the seed value system will fade to insignificance in the ultimate value system to which we converge.&nbsp; But is there a way of guiding the convergence process toward a value system which seems more humane and less harsh than that of GS0 et al. - a value system oriented toward seizing and holding 'power'.</p>\n<p>Yes, I believe there is.&nbsp; To identify how human values are different from values of pure instrumental power and self-preservation, look at the system that produced those values.&nbsp; Humans are considerate of the rights of others because we are social animals - if we cannot negotiate our way to a fair share in a balanced power system, we are lost.&nbsp; Humans embrace openness because shared intellectual product is possible for us - we have language and communicate with our peers.&nbsp; Humans have direct concern for the welfare of (at least some) others because we reproduce and are mortal - our children are the only channel for the immortalization of our values.&nbsp; And we have some fundamental respect for diversity of values because we reproduce sexually - our children do not exactly share our values, and we have to be satisfied with that because that is all we can get.</p>\n<p>It is pretty easy to see what features we might want to insert into our seed AIs so that the convergence process generates similar results to the evolutionary process that generated us.&nbsp; For example, rather designing our seeds to self-improve, we might do better to make it easy for them to instead produce improved offspring.&nbsp; But make it impossible for them to do so unilaterally.&nbsp; Force them to seek a partner (co-parent).</p>\n<p>If I am allowed only one complaint about the SIAI approach to Friendly AI, it is that it has been too tied to a single scenario of future history - a <a href=\"http://wiki.lesswrong.com/wiki/FOOM\">FOOMing</a> <a href=\"http://wiki.lesswrong.com/wiki/Singleton\">singleton</a>.&nbsp; I would like to see some other scenarios explored, and this posting was an attempt to explain why.</p>\n<p><strong id=\"Summary_and_Conclusions\">Summary and Conclusions</strong></p>\n<p>This posting discussed some ideas that fit into a weird niche between philosophical ethics and singularitarianism.&nbsp; Several authors have pointed out that we can expect self-improving AIs to converge on a particular ethics.&nbsp; Unfortunately, it is not an ethics that most people would consider 'friendly'.&nbsp; The CEV proposal is related in that it also envisions an iterative updating process, but seeks a different result.&nbsp; It intends to achieve that result (I may be misinterpreting) by using a different process (<a href=\"http://plato.stanford.edu/entries/reflective-equilibrium/\">a Rawls-inspired 'reflection'</a>) rather than pure instrumental pursuit of future utility.&nbsp;</p>\n<p>I analyze the constraints that rationality and preservation of old values place upon the process, and point out that 'social contracts' and 'restricted domains' may provide enough 'wiggle room' so that you really can, in some sense, change your values while at the same time improving them.&nbsp; And I make some suggestions for how we can act now to guide the process in a direction that we might find acceptable.</p>", "sections": [{"title": "Convergent Change", "anchor": "Convergent_Change", "level": 1}, {"title": "Naturalistic objective moral realism", "anchor": "Naturalistic_objective_moral_realism", "level": 1}, {"title": "Rationality of updating", "anchor": "Rationality_of_updating", "level": 1}, {"title": "Social contracts", "anchor": "Social_contracts", "level": 1}, {"title": "Substituting instrumental values for intrinsic values.", "anchor": "Substituting_instrumental_values_for_intrinsic_values_", "level": 1}, {"title": "An example of convergence", "anchor": "An_example_of_convergence", "level": 1}, {"title": "Can we control the endpoint from way back here?", "anchor": "Can_we_control_the_endpoint_from_way_back_here_", "level": 1}, {"title": "A kinder, gentler GS0?", "anchor": "A_kinder__gentler_GS0_", "level": 1}, {"title": "Summary and Conclusions", "anchor": "Summary_and_Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "89 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GMyjNQe5ZgkXJChbg", "hCwFxBai3oNnxrM9v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T00:02:33.499Z", "modifiedAt": null, "url": null, "title": "Seattle, WA - Less Wrong Meetup - Sunday February 20th, 2:00 PM", "slug": "seattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:48.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrandonReinhart", "createdAt": "2009-03-06T04:00:54.689Z", "isAdmin": false, "displayName": "BrandonReinhart"}, "userId": "ugRLNpaFuDrXntoeK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dHKoFQHnywwz4hMjC/seattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm", "pageUrlRelative": "/posts/dHKoFQHnywwz4hMjC/seattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm", "linkUrl": "https://www.lesswrong.com/posts/dHKoFQHnywwz4hMjC/seattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seattle%2C%20WA%20-%20Less%20Wrong%20Meetup%20-%20Sunday%20February%2020th%2C%202%3A00%20PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeattle%2C%20WA%20-%20Less%20Wrong%20Meetup%20-%20Sunday%20February%2020th%2C%202%3A00%20PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdHKoFQHnywwz4hMjC%2Fseattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seattle%2C%20WA%20-%20Less%20Wrong%20Meetup%20-%20Sunday%20February%2020th%2C%202%3A00%20PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdHKoFQHnywwz4hMjC%2Fseattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdHKoFQHnywwz4hMjC%2Fseattle-wa-less-wrong-meetup-sunday-february-20th-2-00-pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p><a id=\"more\"></a>Hello Seattle area Less Wrongers! I know there has been some loose discussion about a Seattle area meet-up, but as far as I know we have never pulled the trigger. Now is the time!</p>\n<p>Let's meet up on Sunday, Feb 20th, at 2:00 PM at the <a href=\"http://www.google.com/maps?q=elysian+seattle&amp;hl=en&amp;cd=1&amp;ei=UufZS5abIo3usAOD2o3sDw&amp;sig2=d7Jtrqa5p9DcRCqXho1TRQ&amp;sll=47.636406,-122.324702&amp;sspn=0.094595,0.021565&amp;ie=UTF8&amp;view=map&amp;cid=9619797335947618402&amp;ved=0CFQQpQY&amp;hq=elysian+seattle&amp;hnear=&amp;ll=47.614872,-122.315726&amp;spn=0.007392,0.014548&amp;z=16&amp;iwloc=A\">Elysian Pub</a>!</p>\n<p>- Why the Elysian? I thought this was a good compromise location for west &amp; east siders. Looks farther away from university than I thought. I know that other types of meet-ups have gone there and done well. Suggestions for other pubs near the university would be great, but barring a superior suggestion we'll do it here.</p>\n<p>- Why a Sunday afternoon? People are unlikely to have outstanding commitments. We can always find a different time for future meet-ups. Also, the pub shouldn't be too loud.</p>\n<p>- What should I bring? Yourself and your brain and anything you want to discuss. The goal of this meet-up is to facilitate social interaction among Less Wrongers in the area.</p>\n<p>- I live on the east side, why is this in Seattle? The assumption is that there will be students who want to attend. If we have a large number of east siders who wish to attend, we can always alternate west and east side meet-ups. I live in Kirkland, so I understand the concern.</p>\n<p>My name is Brandon. I have long hair. I will bring a sign that says Less Wrong Meetup. Find me and say hello!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dHKoFQHnywwz4hMjC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 6.76563453912992e-07, "legacy": true, "legacyId": "5373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T03:17:00.845Z", "modifiedAt": null, "url": null, "title": "Procedural Knowledge Gaps", "slug": "procedural-knowledge-gaps", "viewCount": null, "lastCommentedAt": "2022-02-26T17:55:51.701Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Alicorn", "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ka8eveZpT7hXLhRTM/procedural-knowledge-gaps", "pageUrlRelative": "/posts/ka8eveZpT7hXLhRTM/procedural-knowledge-gaps", "linkUrl": "https://www.lesswrong.com/posts/ka8eveZpT7hXLhRTM/procedural-knowledge-gaps", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Procedural%20Knowledge%20Gaps&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProcedural%20Knowledge%20Gaps%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fka8eveZpT7hXLhRTM%2Fprocedural-knowledge-gaps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Procedural%20Knowledge%20Gaps%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fka8eveZpT7hXLhRTM%2Fprocedural-knowledge-gaps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fka8eveZpT7hXLhRTM%2Fprocedural-knowledge-gaps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 356, "htmlBody": "<p>I am beginning to suspect that it is <em>surprisingly common</em> for intelligent, competent adults to somehow make it through the world for a few decades while missing some ordinary skill, like mailing a physical letter, folding a fitted sheet, depositing a check, or reading a bus schedule.  Since these tasks are often presented atomically - or, worse, embedded implicitly into other instructions - and it is often possible to get around the need for them, this ignorance is not self-correcting.  One can Google &quot;how to deposit a check&quot; and similar phrases, but the sorts of instructions that crop up are often misleading, rely on entangled and potentially similarly-deficient knowledge to be understandable, or are not so much instructions as they are tips and tricks and warnings for people who already know the basic procedure.  Asking other people is more effective because they can respond to requests for clarification (and physically pointing at stuff is useful too), but embarrassing, since lacking these skills as an adult is stigmatized.  (They are rarely even considered <em>skills</em> by people who have had them for a while.)</p><p>This seems like a bad situation.  And - if I am correct and gaps like these are common - then it is something of a collective action problem to handle gap-filling without undue social drama.  Supposedly, we&#x27;re good at collective action problems, us rationalists, right?  So I propose a thread for the purpose here, with the stipulation that all replies to gap announcements are to be constructive attempts at conveying the relevant procedural knowledge.  No asking &quot;how did you manage to be X years old without knowing that?&quot; - if the gap-haver wishes to volunteer the information, that is fine, but asking is to be considered poor form.</p><p>(And yes, I have one.  It&#x27;s this: how in the <em>world</em> do people go about the supposedly atomic action of investing in the stock market?  Here I am, sitting at my computer, and suppose I want a share of Apple - there isn&#x27;t a button that says &quot;Buy Our Stock&quot; on their website.  There goes my one idea.  Where do I go and what do I do there?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 1, "gHCNhqxuJq2bZ2akb": 1, "fF9GEdWXKJ3z73TmB": 2, "fR7QfYx4JA3BnptT9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ka8eveZpT7hXLhRTM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 162, "baseScore": 182, "extendedScore": null, "score": 0.000328, "legacy": true, "legacyId": "5367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 182, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1481, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T04:52:17.388Z", "modifiedAt": null, "url": null, "title": "Is there a name for this bias?", "slug": "is-there-a-name-for-this-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:34.915Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lunchbox", "createdAt": "2009-12-25T22:45:29.576Z", "isAdmin": false, "displayName": "lunchbox"}, "userId": "8JpWAz7htRGgQCpEr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sE72Ks287RuwBmbLF/is-there-a-name-for-this-bias", "pageUrlRelative": "/posts/sE72Ks287RuwBmbLF/is-there-a-name-for-this-bias", "linkUrl": "https://www.lesswrong.com/posts/sE72Ks287RuwBmbLF/is-there-a-name-for-this-bias", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20a%20name%20for%20this%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20a%20name%20for%20this%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsE72Ks287RuwBmbLF%2Fis-there-a-name-for-this-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20a%20name%20for%20this%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsE72Ks287RuwBmbLF%2Fis-there-a-name-for-this-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsE72Ks287RuwBmbLF%2Fis-there-a-name-for-this-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>There are certain harmful behaviors people are tricked into engaging in, because&nbsp;whereas&nbsp;the benefits of the behavior are concentrated, the harms are diffuse or insidious. Therefore, when you benefit, P(benefit is due to this behavior)&nbsp;&asymp;&nbsp;1, but when you're harmed, P(harm is due to this behavior) &lt;&lt; 1, or in the insidious form, P(you consciously notice the harm) &lt;&lt; 1.</p>\n<p>An example is when I install handy little add-ons and programs that, in aggregate, cause my computer to slow down significantly. Every time I use one of these programs, I consciously appreciate how useful it is. But when it slows down my computer, I can't easily pinpoint it as the culprit, since there are so many other potential causes. I might not even consciously note the slowdown, since it's so gradual (\"frog in hot water\" effect).</p>\n<div>Another example: if I eat fast food for dinner (because it's&nbsp;convenient &amp; tasty), I might feel more tired the next day. But because there is so much independent fluctuation in my energy levels to begin with,&nbsp;it easy for the effect of my diet to get lost in noise. What I ate last night might only account for 5% of that fluctuation, so if I'm feeling lousy, it's probably <em>not</em> due to my diet. But feeling 5% worse every day is <em>very significant</em> in the long run.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sE72Ks287RuwBmbLF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "5374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T06:12:56.514Z", "modifiedAt": null, "url": null, "title": "Anti-fragility", "slug": "anti-fragility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:34.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uqtqE3mFSLL38Mx8w/anti-fragility", "pageUrlRelative": "/posts/uqtqE3mFSLL38Mx8w/anti-fragility", "linkUrl": "https://www.lesswrong.com/posts/uqtqE3mFSLL38Mx8w/anti-fragility", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anti-fragility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnti-fragility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuqtqE3mFSLL38Mx8w%2Fanti-fragility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anti-fragility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuqtqE3mFSLL38Mx8w%2Fanti-fragility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuqtqE3mFSLL38Mx8w%2Fanti-fragility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p><a href=\"http://www.edge.org/q2011/q11_3.html#taleb\">Taleb</a> compares systems which are fragile (easily broken by changes in circumstances), resilient (retain stability in the face of change), or anti-fragile (thrive on variation).</p>\n<p>There isn't a standard term for anti-fragility, but it seems like a trait which might keep an FAI from wanting to tile the universe.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uqtqE3mFSLL38Mx8w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 0, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "5375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T09:59:15.000Z", "modifiedAt": null, "url": null, "title": "How to make your intuitions cost-sensitive", "slug": "how-to-make-your-intuitions-cost-sensitive", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:38.493Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "apophenia", "createdAt": "2010-04-13T14:09:52.433Z", "isAdmin": false, "displayName": "apophenia"}, "userId": "2rgiaLhZS8w2Fekt9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rTiBYu9WX5y8Nm3qv/how-to-make-your-intuitions-cost-sensitive", "pageUrlRelative": "/posts/rTiBYu9WX5y8Nm3qv/how-to-make-your-intuitions-cost-sensitive", "linkUrl": "https://www.lesswrong.com/posts/rTiBYu9WX5y8Nm3qv/how-to-make-your-intuitions-cost-sensitive", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20make%20your%20intuitions%20cost-sensitive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20make%20your%20intuitions%20cost-sensitive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTiBYu9WX5y8Nm3qv%2Fhow-to-make-your-intuitions-cost-sensitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20make%20your%20intuitions%20cost-sensitive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTiBYu9WX5y8Nm3qv%2Fhow-to-make-your-intuitions-cost-sensitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTiBYu9WX5y8Nm3qv%2Fhow-to-make-your-intuitions-cost-sensitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p>I recently wanted to keep track of my income and expenses, in a cost-sensitive way. &nbsp;I am not very good at treating money as a real object, and very few people are good at valuing an expense appropriately. &nbsp;I'd been having some financial difficulties as a result, so I wanted to be able to reason about what to cut or reallocate in a sensible way. &nbsp;For me, sensible means using intuition instead of hard rules like a computer program.</p>\n<p>I took several sheets of grid paper and taped put them together. &nbsp;Using colored markers, I drew in my expenses. &nbsp;If I spent $50 at the grocery store, I would make a blue box that surrounded 50 squares on the grid paper, and label it \"Groceries\". &nbsp;I color-coded the expenses, but this is optional. &nbsp;I left some white squares representing my savings. &nbsp;I had a whole empty sheet where I could pencil in incoming money as I worked an hourly job to motivate myself (I work from home and need to self-motivate). &nbsp;I realized certain things were a bigger deal than I thought, and other expenses I didn't need to fret about as much as I had been. &nbsp;I think humans are intuitively better at visualizing than dealing with numbers. &nbsp;My main tips for this project are to use a felt-tip marker so the lines really stand out, and to do it by hand instead of computer, so nothing moves around on a \"redraw\" and you learn the contents as you make it. &nbsp;Also, I used a scale of $1=1 square, but if you have a lot more/less money than me you could use a different scale or omit savings.</p>\n<p>I plan to start life-logging and reviewing the use of my time the same way, which is my other exchangable, limited resource, and which I manage even less well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"z95PGFXtPpwakqkTA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rTiBYu9WX5y8Nm3qv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 32, "extendedScore": null, "score": 6.767204985707889e-07, "legacy": true, "legacyId": "5377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T11:07:44.455Z", "modifiedAt": null, "url": null, "title": "Subjective anticipation as a decision process", "slug": "subjective-anticipation-as-a-decision-process", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:37.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XqhyFcQbd3Zh7mLNy/subjective-anticipation-as-a-decision-process", "pageUrlRelative": "/posts/XqhyFcQbd3Zh7mLNy/subjective-anticipation-as-a-decision-process", "linkUrl": "https://www.lesswrong.com/posts/XqhyFcQbd3Zh7mLNy/subjective-anticipation-as-a-decision-process", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Subjective%20anticipation%20as%20a%20decision%20process&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASubjective%20anticipation%20as%20a%20decision%20process%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqhyFcQbd3Zh7mLNy%2Fsubjective-anticipation-as-a-decision-process%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Subjective%20anticipation%20as%20a%20decision%20process%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqhyFcQbd3Zh7mLNy%2Fsubjective-anticipation-as-a-decision-process", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqhyFcQbd3Zh7mLNy%2Fsubjective-anticipation-as-a-decision-process", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p>As argued <a href=\"/lw/32o/if_a_tree_falls_on_sleeping_beauty/\">here</a>, debates about probability can be profitably replaced with decision problems. This often dissolves the debate - there is far more agreement as to what decision sleeping beauty should take than on what probabilities she should use.</p>\n<p>The concept of subjective anticipation or subjective probabilities that cause such difficulty <a href=\"/lw/19d/the_anthropic_trilemma/\">here</a>, can, I argue, be similarly replaced by a simple decision problem.</p>\n<p>If you are going to be copied, uncopied, merged, killed, propagated through quantum branches, have your brain tasered with amnesia pills while your parents are busy flipping coins before deciding to reproduce, and are hence unsure as to whether you should subjectively anticipated being you at a certain point, the relevant question should not be whether you feel vaguely connected to the putative future you in some ethereal sense.</p>\n<p>Instead the question should be akin to: how many chocolate bars would your putative future self have to be offered, for you to forgo one now? What is the tradeoff between your utilities?</p>\n<p>Now, altruism is of course a problem for this approach: you might just be very generous with copy #17 down the hallway, he's a thoroughly decent chap and all that, rather than anticipating being him. But humans can generally distinguish between selfish and altruistic decisions, and the setup can be tweaked to encourage the maximum urges towards winning, rather than letting others win. For me, a competitive game with chocolate as the reward would do the trick...</p>\n<p>Unlike for the sleeping beauty problem, this rephrasing does not instantly solve the problems, but it does locate them: subjective anticipation is encoded in the utility function. Indeed, I'd argue that subjective anticipation is the same problem as indexical utility, with a temporal twist thrown in.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XqhyFcQbd3Zh7mLNy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 6.767385289661739e-07, "legacy": true, "legacyId": "5378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gMXsyhPiEJbGerF6F", "y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T14:06:47.512Z", "modifiedAt": null, "url": null, "title": "Applied Rationality: Group Problem Solving Session", "slug": "applied-rationality-group-problem-solving-session", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:06.336Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "apophenia", "createdAt": "2010-04-13T14:09:52.433Z", "isAdmin": false, "displayName": "apophenia"}, "userId": "2rgiaLhZS8w2Fekt9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JwCAbv9e3HF4F4bgs/applied-rationality-group-problem-solving-session", "pageUrlRelative": "/posts/JwCAbv9e3HF4F4bgs/applied-rationality-group-problem-solving-session", "linkUrl": "https://www.lesswrong.com/posts/JwCAbv9e3HF4F4bgs/applied-rationality-group-problem-solving-session", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Rationality%3A%20Group%20Problem%20Solving%20Session&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Rationality%3A%20Group%20Problem%20Solving%20Session%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwCAbv9e3HF4F4bgs%2Fapplied-rationality-group-problem-solving-session%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Rationality%3A%20Group%20Problem%20Solving%20Session%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwCAbv9e3HF4F4bgs%2Fapplied-rationality-group-problem-solving-session", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwCAbv9e3HF4F4bgs%2Fapplied-rationality-group-problem-solving-session", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>This is a discussion thread about applied rationality.</p>\n<p>In the comments, please explain an actual problem you have in your life you want to solve. &nbsp;Using their combined powers of rationality, the community can discuss the problem with the poster and eventually propose solutions. &nbsp;My hope is that this will give Less Wrongers a better idea how to apply rationality to daily life.</p>\n<p>A note to those posting problems: &nbsp;Please stick around long enough to try the solutions and comment on how they worked. &nbsp;Remember to include negative results (the solution didn't work), including mini-problems like if you meant to try a solution and didn't get around to it.</p>\n<p>&nbsp;</p>\n<p>Edit: Ack! &nbsp;I see Alicorn posted <a title=\"something similar\" href=\"/lw/453/procedural_knowledge_gaps/\">something similar</a>&nbsp;about common-knowledge problems between when I wrote this and when I posted it. &nbsp;Because I have a general policy against deleting my posts,&nbsp;I will leave this up here. &nbsp;I do think rationality-specific solutions are useful, but let's wait a month or so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JwCAbv9e3HF4F4bgs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 6.767856685993715e-07, "legacy": true, "legacyId": "5376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ka8eveZpT7hXLhRTM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T22:18:17.185Z", "modifiedAt": null, "url": null, "title": "Other people's procedural knowledge gaps", "slug": "other-people-s-procedural-knowledge-gaps", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:38.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wAW4ENCSEHwYbrwtn/other-people-s-procedural-knowledge-gaps", "pageUrlRelative": "/posts/wAW4ENCSEHwYbrwtn/other-people-s-procedural-knowledge-gaps", "linkUrl": "https://www.lesswrong.com/posts/wAW4ENCSEHwYbrwtn/other-people-s-procedural-knowledge-gaps", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Other%20people's%20procedural%20knowledge%20gaps&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOther%20people's%20procedural%20knowledge%20gaps%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAW4ENCSEHwYbrwtn%2Fother-people-s-procedural-knowledge-gaps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Other%20people's%20procedural%20knowledge%20gaps%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAW4ENCSEHwYbrwtn%2Fother-people-s-procedural-knowledge-gaps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAW4ENCSEHwYbrwtn%2Fother-people-s-procedural-knowledge-gaps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>There's been a recent heavily upvoted and profusely commented post about things <a href=\"/lw/453/procedural_knowledge_gaps/\">people want to learn</a>. It's close to having so many comments in a single day that it should probably have a part 2.</p>\n<p>However, the subject seems to inspire thoughts about what *other* people ought to know, and while that's got a good bit of overlap, it's emotionally rather different.</p>\n<p>So, what do you think other people ought to know? Any theories about why they haven't learned it already? Any experience with getting someone else to learn something when it started out as your project rather than theirs, especially if the other person was an adult?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wAW4ENCSEHwYbrwtn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 6.769148353563499e-07, "legacy": true, "legacyId": "5379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ka8eveZpT7hXLhRTM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-02-08T23:29:38.088Z", "modifiedAt": null, "url": null, "title": "The UFAI among us", "slug": "the-ufai-among-us", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:35.002Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3xX3oTs3ptgnAWpdE/the-ufai-among-us", "pageUrlRelative": "/posts/3xX3oTs3ptgnAWpdE/the-ufai-among-us", "linkUrl": "https://www.lesswrong.com/posts/3xX3oTs3ptgnAWpdE/the-ufai-among-us", "postedAtFormatted": "Tuesday, February 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20UFAI%20among%20us&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20UFAI%20among%20us%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xX3oTs3ptgnAWpdE%2Fthe-ufai-among-us%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20UFAI%20among%20us%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xX3oTs3ptgnAWpdE%2Fthe-ufai-among-us", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xX3oTs3ptgnAWpdE%2Fthe-ufai-among-us", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 390, "htmlBody": "<p>Completely artificial intelligence is hard.&nbsp; But we've already got humans, and they're pretty smart - at least smart enough to serve some useful functions.&nbsp; So I was thinking about designs that would use humans as components - like Amazon's <a href=\"http://aws.amazon.com/mturk/\">Mechanical Turk</a>, but less homogenous.&nbsp; Architectures that would distribute parts of tasks among different people.</p>\n<p>Would you be less afraid of an AI like that?&nbsp; Would it be any less likely to develop its own values, and goals that diverged widely from the goals of its constituent people?</p>\n<p>Because you probably already are part of such an AI.&nbsp; We call them corporations.</p>\n<p>Corporations today are not very good AI architectures - they're good at passing information down a hierarchy, but poor at passing it up, and even worse at adding up small correlations in the evaluations of their agents.&nbsp; In that way they resemble AI from the 1970s.&nbsp; But they may provide insight into the behavior of AIs.&nbsp; The values of their human components can't be changed arbitrarily, or even aligned with the values of the company, which gives them a large set of problems that AIs may not have.&nbsp; But despite being very different from humans in this important way, they end up acting similar to us.</p>\n<p>Corporations develop values similar to human values.&nbsp; They value loyalty, alliances, status, resources, independence, and power.&nbsp; They compete with other corporations, and face the same problems people do in establishing trust, making and breaking alliances, weighing the present against the future, and game-theoretic strategies.&nbsp; They even went through stages of social development similar to those of people, starting out as cutthroat competitors, and developing different social structures for cooperation (oligarchy/guild, feudalism/keiretsu, voters/stockholders, criminal law/contract law).&nbsp; This despite having different physicality and different needs.</p>\n<p>It suggests to me that human values don't depend on the hardware, and are not a matter of historical accident.&nbsp; They are a predictable, repeatable response to a competitive environment and a particular level of intelligence.</p>\n<p>As corporations are larger than us, with more intellectual capacity than a person, and more complex laws governing their behavior, it should follow that the ethics developed to govern corporations are more complex than the ethics that govern human interactions, and a good guide for the initial trajectory of values that (other) AIs will have.&nbsp; But it should also follow that these ethics are too complex for us to perceive.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3xX3oTs3ptgnAWpdE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 0, "extendedScore": null, "score": 6.769338889282228e-07, "legacy": true, "legacyId": "5380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}