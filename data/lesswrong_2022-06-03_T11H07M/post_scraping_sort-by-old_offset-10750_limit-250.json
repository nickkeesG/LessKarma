{"results": [{"createdAt": null, "postedAt": "2014-04-29T14:08:47.844Z", "modifiedAt": null, "url": null, "title": "European Community Weekend 2014 retrospective", "slug": "european-community-weekend-2014-retrospective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:06.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/giX36Wqw38xdXhKQc/european-community-weekend-2014-retrospective", "pageUrlRelative": "/posts/giX36Wqw38xdXhKQc/european-community-weekend-2014-retrospective", "linkUrl": "https://www.lesswrong.com/posts/giX36Wqw38xdXhKQc/european-community-weekend-2014-retrospective", "postedAtFormatted": "Tuesday, April 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20European%20Community%20Weekend%202014%20retrospective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEuropean%20Community%20Weekend%202014%20retrospective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgiX36Wqw38xdXhKQc%2Feuropean-community-weekend-2014-retrospective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=European%20Community%20Weekend%202014%20retrospective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgiX36Wqw38xdXhKQc%2Feuropean-community-weekend-2014-retrospective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgiX36Wqw38xdXhKQc%2Feuropean-community-weekend-2014-retrospective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 732, "htmlBody": "<p>So finally - with two weeks distance to the first <a href=\"/lw/jjw/european_community_weekend_in_berlin/\">European LessWrong Community Weekend</a> - we want to share the organizers&rsquo; perception of the event, including a short overview of what went well, what did not and what exceeded our expectations.</p>\n<p>First and foremost we thank all the participants and speakers for helping us in making this such a great weekend. We had an incredible time and are very happy everything worked out as well as it did. In our opinion the event was a great success! Meeting everyone was excellent and we look forward to running a similar yet improved event in the future.</p>\n<p><a id=\"more\"></a>One of the awesome things about the event were the group dynamics. The general feeling was that the participants were open and had a generally positive attitude towards each other. Even some people who usually prefer to avoid crowds expressed that it was a safe place to try different things and to improve their social skills.</p>\n<p>Two things that strongly supported the pervasive feeling of community and friendship were the extraordinarily high frequency of hugs and the cheerful sentiment of the <a href=\"/lw/juh/less_wrong_study_hall_year_1_retrospective/\">LessWrong study hall people</a> that spilled over to the Community Weekend participants.</p>\n<p>We wanted to encourage hugging by letting people put a &ldquo;accepting hugs as a form of greeting&rdquo; sticker on their extended name tags. To our surprise it was adopted by a huge majority and had an immense effect on social interactions by creating an atmosphere of familiarity. One story about this (anonymously shared in the post event survey) reads:</p>\n<blockquote>\n<p>&ldquo;OMFGWTF Yay Hugs sticker :D The social protocol that cuddly people are generally free to hug each other as much as they want pretty much blew my mind. I got two orders of magnitude more hugs than I usually do, and I loved it.&rdquo;</p>\n</blockquote>\n<p>The workshop presentations strongly engaged the audience and made us all participants. While this was a great thing in itself the downside was that the time planning for most of the speakers didn&rsquo;t work out. It cost us a considerable fraction of the planned 30min breaks between talks. The lesson learned: reserve way more time for questions than usual when talking to a LW crowd and actively moderate, too.</p>\n<p>A lot of people (including us) noticed that this way the talks took too much of the time meant for discussion and socializing. Building new connections between the LWers of Europe and strengthening existing ones was the main focus of the event. A lot of this happened in the evenings when people just went to a park to play ultimate frisbee or to climb trees and learn partner acrobatics.</p>\n<p>The wide range of topics and and the high quality of discussions in general was amazing. One especially notable case is a socratic dialogue that emerged from one of the the group discussions on the first evening. The depth of discussion and the clear thinking we achieved was amazing. The moderator of the discussion will write a detailed post on the specifics soon.</p>\n<p>Our estimations of the number of interested LWers were way too pessimistic. Even our 90% confidence intervals fell short of the actual number of participants signing up. While we were able to increase the size of the event beyond the planned maximum by 25% we still had to reject many applications.</p>\n<p>Organizing this event was a great experience for us and we intend to do this again. We have learned a lot and got great feedback: The next event will be even more awesome, with more time and space for discussions and and social activities. We have already started planning and preparing the bigger and better:</p>\n<p style=\"padding-left: 30px;\">+++ European LessWrong Community Weekend 2015 +++</p>\n<p>Our hope is that this will become a regular event providing a meeting and socializing space for the LWers in Europe. Other groups around Europe already showed interest in hosting similar events so that it might be alternating between cities in the future.</p>\n<p>Upcoming posts to look out for:</p>\n<ul>\n<li>The date for the European LessWrong Community Weekend 2015 will be announced by August (at latest)</li>\n<li>We are currently evaluating the results of the post-event survey and will post an overview somewhen in the next two weeks. (it&rsquo;s still possible to take part in the survey and have your opinion considered if you haven't done so yet)</li>\n</ul>\n<p>Looking forward to seeing you again<br />John, Tristan, Alexander, Matthias, Christian&hellip; &amp; everyone else from the Berlin LessWrong meetup</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "giX36Wqw38xdXhKQc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 36, "extendedScore": null, "score": 1.696244268116228e-06, "legacy": true, "legacyId": "26113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jSCEaE5TbYnbFCQYB", "EYKRYNzdrZ4LH2jma"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-29T14:11:15.791Z", "modifiedAt": null, "url": null, "title": "MIRI Donation Collaboration Station", "slug": "miri-donation-collaboration-station", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:04.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Skeptityke", "createdAt": "2013-05-06T21:18:20.431Z", "isAdmin": false, "displayName": "Skeptityke"}, "userId": "H5HExNf3BsmBRvKJG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AnFafRss5e5nZRtnP/miri-donation-collaboration-station", "pageUrlRelative": "/posts/AnFafRss5e5nZRtnP/miri-donation-collaboration-station", "linkUrl": "https://www.lesswrong.com/posts/AnFafRss5e5nZRtnP/miri-donation-collaboration-station", "postedAtFormatted": "Tuesday, April 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI%20Donation%20Collaboration%20Station&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI%20Donation%20Collaboration%20Station%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnFafRss5e5nZRtnP%2Fmiri-donation-collaboration-station%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI%20Donation%20Collaboration%20Station%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnFafRss5e5nZRtnP%2Fmiri-donation-collaboration-station", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnFafRss5e5nZRtnP%2Fmiri-donation-collaboration-station", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 484, "htmlBody": "<p>As you may know, on May 6, there will be a large one-day price-matching fundraiser for Bay Area Charities.</p>\n<p>The relevant details are right here at <a href=\"http://intelligence.org/2014/04/25/may-6th-miri-participating-in-massive-24-hour-online-fundraiser/\" target=\"_blank\">MIRI's official website.</a></p>\n<p>And this is the webpage to visit <a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\" target=\"_blank\">to donate.</a></p>\n<p>For those of you who didn't read the two links above, here's the important information.</p>\n<blockquote>\n<p>On May 6, MIRI is participating in Silicon Valley Gives...</p>\n<p>Why is this exciting for supporters of MIRI? Many reasons, but here are a few.</p>\n<p>&nbsp;</p>\n<ul>\n<li><strong>Over $250,000 of matching prizes and funds up for grabs, from sources that normally wouldn't contribute to MIRI:</strong></li>\n<li>Two-to-one dollar match up to $50,000 during the midnight hour.</li>\n<li>$2,000 dollar prize for the nonprofit that has the most individual gift in an hour, every hour, for 24 hours.</li>\n<li>$150 added to a random donation each hour, every hour for 24 hours.</li>\n<li>Dollar for Dollar match up to $35,000 during the 7AM hour, and $50,000 during the noon, 6 PM, and 7 PM hours.</li>\n<li>Local NBC stations, radio stations, businesses, and Bay Area foundations will be promoting the Silicon Valley Day of Giving on May 6th. So if MIRI is making a splash with our fundraising that day, it's possible we'll draw attention from media and by extension new donors.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Making the most of this opportunity will require some cleverness and a lot of coordination. We are going to need all the help we can get...</p>\n<p>&nbsp;</p>\n<ol>\n<li>If you are interested in supporting MIRI with a large donation during the fundraiser... Get in touch with Malo at malo@intelligence.org</li>\n<li>All MIRI supporters have the potential to make a big impact if we can all work together in a coordinated manner. Sign up below (The sign up sheet is on the MIRI announcement page) to receive updates on our strategy leading up to the event, and updates throughout the fundraiser on the best times to give and promote the event.</li>\n</ol>\n<p>&nbsp;</p>\n</blockquote>\n<div>A group coordination problem? We can do that easily, right?</div>\n<div>So, in the comments, feel free to discuss donation strategy, coordinate groups of people, provide positive reinforcement for donors, etc.</div>\n<div><br /></div>\n<div>With a preliminary scan through the <a href=\"http://svgives.razoo.com/giving_events/svg14/prizes\" target=\"_blank\">prize list</a>, a viable strategy seems to be to funnel most of the donations in during the first hour (midnight) for the two-to-one matching until the matching limit, and do the the rest of the donations during 6 and 7 PM. The small donors can all coordinate on a few hours where few people will be donating (inconvenient morning hours should work well for this) to secure a 2000 dollar prize or two, and since 1000 dollars will be given to the first 50 organizations to receive a gift in the 5 PM hour, getting a handful of people together to donate right at 5 also seems useful.</div>\n<div><br /></div>\n<div>Mark your calendars!</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AnFafRss5e5nZRtnP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 30, "extendedScore": null, "score": 1.6962475670398923e-06, "legacy": true, "legacyId": "26114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-29T17:57:09.854Z", "modifiedAt": "2020-03-05T13:39:17.028Z", "url": null, "title": "The Universal Medical Journal Article Error", "slug": "the-universal-medical-journal-article-error", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:08.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q4o5CMYCHwwAoSaAS/the-universal-medical-journal-article-error", "pageUrlRelative": "/posts/q4o5CMYCHwwAoSaAS/the-universal-medical-journal-article-error", "linkUrl": "https://www.lesswrong.com/posts/q4o5CMYCHwwAoSaAS/the-universal-medical-journal-article-error", "postedAtFormatted": "Tuesday, April 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Universal%20Medical%20Journal%20Article%20Error&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Universal%20Medical%20Journal%20Article%20Error%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq4o5CMYCHwwAoSaAS%2Fthe-universal-medical-journal-article-error%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Universal%20Medical%20Journal%20Article%20Error%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq4o5CMYCHwwAoSaAS%2Fthe-universal-medical-journal-article-error", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq4o5CMYCHwwAoSaAS%2Fthe-universal-medical-journal-article-error", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1397, "htmlBody": "<blockquote> TL;DR:  When people read a journal article that concludes, &quot;We have proved that it is not the case that for every X, P(X)&quot;, they generally credit the article with having provided at least weak evidence in favor of the proposition &#x2200;x !P(x).  This is not necessarily so.<br>  <br> Authors using statistical tests are making precise claims, which must be quantified correctly.  Pretending that all quantifiers are universal because we are speaking English is one error.  It is not, as many commenters are claiming, a small error.  &#x2200;x !P(x) is very different from !&#x2200;x P(x).<br>  <br> A more-subtle problem is that when an article uses an F-test on a hypothesis, it is possible (and common) to fail the F-test for P(x) with data that supports the hypothesis P(x).  The 95% confidence level was chosen for the F-test in order to count false positives as much more expensive than false negatives.  Applying it therefore removes us from the world of Bayesian logic.  You cannot interpret the failure of an F-test for P(x) as being even weak evidence for not P(x). </blockquote><br><br><p>I used to teach logic to undergraduates, and they regularly made the same simple mistake with logical quantifiers.  Take the statement &quot;For every X there is some Y such that P(X,Y)&quot; and represent it symbolically:</p><p>&#x2200;x&#x2203;y P(x,y)</p><br><p>Now negate it:</p><p>!&#x2200;x&#x2203;y P(x,y)</p><br><p>You often don&apos;t want a negation to be outside quantifiers.  My undergraduates would often just push it inside, like this:</p><p>&#x2200;x&#x2203;y !P(x,y)</p><br><p>If you could just move the negation inward like that, then these claims would mean the same thing:</p><p>A) Not everything is a raven:  !&#x2200;x raven(x)</p><p>B) Everything is not a raven: &#x2200;x !raven(x)</p><br><p>To move a negation inside quantifiers, flip each quantifier that you move it past.</p><p>!&#x2200;x&#x2203;y P(x,y) = &#x2203;x!&#x2203;y P(x,y) = &#x2203;x&#x2200;y !P(x,y)</p><br><p>Here&apos;s the findings of a 1982 <u><a href=\"http://archpsyc.jamanetwork.com/article.aspx?articleid=492570\">article</a></u> [1] from <em>JAMA Psychiatry</em> (formerly <em>Archives of General Psychiatry</em>), back in the days when the medical establishment was busy denouncing the Feingold diet:</p><br><blockquote> Previous studies have not conclusively demonstrated behavioral effects of artificial food colorings ...  This study, which was designed to maximize the likelihood of detecting a dietary effect, found none.<br> </blockquote><br><p>Now pay attention; this is the part everyone gets wrong, including most of the commenters below.</p><br><p>The methodology used in this study, and in most studies, is as follows:</p><ul><li>Divide subjects into a test group and a control group.</li><li>Administer the intervention to the test group, and a placebo to the control group.</li><li>Take some measurement that is supposed to reveal the effect they are looking for.</li><li>Compute the mean and standard deviation of that measure for the test and control groups.</li><li>Do either a <a href=\"http://en.wikipedia.org/wiki/Student%27s_t-test\">t-test</a> or an <a href=\"http://en.wikipedia.org/wiki/F-test\">F-test</a> of the hypothesis that the intervention causes a statistically-significant effect on all subjects.</li><li>If the test succeeds, conclude that the intervention causes a statistically-significant effect (CORRECT).</li><li>If the test does not succeed, conclude that the intervention does not cause any effect to any subjects (ERROR).<br></li></ul><br><p>People make the error because they forget to explicitly state what quantifiers they&apos;re using.  Both the t-test and the F-test work by assuming that every subject has <em>the same response function</em> to the intervention:</p><p><em>response = effect + normally distributed error</em></p><br><p>where the effect is the same for every subject.  If you don&apos;t understand why that is so, read the articles about the t-test and the F-test.  The null hypothesis is that the responses of all subjects in both groups were drawn from the same distribution.  The one-tailed versions of the tests take a confidence level C and compute a cutoff Z such that, if the null hypothesis is false,</p><p>P(average effect(test) - average effect(control)) &lt; Z = C</p><br><p>ADDED:  People are making comments proving they don&apos;t understand how the F-test works.  This is how it works:  You are testing the hypothesis that two groups respond <em>differently</em> to food dye.</p><br><p>Suppose you measured the number of times a kid shouted or jumped, and you found that kids fed food dye shouted or jumped an average of 20 times per hour, and kids not fed food dye shouted or jumped an average of 17 times per hour.  When you run your F-test, you compute that, assuming all kids respond to food dye the same way, you need a difference of 4 to conclude with 95% confidence that the two distributions (test and control) are different.</p><br><p>If the food dye kids had shouted/jumped 21 times per hour, the study would conclude that food dye causes hyperactivity.  Because they shouted/jumped only 20 times per hour, it failed to prove that food dye affects hyperactivity.  You can only conclude that food dye affects behavior with 84% confidence, rather than the 95% you desired.</p><br><p>Finding that food dye affects behavior with 84% confidence should not be presented as proof that food dye does not affect behavior!</p><br><p>If half your subjects have a genetic background that makes them resistant to the effect, the threshold for the t-test or F-test will be much too high to detect that.  If 10% of kids become more hyperactive and 10% become less hyperactive after eating food coloring, such a methodology will never, ever detect it.  A test done in this way can <em>only</em> accept or reject the hypothesis that <em>for every</em> subject x, the effect of the intervention is different than the effect of the placebo.<br></p><br><p>So.  Rephrased to say precisely what the study found:</p><br><blockquote> This study tested and rejected the hypothesis that artificial food coloring affects behavior in all children. </blockquote><br><p>Converted to logic (ignoring time):</p><p>!( &#x2200;child ( eats(child, coloring) &#x21E8; behaviorChange(child) ) )</p><br><p>Move the negation inside the quantifier:</p><p>&#x2203;child !( eats(child, coloring) &#x21E8; behaviorChange(child) )</p><br><p>Translated back into English, this study proved:</p><br><blockquote> There exist children for whom artificial food coloring does not affect behavior.</blockquote><br><p>However, this is the actual final sentence of that paper:</p><br><blockquote> The results of this study indicate that artificial food colorings do not affect the behavior of school-age children who are claimed to be sensitive to these agents.</blockquote><br><p>Translated into logic:</p><p>!&#x2203;child ( eats(child, coloring) &#x21E8; hyperactive(child) ) )</p><br><p>or, equivalently,</p><p> &#x2200;child !( eats(child, coloring) &#x21E8; hyperactive(child) ) )</p><br><p>This refereed medical journal article, like many others, made the same mistake as my undergraduate logic students, moving the negation across the quantifier without changing the quantifier.  I cannot recall ever seeing a medical journal article prove a negation and <em>not</em> make this mistake when stating its conclusions.</p><br><p>A lot of people are complaining that I should just interpret their statement as meaning &quot;Food colorings do not affect the behavior of MOST school-age children.&quot;</p><br><p>But they didn&apos;t prove that food colorings do not affect the behavior of most school-age children.  They proved that there exists at least one child whose behavior food coloring does not affect.  That isn&apos;t remotely close to what they have claimed.</p><br><p>For the record, the conclusion is wrong.  Studies that did not assume that all children were identical, such as studies that used each child as his or her own control by randomly giving them cookies containing or not containing food dye [2], or a recent study that partitioned the children according to single-nucleotide polymorphisms (SNPs) in genes related to food metabolism [3], found large, significant effects in some children or some genetically-defined groups of children.  Unfortunately, reviews failed to distinguish the logically sound from the logically unsound articles, and the medical community insisted that food dyes had no influence on behavior until thirty years after their influence had been repeatedly proven.</p><br><br><p>[1] Jeffrey A. Mattes &amp; Rachel Gittelman (1981). Effects of Artificial Food Colorings in Children With Hyperactive Symptoms: A Critical Review and Results of a Controlled Study. <em>Archives of General Psychiatry </em>38(6):714-718. doi:10.1001/archpsyc.1981.01780310114012.</p><br><p>[2] K.S. Rowe &amp; K.J. Rowe (1994). Synthetic food coloring and behavior: a dose response effect in a double-blind, placebo-controlled, repeated-measures study. <em>The Journal of Pediatrics</em> Nov;125(5 Pt 1):691-8.</p><p><br></p><p>[3] Stevenson, Sonuga-Barke, McCann et al. (2010). The Role of Histamine Degradation Gene Polymorphisms in Moderating the Effects of Food Additives on Children&#x2019;s ADHD Symptoms. Am J Psychiatry 167:1108-1115.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q4o5CMYCHwwAoSaAS", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 10, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "22218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 191, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-29T23:06:53.387Z", "modifiedAt": null, "url": null, "title": "Cognito Mentoring and zero-sum competition", "slug": "cognito-mentoring-and-zero-sum-competition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:04.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ZqNcmAo7Lsu6QdYM/cognito-mentoring-and-zero-sum-competition", "pageUrlRelative": "/posts/3ZqNcmAo7Lsu6QdYM/cognito-mentoring-and-zero-sum-competition", "linkUrl": "https://www.lesswrong.com/posts/3ZqNcmAo7Lsu6QdYM/cognito-mentoring-and-zero-sum-competition", "postedAtFormatted": "Tuesday, April 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cognito%20Mentoring%20and%20zero-sum%20competition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACognito%20Mentoring%20and%20zero-sum%20competition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZqNcmAo7Lsu6QdYM%2Fcognito-mentoring-and-zero-sum-competition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cognito%20Mentoring%20and%20zero-sum%20competition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZqNcmAo7Lsu6QdYM%2Fcognito-mentoring-and-zero-sum-competition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZqNcmAo7Lsu6QdYM%2Fcognito-mentoring-and-zero-sum-competition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 809, "htmlBody": "<p>I've recently made some posts (<a href=\"/lw/k0b/a_puzzle_concerning_cs_major_vs_engineering_major/\">[1]</a>, <a href=\"/lw/k0w/how_much_does_where_you_go_to_college_affect/\">[2]</a>, <a href=\"/lw/k29/earnings_of_economics_majors_general/\">[3]</a>, <a href=\"/lw/k2v/economics_majors_and_earnings_further_exploration/\">[4]</a>) geared toward helping high school and college students increase their expected future earnings, as a part of my work for <a href=\"http://www.cognitomentoring.org/\">Cognito Mentoring</a>. A LWer recently asked how closely aligned this is with our goal of producing social value. Specifically, he noted that in some cases the effect of our advice will be to help people win zero-sum competitions. For example, to the extent that majoring in economics has signaling benefits, if somebody majors in economics based on our information, the signaling benefits to him or her will be counterbalanced by reduced signaling benefits for others. At first glance, this appears to lack social value, raising the question of why we're working to provide this information. We recognize the tension, and this post responds to this question.</p>\n<p><a id=\"more\"></a></p>\n<p>There are a number of relevant considerations (which are largely unrelated to each other):</p>\n<p><strong>We need to provide a service that people find useful in order to help people create social value (whether by design or incidentally)</strong></p>\n<p>The primary reason that we're generating and offering advice that helps students win zero sum competitions is to gain traction so as to be better positioned to disseminate information that does have social value.</p>\n<p>Our strategy for disseminating information that has social value is to become a central hub for intellectually talented students to find information on all subjects that are of personal interest to them. Some of what they're interested in is winning zero sum competitions, and we wouldn't be as attractive and visible to our target audience if we didn't provide them with relevant information.&nbsp;</p>\n<p>We need to win people's trust and prove useful to them in order to have further impact. Some of the people will then be open to, and interested in, making decisions that contribute social value.</p>\n<p><strong>It's hard to help people without helping them win zero sum competitions, and that's ok</strong></p>\n<p>Even advice that superficially has no relation to zero sum competitions often has hidden zero sum elements. Consider the case of helping someone&nbsp;<a href=\"/lw/jp2/methods_for_treating_depression/\">overcome depression</a>. If one is successful, one (in expectation) enables the person to be a more productive worker, which will help him or her get better jobs over other job candidates, reducing their expected incomes. But helping someone overcome depression is a positive thing on balance, despite the fact that it may hurt others to some degree.</p>\n<p><strong>Providing public information on how to win zero sum competitions levels the playing field, which is on balance positive</strong></p>\n<p>Consider the case of helping people with their job applications. This is zero sum from the point of view of employees: the job market is competitive, some jobs are (on average) better to have than others, and helping people make themselves look better relatively better to employers makes other people look relatively worse to employers: one person's gain is another person's loss.</p>\n<p>But helping job applicants present themselves better to employers helps <em>the employers</em> offer a better product, which in turn helps the customers, provided that one is helping all job applicants <em>equally</em>. This is because it increases the signal-to-noise ratio. Leveling the playing field makes it matter less who <em>happens</em> to be better at presenting themselves (for example, owing to natural aptitude, or coming from a privileged background): if everyone has equal presentation skills, then people's actual abilities come across more clearly to employers, allowing them to make better hiring decisions.</p>\n<p>We haven't been focused on helping people with their job applications, and it's not something that's part of our current plans, but the general principle applies to some of the domains where we offer advice that helps people win zero sum competitions: because most of our information is public, we're increasing the reliability of signals, which helps match people up with resources more efficiently (though we don't think that this is the dimension on which we can contribute the most social value).</p>\n<p><strong>Our resources devoted to primarily zero-sum aspects are small</strong></p>\n<p>We do have some pages that are almost exclusively useful for winning zero sum competitions, such as our page on&nbsp;<a href=\"http://info.cognitomentoring.org/wiki/Standardized_tests\">standardized tests</a>. But these reflect a very small fraction (perhaps ~ 2%) of the resources that we've spent on Cognito Mentoring.</p>\n<p>We did a review of our personalized advising, and found that there too, a very large majority of our advising has been about subjects such as course selection, major selection, the resources available for learning different subjects, and personal well-being and productivity, which aren't particularly related to zero sum competitions (though see the point above).</p>\n<p><strong>Our research on zero-sum competitions is largely undertaken with a view toward finding ways for people to do well at them at a lower cost</strong></p>\n<p>For example, we investigated <a href=\"/lw/jys/what_colleges_look_for_in_extracurricular/\">what colleges look for in extracurricular activities</a> with a view toward finding ways in which high school students can build their human capital and contribute social value without it coming at the cost of decreased chances of getting into prestigious colleges.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ZqNcmAo7Lsu6QdYM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "26082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've recently made some posts (<a href=\"/lw/k0b/a_puzzle_concerning_cs_major_vs_engineering_major/\">[1]</a>, <a href=\"/lw/k0w/how_much_does_where_you_go_to_college_affect/\">[2]</a>, <a href=\"/lw/k29/earnings_of_economics_majors_general/\">[3]</a>, <a href=\"/lw/k2v/economics_majors_and_earnings_further_exploration/\">[4]</a>) geared toward helping high school and college students increase their expected future earnings, as a part of my work for <a href=\"http://www.cognitomentoring.org/\">Cognito Mentoring</a>. A LWer recently asked how closely aligned this is with our goal of producing social value. Specifically, he noted that in some cases the effect of our advice will be to help people win zero-sum competitions. For example, to the extent that majoring in economics has signaling benefits, if somebody majors in economics based on our information, the signaling benefits to him or her will be counterbalanced by reduced signaling benefits for others. At first glance, this appears to lack social value, raising the question of why we're working to provide this information. We recognize the tension, and this post responds to this question.</p>\n<p><a id=\"more\"></a></p>\n<p>There are a number of relevant considerations (which are largely unrelated to each other):</p>\n<p><strong id=\"We_need_to_provide_a_service_that_people_find_useful_in_order_to_help_people_create_social_value__whether_by_design_or_incidentally_\">We need to provide a service that people find useful in order to help people create social value (whether by design or incidentally)</strong></p>\n<p>The primary reason that we're generating and offering advice that helps students win zero sum competitions is to gain traction so as to be better positioned to disseminate information that does have social value.</p>\n<p>Our strategy for disseminating information that has social value is to become a central hub for intellectually talented students to find information on all subjects that are of personal interest to them. Some of what they're interested in is winning zero sum competitions, and we wouldn't be as attractive and visible to our target audience if we didn't provide them with relevant information.&nbsp;</p>\n<p>We need to win people's trust and prove useful to them in order to have further impact. Some of the people will then be open to, and interested in, making decisions that contribute social value.</p>\n<p><strong id=\"It_s_hard_to_help_people_without_helping_them_win_zero_sum_competitions__and_that_s_ok\">It's hard to help people without helping them win zero sum competitions, and that's ok</strong></p>\n<p>Even advice that superficially has no relation to zero sum competitions often has hidden zero sum elements. Consider the case of helping someone&nbsp;<a href=\"/lw/jp2/methods_for_treating_depression/\">overcome depression</a>. If one is successful, one (in expectation) enables the person to be a more productive worker, which will help him or her get better jobs over other job candidates, reducing their expected incomes. But helping someone overcome depression is a positive thing on balance, despite the fact that it may hurt others to some degree.</p>\n<p><strong id=\"Providing_public_information_on_how_to_win_zero_sum_competitions_levels_the_playing_field__which_is_on_balance_positive\">Providing public information on how to win zero sum competitions levels the playing field, which is on balance positive</strong></p>\n<p>Consider the case of helping people with their job applications. This is zero sum from the point of view of employees: the job market is competitive, some jobs are (on average) better to have than others, and helping people make themselves look better relatively better to employers makes other people look relatively worse to employers: one person's gain is another person's loss.</p>\n<p>But helping job applicants present themselves better to employers helps <em>the employers</em> offer a better product, which in turn helps the customers, provided that one is helping all job applicants <em>equally</em>. This is because it increases the signal-to-noise ratio. Leveling the playing field makes it matter less who <em>happens</em> to be better at presenting themselves (for example, owing to natural aptitude, or coming from a privileged background): if everyone has equal presentation skills, then people's actual abilities come across more clearly to employers, allowing them to make better hiring decisions.</p>\n<p>We haven't been focused on helping people with their job applications, and it's not something that's part of our current plans, but the general principle applies to some of the domains where we offer advice that helps people win zero sum competitions: because most of our information is public, we're increasing the reliability of signals, which helps match people up with resources more efficiently (though we don't think that this is the dimension on which we can contribute the most social value).</p>\n<p><strong id=\"Our_resources_devoted_to_primarily_zero_sum_aspects_are_small\">Our resources devoted to primarily zero-sum aspects are small</strong></p>\n<p>We do have some pages that are almost exclusively useful for winning zero sum competitions, such as our page on&nbsp;<a href=\"http://info.cognitomentoring.org/wiki/Standardized_tests\">standardized tests</a>. But these reflect a very small fraction (perhaps ~ 2%) of the resources that we've spent on Cognito Mentoring.</p>\n<p>We did a review of our personalized advising, and found that there too, a very large majority of our advising has been about subjects such as course selection, major selection, the resources available for learning different subjects, and personal well-being and productivity, which aren't particularly related to zero sum competitions (though see the point above).</p>\n<p><strong id=\"Our_research_on_zero_sum_competitions_is_largely_undertaken_with_a_view_toward_finding_ways_for_people_to_do_well_at_them_at_a_lower_cost\">Our research on zero-sum competitions is largely undertaken with a view toward finding ways for people to do well at them at a lower cost</strong></p>\n<p>For example, we investigated <a href=\"/lw/jys/what_colleges_look_for_in_extracurricular/\">what colleges look for in extracurricular activities</a> with a view toward finding ways in which high school students can build their human capital and contribute social value without it coming at the cost of decreased chances of getting into prestigious colleges.</p>", "sections": [{"title": "We need to provide a service that people find useful in order to help people create social value (whether by design or incidentally)", "anchor": "We_need_to_provide_a_service_that_people_find_useful_in_order_to_help_people_create_social_value__whether_by_design_or_incidentally_", "level": 1}, {"title": "It's hard to help people without helping them win zero sum competitions, and that's ok", "anchor": "It_s_hard_to_help_people_without_helping_them_win_zero_sum_competitions__and_that_s_ok", "level": 1}, {"title": "Providing public information on how to win zero sum competitions levels the playing field, which is on balance positive", "anchor": "Providing_public_information_on_how_to_win_zero_sum_competitions_levels_the_playing_field__which_is_on_balance_positive", "level": 1}, {"title": "Our resources devoted to primarily zero-sum aspects are small", "anchor": "Our_resources_devoted_to_primarily_zero_sum_aspects_are_small", "level": 1}, {"title": "Our research on zero-sum competitions is largely undertaken with a view toward finding ways for people to do well at them at a lower cost", "anchor": "Our_research_on_zero_sum_competitions_is_largely_undertaken_with_a_view_toward_finding_ways_for_people_to_do_well_at_them_at_a_lower_cost", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gPhKgMoGsj9STjvgf", "eEuR3GKjPRtnKTbJF", "rWxWsa5n93Qae7gZx", "keLuabJ8Pierta26T", "sagbSbm3KdH6tHQKP", "NPjMTNNpstXBSyAu6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T02:24:44.665Z", "modifiedAt": null, "url": null, "title": "Positive Queries - How Fetching", "slug": "positive-queries-how-fetching", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:40.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EgHDAa5j5NAxS9Zyb/positive-queries-how-fetching", "pageUrlRelative": "/posts/EgHDAa5j5NAxS9Zyb/positive-queries-how-fetching", "linkUrl": "https://www.lesswrong.com/posts/EgHDAa5j5NAxS9Zyb/positive-queries-how-fetching", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Positive%20Queries%20-%20How%20Fetching&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APositive%20Queries%20-%20How%20Fetching%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEgHDAa5j5NAxS9Zyb%2Fpositive-queries-how-fetching%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Positive%20Queries%20-%20How%20Fetching%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEgHDAa5j5NAxS9Zyb%2Fpositive-queries-how-fetching", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEgHDAa5j5NAxS9Zyb%2Fpositive-queries-how-fetching", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2021, "htmlBody": "<p>&nbsp;</p>\n<blockquote class=\"twitter-tweet\" style=\"quotes: none; font-style: italic; margin: 24px 40px; lang=\">\n<p style=\"margin: 0px 0px 24px;\">Help, having a brain blank. I can come up w examples of times something happened, but not times something didnt-happen. What heuristic?</p>\n<p style=\"margin: 0px 0px 24px;\">&mdash; Kate Donovan (@donovanable) <a href=\"https://twitter.com/donovanable/statuses/461227618725924864\">April 29, 2014</a></p>\n</blockquote>\n<p style=\"margin: 0px 0px 24px;\">If I tell 100 people not to think of an elephant, what's the single thing they're all most likely to think about over the next five minutes, aside from sex?</p>\n<p style=\"margin: 0px 0px 24px;\">An elephant, of course.</p>\n<p style=\"margin: 0px 0px 24px;\">Negation and oppositeness are perfectly intelligible semantic concepts - in general, no one is confused about what \"Don't think of an elephant\" means - or, more generally, \"Don't do [X],\" where X is any intelligible behavior. And people would know how to comply, if [X] were a physical action like sitting down. But even if they wanted to, they don't know how to not think of an elephant - even though that's a behavior they exhibit most of their waking lives, and in some sense on purpose.</p>\n<p style=\"margin: 0px 0px 24px;\">Even for physical actions we are not only admonished to refrain from, but have a strong personal interest in not doing, we feel an <a href=\"http://en.wikipedia.org/wiki/The_Imp_of_the_Perverse_(short_story)\">impulse</a> to do them anyway. Standing on a narrow ledge, afraid of falling, you might feel a strong urge to jump. Why?</p>\n<p style=\"margin: 0px 0px 24px;\">Because a part of your mind that is trying to take care of you is thinking, as hard as it can, \"Don't jump!\" And there's another part of your mind, whose job it is to fetch ideas related to the things you're interested in. This fetcher doesn't understand words like \"don't,\" but it does understand that you're very interested in the idea of jumping off that ledge, so it helpfully suggests ways to do so.</p>\n<p style=\"margin: 0px 0px 24px;\">Oops.</p>\n<p style=\"margin: 0px 0px 24px;\">This can be a big problem if you're trying to find ways not to do something, or for something not to happen.</p>\n<blockquote style=\"margin: 0px 0px 24px;\">It is not possible to find ways for something not to happen.</blockquote>\n<p style=\"margin: 0px 0px 24px;\">Knowing this, how should we use our brains differently than we did before? For obvious reasons, I am not just going to tell you to avoid thinking of the things you want in terms of negations. Instead, I'm going to tell you some stories of how I used techniques designed with this in mind, to win at life.</p>\n<p style=\"margin: 0px 0px 24px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 24px;\"><strong><span style=\"text-decoration: underline;\">The Case of the Missing Car Keys</span></strong></p>\n<p style=\"margin: 0px 0px 24px;\">A few days ago, I was on my way to an eagerly anticipated debate presided over by the incomparable <a href=\"http://www.patheos.com/blogs/unequallyyoked/\">Leah</a>. I had gotten my scheduled prior weekend chores out of the way, and even had time to stop by the local Le Pain Quotidien for a leisurely brunch (for which the service was no more intolerably slow than usual, but this time they apologized without prompting and comped about half the meal), and read a chapter of <a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501\">Global Catastrophic Risks</a>. In short, everything was going horribly right. Right in precisely that way that makes the bad news so upsetting by contrast.</p>\n<p style=\"margin: 0px 0px 24px;\">This was the day I discovered that I am not smart enough to hold onto car keys, but I am smart enough to avoid getting defensive and starting a fight about it. They fell out of my pocket, either on the sidewalk or at the restaurant, or at the Whole Foods where I had plenty of time to pick up snacks for the event. I retraced my steps and asked after the keys at both places I'd been. No luck. I got back to the debate location just in time, and despondent. It didn't ruin the debate for me, since that was a pleasant and engrossing distraction with lots of happy people talking about interesting things, but afterwards I had to ask my girlfriend to come bring me the spare key so I could bring the car home.</p>\n<p style=\"margin: 0px 0px 24px;\">Not only was I upset that I lost time waiting for the keys, and feeling bad about myself for losing them, and anticipating the hassle of going to the dealer to get another extra key (if that's even possible) - but I also put my girlfriend in a bad mood, which made me expect to be criticized for losing the keys. My brain was looking for ways to preemptively blame her. (There were plausible ways to argue it, but nothing that could be accurately described as her fault to anyone except my increasingly desperate defensive brain.)</p>\n<p style=\"margin: 0px 0px 24px;\">I managed to suppress that particular comment preemptively blaming her, but on the car ride home, she brought up a few more things that could have turned into fights. But I (just barely) managed to say, \"let's talk about these things if you still think that's a problem when we're both in better moods.\"</p>\n<p style=\"margin: 0px 0px 24px;\">Haha, fightbrain, YOU LOSE! (For now.)</p>\n<p style=\"margin: 0px 0px 24px;\">I would have totally failed at this as recently as a couple of months ago. What changed?</p>\n<p style=\"margin: 0px 0px 24px;\">Well, over the past few months, I've been meditating for about 10 minutes a day, on average. More recently I even set up a <a href=\"https://www.beeminder.com/benquo/goals/meditate\">Beeminder goal</a> for this. I'm not meditating for spiritual insights or inner calm - I'm meditating to train my mind to do what I want. In particular, I'm practicing this pattern:</p>\n<blockquote style=\"margin: 0px 0px 24px;\">Me: I'm going to focus on X.<br />My brain: Y! Y! Y Y Y Y Y Y Y Y!<br />Me: I notice that I'm thinking about Y. Now let's think about X.</blockquote>\n<p style=\"margin: 0px 0px 24px;\">Over and over again, for as long as it takes. Not fighting the passing thought - not responding to \"Y\" with \"not-Y\" (which as we now know just gets parsed as \"Y\") - but gently redirecting my attention back to X, where X can be the feeling of my breath as it moves through the bottom of my nostrils, or the task of bringing the car safely home.</p>\n<p style=\"margin: 0px 0px 24px;\">I still had to expend some WILLPOWER, which is evil, and means I'm not as good at this as I want to be, but in the past I would have lost and picked a fight. This time I won, and put off the conversations about what happened and what needed to change until I could engage productively.</p>\n<p style=\"margin: 0px 0px 24px;\">Another thing I did in between getting upset and having a calm conversation about the keys, was talk with people whom my brain did not want to get mad at. People totally uninvolved with the conflict. This got my brain into a mode of thinking about my losing the car keys that had nothing to do with blaming or being blamed or defending or attacking - I was just explaining what happened and thinking about how I could hold onto my car keys better in the future.</p>\n<p style=\"margin: 0px 0px 24px;\">(If you have ideas, I want to hear them! My pocket obviously isn't reliable. I'm likely enough to lose a bag that it's no better. A carabiner can come off, and a regular clip is even worse. I've considered using a combination padlock to hold the keys onto my belt, but that seems more hassleful than it's worth. )</p>\n<p style=\"margin: 0px 0px 24px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 24px;\"><span style=\"text-decoration: underline;\"><strong>How I Come Up With Ideas When I Can't Come Up With Any Ideas</strong></span></p>\n<p style=\"margin: 0px 0px 24px;\">Let's say I have something I want to do, and I can't think of any good ways it can be done. Like improving my emotional vocabulary - I want to figure out what exercises I can do that will increase the number of emotions I can recognize and name in the moment, and the rate at which I remember them afterwards. At first I thought I couldn't think of anything good.</p>\n<p style=\"margin: 0px 0px 24px;\">Then I tried to come up with ten terrible ideas.</p>\n<p style=\"margin: 0px 0px 24px;\">My working model of how this happens is that I implicitly have a <a href=\"http://en.wikipedia.org/wiki/Stack_(abstract_data_type)\">stack</a> of ideas, and my idea-fetcher assumes that the top of the stack is probably the best idea, so when I query my mind for \"ideas about how to do X\" the fetcher inspects the top item, finds it terrible, and decides that there are no ideas. If I ask again, the fetcher goes back to the stack, inspects the same top item, judges it unacceptable, and returns \"no results\" again.</p>\n<p style=\"margin: 0px 0px 24px;\">So why does asking for terrible ideas fix this? Because it's not actually possible to query my mind for terrible ideas. Appending the word \"terrible\" doesn't actually suppress the good ideas - it just stops me from suppressing the bad ones. And once I've retrieved the top idea from the stack (even though it often is pretty terrible), my fetcher will turn up something different when I query it again. So I can inspect the second, and third, etc. Often, in my list of ten \"terrible\" ideas, some will obviously be good ones, and some others will be bad but improvable. And you can make a lot more improvements to a bad idea you are considering, than a bad idea you aren't even thinking of.</p>\n<p style=\"margin: 0px 0px 24px;\">A few months ago, I asked <a href=\"http://reflectivedisequilibrium.blogspot.com/\">Carl Shulman</a> for ideas about how to build the forecasting and reasoning skills necessary to judge the importance of different existential risks, and he gave me about fifteen different really good ideas in about five minutes. It felt like magic, and I regret to report that at the time, it didn't occur to me to ask him how he was so good at coming up with ideas. But I think he was just using some version of this technique - at any rate, looking back, it doesn't feel like it would have been impossible for me to come up with those ideas anymore. My <a href=\"http://hpmor.com/chapter/16\">censors are off</a>. I have the Intent To Solve The Problem. I will accept even terrible ideas.</p>\n<p style=\"margin: 0px 0px 24px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 24px;\"><strong><span style=\"text-decoration: underline;\">Swim Parallel to the Shore</span></strong></p>\n<p style=\"margin: 0px 0px 24px;\">Let's say I am going into a social interaction and am nervous that it will be awkward because I'm not good with strangers. We now know that \"don't be awkward\" is not a query that will produce useful plans. Even \"be socially skilled\" is a problem - if you're worried about being awkward, you don't necessarily have a strong and vivid an image of what a generic successful conversation looks like - but you sure know what an awkward one looks like. Even if the explicit verbal instruction you give your mind is \"tell me how to be socially skilled in this conversation,\" it will get parsed as \"tell me how to be not awkward\" and your fetcher will in turn parse that as \"be awkward\" and helpfully suggest ways to accomplish that goal.</p>\n<p style=\"margin: 0px 0px 24px;\">Instead, you might want to make the other person laugh, or get some information from them, or ask them for a favor, or just let them know that you like them and want to be their friend. Pick a goal - or more than one - that is sideways relative to awkwardness, and optimize for that. Your conversation won't be perfect, but it will be a lot less awkward than if you spend all your energy thinking about how to be awkward.</p>\n<p style=\"margin: 0px 0px 24px;\">Do the same thing you're supposed to do when you're swimming in the ocean, and the undertow threatens to draw you out to sea. They don't just tell you not to fight the tide, though - they tell you to swim orthogonally to it, parallel to the shore. Pick a new direction, and optimize for that.</p>\n<p style=\"margin: 0px 0px 24px;\"><strong><br /><span style=\"text-decoration: underline;\">An Alternative Approach: Flip The Sign</span></strong></p>\n<p style=\"margin: 0px 0px 24px;\"><a href=\"http://freethoughtblogs.com/gruntled/2014/04/29/aversive-and-achievement-goals/\">Kate</a> unsurprisingly has her own interesting take on this. She talks about flipping ideas around so if you don't want X, then you can create a positive goal that's the complement of X. For example, she turns the aversive goal \"I don&rsquo;t want to be the sort of person who avoids things because they&rsquo;re emotionally weighty\" into the positive goal \"I want to be the sort of person who tackles emotionally weighty conflicts\".</p>\n<p style=\"margin: 0px 0px 24px;\">I think this is likely to be a problem because your brain may be stupid but it's also smart. It can sometimes tell when your oh-so-positive wording is just a tricky way of circumlocuting a negation. I'd expect more success with something like, \"I want to be <em>compassionate</em> during emotionally weighty conflicts,\" since that goal pushes sideways, not against the aversion.</p>\n<p style=\"margin: 0px 0px 24px;\">You the reader should be happy we disagree, since it means you're more likely to have found a technique that will work for you. If one of our ideas doesn't work for you, try the other. If one works, try the other anyway. <a href=\"/lw/5a5/no_seriously_just_try_it/\">Try</a> <a href=\"/lw/4ln/go_try_things/\">lots</a> of <a href=\"/lw/jgy/try_more_things/\">things</a>! Then keep doing the ones that work.</p>\n<p style=\"margin: 0px 0px 24px;\"><a href=\"http://benjaminrosshoffman.com/positive-queries-how-fetching/\">cross-posted</a> at my <a href=\"http://www.benjaminrosshoffman.com\">personal blog</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2wjPMY34by2gXEXA2": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EgHDAa5j5NAxS9Zyb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 36, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "26115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<blockquote class=\"twitter-tweet\" style=\"quotes: none; font-style: italic; margin: 24px 40px; lang=\">\n<p style=\"margin: 0px 0px 24px;\">Help, having a brain blank. I can come up w examples of times something happened, but not times something didnt-happen. What heuristic?</p>\n<p style=\"margin: 0px 0px 24px;\">\u2014 Kate Donovan (@donovanable) <a href=\"https://twitter.com/donovanable/statuses/461227618725924864\">April 29, 2014</a></p>\n</blockquote>\n<p style=\"margin: 0px 0px 24px;\">If I tell 100 people not to think of an elephant, what's the single thing they're all most likely to think about over the next five minutes, aside from sex?</p>\n<p style=\"margin: 0px 0px 24px;\">An elephant, of course.</p>\n<p style=\"margin: 0px 0px 24px;\">Negation and oppositeness are perfectly intelligible semantic concepts - in general, no one is confused about what \"Don't think of an elephant\" means - or, more generally, \"Don't do [X],\" where X is any intelligible behavior. And people would know how to comply, if [X] were a physical action like sitting down. But even if they wanted to, they don't know how to not think of an elephant - even though that's a behavior they exhibit most of their waking lives, and in some sense on purpose.</p>\n<p style=\"margin: 0px 0px 24px;\">Even for physical actions we are not only admonished to refrain from, but have a strong personal interest in not doing, we feel an <a href=\"http://en.wikipedia.org/wiki/The_Imp_of_the_Perverse_(short_story)\">impulse</a> to do them anyway. Standing on a narrow ledge, afraid of falling, you might feel a strong urge to jump. Why?</p>\n<p style=\"margin: 0px 0px 24px;\">Because a part of your mind that is trying to take care of you is thinking, as hard as it can, \"Don't jump!\" And there's another part of your mind, whose job it is to fetch ideas related to the things you're interested in. This fetcher doesn't understand words like \"don't,\" but it does understand that you're very interested in the idea of jumping off that ledge, so it helpfully suggests ways to do so.</p>\n<p style=\"margin: 0px 0px 24px;\">Oops.</p>\n<p style=\"margin: 0px 0px 24px;\">This can be a big problem if you're trying to find ways not to do something, or for something not to happen.</p>\n<blockquote style=\"margin: 0px 0px 24px;\">It is not possible to find ways for something not to happen.</blockquote>\n<p style=\"margin: 0px 0px 24px;\">Knowing this, how should we use our brains differently than we did before? For obvious reasons, I am not just going to tell you to avoid thinking of the things you want in terms of negations. Instead, I'm going to tell you some stories of how I used techniques designed with this in mind, to win at life.</p>\n<p style=\"margin: 0px 0px 24px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 24px;\"><strong id=\"The_Case_of_the_Missing_Car_Keys\"><span style=\"text-decoration: underline;\">The Case of the Missing Car Keys</span></strong></p>\n<p style=\"margin: 0px 0px 24px;\">A few days ago, I was on my way to an eagerly anticipated debate presided over by the incomparable <a href=\"http://www.patheos.com/blogs/unequallyyoked/\">Leah</a>. I had gotten my scheduled prior weekend chores out of the way, and even had time to stop by the local Le Pain Quotidien for a leisurely brunch (for which the service was no more intolerably slow than usual, but this time they apologized without prompting and comped about half the meal), and read a chapter of <a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501\">Global Catastrophic Risks</a>. In short, everything was going horribly right. Right in precisely that way that makes the bad news so upsetting by contrast.</p>\n<p style=\"margin: 0px 0px 24px;\">This was the day I discovered that I am not smart enough to hold onto car keys, but I am smart enough to avoid getting defensive and starting a fight about it. They fell out of my pocket, either on the sidewalk or at the restaurant, or at the Whole Foods where I had plenty of time to pick up snacks for the event. I retraced my steps and asked after the keys at both places I'd been. No luck. I got back to the debate location just in time, and despondent. It didn't ruin the debate for me, since that was a pleasant and engrossing distraction with lots of happy people talking about interesting things, but afterwards I had to ask my girlfriend to come bring me the spare key so I could bring the car home.</p>\n<p style=\"margin: 0px 0px 24px;\">Not only was I upset that I lost time waiting for the keys, and feeling bad about myself for losing them, and anticipating the hassle of going to the dealer to get another extra key (if that's even possible) - but I also put my girlfriend in a bad mood, which made me expect to be criticized for losing the keys. My brain was looking for ways to preemptively blame her. (There were plausible ways to argue it, but nothing that could be accurately described as her fault to anyone except my increasingly desperate defensive brain.)</p>\n<p style=\"margin: 0px 0px 24px;\">I managed to suppress that particular comment preemptively blaming her, but on the car ride home, she brought up a few more things that could have turned into fights. But I (just barely) managed to say, \"let's talk about these things if you still think that's a problem when we're both in better moods.\"</p>\n<p style=\"margin: 0px 0px 24px;\">Haha, fightbrain, YOU LOSE! (For now.)</p>\n<p style=\"margin: 0px 0px 24px;\">I would have totally failed at this as recently as a couple of months ago. What changed?</p>\n<p style=\"margin: 0px 0px 24px;\">Well, over the past few months, I've been meditating for about 10 minutes a day, on average. More recently I even set up a <a href=\"https://www.beeminder.com/benquo/goals/meditate\">Beeminder goal</a> for this. I'm not meditating for spiritual insights or inner calm - I'm meditating to train my mind to do what I want. In particular, I'm practicing this pattern:</p>\n<blockquote style=\"margin: 0px 0px 24px;\">Me: I'm going to focus on X.<br>My brain: Y! Y! Y Y Y Y Y Y Y Y!<br>Me: I notice that I'm thinking about Y. Now let's think about X.</blockquote>\n<p style=\"margin: 0px 0px 24px;\">Over and over again, for as long as it takes. Not fighting the passing thought - not responding to \"Y\" with \"not-Y\" (which as we now know just gets parsed as \"Y\") - but gently redirecting my attention back to X, where X can be the feeling of my breath as it moves through the bottom of my nostrils, or the task of bringing the car safely home.</p>\n<p style=\"margin: 0px 0px 24px;\">I still had to expend some WILLPOWER, which is evil, and means I'm not as good at this as I want to be, but in the past I would have lost and picked a fight. This time I won, and put off the conversations about what happened and what needed to change until I could engage productively.</p>\n<p style=\"margin: 0px 0px 24px;\">Another thing I did in between getting upset and having a calm conversation about the keys, was talk with people whom my brain did not want to get mad at. People totally uninvolved with the conflict. This got my brain into a mode of thinking about my losing the car keys that had nothing to do with blaming or being blamed or defending or attacking - I was just explaining what happened and thinking about how I could hold onto my car keys better in the future.</p>\n<p style=\"margin: 0px 0px 24px;\">(If you have ideas, I want to hear them! My pocket obviously isn't reliable. I'm likely enough to lose a bag that it's no better. A carabiner can come off, and a regular clip is even worse. I've considered using a combination padlock to hold the keys onto my belt, but that seems more hassleful than it's worth. )</p>\n<p style=\"margin: 0px 0px 24px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 24px;\"><span style=\"text-decoration: underline;\"><strong>How I Come Up With Ideas When I Can't Come Up With Any Ideas</strong></span></p>\n<p style=\"margin: 0px 0px 24px;\">Let's say I have something I want to do, and I can't think of any good ways it can be done. Like improving my emotional vocabulary - I want to figure out what exercises I can do that will increase the number of emotions I can recognize and name in the moment, and the rate at which I remember them afterwards. At first I thought I couldn't think of anything good.</p>\n<p style=\"margin: 0px 0px 24px;\">Then I tried to come up with ten terrible ideas.</p>\n<p style=\"margin: 0px 0px 24px;\">My working model of how this happens is that I implicitly have a <a href=\"http://en.wikipedia.org/wiki/Stack_(abstract_data_type)\">stack</a> of ideas, and my idea-fetcher assumes that the top of the stack is probably the best idea, so when I query my mind for \"ideas about how to do X\" the fetcher inspects the top item, finds it terrible, and decides that there are no ideas. If I ask again, the fetcher goes back to the stack, inspects the same top item, judges it unacceptable, and returns \"no results\" again.</p>\n<p style=\"margin: 0px 0px 24px;\">So why does asking for terrible ideas fix this? Because it's not actually possible to query my mind for terrible ideas. Appending the word \"terrible\" doesn't actually suppress the good ideas - it just stops me from suppressing the bad ones. And once I've retrieved the top idea from the stack (even though it often is pretty terrible), my fetcher will turn up something different when I query it again. So I can inspect the second, and third, etc. Often, in my list of ten \"terrible\" ideas, some will obviously be good ones, and some others will be bad but improvable. And you can make a lot more improvements to a bad idea you are considering, than a bad idea you aren't even thinking of.</p>\n<p style=\"margin: 0px 0px 24px;\">A few months ago, I asked <a href=\"http://reflectivedisequilibrium.blogspot.com/\">Carl Shulman</a> for ideas about how to build the forecasting and reasoning skills necessary to judge the importance of different existential risks, and he gave me about fifteen different really good ideas in about five minutes. It felt like magic, and I regret to report that at the time, it didn't occur to me to ask him how he was so good at coming up with ideas. But I think he was just using some version of this technique - at any rate, looking back, it doesn't feel like it would have been impossible for me to come up with those ideas anymore. My <a href=\"http://hpmor.com/chapter/16\">censors are off</a>. I have the Intent To Solve The Problem. I will accept even terrible ideas.</p>\n<p style=\"margin: 0px 0px 24px;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 24px;\"><strong id=\"Swim_Parallel_to_the_Shore\"><span style=\"text-decoration: underline;\">Swim Parallel to the Shore</span></strong></p>\n<p style=\"margin: 0px 0px 24px;\">Let's say I am going into a social interaction and am nervous that it will be awkward because I'm not good with strangers. We now know that \"don't be awkward\" is not a query that will produce useful plans. Even \"be socially skilled\" is a problem - if you're worried about being awkward, you don't necessarily have a strong and vivid an image of what a generic successful conversation looks like - but you sure know what an awkward one looks like. Even if the explicit verbal instruction you give your mind is \"tell me how to be socially skilled in this conversation,\" it will get parsed as \"tell me how to be not awkward\" and your fetcher will in turn parse that as \"be awkward\" and helpfully suggest ways to accomplish that goal.</p>\n<p style=\"margin: 0px 0px 24px;\">Instead, you might want to make the other person laugh, or get some information from them, or ask them for a favor, or just let them know that you like them and want to be their friend. Pick a goal - or more than one - that is sideways relative to awkwardness, and optimize for that. Your conversation won't be perfect, but it will be a lot less awkward than if you spend all your energy thinking about how to be awkward.</p>\n<p style=\"margin: 0px 0px 24px;\">Do the same thing you're supposed to do when you're swimming in the ocean, and the undertow threatens to draw you out to sea. They don't just tell you not to fight the tide, though - they tell you to swim orthogonally to it, parallel to the shore. Pick a new direction, and optimize for that.</p>\n<p style=\"margin: 0px 0px 24px;\"><strong id=\"An_Alternative_Approach__Flip_The_Sign\"><br><span style=\"text-decoration: underline;\">An Alternative Approach: Flip The Sign</span></strong></p>\n<p style=\"margin: 0px 0px 24px;\"><a href=\"http://freethoughtblogs.com/gruntled/2014/04/29/aversive-and-achievement-goals/\">Kate</a> unsurprisingly has her own interesting take on this. She talks about flipping ideas around so if you don't want X, then you can create a positive goal that's the complement of X. For example, she turns the aversive goal \"I don\u2019t want to be the sort of person who avoids things because they\u2019re emotionally weighty\" into the positive goal \"I want to be the sort of person who tackles emotionally weighty conflicts\".</p>\n<p style=\"margin: 0px 0px 24px;\">I think this is likely to be a problem because your brain may be stupid but it's also smart. It can sometimes tell when your oh-so-positive wording is just a tricky way of circumlocuting a negation. I'd expect more success with something like, \"I want to be <em>compassionate</em> during emotionally weighty conflicts,\" since that goal pushes sideways, not against the aversion.</p>\n<p style=\"margin: 0px 0px 24px;\">You the reader should be happy we disagree, since it means you're more likely to have found a technique that will work for you. If one of our ideas doesn't work for you, try the other. If one works, try the other anyway. <a href=\"/lw/5a5/no_seriously_just_try_it/\">Try</a> <a href=\"/lw/4ln/go_try_things/\">lots</a> of <a href=\"/lw/jgy/try_more_things/\">things</a>! Then keep doing the ones that work.</p>\n<p style=\"margin: 0px 0px 24px;\"><a href=\"http://benjaminrosshoffman.com/positive-queries-how-fetching/\">cross-posted</a> at my <a href=\"http://www.benjaminrosshoffman.com\">personal blog</a></p>", "sections": [{"title": "The Case of the Missing Car Keys", "anchor": "The_Case_of_the_Missing_Car_Keys", "level": 1}, {"title": "Swim Parallel to the Shore", "anchor": "Swim_Parallel_to_the_Shore", "level": 1}, {"title": "An Alternative Approach: Flip The Sign", "anchor": "An_Alternative_Approach__Flip_The_Sign", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Zmfo388RA9oky3KYe", "ADaZaEsmJMnKKhRqS", "ZzCxs2AFThcTfFeKr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T05:26:14.683Z", "modifiedAt": null, "url": null, "title": "Link: Study finds that using a foreign language changes moral decisions", "slug": "link-study-finds-that-using-a-foreign-language-changes-moral", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:24.115Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Golovin", "createdAt": "2009-02-28T13:32:31.085Z", "isAdmin": false, "displayName": "Vladimir_Golovin"}, "userId": "Me2m84AhCn9H49riY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PSD4LMnonitoJW6MP/link-study-finds-that-using-a-foreign-language-changes-moral", "pageUrlRelative": "/posts/PSD4LMnonitoJW6MP/link-study-finds-that-using-a-foreign-language-changes-moral", "linkUrl": "https://www.lesswrong.com/posts/PSD4LMnonitoJW6MP/link-study-finds-that-using-a-foreign-language-changes-moral", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Study%20finds%20that%20using%20a%20foreign%20language%20changes%20moral%20decisions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Study%20finds%20that%20using%20a%20foreign%20language%20changes%20moral%20decisions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSD4LMnonitoJW6MP%2Flink-study-finds-that-using-a-foreign-language-changes-moral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Study%20finds%20that%20using%20a%20foreign%20language%20changes%20moral%20decisions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSD4LMnonitoJW6MP%2Flink-study-finds-that-using-a-foreign-language-changes-moral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSD4LMnonitoJW6MP%2Flink-study-finds-that-using-a-foreign-language-changes-moral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<blockquote>\n<p><span style=\"color: #070809; font-family: Helvetica, Arial, sans-serif; font-size: 13px; line-height: 15.600000381469727px;\"><em>In the new study, two experiments using the well-known \"trolley dilemma\" tested the hypothesis that when faced with moral choices in a foreign language, people are more likely to respond with a utilitarian approach that is less emotional.</em></span></p>\n<p><span style=\"color: #070809; font-family: Helvetica, Arial, sans-serif; font-size: 13px; line-height: 15.600000381469727px;\"><em>The researchers collected data from people in the U.S., Spain, Korea, France and Israel. Across all populations, more participants selected the utilitarian choice -- to save five by killing one -- when the dilemmas were presented in the foreign language than when they did the problem in their native tongue.</em></span></p>\n</blockquote>\n<p><span style=\"font-family: Helvetica, Arial, sans-serif; color: #070809;\"><span style=\"line-height: 15.600000381469727px;\">The article:<br /></span></span>http://www.sciencedaily.com/releases/2014/04/140428120659.htm</p>\n<p><span style=\"font-family: Helvetica, Arial, sans-serif; color: #070809;\"><span style=\"line-height: 15.600000381469727px;\">The publication:<br /></span></span>http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0094842</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PSD4LMnonitoJW6MP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 1.697472511762359e-06, "legacy": true, "legacyId": "26118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T07:35:52.711Z", "modifiedAt": null, "url": null, "title": "Summary of the first SoCal FAI Workshop", "slug": "summary-of-the-first-socal-fai-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:05.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xbL384ciahPehxxLx/summary-of-the-first-socal-fai-workshop", "pageUrlRelative": "/posts/xbL384ciahPehxxLx/summary-of-the-first-socal-fai-workshop", "linkUrl": "https://www.lesswrong.com/posts/xbL384ciahPehxxLx/summary-of-the-first-socal-fai-workshop", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Summary%20of%20the%20first%20SoCal%20FAI%20Workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASummary%20of%20the%20first%20SoCal%20FAI%20Workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxbL384ciahPehxxLx%2Fsummary-of-the-first-socal-fai-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Summary%20of%20the%20first%20SoCal%20FAI%20Workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxbL384ciahPehxxLx%2Fsummary-of-the-first-socal-fai-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxbL384ciahPehxxLx%2Fsummary-of-the-first-socal-fai-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 496, "htmlBody": "<p>Last Saturday, nine people met for the Southern California FAI Workshop. Unsurprisingly, we did not come up with any major results, but I know some people were curious about this experiment, so I a providing a summary anyway.&nbsp;</p>\n<p>First, I would like to say that I consider this first meeting a success. The turnout was higher than I expected. We had 9 participants, and there were 2 other people who did not show up due to scheduling conflicts. We basically stayed on topic the entire 7 hours from 10:00 to 5:00, and then we had dinner at 5:00, generously provided by MIRI. We will be hosting these workshops again. In fact, we have decided to hold them monthly. We will probably follow a schedule of meeting the first Saturday of each month, starting in June. I will make another post announcing the second meetup once this date is finalized.&nbsp;</p>\n<p>We talked about various ideas participants had about FAI, but most of our time was spent thinking about probability distributions on consistent theories. One thing we observed that if you view the space of all probability assignments to logical sentences as living inside the vector space of all functions from sentences to the real numbers, then the collection of coherent probability assignments (those which correspond to probability distributions on consistent theories), is an affine subspace. This is exciting, because we can set up an inner product on this vector space and orthogonally project probability assignments onto the closest point on this subspace (i.e. find a nearby coherent probability assignment to a given probability assignment). Further, while this projection is not computable, there exists a computable procedure which converges to this point. However, I am now convinced that this idea is a dead end, for the following reason: Just because the point you start with has all coordinates between 0 and 1, does not mean that the projection to the subspace containing coherent assignments still has all coordinates between 0 and 1. (Imagine a 3d unit cube, and imagine that a theory is coherent if x+y+z=1. If you project 1,1,0 onto this subspace, you get 2/3,2/3,-1/3, which is not a valid probability assignment) I am now convinced that this idea will not be fruitful.</p>\n<p>However, we did get several good things out of the meeting. First, we introduced several new mathy people to the problems associated with FAI. Second, we set up an email list, so that we can bounce ideas we have off of people that we know personally and who are interested in this stuff. Third, and most importantly we have become excited about doing more. I personally spent most the day after the workshop writing up lots of stuff related to what we observed above (This was before I discovered that it did not work), and I know I am not the only one to have this reaction.</p>\n<p>Thanks to all of the participants, and please let me know if you would be interested in joining us next time!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xbL384ciahPehxxLx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 1.6976461872720284e-06, "legacy": true, "legacyId": "26123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T13:34:43.181Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup", "slug": "meetup-london-social-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L9ckM4c3uQr3XNpXR/meetup-london-social-meetup-1", "pageUrlRelative": "/posts/L9ckM4c3uQr3XNpXR/meetup-london-social-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/L9ckM4c3uQr3XNpXR/meetup-london-social-meetup-1", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL9ckM4c3uQr3XNpXR%2Fmeetup-london-social-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL9ckM4c3uQr3XNpXR%2Fmeetup-london-social-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL9ckM4c3uQr3XNpXR%2Fmeetup-london-social-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zt'>London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our weekly meetings continue! As usual, we'll be meeting in the Shakespeare's Head in Holborn, from 2pm until we get bored (which is almost always after 6pm, but some people tend to leave sooner).</p>\n\n<p>If you have difficulty finding us, you can contact me on 07792009646.</p>\n\n<p>See also: London LW <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> and <a href=\"https://www.facebook.com/groups/380103898766356\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zt'>London social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L9ckM4c3uQr3XNpXR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "26124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/zt\">London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our weekly meetings continue! As usual, we'll be meeting in the Shakespeare's Head in Holborn, from 2pm until we get bored (which is almost always after 6pm, but some people tend to leave sooner).</p>\n\n<p>If you have difficulty finding us, you can contact me on 07792009646.</p>\n\n<p>See also: London LW <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> and <a href=\"https://www.facebook.com/groups/380103898766356\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/zt\">London social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T16:21:31.652Z", "modifiedAt": null, "url": null, "title": "[Sequence announcement] Introduction to Mechanism Design", "slug": "sequence-announcement-introduction-to-mechanism-design", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:07.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6THwih6NrvS4uaHkH/sequence-announcement-introduction-to-mechanism-design", "pageUrlRelative": "/posts/6THwih6NrvS4uaHkH/sequence-announcement-introduction-to-mechanism-design", "linkUrl": "https://www.lesswrong.com/posts/6THwih6NrvS4uaHkH/sequence-announcement-introduction-to-mechanism-design", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSequence%20announcement%5D%20Introduction%20to%20Mechanism%20Design&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSequence%20announcement%5D%20Introduction%20to%20Mechanism%20Design%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6THwih6NrvS4uaHkH%2Fsequence-announcement-introduction-to-mechanism-design%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSequence%20announcement%5D%20Introduction%20to%20Mechanism%20Design%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6THwih6NrvS4uaHkH%2Fsequence-announcement-introduction-to-mechanism-design", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6THwih6NrvS4uaHkH%2Fsequence-announcement-introduction-to-mechanism-design", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Mechanism_design\">Mechanism design</a> is the theory of how to construct institutions for strategic agents, spanning applications like voting systems, school admissions, regulation of monopolists, and auction design. Think of it as the engineering side of game theory, building algorithms for strategic agents. While it doesn't have much to say about rationality directly, mechanism design provides tools and results for anyone interested in world optimization.</p>\n<p>In this sequence, I'll touch on</p>\n<ul>\n<li>The basic mechanism design framework, including the <a href=\"http://en.wikipedia.org/wiki/Mechanism_design#Revelation_principle\">revelation principle</a> and incentive compatibility.</li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Gibbard-Satterthwaite_theorem\">Gibbard-Satterthwaite impossibility theorem</a> for strategyproof implementation (a close analogue of Arrow's Theorem), and restricted domains like single-peaked or quasilinear preference where we do have positive results.</li>\n<li>The power and limitations of <a href=\"http://en.wikipedia.org/wiki/Vickrey%E2%80%93Clarke%E2%80%93Groves_auction\">Vickrey-Clarke-Groves mechanisms</a> for efficiently allocating goods, generalizing Vickrey's second-price auction.</li>\n<li>Characterizations of incentive-compatible mechanisms and the revenue equivalence theorem.</li>\n<li>Profit-maximizing auctions.</li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Myerson-Satterthwaite_theorem\">Myerson-Satterthwaite</a> impossibility for bilateral trade.</li>\n<li>Two-sided matching markets &agrave; la <a href=\"http://en.wikipedia.org/wiki/Stable_marriage_problem\">Gale and Shapley</a>, school choice, and kidney exchange.</li>\n</ul>\n<p>As the list above suggests, this sequence is going to be semi-technical, but my foremost goal is to convey the intuition behind these results. Since mechanism design builds on game theory, take a look at Yvain's <a href=\"/lw/dbe/introduction_to_game_theory_sequence_guide/\">Game Theory Intro</a> if you want to brush up.</p>\n<p>Various resources:</p>\n<ul>\n<li>For further introduction, you can start with the <a href=\"http://www.nobelprize.org/nobel_prizes/economic-sciences/laureates/2007/popular-economicsciences2007.pdf\">popular</a> or <a href=\"http://www.nobelprize.org/nobel_prizes/economic-sciences/laureates/2007/advanced-economicsciences2007.pdf\">more scholarly survey</a> of mechanism design from the 2007 Nobel memoriam prize in economics.</li>\n<li>Jeff Ely has <a href=\"http://cheaptalk.org/jeffs-intermediate-micro-course/\">lecture notes and short videos</a> to accompany an undergraduate class in microeconomic theory from the perspective of mechanism design.</li>\n<li>The textbook <a href=\"http://www.amazon.com/Toolbox-Economic-Design-Dimitrios-Diamantaras/dp/0230610609/\">A Toolbox for Economic Design</a> by Dimitrios Diamantaras is very accessible and comprehensive if you can get ahold of a copy.</li>\n<li>Tilman B&ouml;rgers has a <a href=\"http://www-personal.umich.edu/~tborgers/TheoryOfMechanismDesign.pdf\">draft textbook</a>&nbsp;intended for graduate students.</li>\n<li>Chapters 9-16 of <a href=\"http://www.cambridge.org/journals/nisan/downloads/Nisan_Non-printable.pdf\">Algorithmic Game Theory</a>&nbsp;and chapters 10-11 of <a href=\"http://www.masfoundations.org/download.html\">Multiagent Systems</a> cover various topics in mechanism design from the perspective of computer scientists.</li>\n<li><a href=\"http://www.cs.cmu.edu/~arielpro/summer/schedule.html\">Video lectures</a> introducing market design and computational aspects of mechanism design.</li>\n</ul>\n<p>I plan on following up on this sequence with another focusing on group rationality and information aggregation, surveying scoring rules and prediction markets among other topics.</p>\n<p>Suggestions and comments are very welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 12, "xexCWMyds6QLWognu": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6THwih6NrvS4uaHkH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 90, "extendedScore": null, "score": 0.000261, "legacy": true, "legacyId": "26125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 90, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QxZs5Za4qXBegXCgu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T18:37:20.875Z", "modifiedAt": null, "url": null, "title": "Mechanism Design: Constructing Algorithms for Strategic Agents", "slug": "mechanism-design-constructing-algorithms-for-strategic", "viewCount": null, "lastCommentedAt": "2021-04-01T01:48:13.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xTvdaCwaeZnePMuX5/mechanism-design-constructing-algorithms-for-strategic", "pageUrlRelative": "/posts/xTvdaCwaeZnePMuX5/mechanism-design-constructing-algorithms-for-strategic", "linkUrl": "https://www.lesswrong.com/posts/xTvdaCwaeZnePMuX5/mechanism-design-constructing-algorithms-for-strategic", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mechanism%20Design%3A%20Constructing%20Algorithms%20for%20Strategic%20Agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMechanism%20Design%3A%20Constructing%20Algorithms%20for%20Strategic%20Agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxTvdaCwaeZnePMuX5%2Fmechanism-design-constructing-algorithms-for-strategic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mechanism%20Design%3A%20Constructing%20Algorithms%20for%20Strategic%20Agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxTvdaCwaeZnePMuX5%2Fmechanism-design-constructing-algorithms-for-strategic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxTvdaCwaeZnePMuX5%2Fmechanism-design-constructing-algorithms-for-strategic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2803, "htmlBody": "<p style=\"padding-left: 30px;\"><em>tl;dr Mechanism design studies how to design incentives for fun and profit. A puzzle about whether or not to paint a room is posed. A modeling framework is introduced, with lots of corresponding notation.</em></p>\n<p><em>Mechanism design</em> is a framework for constructing institutions for group interactions, giving us a language for the design of everything from voting systems to school admissions to auctions to crowdsourcing. Think of it as the engineering side of game theory, building algorithms for strategic agents. In game theory, the primary goal is to answer the question, &ldquo;Given agents who can take some actions that will lead to some payoffs, what do we expect to happen when the agents strategically interact?&rdquo; In other words, game theory describes the outcomes of fixed scenarios. In contrast, mechanism design flips the question around and asks, &ldquo;Given some goals, what payoffs should agents be assigned for the right outcome to occur when agents strategically interact?&rdquo; The rules of the game are ours to choose, and, within some design constraints, we want to find the best possible ones for a situation.</p>\n<p>Although many people, even <a href=\"http://arielrubinstein.tau.ac.il/papers/afterwards.pdf\">high-profile theorists</a>, doubt the usefulness of game theory, its application in mechanism design is one of the major success stories of modern economics. <a href=\"http://en.wikipedia.org/wiki/Spectrum_auction\">Spectrum license auctions</a> designed by economists paved the way for modern cell-phone networks and garnered billions in revenue for the US and European governments. Tech companies like Google and Microsoft employ theorists to improve advertising auctions. Economists like <a href=\"http://www.forbes.com/forbes/2010/0809/opinions-harvard-alvin-roth-freakonomics-ideas-opinions.html\">Al Roth</a> and computer scientists like <a href=\"http://www.sciencedaily.com/releases/2010/11/101116122859.htm\">Tuomas Sandholm</a> have been instrumental in establishing kidney exchanges to facilitate organ transplants, while others have been active in the redesign of public school admissions in Boston, Chicago, and New Orleans.</p>\n<p>The objective of this post is to introduce all the pieces of a mechanism design problem, providing the setup for actual conclusions later on. I assume you have some basic familiarity with game theory, at the level of understanding the concept of a <a href=\"http://en.wikipedia.org/wiki/Dominance_(game_theory)\">dominant strategies</a> and <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilbria</a>. Take a look at Yvain&rsquo;s <a href=\"/lw/dbe/introduction_to_game_theory_sequence_guide/\">Game Theory Intro</a> if you&rsquo;d like to brush up.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"overly-optimizing-whether-or-not-to-paint-an-room\">Overly optimizing whether or not to paint an room</h2>\n<p>Let&rsquo;s start with a concrete example of a group choice: Jack, an economics student, and Jill, a computer science student, are housemates. To procrastinate studying for finals, they are considering whether to repaint their living room. Conveniently, they agree on what color they would choose, but are unsure whether it&rsquo;s worth doing at all. Neither Jack nor Jill would pay the known fixed cost of $300 entirely on their own. They&rsquo;re not even sure the cost is less than their joint value<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>, so it&rsquo;s not only a matter of bargaining to split the cost (a non-trivial question on its own). The decision to paint the room depends on information neither person fully knows, since each knows their own value, but not the value to the other person.</p>\n<p>The lack of complete information is what makes the problem interesting. If the total value is obviously greater than $300, forcing them to paint the room and split the cost evenly would be utility-maximizing<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>. One person might be worse off, but the other would be correspondingly better off. This solution corresponds to funding a <a href=\"http://en.wikipedia.org/wiki/Public_good\">public good</a> (in the technical sense of something non-excludable and non-rivalrous) through taxation. Alternatively, if the total value is obviously less than $300, then the project shouldn&rsquo;t be done, and the question of how to split the cost becomes moot. With some overall uncertainty, we now have to worry that either housemate might misrepresent their value to get a better deal, causing the room to be painted when it shouldn&rsquo;t be or vice versa.</p>\n<p>Assuming the two want to repaint the room if and only if their total value is greater than $300, how would you advise they decide whether to do the project and how much each should contribute?</p>\n<p>Pause a moment to ponder this puzzle&hellip;</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>Some naive solutions would be to:</p>\n<ul>\n<li>Vote on painting the room, and if both say yes, do the project with each contributing $150.</li>\n<li>Vote, and if either one says yes, do the project. If both say yes, both contribute $150. If only one says yes, that person contributes $225 and the other contributes $75.</li>\n<li>Both simultaneously write down a number. If the total is greater than $300, do the project. Each contributes a share of the $300 proportional their number.</li>\n<li>Flip a coin to decide who makes an initial offer of how to split the cost, i.e.&nbsp;Jack paying $50 and Jill paying $250. The other person can accept the proposal, in which case they do the project with those contributions. Otherwise, that person makes a new proposal. Alternating proposals continue until one is accepted. If 100 rounds pass without an accepted proposal, don&rsquo;t paint the room.</li>\n</ul>\n<p>None of these will guarantee the room is painted exactly when it&rsquo;s worth the cost. The first procedure never paints the room when it shouldn&rsquo;t be, but sometimes fails to paint when it would be worthwhile, like when Jack values the renovation at $120 and Jill values it at $200. Jack would vote &ldquo;no&rdquo;, even though their total value is $320. The second procedure can make mistakes of both kinds and can also result in someone contributing more than their value. The other two are more difficult to analyze, but still turn out to be non-optimal.</p>\n<p>These protocols barely scratch the surface of all the possible institutions out there. Maybe we just need to be a little more creative to find something better. To definitively solve this problem, some formalism is in order.</p>\n<h2 id=\"framework-for-institutional-design\">Framework for institutional design</h2>\n<p style=\"padding-left: 30px;\"><em style=\"font-size: small;\">Trigger warning: Greek letters, subscripts, sets, and functions.</em></p>\n<p>Getting a little more abstract, let&rsquo;s specify all the relevant elements in an institutional design setting. First of all, the agents involved in our process need to be identified. Typically, this doesn&rsquo;t need to go beyond labeling each agent. For instance, we might assign each a number from 1 to <span class=\"math\"><em>n</em></span>, with <span class=\"math\"><em>i</em></span> representing a generic agent. Above, we have two agents named Jack and Jill.</p>\n<blockquote>\n<p><em>Notes on notation:</em> A generic agent has the label <span class=\"math\"><em>i</em></span>. A set or variable <span class=\"math\"><em>Z</em><sub><em>i</em></sub></span> belongs to agent <span class=\"math\"><em>i</em></span>. Typically, variables are lowercase, and the set a variable lives in is uppercase. Without a subscript, <span class=\"math\"><em>Z</em></span> refers to the vector <span class=\"math\"><em>Z</em> =\u2004(<em>Z</em><sub>1</sub>,\u2006&hellip;,\u2006<em>Z</em><sub><em>i</em></sub>,\u2006&hellip;,\u2006<em>Z</em><sub><em>N</em></sub>)</span> with one element for each agent. It&rsquo;s often useful to talk about the vector <span class=\"math\"><em>Z</em><sub> &minus;\u2005<em>i</em></sub> =\u2004(<em>Z</em><sub>1</sub>,\u2006&hellip;,\u2006<em>Z</em><sub><em>i</em> &minus;\u20051</sub>,\u2006<em>Z</em><sub><em>i</em> +\u20051</sub>,\u2006&hellip;,\u2006<em>Z</em><sub><em>N</em></sub>)</span> for all agents <em>except</em> agent <span class=\"math\"><em>i</em></span> (think of deleting <span class=\"math\"><em>Z</em><sub><em>i</em></sub></span> from the vector <span class=\"math\"><em>Z</em></span>). This enables us to write <span class=\"math\"><em>Z</em></span> as <span class=\"math\">(<em>Z</em><sub><em>i</em></sub>,\u2006<em>Z</em><sub> &minus;\u2005<em>i</em></sub>)</span>, highlighting agent <span class=\"math\"><em>i</em></span>&rsquo;s part in the profile.</p>\n</blockquote>\n<p>Once we&rsquo;ve established who&rsquo;s involved, the next step is determining the relevant traits of agents that aren&rsquo;t obviously known. This unknown data is summarized as that agent&rsquo;s <em>type</em>. Types can describe an agent&rsquo;s preferences, their capabilities, their beliefs about the state of the world, their beliefs about others&rsquo; types, their beliefs about others&rsquo; beliefs, etc. The set of possible types for each agent is their <em>type space</em>. A typical notation for the type of agent <span class=\"math\"><em>i</em></span> is <span class=\"math\"><em>&theta;</em><sub><em>i</em></sub></span>, an element of the type space <span class=\"math\">&Theta;\u2006<sub><em>i</em></sub></span>. If an assumption about an agent&rsquo;s preferences or beliefs seems unreasonable, that&rsquo;s a sign we should enlarge the set of possible types, allowing for more variety in behavior. In our housemate scenario, the type of each agent needs to &ndash; at a bare minimum &ndash; specify the value each puts on having new paint. If knowing that value fully specifies the person&rsquo;s preferences and beliefs, we&rsquo;re done. Otherwise, we might need to stick more information inside the person&rsquo;s type. Of course, there is a trade-off between realism and tractibility that guides the modeling choice of how to specify types.</p>\n<p>Next, we need to consider all possible outcomes that might result from the group interaction. This could be an overarching outcome, like having one candidate elected to office, or a specification of the sub-outcomes for each agent, like the job each one is assigned. Let&rsquo;s call the set of all outcomes <span class=\"math\"><em>X</em></span>. In the housemate scenario, the outcomes consist of the binary choice of whether the room is painted and the payment each person makes. Throwing some notation on this, each outcome is a triple <span class=\"math\">(<em>q</em>,\u2006<em>P</em><sub>Jack</sub>,\u2006<em>P</em><sub>Jill</sub>)\u2004&isin;\u2004<em>X</em> =\u2004{0,\u20061}\u2005&times;\u2005R\u2005&times;\u2005R</span>.</p>\n<p>To talk about the incentives of agents, their preferences over each outcome must be specified as a function of their type. In general, preferences could be any ranking of the outcomes, but we often assume a particular utility function. For instance, we might numerically represent the preferences of Jack or Jill as <span class=\"math\"><em>u</em><sub><em>i</em></sub>(<em>q</em>,\u2006<em>P</em><sub><em>i</em></sub>,\u2006<em>&theta;</em><sub><em>i</em></sub>)\u2004=\u2004<em>q</em><em>&theta;</em><sub><em>i</em></sub> &minus;\u2005<em>P</em><sub><em>i</em></sub></span>, meaning each gets benefit <span class=\"math\"><em>&theta;</em><sub><em>i</em></sub></span> if and only if the room is painted, minus their payment, and with no direct preference over the payment of the other person.</p>\n<p>After establishing what an agent wants, we need to describe what an agent believes, again conditional on their type. Usually, this is done by assuming agents are Bayesians with a common prior over the state of the world and the types of others, who then update their beliefs after learning their type<sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>. For instance, Jack and Jill might both think the value of the other person is distributed uniformly between $0 and $300, independently of their own type.</p>\n<p>Based on the agents&rsquo; preferences and beliefs, we now need to have some theory of how they choose actions. One standard assumption is that everyone is an expected utility maximizer who takes actions in Nash equilibrium with everyone else. Alternatively, we might consider agents who reason based on the worst case actions of everyone else, who minimize maximum regret, who are boundedly rational, who are willing to tell the truth as long as they only lose a small amount of utility, or who can only be trusted to play dominant strategies rather than Nash equilibrium strategies, etc. What&rsquo;s impossible under one behavioral theory or solution concept can be possible under another.</p>\n<p>In summary so far, a design setting consists of:</p>\n<ol style=\"list-style-type: decimal\">\n<li>The agents involved.</li>\n<li>The potential types of each agent, representing all relevant private information the agent has.</li>\n<li>The potential outcomes available.</li>\n<li>The agents&rsquo; preferences over each outcome for each type, possibly expressed as a utility function.</li>\n<li>The beliefs of each agent as a function of their type.</li>\n<li>A theory about the behavior of agents.</li>\n</ol>\n<p>In our housemate scenario, these could be modeled as following:</p>\n<ol style=\"list-style-type: decimal\">\n<li><strong>Agents involved:</strong> Two people, Jack and Jill.</li>\n<li><strong>Potential types:</strong> The maximum dollar amounts, <span class=\"math\"><em>&theta;</em><sub>Jack</sub></span> and <span class=\"math\"><em>&theta;</em><sub>Jill</sub></span>, each would be willing to contribute, contained in the type spaces <span class=\"math\">&Theta;\u2006<sub>Jack</sub> =\u2004&Theta;\u2006<sub>Jill</sub> =\u2004[0,\u2006300]</span>.</li>\n<li><strong>Potential outcomes:</strong> A binary decision variable <span class=\"math\"><em>q</em> =\u20041</span> if the room is repainted and <span class=\"math\"><em>q</em> =\u20040</span> if not, as well as the amounts paid <span class=\"math\"><em>p</em><sub>Jack</sub></span> and <span class=\"math\"><em>p</em><sub>Jill</sub></span>, contained in the outcome space <span class=\"math\"><em>X</em> =\u2004{0,\u20061}\u2005&times;\u2005R\u2005&times;\u2005R</span>.</li>\n<li><strong>Preferences over each outcome for each type:</strong> A numerical representation of how much each agent likes each outcome <span class=\"math\"><em>u</em><sub><em>i</em></sub>(<em>q</em>,\u2006<em>p</em><sub><em>i</em></sub>,\u2006<em>&theta;</em><sub><em>i</em></sub>)\u2004=\u2004<em>q</em><em>&theta;</em><sub><em>i</em></sub> &minus;\u2005<em>p</em><sub><em>i</em></sub></span>.</li>\n<li><strong>Beliefs:</strong> Independently of their own valuation, each thinks the valuation of the other is uniformly distributed between $0 and $300.</li>\n<li><strong>Behavioral theory:</strong> Each housemate is an expected utility maximizer, and we expect them to play strategies in Nash equilibrium with each other.</li>\n</ol>\n<p>Given a design setting, we presumably have some goals about what should happen, once again conditional on the types of each agent. In particular, we might want specific outcomes to occur for each profile of types. A goal like this is called a <em>social choice function</em><sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>, specifying a mapping from profiles of agent types to outcomes <span class=\"math\"><em>f</em>:\u2006\u2004&prod;\u2006<sub><em>i</em></sub> &Theta;\u2006<sub><em>i</em></sub> &rarr;\u2004<em>X</em></span>. A social choice function <span class=\"math\"><em>f</em></span> says, &ldquo;If the types of agents are <span class=\"math\"><em>&theta;</em><sub>1</sub></span> through <span class=\"math\"><em>&theta;</em><sub><em>N</em></sub></span>, the outcome <span class=\"math\"><em>f</em>(<em>&theta;</em><sub>1</sub>,\u2006&hellip;,\u2006<em>&theta;</em><sub><em>N</em></sub>)\u2004&isin;\u2004<em>X</em></span> should happen&rdquo;. A social choice fucntion could be defined indirectly as whatever outcome maximizes some objective subject to design constraints. For instance, we could find the social choice function that maximizes agents&rsquo; utility or the one that maximizes the total payments collected from the agents in an auction, conditional on no agent being worse off for participating.</p>\n<h2 id=\"putting-the-mechanism-in-mechanism-design\">Putting the mechanism in mechanism design</h2>\n<p>So far, this has been an exercise in precisely specifying the setting we&rsquo;re working in. With all this in hand, we now want to create a protocol/game/institution where agents will interact to produce outcomes according to our favorite social choice function. We&rsquo;ll formalize any possible institution as a <em>mechanism</em> <span class=\"math\">(<em>M</em>,\u2006<em>g</em>)</span> consisting of a set of messages <span class=\"math\"><em>M</em><sub><em>i</em></sub></span> for each agent and an outcome function <span class=\"math\"><em>g</em>:\u2006\u2004&prod;\u2006<sub><em>i</em></sub> <em>M</em><sub><em>i</em></sub> &rarr;\u2004<em>X</em></span> that assigns an outcome for each profile of messages received from agents. The messages could be very simple, like a binary vote, or very complex, like the source code of a program. We can force any set of rules into this formalism by having agents submit programs to act as a their proxy. If we wanted the housemates to bargain back and forth about how to split the cost, their messages could be parameters for a pre-written bargaining program or full programs that make initial and subsequent offers, depending on how much flexibility we allow the agents. Messages represent strategies we&rsquo;re making available to the agent, which are then translated into outcomes by <span class=\"math\"><em>g</em></span>.</p>\n<p>When agents interact together in the mechanism, each chooses the message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>&theta;</em><sub><em>i</em></sub>)</span> they&rsquo;ll send as a function of their type, which then results in the overall outcome <span class=\"math\"><em>g</em>(<em>m</em><sub>1</sub>(<em>&theta;</em><sub>1</sub>),\u2006&hellip;,\u2006<em>m</em><sub><em>n</em></sub>(<em>&theta;</em><sub><em>n</em></sub>))</span>. The mechanism <span class=\"math\">(<em>M</em>,\u2006<em>g</em>)</span> <em>implements</em> a social choice function <span class=\"math\"><em>f</em></span> if, for all profiles of types <span class=\"math\"><em>&theta;</em></span>, the outcome we get under the mechanism equals the outcome we want, i.e.</p>\n<p style=\"padding-left: 30px;\"><br /><span class=\"math\"><em>g</em>(<em>m</em><sub>1</sub>(<em>&theta;</em><sub>1</sub>),\u2006&hellip;,\u2006<em>m</em><sub><em>n</em></sub>(<em>&theta;</em><sub><em>n</em></sub>))\u2004=\u2004<em>f</em>(<em>&theta;</em><sub>1</sub>,\u2006&hellip;,\u2006<em>&theta;</em><sub><em>n</em></sub>),\u2006\u2004for all profiles\u2004(<em>&theta;</em><sub>1</sub>,\u2006&hellip;,\u2006<em>&theta;</em><sub><em>n</em></sub>)\u2004&isin;\u2004&Theta;\u2006\u2004=\u2004&prod;\u2006<sub><em>i</em></sub> &Theta;\u2006<sub><em>i</em></sub></span></p>\n<p>In other words, we want the strategies (determined by whatever behavioral theory we have for each agent) to compose with the outcome function (which we are free to choose, up to design constraints) to match up with our goal, as shown in the following diagram:</p>\n<p style=\"padding-left: 120px;\"><img style=\"vertical-align: middle\" src=\"http://images.lesswrong.com/t3_k5r_0.png?v=e2894a1f17d9820d42454b411b29b234\" alt=\"Reiter diagram\" width=\"377\" height=\"307\" /></p>\n<p>A social choice function <span class=\"math\"><em>f</em></span> is <em>implementable</em> if some mechanism exists that implements it. Whether a social choice function is implementable depends on our behavioral theory. If we think agents choose strategies in Nash equilibrium with each other, we&rsquo;ll have more flexiblity in finding a mechanism than if agents need the stronger incentive of a dominant strategy, since more Nash equilibria exist than dominant-strategy equilibria. Rather than assuming agents choose strategically based on their preferences, perhaps we think agents are naively honest (maybe because they are computer programs we&rsquo;ve programmed ourselves). In this case, we can trivially implement a social choice rule by having each agent tell us their full type and simply choosing the corresponding goal by picking <span class=\"math\"><em>M</em><sub><em>i</em></sub> =\u2004&Theta;\u2006<sub><em>i</em></sub></span> and <span class=\"math\"><em>g</em> =\u2004<em>f</em></span>. Here the interesting question is instead which mechanism can implement <span class=\"math\"><em>f</em></span> with the minimal amount of communication, either by minimizing the number of dimensions or bits in each message. It&rsquo;s also worth asking whether social choice rules satisfying certain properties can exist at all (much less whether we can implement them), along the lines of Arrow&rsquo;s Impossibility Theorem.</p>\n<h2 id=\"wrapping-up\">Wrapping up</h2>\n<p>In summary, agents have <em>types</em> that describe all their relevant information unknown to the mechanism designer. Once we have a <em>social choice function</em> describing a goal of which outcomes should occur for each realization of types, we can build a <em>mechanism</em> consisting of sets of messages or strategies for each agent and a function that assigns outcomes based on the messages received. The hope is that the outcome realized by the agents&rsquo; choice of messages based on their type matches up with the intended outcome. If so, that mechanism <em>implements</em> the social choice function.</p>\n<p>How can we actually determine whether a social choice function is implementable though? If we can find a mechanism that implements it, we&rsquo;ve answered our question. In the reverse though, how would we go about showing that no such mechanism exists? We&rsquo;re back to the problem of searching over all possible ways the agents could interact and hoping we&rsquo;re creative enough.</p>\n<p>In the next post, I&rsquo;ll discuss the Revelation Principle, which allows us to cut through all this complexity via <em>incentive&nbsp;</em><em>compatibility</em>, along with a solution to the painting puzzle.</p>\n<p>&nbsp;</p>\n<p><em>Next up:</em> <a href=\"/lw/k69/incentive_compatibility_and_the_revelation/\">Incentive compatibility and the Revelation Principle</a></p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>To clarify if necessary, Jack&rsquo;s value for the project is the amount of money that he&rsquo;d just barely be willing to spend on the project. If his value is $200, then he would be willing to pay $199 since that leaves a dollar of value left over as <a href=\"http://en.wikipedia.org/wiki/Consumer_surplus\">economic surplus</a>, but he wouldn&rsquo;t pay $201 dollars. If the cost was $200, identical to his value, then he&rsquo;s indifferent between making the purchase and not. The joint value of Jack and Jill is just the sum of their individual valuations.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Assuming dollars map roughly equally onto utilities for each. In general, maximizing total willingness-to-pay is known as <a href=\"http://en.wikipedia.org/wiki/Kaldor%E2%80%93Hicks_efficiency\">Kaldor-Hicks efficiency</a>.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p>This idea is originally due to John Harsanyi. As long as the type spaces are big enough, this can be done without loss of generality as formalized by Mertens and Zamir (1985).<a href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>If multiple outcomes are acceptable for individual profiles, we have a <em>social choice correspondence</em>.<a href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 12, "b8FHrKqyXuYGWc6vn": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xTvdaCwaeZnePMuX5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 69, "extendedScore": null, "score": 0.000187, "legacy": true, "legacyId": "26127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\"><em>tl;dr Mechanism design studies how to design incentives for fun and profit. A puzzle about whether or not to paint a room is posed. A modeling framework is introduced, with lots of corresponding notation.</em></p>\n<p><em>Mechanism design</em> is a framework for constructing institutions for group interactions, giving us a language for the design of everything from voting systems to school admissions to auctions to crowdsourcing. Think of it as the engineering side of game theory, building algorithms for strategic agents. In game theory, the primary goal is to answer the question, \u201cGiven agents who can take some actions that will lead to some payoffs, what do we expect to happen when the agents strategically interact?\u201d In other words, game theory describes the outcomes of fixed scenarios. In contrast, mechanism design flips the question around and asks, \u201cGiven some goals, what payoffs should agents be assigned for the right outcome to occur when agents strategically interact?\u201d The rules of the game are ours to choose, and, within some design constraints, we want to find the best possible ones for a situation.</p>\n<p>Although many people, even <a href=\"http://arielrubinstein.tau.ac.il/papers/afterwards.pdf\">high-profile theorists</a>, doubt the usefulness of game theory, its application in mechanism design is one of the major success stories of modern economics. <a href=\"http://en.wikipedia.org/wiki/Spectrum_auction\">Spectrum license auctions</a> designed by economists paved the way for modern cell-phone networks and garnered billions in revenue for the US and European governments. Tech companies like Google and Microsoft employ theorists to improve advertising auctions. Economists like <a href=\"http://www.forbes.com/forbes/2010/0809/opinions-harvard-alvin-roth-freakonomics-ideas-opinions.html\">Al Roth</a> and computer scientists like <a href=\"http://www.sciencedaily.com/releases/2010/11/101116122859.htm\">Tuomas Sandholm</a> have been instrumental in establishing kidney exchanges to facilitate organ transplants, while others have been active in the redesign of public school admissions in Boston, Chicago, and New Orleans.</p>\n<p>The objective of this post is to introduce all the pieces of a mechanism design problem, providing the setup for actual conclusions later on. I assume you have some basic familiarity with game theory, at the level of understanding the concept of a <a href=\"http://en.wikipedia.org/wiki/Dominance_(game_theory)\">dominant strategies</a> and <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilbria</a>. Take a look at Yvain\u2019s <a href=\"/lw/dbe/introduction_to_game_theory_sequence_guide/\">Game Theory Intro</a> if you\u2019d like to brush up.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Overly_optimizing_whether_or_not_to_paint_an_room\">Overly optimizing whether or not to paint an room</h2>\n<p>Let\u2019s start with a concrete example of a group choice: Jack, an economics student, and Jill, a computer science student, are housemates. To procrastinate studying for finals, they are considering whether to repaint their living room. Conveniently, they agree on what color they would choose, but are unsure whether it\u2019s worth doing at all. Neither Jack nor Jill would pay the known fixed cost of $300 entirely on their own. They\u2019re not even sure the cost is less than their joint value<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>, so it\u2019s not only a matter of bargaining to split the cost (a non-trivial question on its own). The decision to paint the room depends on information neither person fully knows, since each knows their own value, but not the value to the other person.</p>\n<p>The lack of complete information is what makes the problem interesting. If the total value is obviously greater than $300, forcing them to paint the room and split the cost evenly would be utility-maximizing<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>. One person might be worse off, but the other would be correspondingly better off. This solution corresponds to funding a <a href=\"http://en.wikipedia.org/wiki/Public_good\">public good</a> (in the technical sense of something non-excludable and non-rivalrous) through taxation. Alternatively, if the total value is obviously less than $300, then the project shouldn\u2019t be done, and the question of how to split the cost becomes moot. With some overall uncertainty, we now have to worry that either housemate might misrepresent their value to get a better deal, causing the room to be painted when it shouldn\u2019t be or vice versa.</p>\n<p>Assuming the two want to repaint the room if and only if their total value is greater than $300, how would you advise they decide whether to do the project and how much each should contribute?</p>\n<p>Pause a moment to ponder this puzzle\u2026</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>Some naive solutions would be to:</p>\n<ul>\n<li>Vote on painting the room, and if both say yes, do the project with each contributing $150.</li>\n<li>Vote, and if either one says yes, do the project. If both say yes, both contribute $150. If only one says yes, that person contributes $225 and the other contributes $75.</li>\n<li>Both simultaneously write down a number. If the total is greater than $300, do the project. Each contributes a share of the $300 proportional their number.</li>\n<li>Flip a coin to decide who makes an initial offer of how to split the cost, i.e.&nbsp;Jack paying $50 and Jill paying $250. The other person can accept the proposal, in which case they do the project with those contributions. Otherwise, that person makes a new proposal. Alternating proposals continue until one is accepted. If 100 rounds pass without an accepted proposal, don\u2019t paint the room.</li>\n</ul>\n<p>None of these will guarantee the room is painted exactly when it\u2019s worth the cost. The first procedure never paints the room when it shouldn\u2019t be, but sometimes fails to paint when it would be worthwhile, like when Jack values the renovation at $120 and Jill values it at $200. Jack would vote \u201cno\u201d, even though their total value is $320. The second procedure can make mistakes of both kinds and can also result in someone contributing more than their value. The other two are more difficult to analyze, but still turn out to be non-optimal.</p>\n<p>These protocols barely scratch the surface of all the possible institutions out there. Maybe we just need to be a little more creative to find something better. To definitively solve this problem, some formalism is in order.</p>\n<h2 id=\"Framework_for_institutional_design\">Framework for institutional design</h2>\n<p style=\"padding-left: 30px;\"><em style=\"font-size: small;\">Trigger warning: Greek letters, subscripts, sets, and functions.</em></p>\n<p>Getting a little more abstract, let\u2019s specify all the relevant elements in an institutional design setting. First of all, the agents involved in our process need to be identified. Typically, this doesn\u2019t need to go beyond labeling each agent. For instance, we might assign each a number from 1 to <span class=\"math\"><em>n</em></span>, with <span class=\"math\"><em>i</em></span> representing a generic agent. Above, we have two agents named Jack and Jill.</p>\n<blockquote>\n<p><em>Notes on notation:</em> A generic agent has the label <span class=\"math\"><em>i</em></span>. A set or variable <span class=\"math\"><em>Z</em><sub><em>i</em></sub></span> belongs to agent <span class=\"math\"><em>i</em></span>. Typically, variables are lowercase, and the set a variable lives in is uppercase. Without a subscript, <span class=\"math\"><em>Z</em></span> refers to the vector <span class=\"math\"><em>Z</em> =\u2004(<em>Z</em><sub>1</sub>,\u2006\u2026,\u2006<em>Z</em><sub><em>i</em></sub>,\u2006\u2026,\u2006<em>Z</em><sub><em>N</em></sub>)</span> with one element for each agent. It\u2019s often useful to talk about the vector <span class=\"math\"><em>Z</em><sub> \u2212\u2005<em>i</em></sub> =\u2004(<em>Z</em><sub>1</sub>,\u2006\u2026,\u2006<em>Z</em><sub><em>i</em> \u2212\u20051</sub>,\u2006<em>Z</em><sub><em>i</em> +\u20051</sub>,\u2006\u2026,\u2006<em>Z</em><sub><em>N</em></sub>)</span> for all agents <em>except</em> agent <span class=\"math\"><em>i</em></span> (think of deleting <span class=\"math\"><em>Z</em><sub><em>i</em></sub></span> from the vector <span class=\"math\"><em>Z</em></span>). This enables us to write <span class=\"math\"><em>Z</em></span> as <span class=\"math\">(<em>Z</em><sub><em>i</em></sub>,\u2006<em>Z</em><sub> \u2212\u2005<em>i</em></sub>)</span>, highlighting agent <span class=\"math\"><em>i</em></span>\u2019s part in the profile.</p>\n</blockquote>\n<p>Once we\u2019ve established who\u2019s involved, the next step is determining the relevant traits of agents that aren\u2019t obviously known. This unknown data is summarized as that agent\u2019s <em>type</em>. Types can describe an agent\u2019s preferences, their capabilities, their beliefs about the state of the world, their beliefs about others\u2019 types, their beliefs about others\u2019 beliefs, etc. The set of possible types for each agent is their <em>type space</em>. A typical notation for the type of agent <span class=\"math\"><em>i</em></span> is <span class=\"math\"><em>\u03b8</em><sub><em>i</em></sub></span>, an element of the type space <span class=\"math\">\u0398\u2006<sub><em>i</em></sub></span>. If an assumption about an agent\u2019s preferences or beliefs seems unreasonable, that\u2019s a sign we should enlarge the set of possible types, allowing for more variety in behavior. In our housemate scenario, the type of each agent needs to \u2013 at a bare minimum \u2013 specify the value each puts on having new paint. If knowing that value fully specifies the person\u2019s preferences and beliefs, we\u2019re done. Otherwise, we might need to stick more information inside the person\u2019s type. Of course, there is a trade-off between realism and tractibility that guides the modeling choice of how to specify types.</p>\n<p>Next, we need to consider all possible outcomes that might result from the group interaction. This could be an overarching outcome, like having one candidate elected to office, or a specification of the sub-outcomes for each agent, like the job each one is assigned. Let\u2019s call the set of all outcomes <span class=\"math\"><em>X</em></span>. In the housemate scenario, the outcomes consist of the binary choice of whether the room is painted and the payment each person makes. Throwing some notation on this, each outcome is a triple <span class=\"math\">(<em>q</em>,\u2006<em>P</em><sub>Jack</sub>,\u2006<em>P</em><sub>Jill</sub>)\u2004\u2208\u2004<em>X</em> =\u2004{0,\u20061}\u2005\u00d7\u2005R\u2005\u00d7\u2005R</span>.</p>\n<p>To talk about the incentives of agents, their preferences over each outcome must be specified as a function of their type. In general, preferences could be any ranking of the outcomes, but we often assume a particular utility function. For instance, we might numerically represent the preferences of Jack or Jill as <span class=\"math\"><em>u</em><sub><em>i</em></sub>(<em>q</em>,\u2006<em>P</em><sub><em>i</em></sub>,\u2006<em>\u03b8</em><sub><em>i</em></sub>)\u2004=\u2004<em>q</em><em>\u03b8</em><sub><em>i</em></sub> \u2212\u2005<em>P</em><sub><em>i</em></sub></span>, meaning each gets benefit <span class=\"math\"><em>\u03b8</em><sub><em>i</em></sub></span> if and only if the room is painted, minus their payment, and with no direct preference over the payment of the other person.</p>\n<p>After establishing what an agent wants, we need to describe what an agent believes, again conditional on their type. Usually, this is done by assuming agents are Bayesians with a common prior over the state of the world and the types of others, who then update their beliefs after learning their type<sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>. For instance, Jack and Jill might both think the value of the other person is distributed uniformly between $0 and $300, independently of their own type.</p>\n<p>Based on the agents\u2019 preferences and beliefs, we now need to have some theory of how they choose actions. One standard assumption is that everyone is an expected utility maximizer who takes actions in Nash equilibrium with everyone else. Alternatively, we might consider agents who reason based on the worst case actions of everyone else, who minimize maximum regret, who are boundedly rational, who are willing to tell the truth as long as they only lose a small amount of utility, or who can only be trusted to play dominant strategies rather than Nash equilibrium strategies, etc. What\u2019s impossible under one behavioral theory or solution concept can be possible under another.</p>\n<p>In summary so far, a design setting consists of:</p>\n<ol style=\"list-style-type: decimal\">\n<li>The agents involved.</li>\n<li>The potential types of each agent, representing all relevant private information the agent has.</li>\n<li>The potential outcomes available.</li>\n<li>The agents\u2019 preferences over each outcome for each type, possibly expressed as a utility function.</li>\n<li>The beliefs of each agent as a function of their type.</li>\n<li>A theory about the behavior of agents.</li>\n</ol>\n<p>In our housemate scenario, these could be modeled as following:</p>\n<ol style=\"list-style-type: decimal\">\n<li><strong>Agents involved:</strong> Two people, Jack and Jill.</li>\n<li><strong>Potential types:</strong> The maximum dollar amounts, <span class=\"math\"><em>\u03b8</em><sub>Jack</sub></span> and <span class=\"math\"><em>\u03b8</em><sub>Jill</sub></span>, each would be willing to contribute, contained in the type spaces <span class=\"math\">\u0398\u2006<sub>Jack</sub> =\u2004\u0398\u2006<sub>Jill</sub> =\u2004[0,\u2006300]</span>.</li>\n<li><strong>Potential outcomes:</strong> A binary decision variable <span class=\"math\"><em>q</em> =\u20041</span> if the room is repainted and <span class=\"math\"><em>q</em> =\u20040</span> if not, as well as the amounts paid <span class=\"math\"><em>p</em><sub>Jack</sub></span> and <span class=\"math\"><em>p</em><sub>Jill</sub></span>, contained in the outcome space <span class=\"math\"><em>X</em> =\u2004{0,\u20061}\u2005\u00d7\u2005R\u2005\u00d7\u2005R</span>.</li>\n<li><strong>Preferences over each outcome for each type:</strong> A numerical representation of how much each agent likes each outcome <span class=\"math\"><em>u</em><sub><em>i</em></sub>(<em>q</em>,\u2006<em>p</em><sub><em>i</em></sub>,\u2006<em>\u03b8</em><sub><em>i</em></sub>)\u2004=\u2004<em>q</em><em>\u03b8</em><sub><em>i</em></sub> \u2212\u2005<em>p</em><sub><em>i</em></sub></span>.</li>\n<li><strong>Beliefs:</strong> Independently of their own valuation, each thinks the valuation of the other is uniformly distributed between $0 and $300.</li>\n<li><strong>Behavioral theory:</strong> Each housemate is an expected utility maximizer, and we expect them to play strategies in Nash equilibrium with each other.</li>\n</ol>\n<p>Given a design setting, we presumably have some goals about what should happen, once again conditional on the types of each agent. In particular, we might want specific outcomes to occur for each profile of types. A goal like this is called a <em>social choice function</em><sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>, specifying a mapping from profiles of agent types to outcomes <span class=\"math\"><em>f</em>:\u2006\u2004\u220f\u2006<sub><em>i</em></sub> \u0398\u2006<sub><em>i</em></sub> \u2192\u2004<em>X</em></span>. A social choice function <span class=\"math\"><em>f</em></span> says, \u201cIf the types of agents are <span class=\"math\"><em>\u03b8</em><sub>1</sub></span> through <span class=\"math\"><em>\u03b8</em><sub><em>N</em></sub></span>, the outcome <span class=\"math\"><em>f</em>(<em>\u03b8</em><sub>1</sub>,\u2006\u2026,\u2006<em>\u03b8</em><sub><em>N</em></sub>)\u2004\u2208\u2004<em>X</em></span> should happen\u201d. A social choice fucntion could be defined indirectly as whatever outcome maximizes some objective subject to design constraints. For instance, we could find the social choice function that maximizes agents\u2019 utility or the one that maximizes the total payments collected from the agents in an auction, conditional on no agent being worse off for participating.</p>\n<h2 id=\"Putting_the_mechanism_in_mechanism_design\">Putting the mechanism in mechanism design</h2>\n<p>So far, this has been an exercise in precisely specifying the setting we\u2019re working in. With all this in hand, we now want to create a protocol/game/institution where agents will interact to produce outcomes according to our favorite social choice function. We\u2019ll formalize any possible institution as a <em>mechanism</em> <span class=\"math\">(<em>M</em>,\u2006<em>g</em>)</span> consisting of a set of messages <span class=\"math\"><em>M</em><sub><em>i</em></sub></span> for each agent and an outcome function <span class=\"math\"><em>g</em>:\u2006\u2004\u220f\u2006<sub><em>i</em></sub> <em>M</em><sub><em>i</em></sub> \u2192\u2004<em>X</em></span> that assigns an outcome for each profile of messages received from agents. The messages could be very simple, like a binary vote, or very complex, like the source code of a program. We can force any set of rules into this formalism by having agents submit programs to act as a their proxy. If we wanted the housemates to bargain back and forth about how to split the cost, their messages could be parameters for a pre-written bargaining program or full programs that make initial and subsequent offers, depending on how much flexibility we allow the agents. Messages represent strategies we\u2019re making available to the agent, which are then translated into outcomes by <span class=\"math\"><em>g</em></span>.</p>\n<p>When agents interact together in the mechanism, each chooses the message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>\u03b8</em><sub><em>i</em></sub>)</span> they\u2019ll send as a function of their type, which then results in the overall outcome <span class=\"math\"><em>g</em>(<em>m</em><sub>1</sub>(<em>\u03b8</em><sub>1</sub>),\u2006\u2026,\u2006<em>m</em><sub><em>n</em></sub>(<em>\u03b8</em><sub><em>n</em></sub>))</span>. The mechanism <span class=\"math\">(<em>M</em>,\u2006<em>g</em>)</span> <em>implements</em> a social choice function <span class=\"math\"><em>f</em></span> if, for all profiles of types <span class=\"math\"><em>\u03b8</em></span>, the outcome we get under the mechanism equals the outcome we want, i.e.</p>\n<p style=\"padding-left: 30px;\"><br><span class=\"math\"><em>g</em>(<em>m</em><sub>1</sub>(<em>\u03b8</em><sub>1</sub>),\u2006\u2026,\u2006<em>m</em><sub><em>n</em></sub>(<em>\u03b8</em><sub><em>n</em></sub>))\u2004=\u2004<em>f</em>(<em>\u03b8</em><sub>1</sub>,\u2006\u2026,\u2006<em>\u03b8</em><sub><em>n</em></sub>),\u2006\u2004for all profiles\u2004(<em>\u03b8</em><sub>1</sub>,\u2006\u2026,\u2006<em>\u03b8</em><sub><em>n</em></sub>)\u2004\u2208\u2004\u0398\u2006\u2004=\u2004\u220f\u2006<sub><em>i</em></sub> \u0398\u2006<sub><em>i</em></sub></span></p>\n<p>In other words, we want the strategies (determined by whatever behavioral theory we have for each agent) to compose with the outcome function (which we are free to choose, up to design constraints) to match up with our goal, as shown in the following diagram:</p>\n<p style=\"padding-left: 120px;\"><img style=\"vertical-align: middle\" src=\"http://images.lesswrong.com/t3_k5r_0.png?v=e2894a1f17d9820d42454b411b29b234\" alt=\"Reiter diagram\" width=\"377\" height=\"307\"></p>\n<p>A social choice function <span class=\"math\"><em>f</em></span> is <em>implementable</em> if some mechanism exists that implements it. Whether a social choice function is implementable depends on our behavioral theory. If we think agents choose strategies in Nash equilibrium with each other, we\u2019ll have more flexiblity in finding a mechanism than if agents need the stronger incentive of a dominant strategy, since more Nash equilibria exist than dominant-strategy equilibria. Rather than assuming agents choose strategically based on their preferences, perhaps we think agents are naively honest (maybe because they are computer programs we\u2019ve programmed ourselves). In this case, we can trivially implement a social choice rule by having each agent tell us their full type and simply choosing the corresponding goal by picking <span class=\"math\"><em>M</em><sub><em>i</em></sub> =\u2004\u0398\u2006<sub><em>i</em></sub></span> and <span class=\"math\"><em>g</em> =\u2004<em>f</em></span>. Here the interesting question is instead which mechanism can implement <span class=\"math\"><em>f</em></span> with the minimal amount of communication, either by minimizing the number of dimensions or bits in each message. It\u2019s also worth asking whether social choice rules satisfying certain properties can exist at all (much less whether we can implement them), along the lines of Arrow\u2019s Impossibility Theorem.</p>\n<h2 id=\"Wrapping_up\">Wrapping up</h2>\n<p>In summary, agents have <em>types</em> that describe all their relevant information unknown to the mechanism designer. Once we have a <em>social choice function</em> describing a goal of which outcomes should occur for each realization of types, we can build a <em>mechanism</em> consisting of sets of messages or strategies for each agent and a function that assigns outcomes based on the messages received. The hope is that the outcome realized by the agents\u2019 choice of messages based on their type matches up with the intended outcome. If so, that mechanism <em>implements</em> the social choice function.</p>\n<p>How can we actually determine whether a social choice function is implementable though? If we can find a mechanism that implements it, we\u2019ve answered our question. In the reverse though, how would we go about showing that no such mechanism exists? We\u2019re back to the problem of searching over all possible ways the agents could interact and hoping we\u2019re creative enough.</p>\n<p>In the next post, I\u2019ll discuss the Revelation Principle, which allows us to cut through all this complexity via <em>incentive&nbsp;</em><em>compatibility</em>, along with a solution to the painting puzzle.</p>\n<p>&nbsp;</p>\n<p><em>Next up:</em> <a href=\"/lw/k69/incentive_compatibility_and_the_revelation/\">Incentive compatibility and the Revelation Principle</a></p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p>To clarify if necessary, Jack\u2019s value for the project is the amount of money that he\u2019d just barely be willing to spend on the project. If his value is $200, then he would be willing to pay $199 since that leaves a dollar of value left over as <a href=\"http://en.wikipedia.org/wiki/Consumer_surplus\">economic surplus</a>, but he wouldn\u2019t pay $201 dollars. If the cost was $200, identical to his value, then he\u2019s indifferent between making the purchase and not. The joint value of Jack and Jill is just the sum of their individual valuations.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Assuming dollars map roughly equally onto utilities for each. In general, maximizing total willingness-to-pay is known as <a href=\"http://en.wikipedia.org/wiki/Kaldor%E2%80%93Hicks_efficiency\">Kaldor-Hicks efficiency</a>.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p>This idea is originally due to John Harsanyi. As long as the type spaces are big enough, this can be done without loss of generality as formalized by Mertens and Zamir (1985).<a href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>If multiple outcomes are acceptable for individual profiles, we have a <em>social choice correspondence</em>.<a href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Overly optimizing whether or not to paint an room", "anchor": "Overly_optimizing_whether_or_not_to_paint_an_room", "level": 1}, {"title": "Framework for institutional design", "anchor": "Framework_for_institutional_design", "level": 1}, {"title": "Putting the mechanism in mechanism design", "anchor": "Putting_the_mechanism_in_mechanism_design", "level": 1}, {"title": "Wrapping up", "anchor": "Wrapping_up", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QxZs5Za4qXBegXCgu", "N4gDA5HPpGC4mbTEZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T19:40:46.688Z", "modifiedAt": null, "url": null, "title": "Rebutting radical scientific skepticism", "slug": "rebutting-radical-scientific-skepticism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:30.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "asr", "createdAt": "2011-06-02T01:42:05.591Z", "isAdmin": false, "displayName": "asr"}, "userId": "w2EyaugHx6wxwdbva", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rnT3c7n7kZYfTXuYp/rebutting-radical-scientific-skepticism", "pageUrlRelative": "/posts/rnT3c7n7kZYfTXuYp/rebutting-radical-scientific-skepticism", "linkUrl": "https://www.lesswrong.com/posts/rnT3c7n7kZYfTXuYp/rebutting-radical-scientific-skepticism", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rebutting%20radical%20scientific%20skepticism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARebutting%20radical%20scientific%20skepticism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnT3c7n7kZYfTXuYp%2Frebutting-radical-scientific-skepticism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rebutting%20radical%20scientific%20skepticism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnT3c7n7kZYfTXuYp%2Frebutting-radical-scientific-skepticism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnT3c7n7kZYfTXuYp%2Frebutting-radical-scientific-skepticism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 601, "htmlBody": "<p>Suppose you distrusted everything you had ever read about science. How much of modern scientific knowledge could you verify for yourself, using only your own senses and the sort of equipment you could easily obtain? &nbsp;How about if you accept third-party evidence when many thousands of people can easily check the facts?</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a>My purpose with the question isn't to cast radical doubt on science; rather, it's an entertaining game of trying to understand how we know what we know. Thinking through these sorts of questions also helped me notice interesting things in the history of science that I hadn't previously focused on. It might also be of interest from a science education perspective.</p>\n<p>Some things are much easier to check than they used to be. As late as the 19th century, there were people who were publicly skeptical about the curvature of the earth. Skeptics and scientists did careful measurements (notably the <a href=\"http://en.wikipedia.org/wiki/Bedford_Level_experiment\">Bedford Level Experiment</a>) to observe the earth's curvature. Today, you can verify it by phoning a friend a few time zones away and noticing that the sun reaches the zenith at steadily later times as you move west. This only makes sense if the earth is curved.</p>\n<p>Some things are still hard to check. I don't know an easy way to show that the Earth orbits the Sun. The direct way to show it would be to measure stellar parallax. But even the closest stars have a parallax of less than an arcsecond. My understanding is that very few amateurs are able to take measurements with that level of precision.</p>\n<p>Some things are surprisingly easy. There are lots of easily accessible demonstrations of quantum phenomena. For example, a ten dollar spectroscope will show you that an incandescent light bulb has a continuous spectrum, and that LEDs and fluorescent bulbs don't. Bright-line spectra are very much a quantum mechanical phenomenon -- it's a sign that the atoms in the light source have fixed energy transition levels. Spectroscopy was one of the key early lines of evidence for quantum mechanics, and it blows my mind that it's something you can just see whenever you want, with a negligible equipment cost.</p>\n<p>Pretty much all of modern chemistry and solid state physics rests on a quantum foundation, and you can test a great deal of chemistry pretty easily. If you are in doubt that water is a bonded compound of two gasses, you can do the electrolysis very easily yourself. You can observe the periodicity of chemical elements yourself if you buy alkali metals (don't try this one at home!). If you are willing to accept slightly indirect evidence, the entire semiconductor industry is about precisely controlling the conductivity of impure silicon, and this would make no sense if quantum mechanics weren't a reliable guide to electron energy levels in the solid state.</p>\n<p>I don't feel quite as qualified to play this game for biology. I imagine that antibiotic resistance is a well-enough documented case of evolution through natural selection to serve at least as a proof of concept. DNA sequence comparisons across species are emphatic evidence of taxonomic trees, if you trust the scientists not to be part of a vast conspiracy.</p>\n<p>It feels almost impossible that it's easier to see quantum mechanical effects than it is to see that the earth orbits the sun, but it does seem that way.</p>\n<p>Some questions:</p>\n<ul>\n<li>Is there an easily visible consequence of special relativity that you can see without specialized equipment?</li>\n<li>Can you measure the consistency of the velocity of light on your own?</li>\n<li>How much can you directly demonstrate in biology or the social sciences?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rnT3c7n7kZYfTXuYp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 30, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "26128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T21:06:16.359Z", "modifiedAt": null, "url": null, "title": "Discussion: How scientifically sound are MBAs?", "slug": "discussion-how-scientifically-sound-are-mbas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "caffemacchiavelli", "createdAt": "2013-07-10T10:33:15.140Z", "isAdmin": false, "displayName": "caffemacchiavelli"}, "userId": "Cr5Z2f7faDcEpnjhy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XX4QMtYshaN5Lt3t5/discussion-how-scientifically-sound-are-mbas", "pageUrlRelative": "/posts/XX4QMtYshaN5Lt3t5/discussion-how-scientifically-sound-are-mbas", "linkUrl": "https://www.lesswrong.com/posts/XX4QMtYshaN5Lt3t5/discussion-how-scientifically-sound-are-mbas", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Discussion%3A%20How%20scientifically%20sound%20are%20MBAs%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiscussion%3A%20How%20scientifically%20sound%20are%20MBAs%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXX4QMtYshaN5Lt3t5%2Fdiscussion-how-scientifically-sound-are-mbas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Discussion%3A%20How%20scientifically%20sound%20are%20MBAs%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXX4QMtYshaN5Lt3t5%2Fdiscussion-how-scientifically-sound-are-mbas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXX4QMtYshaN5Lt3t5%2Fdiscussion-how-scientifically-sound-are-mbas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 389, "htmlBody": "<p>I'm finishing up the first year of my distance-learning MBA, which has been a very confusing experience.</p>\n<p>I went into the course partially as insurance against \"unknown unknowns\", i.e., lacking concepts important to building or running a business because I didn't know about them or underestimated their importance.</p>\n<p>The first surprise within the course was the relative lack of explaining the utility of the particular models presented. Apart from the section on financial reporting, which did explain how you'd be able to solve practical problems by using particular budgeting tools or costing methods, concepts where generally presented as \"This well-known management thinker came up with this, why don't you play around with it a little\".&nbsp;</p>\n<p>Analysis and (subjective) discussion of course material was also the main focus of student assessment, which consisted of marked reports on a subject of the students' choice, usually a problem they'd be dealing with in their current position.</p>\n<p>There were no quizzes, case studies with objectively correct answers (except for, again, financial reporting) or other problem-solving exercises.</p>\n<p>Essentially, students are handed a lot of maps for whatever territory they're trying to analyze, but don't really receive any tool to judge the accuracy, let alone utility of said maps. For instance, I'd consider \"Concept useful for describing why A does B well\" and  \"Concept useful for helping A do B well\" to be fundamentally different,  but the course did not explicitly separate the two. (I've heard similar  criticisms of Michael Porter's failed Monitor Group, so this may be a  problem of business academics in general)</p>\n<p>I fear that there will be a risk of training the stereotypically smug MBA, who has gained more confidence in their management skills even though they haven't proven their ability of making sound decisions.</p>\n<p>I'm also starting to understand why so many entrepreneurs are dismissive of MBAs. <a href=\"https://www.youtube.com/watch?v=4gnZaEei8_g\">Manoj Bhargava</a> has a whole talk on the issue and <a href=\"http://science.slashdot.org/story/13/11/20/1843208/elon-musk-talks-about-the-importance-of-physics-criticizes-the-mba\">Elon Musk isn't a fan</a>, either. I'm not willing to argue that they're right, but on closer inspection, the field hasn't inspired much confidence in me. Even though I think that business administration should be taught in school, I can't help but feel that we're not quite there yet.</p>\n<p>I still can't get the fog to clear completely and this post isn't as lucid as I want it to be, so I'd be happy to hear more arguments on the issue.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XX4QMtYshaN5Lt3t5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 1.698732618306155e-06, "legacy": true, "legacyId": "26129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-04-30T22:29:44.052Z", "modifiedAt": null, "url": null, "title": "Exploring Botworld", "slug": "exploring-botworld", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:06.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sZEh7zsZ2hM5i836q/exploring-botworld", "pageUrlRelative": "/posts/sZEh7zsZ2hM5i836q/exploring-botworld", "linkUrl": "https://www.lesswrong.com/posts/sZEh7zsZ2hM5i836q/exploring-botworld", "postedAtFormatted": "Wednesday, April 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exploring%20Botworld&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExploring%20Botworld%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZEh7zsZ2hM5i836q%2Fexploring-botworld%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exploring%20Botworld%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZEh7zsZ2hM5i836q%2Fexploring-botworld", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZEh7zsZ2hM5i836q%2Fexploring-botworld", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1728, "htmlBody": "<p>Many people have been asking me this question:</p>\n<blockquote>\n<p>But what am I supposed to <em>do </em>with Botworld?</p>\n</blockquote>\n<p>This indicates a failure of communication on my part. In this post, I'll try to resolve that question. As part of this attempt, I've made some updates to the <a href=\"https://github.com/machine-intelligence/Botworld\">Botworld code</a> (which is now v1.1) which make Botworld a bit more approachable. A changelog and some documentation are included below.</p>\n<p><a id=\"more\"></a></p>\n<h1>Playing with Botworld</h1>\n<p>To clarify, Botworld was devised as a tool for reasoning about different AI formalisms. We're not trying to build a hotbed for programming competitions or anything of the sort. Rather, we need a concrete system with concrete games where we can study how a given agent deals with a given situation. As such, most of our work which references Botworld will describe complex agents playing simple games. A few such posts are in the pipeline.</p>\n<p>Benja and I designed Botworld as a universe where we can distill many 'edge case' scenarios that an AI may face, such as having it's source code read or distributing itself across multiple machines. Such scenarios are often handled clumsily by existing AI formalisms, and it is quite useful to have simple, concrete scenarios where we can envision such events.</p>\n<p>That's how&nbsp;<em>we</em>&nbsp;use Botworld: we throw different formalisms at different edge cases, figure out where their problems might lie, and visualize solutions. If we encounter problems, we use Botworld as a way to find a minimal example. You'll see one such example in an upcoming post.</p>\n<p>So how are&nbsp;<em>you</em>&nbsp;supposed to use Botworld? Well, if you're studying AI formalisms, then you're encouraged to use Botworld to visualize and distill weird scenarios where different agents may run into trouble. Otherwise, we don't really expect you to get direct use out of it.</p>\n<p>Don't get me wrong, you're more than welcome to play around, write amusing games, and design programs that do neat things. It's a flexible and fun little system. If you are just looking to entertain yourself, you're invited to:</p>\n<ul>\n<li>Write some simple robots that cooperate in a Stag Hunt (now included in the <a href=\"https://github.com/machine-intelligence/Botworld/tree/master/games\">games/</a> directory)</li>\n<li>Design your own game</li>\n<li>Write a program that wins in the <a href=\"https://github.com/machine-intelligence/Botworld/tree/master/games/Precommit\">Precommitment</a> game (spoiler alert: a solution can be found in the&nbsp;<em>Ideal.ct</em>&nbsp;file)</li>\n</ul>\n<div>This can definitely help you get a feel for Botworld, and will help you understand the upcoming posts more readily. That said, we don't have plans to make Botworld into more than an explanatory tool.</div>\n<div><br /></div>\n<div>For anyone following our research, it's sufficient to know only the high-level description Botworld (or even just that it exists) so that you may follow along when we talk about different agents playing different Botworld games in the future.</div>\n<div><br /></div>\n<div>If you <em>do </em>want to use Botworld, either to understand our games better, or because you're considering writing your own Botworld games for your own purposes, or just out of curiosity, this new code update makes Botworld much more approachable.</div>\n<div><br /></div>\n<h1>Changelog</h1>\n<ul>\n<li style=\"font-weight: normal; font-style: normal;\"><span style=\"font-style: normal;\">The step function has been split into the Environment phase and the Computation phase. Everything still works in exactly the same way, but this change allows you to pause halfway through a step and see what's happening. The new </span><em>runEnvironment </em>and <em>runRobots</em>&nbsp;functions allow you to partially step a grid through only one of the phases.</li>\n<li style=\"font-weight: normal;\">An <em>Event</em>&nbsp;datatype has been added, which describes the state of a square between phases. This drastically simplifies the register machine input type from <em>(Int, [Robot], [Action], ([Item], [Item], [([Item], [Item])]), Constree) to (Int, Event, Constree)</em>, but does introduce a minor incompatibility in the way that register machine input is encoded. If you have written a Constree program that navigates the input data structure, it will have to be updated accordingly.</li>\n<li>The display code has been extracted into a library and fleshed out. See the <strong>Displaying Botworlds</strong>&nbsp;section below.</li>\n<li>Many tools have been added to make it easier to write Constree programs. See the <strong>Writing Constree</strong>&nbsp;section below.</li>\n<li>Four new games have been added. See the <strong>New Games</strong>&nbsp;section below.</li>\n</ul>\n<div><br /></div>\n<h1>Displaying Botworlds</h1>\n<p>The display code now lives in the <em>Botworld.Display </em>module.&nbsp;New functionality has been added which allows you to gain much more insight into what happens between steps. The Botworld displays are now much prettier (using ASCII box drawing characters) and much less cluttered.</p>\n<p>The new Botworld grid display is quite minimal, displaying only the positions of each robot by color. (Use the <em>displayBotworld </em>function to display a Botworld grid.) Much more data can be had by half-stepping a grid (using the <em>runEnvironment </em>function) and then displaying the resulting <em>EventGrid</em>&nbsp;(see the <em>displayEventGrid</em>&nbsp;function). The Event Grid display works as follows:</p>\n<p>Each cell has two rows. The first shows the position of (up to 5) robots in the square by color. The second has information about what is happening in the cell, using the following flags:</p>\n<ul>\n<li>&darr; at least one item was dropped in this cell</li>\n<li>&uarr; at least one item was lifted in this cell</li>\n<li>\u2195 items were both dropped and lifted in this cell</li>\n<li>&times; at least one robot was destroyed in this cell</li>\n<li>+ at least one robot was created in this cell</li>\n<li>? at least one robot was inspected in this cell</li>\n<li>! at least one robot in this cell executed an invalid action</li>\n</ul>\n<p>Robot movement is shown by arrows breaching the borders between cells. (Diagonal movements are somewhat awkward to display. To see which robots moved in/out of a cell's northwest/northeast neighbors, look at the cell's top border. To see which robots moved in/out of a cell's southwest/southeast neigbors, look at the top border of the cell's southwest/southeast neighbor. It's less confusing than it sounds; the arrows make things fairly clear.)</p>\n<p>To get additional information about what happened in a single cell, you may use the <em>displayChangesAt</em>&nbsp;function. This prints a full description of a cell's event. This includes a full robot list (color, action, speed, register count, inventory) and a full breakdown of items (untouched, dropped, and fallen, with fallen items separated by robot and by fallen part vs fallen inventory).</p>\n<p>&nbsp;</p>\n<h1>Writing Constree</h1>\n<p>A minimal language 'ct' has been added, which makes writing robot programs slightly less painful. The syntax is as follows:</p>\n<p>A register is declared by either a name followed by a colon, or a name followed by a number followed by a colon. The number, if given, is the memory limit of the register. If omitted, the register size will be made to fit the register contents precisely (this is useful when a register holds a subroutine.) Examples:</p>\n<ul>\n<li>REG:</li>\n<li>REG 1024:</li>\n</ul>\n<p>A register definition must be followed by content. Content may be any of:</p>\n<ul>\n<li>Raw constree, such as \"Nil\" or \"Cons Nil Nil\"</li>\n<li>A command, such as \"Inspect 1\"</li>\n<li>An instruction, such as \"CopyIfNil 0 0 0\"</li>\n<li>A list or tuple of the above</li>\n<li>A series of machine instructions</li>\n</ul>\n<p>Machine instructions compile directly into the normal Constree instructions (Nilify, Construct, Destruct, or CopyIfNil), but they allow you to specify registers by name instead of by number, and allow some simple shortcuts. Accepted instructions include:</p>\n<ul>\n<li><strong>NILIFY </strong>target</li>\n<li><strong>CONS </strong>left right target</li>\n<li><strong>DEST </strong>target left right</li>\n<li><strong>COPY </strong>source dest</li>\n<li><strong>CONDCOPY </strong>test source target</li>\n<li><strong>EXEC </strong>source</li>\n<li><strong>CONDEXEC </strong>test source</li>\n<li><strong>PUSH </strong>source stack</li>\n<li><strong>POP </strong>stack target</li>\n<li><strong>WRITE </strong>source</li>\n<li><strong>CONDWRITE </strong>test source</li>\n</ul>\n<div>NILIFY, CONS, DEST, and CONDCOPY compile to Nilify, Construct, Destruct, and CopyIfNil.</div>\n<div><br /></div>\n<div>COPY, EXEC, and WRITE may only be used if you have a register named NIL, and they only work if the register NIL actually contains NIL. They are aliases for COND* NIL * *.</div>\n<div><br /></div>\n<div>PUSH source stack is the same as CONS source stack stack.</div>\n<div>POP stack source is the same as DEST stack source stack.</div>\n<div>EXEC source is the same as COPY source 0 (0 is the program register).</div>\n<div>WRITE source is the same as COPY source 2 (2 is the output register).</div>\n<div><br /></div>\n<div>For an example, see <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Omega.ct\">this file</a>. ct files may be compiled to haskell using the <em>ct2hs</em>&nbsp;binary, which is now bundled with Botworld. This compiler is quite rudimentary, and may get better in the future. For now, it takes two arguments: the name of the ct file (without the .ct extension) and the name of the Haskell module to output (optional, defaults to the file name). It then outputs (to stdout) a Haskell module which defines a <em>machine</em>&nbsp;object containing the compiled machine. Example usage:</div>\n<div><br /></div>\n<pre>&gt; ct2hs Omega &gt; Omega.hs</pre>\n<pre>&gt; ghci Omega.hs</pre>\n<pre>&lambda; :t machine</pre>\n<pre>Memory</pre>\n<div><br /></div>\n<div>Some simple Constree debugging facilities have been added in the <em>Botworld.Debug</em>&nbsp;module, which is now bundled with Botworld.</div>\n<div><br /></div>\n<h1>New Games</h1>\n<p>Four games have been added to the new <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Omega.ct\">games directory</a>.</p>\n<ol>\n<li>A <em>Prisoner's Dilemma:</em>&nbsp;there are two robots, the left robot and the right robot. There are two cells, the left cell and the right cell. The left robot starts in the left cell, which is the left player's home square. The right robot starts in the right cell, which is the right player's home square. Each robot holds an item that their player values at $1, but which the other player values at $2. The game lasts for one timestep. Each robot must decide whether or not to move into the opponent's home square.</li>\n<li>A <em>Stag Hunt: </em>there are two player robots, two hare robots, and one stag robot. Each player robot has just barely enough time to attack exactly one non-player robot, take it's item, and get to it's home square before the game ends. The Hare robot has no shields and holds a low-value item. The Stag robot holds two high-value items, but has one shield, which means that it must be attacked twice in order to drop its items. (The time constraints are such that the attack must be simultaneous, and such that no one player can score both of the stag's items.)</li>\n<li>A <em>Precommitment game</em>, to be explained in an upcoming post.</li>\n<li>A <em>Self-destruction game</em>, to be explained in an upcoming post.</li>\n</ol>\n<div>In order to play these games, you'll have to provide machines for the player robots. This can be written using the <em>ct </em>language and the <em>ct2hs</em>&nbsp;binary as described above. For example:</div>\n<div><br /></div>\n<pre>&gt; ct2hs LeftBot &gt; LeftBot.hs</pre>\n<pre>&gt; ct2hs RightBot &gt; RightBot.hs</pre>\n<pre>&gt; ghci PrisonersDilemma</pre>\n<pre>&lambda; :m +LeftBot</pre>\n<pre>&lambda; :m +RightBot</pre>\n<pre>&lambda; run LeftBot.machine RightBot.machine</pre>\n<pre>...</pre>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "TkZ7MFwCi4D63LJ5n": 1, "HFou6RHqFagkyrKkW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sZEh7zsZ2hM5i836q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 34, "extendedScore": null, "score": 1.6988433423803155e-06, "legacy": true, "legacyId": "26130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Many people have been asking me this question:</p>\n<blockquote>\n<p>But what am I supposed to <em>do </em>with Botworld?</p>\n</blockquote>\n<p>This indicates a failure of communication on my part. In this post, I'll try to resolve that question. As part of this attempt, I've made some updates to the <a href=\"https://github.com/machine-intelligence/Botworld\">Botworld code</a> (which is now v1.1) which make Botworld a bit more approachable. A changelog and some documentation are included below.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Playing_with_Botworld\">Playing with Botworld</h1>\n<p>To clarify, Botworld was devised as a tool for reasoning about different AI formalisms. We're not trying to build a hotbed for programming competitions or anything of the sort. Rather, we need a concrete system with concrete games where we can study how a given agent deals with a given situation. As such, most of our work which references Botworld will describe complex agents playing simple games. A few such posts are in the pipeline.</p>\n<p>Benja and I designed Botworld as a universe where we can distill many 'edge case' scenarios that an AI may face, such as having it's source code read or distributing itself across multiple machines. Such scenarios are often handled clumsily by existing AI formalisms, and it is quite useful to have simple, concrete scenarios where we can envision such events.</p>\n<p>That's how&nbsp;<em>we</em>&nbsp;use Botworld: we throw different formalisms at different edge cases, figure out where their problems might lie, and visualize solutions. If we encounter problems, we use Botworld as a way to find a minimal example. You'll see one such example in an upcoming post.</p>\n<p>So how are&nbsp;<em>you</em>&nbsp;supposed to use Botworld? Well, if you're studying AI formalisms, then you're encouraged to use Botworld to visualize and distill weird scenarios where different agents may run into trouble. Otherwise, we don't really expect you to get direct use out of it.</p>\n<p>Don't get me wrong, you're more than welcome to play around, write amusing games, and design programs that do neat things. It's a flexible and fun little system. If you are just looking to entertain yourself, you're invited to:</p>\n<ul>\n<li>Write some simple robots that cooperate in a Stag Hunt (now included in the <a href=\"https://github.com/machine-intelligence/Botworld/tree/master/games\">games/</a> directory)</li>\n<li>Design your own game</li>\n<li>Write a program that wins in the <a href=\"https://github.com/machine-intelligence/Botworld/tree/master/games/Precommit\">Precommitment</a> game (spoiler alert: a solution can be found in the&nbsp;<em>Ideal.ct</em>&nbsp;file)</li>\n</ul>\n<div>This can definitely help you get a feel for Botworld, and will help you understand the upcoming posts more readily. That said, we don't have plans to make Botworld into more than an explanatory tool.</div>\n<div><br></div>\n<div>For anyone following our research, it's sufficient to know only the high-level description Botworld (or even just that it exists) so that you may follow along when we talk about different agents playing different Botworld games in the future.</div>\n<div><br></div>\n<div>If you <em>do </em>want to use Botworld, either to understand our games better, or because you're considering writing your own Botworld games for your own purposes, or just out of curiosity, this new code update makes Botworld much more approachable.</div>\n<div><br></div>\n<h1 id=\"Changelog\">Changelog</h1>\n<ul>\n<li style=\"font-weight: normal; font-style: normal;\"><span style=\"font-style: normal;\">The step function has been split into the Environment phase and the Computation phase. Everything still works in exactly the same way, but this change allows you to pause halfway through a step and see what's happening. The new </span><em>runEnvironment </em>and <em>runRobots</em>&nbsp;functions allow you to partially step a grid through only one of the phases.</li>\n<li style=\"font-weight: normal;\">An <em>Event</em>&nbsp;datatype has been added, which describes the state of a square between phases. This drastically simplifies the register machine input type from <em>(Int, [Robot], [Action], ([Item], [Item], [([Item], [Item])]), Constree) to (Int, Event, Constree)</em>, but does introduce a minor incompatibility in the way that register machine input is encoded. If you have written a Constree program that navigates the input data structure, it will have to be updated accordingly.</li>\n<li>The display code has been extracted into a library and fleshed out. See the <strong>Displaying Botworlds</strong>&nbsp;section below.</li>\n<li>Many tools have been added to make it easier to write Constree programs. See the <strong>Writing Constree</strong>&nbsp;section below.</li>\n<li>Four new games have been added. See the <strong>New Games</strong>&nbsp;section below.</li>\n</ul>\n<div><br></div>\n<h1 id=\"Displaying_Botworlds\">Displaying Botworlds</h1>\n<p>The display code now lives in the <em>Botworld.Display </em>module.&nbsp;New functionality has been added which allows you to gain much more insight into what happens between steps. The Botworld displays are now much prettier (using ASCII box drawing characters) and much less cluttered.</p>\n<p>The new Botworld grid display is quite minimal, displaying only the positions of each robot by color. (Use the <em>displayBotworld </em>function to display a Botworld grid.) Much more data can be had by half-stepping a grid (using the <em>runEnvironment </em>function) and then displaying the resulting <em>EventGrid</em>&nbsp;(see the <em>displayEventGrid</em>&nbsp;function). The Event Grid display works as follows:</p>\n<p>Each cell has two rows. The first shows the position of (up to 5) robots in the square by color. The second has information about what is happening in the cell, using the following flags:</p>\n<ul>\n<li>\u2193 at least one item was dropped in this cell</li>\n<li>\u2191 at least one item was lifted in this cell</li>\n<li>\u2195 items were both dropped and lifted in this cell</li>\n<li>\u00d7 at least one robot was destroyed in this cell</li>\n<li>+ at least one robot was created in this cell</li>\n<li>? at least one robot was inspected in this cell</li>\n<li>! at least one robot in this cell executed an invalid action</li>\n</ul>\n<p>Robot movement is shown by arrows breaching the borders between cells. (Diagonal movements are somewhat awkward to display. To see which robots moved in/out of a cell's northwest/northeast neighbors, look at the cell's top border. To see which robots moved in/out of a cell's southwest/southeast neigbors, look at the top border of the cell's southwest/southeast neighbor. It's less confusing than it sounds; the arrows make things fairly clear.)</p>\n<p>To get additional information about what happened in a single cell, you may use the <em>displayChangesAt</em>&nbsp;function. This prints a full description of a cell's event. This includes a full robot list (color, action, speed, register count, inventory) and a full breakdown of items (untouched, dropped, and fallen, with fallen items separated by robot and by fallen part vs fallen inventory).</p>\n<p>&nbsp;</p>\n<h1 id=\"Writing_Constree\">Writing Constree</h1>\n<p>A minimal language 'ct' has been added, which makes writing robot programs slightly less painful. The syntax is as follows:</p>\n<p>A register is declared by either a name followed by a colon, or a name followed by a number followed by a colon. The number, if given, is the memory limit of the register. If omitted, the register size will be made to fit the register contents precisely (this is useful when a register holds a subroutine.) Examples:</p>\n<ul>\n<li>REG:</li>\n<li>REG 1024:</li>\n</ul>\n<p>A register definition must be followed by content. Content may be any of:</p>\n<ul>\n<li>Raw constree, such as \"Nil\" or \"Cons Nil Nil\"</li>\n<li>A command, such as \"Inspect 1\"</li>\n<li>An instruction, such as \"CopyIfNil 0 0 0\"</li>\n<li>A list or tuple of the above</li>\n<li>A series of machine instructions</li>\n</ul>\n<p>Machine instructions compile directly into the normal Constree instructions (Nilify, Construct, Destruct, or CopyIfNil), but they allow you to specify registers by name instead of by number, and allow some simple shortcuts. Accepted instructions include:</p>\n<ul>\n<li><strong>NILIFY </strong>target</li>\n<li><strong>CONS </strong>left right target</li>\n<li><strong>DEST </strong>target left right</li>\n<li><strong>COPY </strong>source dest</li>\n<li><strong>CONDCOPY </strong>test source target</li>\n<li><strong>EXEC </strong>source</li>\n<li><strong>CONDEXEC </strong>test source</li>\n<li><strong>PUSH </strong>source stack</li>\n<li><strong>POP </strong>stack target</li>\n<li><strong>WRITE </strong>source</li>\n<li><strong>CONDWRITE </strong>test source</li>\n</ul>\n<div>NILIFY, CONS, DEST, and CONDCOPY compile to Nilify, Construct, Destruct, and CopyIfNil.</div>\n<div><br></div>\n<div>COPY, EXEC, and WRITE may only be used if you have a register named NIL, and they only work if the register NIL actually contains NIL. They are aliases for COND* NIL * *.</div>\n<div><br></div>\n<div>PUSH source stack is the same as CONS source stack stack.</div>\n<div>POP stack source is the same as DEST stack source stack.</div>\n<div>EXEC source is the same as COPY source 0 (0 is the program register).</div>\n<div>WRITE source is the same as COPY source 2 (2 is the output register).</div>\n<div><br></div>\n<div>For an example, see <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Omega.ct\">this file</a>. ct files may be compiled to haskell using the <em>ct2hs</em>&nbsp;binary, which is now bundled with Botworld. This compiler is quite rudimentary, and may get better in the future. For now, it takes two arguments: the name of the ct file (without the .ct extension) and the name of the Haskell module to output (optional, defaults to the file name). It then outputs (to stdout) a Haskell module which defines a <em>machine</em>&nbsp;object containing the compiled machine. Example usage:</div>\n<div><br></div>\n<pre>&gt; ct2hs Omega &gt; Omega.hs</pre>\n<pre>&gt; ghci Omega.hs</pre>\n<pre>\u03bb :t machine</pre>\n<pre>Memory</pre>\n<div><br></div>\n<div>Some simple Constree debugging facilities have been added in the <em>Botworld.Debug</em>&nbsp;module, which is now bundled with Botworld.</div>\n<div><br></div>\n<h1 id=\"New_Games\">New Games</h1>\n<p>Four games have been added to the new <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Omega.ct\">games directory</a>.</p>\n<ol>\n<li>A <em>Prisoner's Dilemma:</em>&nbsp;there are two robots, the left robot and the right robot. There are two cells, the left cell and the right cell. The left robot starts in the left cell, which is the left player's home square. The right robot starts in the right cell, which is the right player's home square. Each robot holds an item that their player values at $1, but which the other player values at $2. The game lasts for one timestep. Each robot must decide whether or not to move into the opponent's home square.</li>\n<li>A <em>Stag Hunt: </em>there are two player robots, two hare robots, and one stag robot. Each player robot has just barely enough time to attack exactly one non-player robot, take it's item, and get to it's home square before the game ends. The Hare robot has no shields and holds a low-value item. The Stag robot holds two high-value items, but has one shield, which means that it must be attacked twice in order to drop its items. (The time constraints are such that the attack must be simultaneous, and such that no one player can score both of the stag's items.)</li>\n<li>A <em>Precommitment game</em>, to be explained in an upcoming post.</li>\n<li>A <em>Self-destruction game</em>, to be explained in an upcoming post.</li>\n</ol>\n<div>In order to play these games, you'll have to provide machines for the player robots. This can be written using the <em>ct </em>language and the <em>ct2hs</em>&nbsp;binary as described above. For example:</div>\n<div><br></div>\n<pre>&gt; ct2hs LeftBot &gt; LeftBot.hs</pre>\n<pre>&gt; ct2hs RightBot &gt; RightBot.hs</pre>\n<pre>&gt; ghci PrisonersDilemma</pre>\n<pre>\u03bb :m +LeftBot</pre>\n<pre>\u03bb :m +RightBot</pre>\n<pre>\u03bb run LeftBot.machine RightBot.machine</pre>\n<pre>...</pre>", "sections": [{"title": "Playing with Botworld", "anchor": "Playing_with_Botworld", "level": 1}, {"title": "Changelog", "anchor": "Changelog", "level": 1}, {"title": "Displaying Botworlds", "anchor": "Displaying_Botworlds", "level": 1}, {"title": "Writing Constree", "anchor": "Writing_Constree", "level": 1}, {"title": "New Games", "anchor": "New_Games", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-01T01:50:11.633Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Games meetup", "slug": "meetup-washington-dc-games-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wYvbAfDKNxA55b4AW/meetup-washington-dc-games-meetup-2", "pageUrlRelative": "/posts/wYvbAfDKNxA55b4AW/meetup-washington-dc-games-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/wYvbAfDKNxA55b4AW/meetup-washington-dc-games-meetup-2", "postedAtFormatted": "Thursday, May 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYvbAfDKNxA55b4AW%2Fmeetup-washington-dc-games-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYvbAfDKNxA55b4AW%2Fmeetup-washington-dc-games-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYvbAfDKNxA55b4AW%2Fmeetup-washington-dc-games-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zu'>Washington DC Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zu'>Washington DC Games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wYvbAfDKNxA55b4AW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.699113540015723e-06, "legacy": true, "legacyId": "26131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Games_meetup\">Discussion article for the meetup : <a href=\"/meetups/zu\">Washington DC Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/zu\">Washington DC Games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-01T02:42:01.079Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, May 1-15 ", "slug": "group-rationality-diary-may-1-15-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:05.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jSQiJJPSmR8DJh3jR/group-rationality-diary-may-1-15-0", "pageUrlRelative": "/posts/jSQiJJPSmR8DJh3jR/group-rationality-diary-may-1-15-0", "linkUrl": "https://www.lesswrong.com/posts/jSQiJJPSmR8DJh3jR/group-rationality-diary-may-1-15-0", "postedAtFormatted": "Thursday, May 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20May%201-15%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20May%201-15%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjSQiJJPSmR8DJh3jR%2Fgroup-rationality-diary-may-1-15-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20May%201-15%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjSQiJJPSmR8DJh3jR%2Fgroup-rationality-diary-may-1-15-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjSQiJJPSmR8DJh3jR%2Fgroup-rationality-diary-may-1-15-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.272727966308594px; text-align: justify;\">This is the public group instrumental rationality diary for May 1-15.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 24.272727966308594px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.272727966308594px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.272727966308594px; text-align: justify;\">Previous diary:&nbsp;<a href=\"/lw/k2q/group_rationality_diary_april_1630/\"><span style=\"color: #8a8a8b;\">A</span>pril 16-30</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.272727966308594px; text-align: justify;\">Next diary: &nbsp;<a href=\"/r/discussion/lw/k8v/group_rationality_diary_may_1631/\">May 16-31</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 24.272727966308594px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jSQiJJPSmR8DJh3jR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xnXqGXDqSMBmmhwpi", "D3vrEJv5S2BxdkBnN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-01T09:45:45.166Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes May 2014", "slug": "rationality-quotes-may-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:07.728Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "elharo", "createdAt": "2012-12-28T14:11:02.335Z", "isAdmin": false, "displayName": "elharo"}, "userId": "cgJcCeZhdRnGtwMMR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BnNZgzC3GAsQh3tSF/rationality-quotes-may-2014", "pageUrlRelative": "/posts/BnNZgzC3GAsQh3tSF/rationality-quotes-may-2014", "linkUrl": "https://www.lesswrong.com/posts/BnNZgzC3GAsQh3tSF/rationality-quotes-may-2014", "postedAtFormatted": "Thursday, May 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20May%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20May%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnNZgzC3GAsQh3tSF%2Frationality-quotes-may-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20May%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnNZgzC3GAsQh3tSF%2Frationality-quotes-may-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnNZgzC3GAsQh3tSF%2Frationality-quotes-may-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<div id=\"entry_t3_jsm\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">\n<li>Please post all quotes separately, so that they can be upvoted or  downvoted separately. (If they are strongly related, reply to your own  comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or  Robin Hanson. If you'd like to revive an old quote from one of those  sources, please do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n<li>Provide sufficient information (URL, title, date, page number, etc.)  to enable a reader to find the place where you read the quote, or its  original source if available. Do not quote with only a name. </li>\n</ul>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BnNZgzC3GAsQh3tSF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 1.6997519101768245e-06, "legacy": true, "legacyId": "26134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 299, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-01T12:44:58.481Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: Rationalist Fun and Games!", "slug": "meetup-canberra-rationalist-fun-and-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WpPCiKGB4veKHguTi/meetup-canberra-rationalist-fun-and-games", "pageUrlRelative": "/posts/WpPCiKGB4veKHguTi/meetup-canberra-rationalist-fun-and-games", "linkUrl": "https://www.lesswrong.com/posts/WpPCiKGB4veKHguTi/meetup-canberra-rationalist-fun-and-games", "postedAtFormatted": "Thursday, May 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20Rationalist%20Fun%20and%20Games!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20Rationalist%20Fun%20and%20Games!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpPCiKGB4veKHguTi%2Fmeetup-canberra-rationalist-fun-and-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20Rationalist%20Fun%20and%20Games!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpPCiKGB4veKHguTi%2Fmeetup-canberra-rationalist-fun-and-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpPCiKGB4veKHguTi%2Fmeetup-canberra-rationalist-fun-and-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zv'>Canberra: Rationalist Fun and Games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 May 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Note the change in location: we will now be meeting in room N101 of the CSIT building.</p>\n\n<p>Also note the different date than normal: Friday the 23rd is the CSSA's Epic Games Night, so we will be meeting on Saturday the 24th instead.</p>\n\n<p>This meetup, we will be having rationalist fun and games! We will start off with 5-minute debiasing, where we will break up into groups and think of ways to attack the scourge of evil biases. Then, we will move on to a game of Liar's Dice, described here: <a href=\"http://en.wikipedia.org/wiki/Liar%27s_dice\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Liar%27s_dice</a> , which will teach us about probability and bluffing. As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the same location.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zv'>Canberra: Rationalist Fun and Games!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WpPCiKGB4veKHguTi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 1.6999926001858455e-06, "legacy": true, "legacyId": "26135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__Rationalist_Fun_and_Games_\">Discussion article for the meetup : <a href=\"/meetups/zv\">Canberra: Rationalist Fun and Games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 May 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Note the change in location: we will now be meeting in room N101 of the CSIT building.</p>\n\n<p>Also note the different date than normal: Friday the 23rd is the CSSA's Epic Games Night, so we will be meeting on Saturday the 24th instead.</p>\n\n<p>This meetup, we will be having rationalist fun and games! We will start off with 5-minute debiasing, where we will break up into groups and think of ways to attack the scourge of evil biases. Then, we will move on to a game of Liar's Dice, described here: <a href=\"http://en.wikipedia.org/wiki/Liar%27s_dice\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Liar%27s_dice</a> , which will teach us about probability and bluffing. As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the same location.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__Rationalist_Fun_and_Games_1\">Discussion article for the meetup : <a href=\"/meetups/zv\">Canberra: Rationalist Fun and Games!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: Rationalist Fun and Games!", "anchor": "Discussion_article_for_the_meetup___Canberra__Rationalist_Fun_and_Games_", "level": 1}, {"title": "Discussion article for the meetup : Canberra: Rationalist Fun and Games!", "anchor": "Discussion_article_for_the_meetup___Canberra__Rationalist_Fun_and_Games_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-01T21:49:20.584Z", "modifiedAt": null, "url": null, "title": "May 2014 Media Thread", "slug": "may-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:13.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/evcubppGGCXEtqCkA/may-2014-media-thread", "pageUrlRelative": "/posts/evcubppGGCXEtqCkA/may-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/evcubppGGCXEtqCkA/may-2014-media-thread", "postedAtFormatted": "Thursday, May 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20May%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMay%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevcubppGGCXEtqCkA%2Fmay-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=May%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevcubppGGCXEtqCkA%2Fmay-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevcubppGGCXEtqCkA%2Fmay-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "evcubppGGCXEtqCkA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 1.7007240416622713e-06, "legacy": true, "legacyId": "26138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-02T14:15:49.787Z", "modifiedAt": null, "url": null, "title": "The Extended Living-Forever Strategy-Space", "slug": "the-extended-living-forever-strategy-space", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.450Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Froolow", "createdAt": "2014-03-25T11:51:16.226Z", "isAdmin": false, "displayName": "Froolow"}, "userId": "BARcGh2ChTXTNhvxK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KjAgw46Zfp3E5Zpsj/the-extended-living-forever-strategy-space", "pageUrlRelative": "/posts/KjAgw46Zfp3E5Zpsj/the-extended-living-forever-strategy-space", "linkUrl": "https://www.lesswrong.com/posts/KjAgw46Zfp3E5Zpsj/the-extended-living-forever-strategy-space", "postedAtFormatted": "Friday, May 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Extended%20Living-Forever%20Strategy-Space&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Extended%20Living-Forever%20Strategy-Space%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjAgw46Zfp3E5Zpsj%2Fthe-extended-living-forever-strategy-space%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Extended%20Living-Forever%20Strategy-Space%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjAgw46Zfp3E5Zpsj%2Fthe-extended-living-forever-strategy-space", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjAgw46Zfp3E5Zpsj%2Fthe-extended-living-forever-strategy-space", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2588, "htmlBody": "<p><em>I wanted to try and write this like a sequence post with a little story at the beginning because the style is hard to beat if you can pull it off. For those that want to skip to the meat of the argument, scroll down to the section titled &lsquo;The Jealous God of Cryonics&rsquo;</em></p>\n<h4>Bizzaro-Pascal</h4>\n<p>The year is 1600BC and Moses is scrambling down the slopes of Mount Sinai under the blazing Egyptian sun, with two stone tablets tucked under his arms - strangely small for the enduring impact they will have on the world. Pausing a moment to take a sip of water from his waterskin, he decided to double-check the words on the tablet were the same as those God had dictated to him before reading them to the Israelites &ndash; it wouldn&rsquo;t do to have a typo <a href=\"http://en.wikipedia.org/wiki/Wicked_Bible\">encouraging adultery</a>! Suddenly, a great shockwave bowled Moses onto the ground. It was simultaneously as loud as the universe tearing itself into two nearly identical copies, but as quiet as the difference between a coin landing on heads rather than tails. Moses - trembling with shock - picked himself up, dusted off the tablets and scratched his beard. He was <em>sure</em> that the Second Commandment looked a bit different, but he couldn&rsquo;t quite put his finger on it...</p>\n<p>More than three thousand years later, Blaise Pascal is about to formulate the Wager that would make him infamous. &ldquo;You see,&rdquo; he says, &ldquo;If God exists then the payoff is infinitely positive for believing in Him and infinitely negative for not, therefore whatever the cost of believing you should do it&rdquo;.</p>\n<p>&ldquo;Well I&rsquo;m sceptical,&rdquo; says his friend, &ldquo;It seems to me that the idea of an infinite payoff is incoherent to begin with, plus you have no particular reason to privilege the hypothesis that the Christian God<em> should</em> and <em>wants</em> to be worshipped, and not to mention the fact that if I were God I&rsquo;d be pretty irritated that people pretended believe in Me because of some probabilistic argument rather than by observing all of My great works&rdquo;</p>\n<p>&ldquo;But don&rsquo;t you see?&rdquo; Pascal rejoins, &ldquo;God in His infinite goodness foresaw your objections and wrote the Second Commandment specifically to take that into account; &lsquo;Thou shall have no other God but me, unless thou feels that thy can maximise thine&rsquo;s utility by ignoring this Commandment and worshipping multiple Gods. Seriously, I don&rsquo;t mind, worship as many Gods as you want with whatever degree of &lsquo;true&rsquo; faithfulness versus rational utility maximising makes you happiest (although I recommend worshipping only Gods that do not prohibit the worship of other Gods, so as to maximise your chances of getting it right and going to heaven)&rsquo; &ldquo;</p>\n<p>&ldquo;Hmm... Yes, come to think of it there has always been something a little different about that Commandment compared to the rest. I didn&rsquo;t think much of it because there exist similar laws in every other major religion, which now I reflect on it should probably have tipped me off to the format of your Wager quite a long time before now&rdquo;</p>\n<p>&ldquo;You see my Wager suggests you should worship the largest subset of non-contradictory Gods you possibly can; although I acknowledge that the probability of selecting the true God out of all of God-space is small (and for that God to both exist and select for heaven based on faithfulness is also unlikely), the payoff is sufficiently wonderful to make it worth the small up-front cost of most religions&rsquo; declarations of faith. I can only imagine what sort of a fanatic would seriously propose this argument in a Universe where all Gods demand you sample only once from God-space!&rdquo;</p>\n<p>In the universe Pascal describes, all you need to do to qualify for eternal life given a particular religion is true is to say out loud that you are a true believer (or go through some non-traumatic initiation rite, like a baptism or Shahadah). The probability of a God existing is still low, and the probability of that God caring that you worship Zir is still low, but it is (almost certainly) rational to take the advice of Pascal and find a maximal subset of Gods that you think maximises your chance of eternal happiness.</p>\n<h4>The Jealous God of Cryonics</h4>\n<p>Cryonics is not like Pascal&rsquo;s Wager except superficially, but this little story attempts to drive an intuition which would appeal to bizzaro-Pascal. In this universe, someone who worshipped only one God would be deeply irrational. They might be able to defend their choice with some applause-light soundbites (&ldquo;I have great faith, so I need no other Gods&rdquo;) but in a purely utility-maximising sense - the sense where we try and maximise the number of happy years we live &ndash; this person is behaving irrationally. But although this seems obvious to us, some (most) cryonics advocates behave as though cryonics is a &lsquo;jealous&rsquo; God (like in our universe) rather more accurately modelling it as a &lsquo;permissive&rsquo; God like in bizzaro-Pascal&rsquo;s universe. Cryonics doesn&rsquo;t care at all if you adopt other strategies for maximising your lifespan except insofar as they conflict with cryonics. So for example high religiosity and cryonics are logically compatible as far as I can see; if brain death <em>really is</em> death (that is to say it is completely irreversible) then at least you have the back-up possibility that an afterlife exists. Yet it seems to me that supporters of cryonics happily stop looking for alternate life-extension strategies almost as soon as they discover cryonics (I hypothesise the actual mechanism is that someone convinces them cryonics is rational and then they forget about the rest of the strategy-space in their excitement). Certainly, I can&rsquo;t find any discussions on cryonics on LessWrong promoting any alternate life-maximisation techniques except perhaps brain plasticisation. This is a shame, because it is possible that some additional life-extension techniques might be costlessly employed by those who want to live forever to greatly increase their expected utility.</p>\n<p>Looking around for literature on this topic. Alcor, for example, have an article entitled &lsquo;<a href=\"http://www.alcor.org/cryonics/cryonics0703.pdf \">The Road Less Travelled</a>&rsquo; talking about potential alternatives to cryonics including desiccation and peat preservation. <a href=\"http://www.gwern.net/plastination\">Brain plasticisation </a>and chemical preservation are seriously discussed as alternatives even amongst those who are strongly in favour of freezing; the consensus is that these techniques are likely to offer a higher success rate once they are perfected, but freezing is the way forward now. I can think of a few more outlandish methods of preservation (such as firing yourself into the heart of a black hole and assuming time dilation means you will still be alive when a recovery technique is developed or standing in a high-radiation environment hoping that your telomerase will re-knit) but these all suffer from the fact they are less likely to work than cryonics, and obviously so. Why would cryonicists waste time thinking about outlandish preservation techniques when they displace a more likely technique? Indeed, even if these techniques were more likely there are good reasons to treat cryonics as a Schelling point unless a new technique obviously dominates; we want future society to spend all of its resources targeting one problem, especially if we are part of the generation that is first experimenting with these techniques. While it surprises me that no cryonicists seem interested in this even as an intellectual exercise, it is at least rational to ignore low-probability techniques which displace higher-probability techniques with the same payoff for all of the above reasons.</p>\n<h4>The Extended Strategy-Space</h4>\n<p>But there seems to be no excuse for failing to consider additional strategies which complement cryonics; there exist a very great number of strategies which could be followed that <em>might</em> result in revivification before cryonics (or instead of cryonics if cryonics turns out to be impossible) and have a cost of strictly less than cryonic freezing. I&rsquo;ve given them short descriptions to enable easy reference in the comments (if anyone is interested) so don&rsquo;t read too much into the names. I&rsquo;ve also ordered them roughly in the order in which I find them plausible; up until the boundary between Social and Simulation Preservation I actual find the arguments <em>more</em> plausible than cryonics:</p>\n<ul>\n<li>Diarist Preservation: Begin recording your phone calls, pay someone to archive your web presence, begin keeping obsessive diaries and blog constantly. Hope that this can be recompiled into a coherent personality at some point in the future, or at the very least be used to plug gaps in the personality of the unfrozen body.</li>\n<li>Genetic Preservation: Take genetic samples of yourself and preserve them in a platinum-iridium bar in binary. Hope that personality is very largely genetic, and the proportion that isn&rsquo;t can be reconstructed from statistical analysis of the time period in which you live (perhaps by employing Diarist Preservation in tandem).</li>\n<li>MRI Preservation: Subject yourself to MRI scans as often as possible (it may be helpful to fake a serious neurological condition). Ask for copies and encode them in microchips that you scatter round the world as you travel. Hope that future societies will find the information useful to constructing an em and will find the chips if they are distributed widely enough.</li>\n<li>Signal Preservation: Obsessively generate long streams of nonsense binary based on tapping randomly at a keyboard. Assume that these long strings must correspond in some way to brain states, and that future mathematics will be advanced enough to untangle the signal from the noise. Post these long strings of text to as many internet sites as possible to preserve them (VERY VERY IMPORTANT NOTE: If you decide to try this strategy you must <em>absolutely</em> ensure that the first few characters of every message are a code known only to you salted with (for example) the current time and then hashed, or the first word of the next string of binary you produce. Otherwise unkind people could claim to be you, post their own strings and screw up your revival. I don't think it is a serious worry that people who can bring you back from the dead will struggle with SHA)</li>\n<li>Social Preservation: Form a hypothesis which says (roughly) &ldquo;The more people who know about me that I can persuade to freeze their brain information with me, the more likely it is that any gap in my own brain-state can be plugged with information from another individual&rsquo;s brainstate&rdquo;. Act ruthlessly on this hypothesis; pay for friends and family to get frozen conditional on their memorising a list of facts about you. Offer to discount a friend&rsquo;s cryo in exchange for them signing up with another organisation to you (in case yours has a damaging but not fatal mishap and you need perfectly-stored redundant information to back yourself up). Attend cryonics conferences like a vulture, and socialise as much as you possibly can. An additional note about this strategy (which every pro-cryonicist knows); it is hugely in your interest to take a large 21<sup>st</sup> Century contingent with you to whatever time you are revived, so that your 21<sup>st</sup> Century contingent can form a natural political bloc. Even better if the majority of that bloc know and like you!</li>\n<li>Simulation Preservation: Bury &lsquo;time-capsules&rsquo; &ndash; lead-lined containers which explain in as many languages as possible who you are and expressing a desire to be resurrected if society has discovered that we live in a simulation and has the power to talk to the simulators. Otherwise ask the society to rebury your letter (after translating the request into all <em>current</em> languages) to await the arrival of a true simulationist society. A stronger version of this is to employ one of the aforementioned Preservation techniques and add in your letter that you would be happy to be resurrected inside a simulation created by <em>this</em> society based on the information preserved by that technique; that insures against the possibility that simulation is logically possible but we have not yet discovered a way to communicate with the simulator.</li>\n<li>Philosophical Preservation: Discover a completely watertight argument which proves &ndash; perhaps probabilistically - that &lsquo;you&rsquo; (the bit of you you hope will survive death) is totally identifiable with something permanent like the information on your Y-chromosome (for men) or the unordered atoms in your brain. Do whatever this argument implies to extend your life. This might sound silly, but many people really do profess to believe their &lsquo;soul&rsquo; survives forever and they can increase their chances of this occurring by correctly interpreting a very old book, so it is highly likely that there is an argument that would convince you, even if that argument is not actually valid. A clever rationalist might even be able to identify a subset of religious/philosophical activities that maximises their chance of eternal life in heaven (as per the introductory story).</li>\n<li>Evolutionary Preservation: Blast genetic samples of yourself into space. Hope that eons later one sample will come to rest on a planet suitable for life and evolve into a creature identical to you except whereas you have mostly true beliefs, this creature will have mostly delusional beliefs that correspond in a one-to-one way with your true beliefs. For example while you truthfully think, &ldquo;I was alive in 2010&rdquo;, this creature will have a delusional belief, &ldquo;I was alive in 2010&rdquo;, plus whatever additional delusional beliefs it needs to make this belief cohere, for example, &ldquo;I must have been stunned sometime in early 2070 (when my beliefs appear to stop) and taken to this strange planet I don&rsquo;t recognise&rdquo;.</li>\n<li>Time-travel Preservation: Do something so marvellous or heinous that if time travel exists, some time travellers will travel to the moment of your triumph/crime to watch. Overpower a time traveller, and take their time machine. You might have a very low prior probability of being able to do something so brilliant/evil as to compete with the whole rest of history, but bear in mind the first successful hijack of a time machine would itself be an event worth watching by future time travellers, so you may not actually need to do anything marvellous in the first place; just make a binding resolution with yourself to steal the first time machine you come across and look to see if any police phone boxes pop up from nowhere. Making this resolution once or twice a day for the rest of your life is almost costless, although perhaps you would want to attend a combat sport class to increase the chances of a successful overpowering.</li>\n</ul>\n<p>Each of these strategies have a number of features which make them attractive; they are (mostly) less expensive than cryonics, they do not strictly lower your chance of cryogenic revival (and in some cases probably increase it) and all have a non-zero chance of preserving your brainstates at least until future society is advanced enough to do something with them. Even better, most of these strategies synergise well with each other; if I decide to get myself frozen I will definitely also pay for fMRIs to record my brainstate as I think about various stimuli and store copies of those recordings with multiple institutions. I don&rsquo;t think this list is exhaustive, but I do think it covers a good amount of the possible &lsquo;live forever&rsquo; strategy space. It does not explore strategies which are absurdly expensive or which interfere with cryonics - so it is still only one small corner of the total strategy space &ndash; but I think it expands the area of the strategy space most people are interested in; the bit in which you and I can act.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KjAgw46Zfp3E5Zpsj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 18, "baseScore": 17, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "26142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>I wanted to try and write this like a sequence post with a little story at the beginning because the style is hard to beat if you can pull it off. For those that want to skip to the meat of the argument, scroll down to the section titled \u2018The Jealous God of Cryonics\u2019</em></p>\n<h4 id=\"Bizzaro_Pascal\">Bizzaro-Pascal</h4>\n<p>The year is 1600BC and Moses is scrambling down the slopes of Mount Sinai under the blazing Egyptian sun, with two stone tablets tucked under his arms - strangely small for the enduring impact they will have on the world. Pausing a moment to take a sip of water from his waterskin, he decided to double-check the words on the tablet were the same as those God had dictated to him before reading them to the Israelites \u2013 it wouldn\u2019t do to have a typo <a href=\"http://en.wikipedia.org/wiki/Wicked_Bible\">encouraging adultery</a>! Suddenly, a great shockwave bowled Moses onto the ground. It was simultaneously as loud as the universe tearing itself into two nearly identical copies, but as quiet as the difference between a coin landing on heads rather than tails. Moses - trembling with shock - picked himself up, dusted off the tablets and scratched his beard. He was <em>sure</em> that the Second Commandment looked a bit different, but he couldn\u2019t quite put his finger on it...</p>\n<p>More than three thousand years later, Blaise Pascal is about to formulate the Wager that would make him infamous. \u201cYou see,\u201d he says, \u201cIf God exists then the payoff is infinitely positive for believing in Him and infinitely negative for not, therefore whatever the cost of believing you should do it\u201d.</p>\n<p>\u201cWell I\u2019m sceptical,\u201d says his friend, \u201cIt seems to me that the idea of an infinite payoff is incoherent to begin with, plus you have no particular reason to privilege the hypothesis that the Christian God<em> should</em> and <em>wants</em> to be worshipped, and not to mention the fact that if I were God I\u2019d be pretty irritated that people pretended believe in Me because of some probabilistic argument rather than by observing all of My great works\u201d</p>\n<p>\u201cBut don\u2019t you see?\u201d Pascal rejoins, \u201cGod in His infinite goodness foresaw your objections and wrote the Second Commandment specifically to take that into account; \u2018Thou shall have no other God but me, unless thou feels that thy can maximise thine\u2019s utility by ignoring this Commandment and worshipping multiple Gods. Seriously, I don\u2019t mind, worship as many Gods as you want with whatever degree of \u2018true\u2019 faithfulness versus rational utility maximising makes you happiest (although I recommend worshipping only Gods that do not prohibit the worship of other Gods, so as to maximise your chances of getting it right and going to heaven)\u2019 \u201c</p>\n<p>\u201cHmm... Yes, come to think of it there has always been something a little different about that Commandment compared to the rest. I didn\u2019t think much of it because there exist similar laws in every other major religion, which now I reflect on it should probably have tipped me off to the format of your Wager quite a long time before now\u201d</p>\n<p>\u201cYou see my Wager suggests you should worship the largest subset of non-contradictory Gods you possibly can; although I acknowledge that the probability of selecting the true God out of all of God-space is small (and for that God to both exist and select for heaven based on faithfulness is also unlikely), the payoff is sufficiently wonderful to make it worth the small up-front cost of most religions\u2019 declarations of faith. I can only imagine what sort of a fanatic would seriously propose this argument in a Universe where all Gods demand you sample only once from God-space!\u201d</p>\n<p>In the universe Pascal describes, all you need to do to qualify for eternal life given a particular religion is true is to say out loud that you are a true believer (or go through some non-traumatic initiation rite, like a baptism or Shahadah). The probability of a God existing is still low, and the probability of that God caring that you worship Zir is still low, but it is (almost certainly) rational to take the advice of Pascal and find a maximal subset of Gods that you think maximises your chance of eternal happiness.</p>\n<h4 id=\"The_Jealous_God_of_Cryonics\">The Jealous God of Cryonics</h4>\n<p>Cryonics is not like Pascal\u2019s Wager except superficially, but this little story attempts to drive an intuition which would appeal to bizzaro-Pascal. In this universe, someone who worshipped only one God would be deeply irrational. They might be able to defend their choice with some applause-light soundbites (\u201cI have great faith, so I need no other Gods\u201d) but in a purely utility-maximising sense - the sense where we try and maximise the number of happy years we live \u2013 this person is behaving irrationally. But although this seems obvious to us, some (most) cryonics advocates behave as though cryonics is a \u2018jealous\u2019 God (like in our universe) rather more accurately modelling it as a \u2018permissive\u2019 God like in bizzaro-Pascal\u2019s universe. Cryonics doesn\u2019t care at all if you adopt other strategies for maximising your lifespan except insofar as they conflict with cryonics. So for example high religiosity and cryonics are logically compatible as far as I can see; if brain death <em>really is</em> death (that is to say it is completely irreversible) then at least you have the back-up possibility that an afterlife exists. Yet it seems to me that supporters of cryonics happily stop looking for alternate life-extension strategies almost as soon as they discover cryonics (I hypothesise the actual mechanism is that someone convinces them cryonics is rational and then they forget about the rest of the strategy-space in their excitement). Certainly, I can\u2019t find any discussions on cryonics on LessWrong promoting any alternate life-maximisation techniques except perhaps brain plasticisation. This is a shame, because it is possible that some additional life-extension techniques might be costlessly employed by those who want to live forever to greatly increase their expected utility.</p>\n<p>Looking around for literature on this topic. Alcor, for example, have an article entitled \u2018<a href=\"http://www.alcor.org/cryonics/cryonics0703.pdf \">The Road Less Travelled</a>\u2019 talking about potential alternatives to cryonics including desiccation and peat preservation. <a href=\"http://www.gwern.net/plastination\">Brain plasticisation </a>and chemical preservation are seriously discussed as alternatives even amongst those who are strongly in favour of freezing; the consensus is that these techniques are likely to offer a higher success rate once they are perfected, but freezing is the way forward now. I can think of a few more outlandish methods of preservation (such as firing yourself into the heart of a black hole and assuming time dilation means you will still be alive when a recovery technique is developed or standing in a high-radiation environment hoping that your telomerase will re-knit) but these all suffer from the fact they are less likely to work than cryonics, and obviously so. Why would cryonicists waste time thinking about outlandish preservation techniques when they displace a more likely technique? Indeed, even if these techniques were more likely there are good reasons to treat cryonics as a Schelling point unless a new technique obviously dominates; we want future society to spend all of its resources targeting one problem, especially if we are part of the generation that is first experimenting with these techniques. While it surprises me that no cryonicists seem interested in this even as an intellectual exercise, it is at least rational to ignore low-probability techniques which displace higher-probability techniques with the same payoff for all of the above reasons.</p>\n<h4 id=\"The_Extended_Strategy_Space\">The Extended Strategy-Space</h4>\n<p>But there seems to be no excuse for failing to consider additional strategies which complement cryonics; there exist a very great number of strategies which could be followed that <em>might</em> result in revivification before cryonics (or instead of cryonics if cryonics turns out to be impossible) and have a cost of strictly less than cryonic freezing. I\u2019ve given them short descriptions to enable easy reference in the comments (if anyone is interested) so don\u2019t read too much into the names. I\u2019ve also ordered them roughly in the order in which I find them plausible; up until the boundary between Social and Simulation Preservation I actual find the arguments <em>more</em> plausible than cryonics:</p>\n<ul>\n<li>Diarist Preservation: Begin recording your phone calls, pay someone to archive your web presence, begin keeping obsessive diaries and blog constantly. Hope that this can be recompiled into a coherent personality at some point in the future, or at the very least be used to plug gaps in the personality of the unfrozen body.</li>\n<li>Genetic Preservation: Take genetic samples of yourself and preserve them in a platinum-iridium bar in binary. Hope that personality is very largely genetic, and the proportion that isn\u2019t can be reconstructed from statistical analysis of the time period in which you live (perhaps by employing Diarist Preservation in tandem).</li>\n<li>MRI Preservation: Subject yourself to MRI scans as often as possible (it may be helpful to fake a serious neurological condition). Ask for copies and encode them in microchips that you scatter round the world as you travel. Hope that future societies will find the information useful to constructing an em and will find the chips if they are distributed widely enough.</li>\n<li>Signal Preservation: Obsessively generate long streams of nonsense binary based on tapping randomly at a keyboard. Assume that these long strings must correspond in some way to brain states, and that future mathematics will be advanced enough to untangle the signal from the noise. Post these long strings of text to as many internet sites as possible to preserve them (VERY VERY IMPORTANT NOTE: If you decide to try this strategy you must <em>absolutely</em> ensure that the first few characters of every message are a code known only to you salted with (for example) the current time and then hashed, or the first word of the next string of binary you produce. Otherwise unkind people could claim to be you, post their own strings and screw up your revival. I don't think it is a serious worry that people who can bring you back from the dead will struggle with SHA)</li>\n<li>Social Preservation: Form a hypothesis which says (roughly) \u201cThe more people who know about me that I can persuade to freeze their brain information with me, the more likely it is that any gap in my own brain-state can be plugged with information from another individual\u2019s brainstate\u201d. Act ruthlessly on this hypothesis; pay for friends and family to get frozen conditional on their memorising a list of facts about you. Offer to discount a friend\u2019s cryo in exchange for them signing up with another organisation to you (in case yours has a damaging but not fatal mishap and you need perfectly-stored redundant information to back yourself up). Attend cryonics conferences like a vulture, and socialise as much as you possibly can. An additional note about this strategy (which every pro-cryonicist knows); it is hugely in your interest to take a large 21<sup>st</sup> Century contingent with you to whatever time you are revived, so that your 21<sup>st</sup> Century contingent can form a natural political bloc. Even better if the majority of that bloc know and like you!</li>\n<li>Simulation Preservation: Bury \u2018time-capsules\u2019 \u2013 lead-lined containers which explain in as many languages as possible who you are and expressing a desire to be resurrected if society has discovered that we live in a simulation and has the power to talk to the simulators. Otherwise ask the society to rebury your letter (after translating the request into all <em>current</em> languages) to await the arrival of a true simulationist society. A stronger version of this is to employ one of the aforementioned Preservation techniques and add in your letter that you would be happy to be resurrected inside a simulation created by <em>this</em> society based on the information preserved by that technique; that insures against the possibility that simulation is logically possible but we have not yet discovered a way to communicate with the simulator.</li>\n<li>Philosophical Preservation: Discover a completely watertight argument which proves \u2013 perhaps probabilistically - that \u2018you\u2019 (the bit of you you hope will survive death) is totally identifiable with something permanent like the information on your Y-chromosome (for men) or the unordered atoms in your brain. Do whatever this argument implies to extend your life. This might sound silly, but many people really do profess to believe their \u2018soul\u2019 survives forever and they can increase their chances of this occurring by correctly interpreting a very old book, so it is highly likely that there is an argument that would convince you, even if that argument is not actually valid. A clever rationalist might even be able to identify a subset of religious/philosophical activities that maximises their chance of eternal life in heaven (as per the introductory story).</li>\n<li>Evolutionary Preservation: Blast genetic samples of yourself into space. Hope that eons later one sample will come to rest on a planet suitable for life and evolve into a creature identical to you except whereas you have mostly true beliefs, this creature will have mostly delusional beliefs that correspond in a one-to-one way with your true beliefs. For example while you truthfully think, \u201cI was alive in 2010\u201d, this creature will have a delusional belief, \u201cI was alive in 2010\u201d, plus whatever additional delusional beliefs it needs to make this belief cohere, for example, \u201cI must have been stunned sometime in early 2070 (when my beliefs appear to stop) and taken to this strange planet I don\u2019t recognise\u201d.</li>\n<li>Time-travel Preservation: Do something so marvellous or heinous that if time travel exists, some time travellers will travel to the moment of your triumph/crime to watch. Overpower a time traveller, and take their time machine. You might have a very low prior probability of being able to do something so brilliant/evil as to compete with the whole rest of history, but bear in mind the first successful hijack of a time machine would itself be an event worth watching by future time travellers, so you may not actually need to do anything marvellous in the first place; just make a binding resolution with yourself to steal the first time machine you come across and look to see if any police phone boxes pop up from nowhere. Making this resolution once or twice a day for the rest of your life is almost costless, although perhaps you would want to attend a combat sport class to increase the chances of a successful overpowering.</li>\n</ul>\n<p>Each of these strategies have a number of features which make them attractive; they are (mostly) less expensive than cryonics, they do not strictly lower your chance of cryogenic revival (and in some cases probably increase it) and all have a non-zero chance of preserving your brainstates at least until future society is advanced enough to do something with them. Even better, most of these strategies synergise well with each other; if I decide to get myself frozen I will definitely also pay for fMRIs to record my brainstate as I think about various stimuli and store copies of those recordings with multiple institutions. I don\u2019t think this list is exhaustive, but I do think it covers a good amount of the possible \u2018live forever\u2019 strategy space. It does not explore strategies which are absurdly expensive or which interfere with cryonics - so it is still only one small corner of the total strategy space \u2013 but I think it expands the area of the strategy space most people are interested in; the bit in which you and I can act.</p>", "sections": [{"title": "Bizzaro-Pascal", "anchor": "Bizzaro_Pascal", "level": 1}, {"title": "The Jealous God of Cryonics", "anchor": "The_Jealous_God_of_Cryonics", "level": 1}, {"title": "The Extended Strategy-Space", "anchor": "The_Extended_Strategy_Space", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-02T16:09:12.436Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-118", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XaQ8baJFPrcPrpzDC/weekly-lw-meetups-118", "pageUrlRelative": "/posts/XaQ8baJFPrcPrpzDC/weekly-lw-meetups-118", "linkUrl": "https://www.lesswrong.com/posts/XaQ8baJFPrcPrpzDC/weekly-lw-meetups-118", "postedAtFormatted": "Friday, May 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXaQ8baJFPrcPrpzDC%2Fweekly-lw-meetups-118%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXaQ8baJFPrcPrpzDC%2Fweekly-lw-meetups-118", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXaQ8baJFPrcPrpzDC%2Fweekly-lw-meetups-118", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 566, "htmlBody": "<p><strong>This summary was posted to LW main on April 25th. The following week's summary is <a href=\"/lw/k67/new_lw_meetup_houston/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/z6\">Christchurch, NZ Inaugural Meetup:&nbsp;<span class=\"date\">27 April 2014 04:30PM</span></a></li>\n<li><a href=\"/meetups/ye\">Las Vegas/Henderson, NV:&nbsp;<span class=\"date\">01 May 2014 06:00PM</span></a></li>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/z8\">Australia Mega-Meetup:&nbsp;<span class=\"date\">09 May 2014 05:00PM</span></a></li>\n<li><a href=\"/meetups/yw\">Bratislava Meetup XII.:&nbsp;<span class=\"date\">28 April 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/zm\">(Buffalo NY) Sunday Meetup:&nbsp;<span class=\"date\">04 May 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/ys\">Hamburg - Report from Berlin:&nbsp;<span class=\"date\">26 April 2014 05:00PM</span></a></li>\n<li><a href=\"/meetups/zk\">Montreal - Easy Lifehacks:&nbsp;<span class=\"date\">28 April 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/zh\">Munich Meetup:&nbsp;<span class=\"date\">11 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zj\">Ottawa - How to Run a Successful Less Wrong Meetup Group:&nbsp;<span class=\"date\">30 April 2014 07:30PM</span></a></li>\n<li><a href=\"/meetups/zl\">Urbana-Champaign, Consciousness:&nbsp;<span class=\"date\">27 April 2014 12:00PM</span></a></li>\n<li><a href=\"/meetups/zg\">Utrecht:&nbsp;<span class=\"date\">03 May 2014 05:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">26 April 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/zi\">(NYC) Effective Altruism 102 (NYC):&nbsp;<span class=\"date\">26 April 2014 05:00PM</span></a></li>\n<li><a href=\"/meetups/zn\">Washington DC Singing Meetup:&nbsp;<span class=\"date\">27 April 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/zo\">West LA&mdash;Honor and Glory:&nbsp;<span class=\"date\">30 April 2014 06:59PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XaQ8baJFPrcPrpzDC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.7022035761425133e-06, "legacy": true, "legacyId": "26079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d78neHtwpKSbHbc6J", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-03T13:38:48.134Z", "modifiedAt": null, "url": null, "title": "Incentive compatibility and the Revelation Principle", "slug": "incentive-compatibility-and-the-revelation-principle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:03.735Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N4gDA5HPpGC4mbTEZ/incentive-compatibility-and-the-revelation-principle", "pageUrlRelative": "/posts/N4gDA5HPpGC4mbTEZ/incentive-compatibility-and-the-revelation-principle", "linkUrl": "https://www.lesswrong.com/posts/N4gDA5HPpGC4mbTEZ/incentive-compatibility-and-the-revelation-principle", "postedAtFormatted": "Saturday, May 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Incentive%20compatibility%20and%20the%20Revelation%20Principle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIncentive%20compatibility%20and%20the%20Revelation%20Principle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN4gDA5HPpGC4mbTEZ%2Fincentive-compatibility-and-the-revelation-principle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Incentive%20compatibility%20and%20the%20Revelation%20Principle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN4gDA5HPpGC4mbTEZ%2Fincentive-compatibility-and-the-revelation-principle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN4gDA5HPpGC4mbTEZ%2Fincentive-compatibility-and-the-revelation-principle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1823, "htmlBody": "<p style=\"padding-left: 30px;\"><em>In which the Revelation Principle is introduced, showing all mechanisms can be reduced to incentive compatible mechanisms. With this insight, a solution (of sorts) is given to the public good problem in the <a href=\"/lw/k5r/mechanism_design_constructing_algorithms_for/\">last post</a>. Limitations of the Revelation Principle are also discussed.</em></p>\n<p>The formalism I introduced last time will now start paying off. We were left with the question of how to check whether a mechanism exists that satisfies some particular goal, naively requiring us to search over all possible procedures. Luckily though, the space of all possible mechanisms can be reduced to something manageable.</p>\n<p>Observe the following: suppose through divine intervention we were granted a mechanism <span class=\"math\">(<em>M</em>,\u2006<em>g</em>)</span> that implements a social choice function <span class=\"math\"><em>f</em></span>. In other words, the outcome when agents of types <span class=\"math\"><em>&theta;</em><sub>1</sub>,\u2006&hellip;,\u2006<em>&theta;</em><sub><em>n</em></sub></span> interact with the mechanism is exactly the outcome prescribed by the function <span class=\"math\"><em>f</em></span> for those types. Since a person&rsquo;s type encodes all relevant variation in their characteristics, preferences, or beliefs, we&rsquo;d know what that person wants to do if we knew their type. In particular, when agent <span class=\"math\"><em>i</em></span> is type <span class=\"math\"><em>&theta;</em><sub><em>i</em></sub></span>, we expect her choice of message to be <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>&theta;</em><sub><em>i</em></sub>)</span>. Once all the messages are sent, the function <span class=\"math\"><em>g</em></span> translates the agents&rsquo; choices into an outcome so that <span class=\"math\"><em>g</em>(<em>m</em><sub>1</sub>(<em>&theta;</em><sub>1</sub>),\u2006&hellip;,\u2006<em>m</em><sub><em>n</em></sub>(<em>&theta;</em><sub><em>n</em></sub>))\u2004=\u2004<em>f</em>(<em>&theta;</em><sub>1</sub>,\u2006&hellip;,\u2006<em>&theta;</em><sub><em>n</em></sub>)</span>.</p>\n<p>But wait! Since we expect a type <span class=\"math\"><em>&theta;</em><sub><em>i</em></sub></span> agent to send message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>&theta;</em><sub><em>i</em></sub>)</span>, why don&rsquo;t we just package that inside the mechanism? Each agent will tell us their type and the mechanism designer will play in proxy for the agent according to the original mechanism. We&rsquo;ll call this the <em>direct mechanism</em> for <span class=\"math\"><em>f</em></span> since all we need to know is that agents tell us their types and then are assigned outcome <span class=\"math\"><em>f</em>(<em>&theta;</em>)</span>, no matter what we&rsquo;ve blackboxed in the middle.</p>\n<p><a id=\"more\"></a></p>\n<p style=\"padding-left: 30px;\"><img src=\"http://images.lesswrong.com/t3_k69_0.png?v=e3a1c82c29f446f676e52f6178d12dc6\" alt=\"\" width=\"633\" height=\"398\" /></p>\n<p>&nbsp;</p>\n<p>Why did we expect an agent of type <span class=\"math\"><em>&theta;</em><sub><em>i</em></sub></span> to send message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>&theta;</em><sub><em>i</em></sub>)</span>? Presumably because that message maximized her utility. In particular, it had to be at least as good as the message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>&theta;</em>\u02b9<sub><em>i</em></sub>)</span> she&rsquo;s send when her type is different, giving us:</p>\n<p style=\"padding-left: 90px;\"><img src=\"http://latex.codecogs.com/gif.latex?\\begin{aligned}u_i(g(m_i(\\theta_i),m_{-i}),\\theta_i)&amp;\\ge&amp;space;u_i(g(m'_i,m_{-i}),\\theta_i),\\quad\\forall&amp;space;m'_i\\in&amp;space;M_i\\quad\\Rightarrow\\\\&amp;space;u_i(g(m_i(\\theta_i),m_{-i}),\\theta_i)&amp;\\ge&amp;space;u_i(g(m_i(\\theta'_i),m_{-i}),\\theta_i),\\quad\\forall\\theta'_i\\in&amp;space;\\Theta_i\\end{aligned}\" alt=\"\" /></p>\n<p>Since the outcomes of the mechanism coincide with&nbsp;<span class=\"math\"><em>f</em></span>, we can conclude</p>\n<p style=\"padding-left: 120px;\"><img src=\"http://latex.codecogs.com/gif.latex?u_i(f(\\theta_i,\\theta_{-i}),\\theta_i)&amp;\\ge&amp;space;u_i(f(\\theta'_i,&amp;space;\\theta_{-i}),\\theta_i),\\quad\\forall\\theta'_i\\in&amp;space;\\Theta_i\" alt=\"\" /></p>\n<p>Even though we started off with some mechanism, this last statement doesn't say anything about the mechanism itself, only the social choice function it implements. Let's call any social choice function <span class=\"math\"><em>f</em></span>&nbsp;that satisfies this constraint for every agent and for all possible types of others&nbsp;<em>strategy-proof</em> or <em>dominant-strategy incentive compatible (DSIC)</em>. Note that this can add up to a lot of constraints.</p>\n<p>These observations lead us to the following powerful tool:</p>\n<blockquote>\n<p><strong>The Revelation Principle (for dominant strategies):</strong> A social choice function <span class=\"math\"><em>f</em></span> is implementable in dominant strategies if and only if it is dominant-strategy incentive compatible. Alternatively, every mechanism that implements <span class=\"math\"><em>f</em></span> in dominant strategies is equivalent to a DSIC direct mechanism.</p>\n</blockquote>\n<p>Rather than requiring an agent to always (weakly) prefer their type&rsquo;s outcome, it might suffice that agents get greater utility on average for their true type:</p>\n<p style=\"padding-left: 90px;\"><img src=\"http://latex.codecogs.com/gif.latex?%5Coperatorname%7BE%7D_%7B%5Ctheta_%7B-i%7D%7D%5B%5C%3B%20u_i%28f%28%5Ctheta_i%2C%20%5Ctheta_%7B-i%7D%29%2C%20%5Ctheta_i%29%20%5C%3B%7C%5C%3B%20%5Ctheta_i%5C%2C%5D%20%5Cge%20%5Coperatorname%7BE%7D_%7B%5Ctheta_%7B-i%7D%7D%5B%5C%3B%20u_i%28f%28%5Ctheta%27_i%2C%20%5Ctheta_%7B-i%7D%29%2C%20%5Ctheta_i%29%20%5C%3B%7C%5C%3B%20%5Ctheta_i%5C%2C%5D%2C%20%5Cquad%20%5Cforall%20%5Ctheta_i%2C%20%5Ctheta%27_i%20%5Cin%20%5CTheta_i\" alt=\"\" /></p>\n<p>Social choice functions <span class=\"math\"><em>f</em></span> that satisfy this constraint for each agent are <em>Bayesian incentive compatible (BIC)</em>. Just as we had a Revelation Principle for dominant strategies, we have an analogous one for Bayes-Nash equilibrium:</p>\n<blockquote>\n<p><strong>The Revelation Principle (for Bayes-Nash equilibrium):</strong> A social choice function <span class=\"math\"><em>f</em></span> is implementable in Bayes-Nash equilibrium if and only if it is Bayes-Nash incentive compatible.</p>\n</blockquote>\n<p>As a check whether you&rsquo;re following, consider these questions:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Suppose you have a social choice function <span class=\"math\"><em>f</em></span> that is (dominant-strategy or Bayesian) incentive compatible for the type spaces <span class=\"math\">&Theta;\u2006<sub>1</sub>,\u2006&hellip;,\u2006&Theta;\u2006<sub><em>n</em></sub></span>. Now, throw out some types for each agent. Is the restriction of <span class=\"math\"><em>f</em></span> to these smaller type spaces still incentive compatible?</li>\n<li>Is it easier for a social choice function to be DSIC or BIC?</li>\n</ol>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><a id=\"more\">Answers in the footnotes.</a><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></p>\n<p>With the reduction provided by the various versions of the Revelation Principle, we can get away with analyzing only incentive compatible direct mechanisms rather than all possible ones. If a social choice function isn&rsquo;t incentive compatible, then it can&rsquo;t be implemented, no matter how fancy we get.</p>\n<p>There are many reasons why we wouldn&rsquo;t want to use a direct mechanism in practice: full types might be too complex to communicate easily, agents might be worried about privacy, or agents might not trust the mechanism operator to use the information like promised (changing the rules of the game after the fact, despite a claimed commitment to a particular outcome)<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup> <sup>,</sup> <sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>. Still, direct mechanisms are very straightforward for people to use. Rather than investing time to figure out how to play the game, the participants only have to consider what their true preferences are.</p>\n<p>In any case, direct mechanisms are very handy theoretically. From this perspective, all incentive problems boil down to encouraging agents to be honest about their private information. The only leeway we have is whether being honest should be a dominant strategy, a best response to the honesty of others, minimize the agent&rsquo;s maximimum regret, etc.</p>\n<h2 id=\"returning-to-the-painting-puzzle\">Returning to the painting puzzle</h2>\n<p>In our housemate story, we don&rsquo;t need to consider all possible mechanisms; we only need to consider procedures where they write down their valuation and both have an incentive to be honest about their true preferences. This is still a big space of possible mechanisms, but is much more manageable. Think for a moment about what direct mechanism you&rsquo;d recommend to the housemates.</p>\n<p>Here is one procedure that always makes the efficient decision, though it&rsquo;s less than ideal: Collect valuations simultaneously. If the sum of the two reports <span class=\"math\"><em>&theta;</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> +\u2005<em>&theta;</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub></span> is at least $300, the room will be painted. Jack&rsquo;s payment will be <span class=\"math\"><em>p</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> =\u2004300\u2005&minus;\u2005<em>&theta;</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub></span> if the room is painted and zero otherwise. Similarly, Jill&rsquo;s payment will be <span class=\"math\"><em>p</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> =\u2004300\u2005&minus;\u2005<em>&theta;</em><sub><em>Jack</em></sub></span>&nbsp;when the room is painted and zero otherwise.</p>\n<p>Take a moment to convince yourself that honesty is (weakly) dominant for each person under this mechanism. For instance, suppose Jack&rsquo;s value is $120. What would happen if he reports above this, say at $150? If Jill reports something less than $150, then the room isn&rsquo;t painted and Jack&rsquo;s utility is zero, the same as when he is honest. If Jill reports between $150 and $180, the room is painted and Jack gets a payoff of <span class=\"math\">120\u2005&minus;\u2005(300\u2005&minus;\u2005<em>&theta;</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub>)\u2004=\u2004<em>&theta;</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> &minus;\u2005180\u2004&lt;\u20040</span>, less than his payoff from honesty where the room isn&rsquo;t painted. If Jill reports above $180, the room is painted whether or not Jack reports honestly at $120 or dishonestly at $150, so the payoff is the same. Similar reasoning goes through for any report Jack considers below $120.</p>\n<p>So what&rsquo;s the issue here? Look at the sum of the payments. When the room is painted, Jack and Jill have a $300 bill, but the mechanism only collects <span class=\"math\">300\u2005&minus;\u2005<em>&theta;</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> +\u2005300\u2005&minus;\u2005<em>&theta;</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> &lt;\u2004300</span>. While the mechanism is efficient, it assumes extra money is coming from somewhere! If the individual values could be anywhere between $0 and $300, then the deficit could be up to $300.</p>\n<p>In an attempt to solve this budget issue, we could tweak the payments to the following: <span class=\"math\"><em>p</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> =\u2004450\u2005&minus;\u2005<em>&theta;</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub></span> and <span class=\"math\"><em>p</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> =\u2004450\u2005&minus;\u2005<em>&theta;</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub></span> when the room is painted and both <span class=\"math\">150</span> when it isn&rsquo;t. Honest reporting is still a dominant strategy and there is never a budget shortfall, but now the two are paying out $300 whether or not the room is painted. Hard to imagine them agreeing to use a procedure that could leave them worse off than never having considered the renovation.</p>\n<p>Is there a mechanism that makes the efficient decision but avoids both these pitfalls, never forcing someone to pay more than their valuation while still covering the bill? Alas, this turns out to be impossible.</p>\n<h2 id=\"second-best-mechanisms-and-the-myerson-satterthwaite-impossibility\">Second-best mechanisms and the Myerson-Satterthwaite impossibility</h2>\n<p><a href=\"http://www.econ.yale.edu/~dirkb/teach/521b-08-09/reading/1983-bilateral-trade.pdf\">Myerson and Satterthwaite (1983)</a> prove no possible mechanism exists that paints the room exactly when efficient without requiring an outside subsidy or making one agent worse off for having participated. The original paper was framed in terms of a single buyer and seller considering whether to trade an item, showing perfectly efficient trade is impossible in the presence of incomplete information when types are independently distributed and trade isn&rsquo;t a foregone conclusion<sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>.</p>\n<p>Rather than looking for the &ldquo;first-best&rdquo; mechanism that always makes the efficient decision, we&rsquo;re going to be forced to find a &ldquo;second-best&rdquo; mechanism that maximizes welfare while still satisfying our constraints. Surprisingly, the naive &ldquo;vote and split the cost&rdquo; mechanism is the best feasible procedure in dominant strategies. Restated in direct mechanism terms, if both submit a valuation greater than $150, the room is painted and they split the cost equally. Jeff Ely provides a very nice graphical proof of this fact <a href=\"http://cheeptalk.files.wordpress.com/2009/07/second-best-mechanisms.pdf\">here</a>.</p>\n<p>Honesty as a dominant strategy is a compelling feature of a mechanism&mdash;agents don&rsquo;t have to put any thought into what others might do. Requiring honesty to be dominant might be overly strict though. Not all games have dominant strategies. On the other hand, we expect all (well-behaved) games to have a Nash equilibrium. If we weaken dominant-strategy IC to Bayes-Nash IC, we can do slightly better, but full efficiency is still impossible. The best feasible direct mechanism&mdash;assuming values are uniform between $0 and $300&mdash;is to paint the room if and only if the values sum to more than $375, with each paying $150 and the person with the high valuation giving the other one-third of the difference in their valuations, i.e.</p>\n<p style=\"padding-left: 120px;\"><img src=\"http://latex.codecogs.com/gif.latex?p_{Jack}=\\left\\{\\begin{aligned}&amp;150+(\\theta_{Jack}-\\theta_{Jill})/3,&amp;\\mbox{&amp;space;if&amp;space;}\\theta_{Jack}+\\theta_{Jill}\\ge375\\\\&amp;0,&amp;\\mbox{&amp;space;if&amp;space;}\\theta_{Jack}+\\theta_{Jill}&lt;375\\end{aligned}\\right.\" alt=\"\" /></p>\n<p>and similarly for Jill. This direct mechanism corresponds to a Bayes-Nash equilibrium of the following, more intuitive mechanism: each writes down a bid and the room is painted if the bids sum to more than $300, with excess total payments over $300 split equally between them (so if Jack bids $200 and Jill bids $150, Jack pays $175 and Jill pays $125).</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>By reducing the design problem to encouraging the roommates to be honest via the Revelation Principle, we can identify the best feasible mechanism and end up uncovering a general impossibility about bargaining along the way. In the next post, I&rsquo;ll delve further into what is possible under dominant-strategy incentive compatibility.</p>\n<p>&nbsp;</p>\n<p><em>Previously on:</em> <a href=\"/lw/k5r/mechanism_design_constructing_algorithms_for/\">Mechanism Design: Constructing Algorithms for Strategic Agents</a></p>\n<p><em>Next up: </em><a href=\"/lw/k8r/strategyproof_mechanisms_impossibilities/\">Strategyproof Mechanisms: Impossibilities</a></p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p><em>Question 1:</em> Yes, it is still incentive compatible since the constraint still holds for all types kept, with the constraints involving all types thrown out being no longer relevant. <em>Question 2:</em> Bayesian incentive compatibility is the weaker condition, implied by dominant-strategy incentive compatibility, since if the constraint holds for every possible realization, then it must hold on average for any distribution across possible types.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Although cryptography could help solve the privacy and commitment problems of direct mechanisms. See <a href=\"http://pdf.aminer.org/000/592/908/privacy_preserving_auctions_and_mechanism_design.pdf\">Naor et al 1999, &ldquo;Privacy preserving auctions and mechanism design&rdquo;</a>.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p>I&rsquo;ve also glossed over the issue of <em>full</em> vs <em>partial</em> implementation. The revelation principle guarantees some equilibrium of the direct mechanism coincides with our social choice function. However, there might be other &ldquo;bad&rdquo; equilibria in the direct mechanism, whil an indirect mechanism might have a unique good equilibrium.<a href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>Luckily, the welfare loss disappears quickly as the size of the market grows. Once there are at least six people on each side of the market, the overall welfare loss due to incomplete info is less than 1% (<a href=\"http://faculty.las.illinois.edu/swillia3/www/research/2013-03-12optimalityvspracticality.pdf\">Satterthwaite et al 2014, &ldquo;Optimality vs Practicality in Market Design</a>)<a href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 10, "z5uy4NcWc2JSRTGHb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N4gDA5HPpGC4mbTEZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 35, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "26145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\"><em>In which the Revelation Principle is introduced, showing all mechanisms can be reduced to incentive compatible mechanisms. With this insight, a solution (of sorts) is given to the public good problem in the <a href=\"/lw/k5r/mechanism_design_constructing_algorithms_for/\">last post</a>. Limitations of the Revelation Principle are also discussed.</em></p>\n<p>The formalism I introduced last time will now start paying off. We were left with the question of how to check whether a mechanism exists that satisfies some particular goal, naively requiring us to search over all possible procedures. Luckily though, the space of all possible mechanisms can be reduced to something manageable.</p>\n<p>Observe the following: suppose through divine intervention we were granted a mechanism <span class=\"math\">(<em>M</em>,\u2006<em>g</em>)</span> that implements a social choice function <span class=\"math\"><em>f</em></span>. In other words, the outcome when agents of types <span class=\"math\"><em>\u03b8</em><sub>1</sub>,\u2006\u2026,\u2006<em>\u03b8</em><sub><em>n</em></sub></span> interact with the mechanism is exactly the outcome prescribed by the function <span class=\"math\"><em>f</em></span> for those types. Since a person\u2019s type encodes all relevant variation in their characteristics, preferences, or beliefs, we\u2019d know what that person wants to do if we knew their type. In particular, when agent <span class=\"math\"><em>i</em></span> is type <span class=\"math\"><em>\u03b8</em><sub><em>i</em></sub></span>, we expect her choice of message to be <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>\u03b8</em><sub><em>i</em></sub>)</span>. Once all the messages are sent, the function <span class=\"math\"><em>g</em></span> translates the agents\u2019 choices into an outcome so that <span class=\"math\"><em>g</em>(<em>m</em><sub>1</sub>(<em>\u03b8</em><sub>1</sub>),\u2006\u2026,\u2006<em>m</em><sub><em>n</em></sub>(<em>\u03b8</em><sub><em>n</em></sub>))\u2004=\u2004<em>f</em>(<em>\u03b8</em><sub>1</sub>,\u2006\u2026,\u2006<em>\u03b8</em><sub><em>n</em></sub>)</span>.</p>\n<p>But wait! Since we expect a type <span class=\"math\"><em>\u03b8</em><sub><em>i</em></sub></span> agent to send message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>\u03b8</em><sub><em>i</em></sub>)</span>, why don\u2019t we just package that inside the mechanism? Each agent will tell us their type and the mechanism designer will play in proxy for the agent according to the original mechanism. We\u2019ll call this the <em>direct mechanism</em> for <span class=\"math\"><em>f</em></span> since all we need to know is that agents tell us their types and then are assigned outcome <span class=\"math\"><em>f</em>(<em>\u03b8</em>)</span>, no matter what we\u2019ve blackboxed in the middle.</p>\n<p><a id=\"more\"></a></p>\n<p style=\"padding-left: 30px;\"><img src=\"http://images.lesswrong.com/t3_k69_0.png?v=e3a1c82c29f446f676e52f6178d12dc6\" alt=\"\" width=\"633\" height=\"398\"></p>\n<p>&nbsp;</p>\n<p>Why did we expect an agent of type <span class=\"math\"><em>\u03b8</em><sub><em>i</em></sub></span> to send message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>\u03b8</em><sub><em>i</em></sub>)</span>? Presumably because that message maximized her utility. In particular, it had to be at least as good as the message <span class=\"math\"><em>m</em><sub><em>i</em></sub>(<em>\u03b8</em>\u02b9<sub><em>i</em></sub>)</span> she\u2019s send when her type is different, giving us:</p>\n<p style=\"padding-left: 90px;\"><img src=\"http://latex.codecogs.com/gif.latex?\\begin{aligned}u_i(g(m_i(\\theta_i),m_{-i}),\\theta_i)&amp;\\ge&amp;space;u_i(g(m'_i,m_{-i}),\\theta_i),\\quad\\forall&amp;space;m'_i\\in&amp;space;M_i\\quad\\Rightarrow\\\\&amp;space;u_i(g(m_i(\\theta_i),m_{-i}),\\theta_i)&amp;\\ge&amp;space;u_i(g(m_i(\\theta'_i),m_{-i}),\\theta_i),\\quad\\forall\\theta'_i\\in&amp;space;\\Theta_i\\end{aligned}\" alt=\"\"></p>\n<p>Since the outcomes of the mechanism coincide with&nbsp;<span class=\"math\"><em>f</em></span>, we can conclude</p>\n<p style=\"padding-left: 120px;\"><img src=\"http://latex.codecogs.com/gif.latex?u_i(f(\\theta_i,\\theta_{-i}),\\theta_i)&amp;\\ge&amp;space;u_i(f(\\theta'_i,&amp;space;\\theta_{-i}),\\theta_i),\\quad\\forall\\theta'_i\\in&amp;space;\\Theta_i\" alt=\"\"></p>\n<p>Even though we started off with some mechanism, this last statement doesn't say anything about the mechanism itself, only the social choice function it implements. Let's call any social choice function <span class=\"math\"><em>f</em></span>&nbsp;that satisfies this constraint for every agent and for all possible types of others&nbsp;<em>strategy-proof</em> or <em>dominant-strategy incentive compatible (DSIC)</em>. Note that this can add up to a lot of constraints.</p>\n<p>These observations lead us to the following powerful tool:</p>\n<blockquote>\n<p><strong>The Revelation Principle (for dominant strategies):</strong> A social choice function <span class=\"math\"><em>f</em></span> is implementable in dominant strategies if and only if it is dominant-strategy incentive compatible. Alternatively, every mechanism that implements <span class=\"math\"><em>f</em></span> in dominant strategies is equivalent to a DSIC direct mechanism.</p>\n</blockquote>\n<p>Rather than requiring an agent to always (weakly) prefer their type\u2019s outcome, it might suffice that agents get greater utility on average for their true type:</p>\n<p style=\"padding-left: 90px;\"><img src=\"http://latex.codecogs.com/gif.latex?%5Coperatorname%7BE%7D_%7B%5Ctheta_%7B-i%7D%7D%5B%5C%3B%20u_i%28f%28%5Ctheta_i%2C%20%5Ctheta_%7B-i%7D%29%2C%20%5Ctheta_i%29%20%5C%3B%7C%5C%3B%20%5Ctheta_i%5C%2C%5D%20%5Cge%20%5Coperatorname%7BE%7D_%7B%5Ctheta_%7B-i%7D%7D%5B%5C%3B%20u_i%28f%28%5Ctheta%27_i%2C%20%5Ctheta_%7B-i%7D%29%2C%20%5Ctheta_i%29%20%5C%3B%7C%5C%3B%20%5Ctheta_i%5C%2C%5D%2C%20%5Cquad%20%5Cforall%20%5Ctheta_i%2C%20%5Ctheta%27_i%20%5Cin%20%5CTheta_i\" alt=\"\"></p>\n<p>Social choice functions <span class=\"math\"><em>f</em></span> that satisfy this constraint for each agent are <em>Bayesian incentive compatible (BIC)</em>. Just as we had a Revelation Principle for dominant strategies, we have an analogous one for Bayes-Nash equilibrium:</p>\n<blockquote>\n<p><strong>The Revelation Principle (for Bayes-Nash equilibrium):</strong> A social choice function <span class=\"math\"><em>f</em></span> is implementable in Bayes-Nash equilibrium if and only if it is Bayes-Nash incentive compatible.</p>\n</blockquote>\n<p>As a check whether you\u2019re following, consider these questions:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Suppose you have a social choice function <span class=\"math\"><em>f</em></span> that is (dominant-strategy or Bayesian) incentive compatible for the type spaces <span class=\"math\">\u0398\u2006<sub>1</sub>,\u2006\u2026,\u2006\u0398\u2006<sub><em>n</em></sub></span>. Now, throw out some types for each agent. Is the restriction of <span class=\"math\"><em>f</em></span> to these smaller type spaces still incentive compatible?</li>\n<li>Is it easier for a social choice function to be DSIC or BIC?</li>\n</ol>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><a id=\"more\">Answers in the footnotes.</a><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></p>\n<p>With the reduction provided by the various versions of the Revelation Principle, we can get away with analyzing only incentive compatible direct mechanisms rather than all possible ones. If a social choice function isn\u2019t incentive compatible, then it can\u2019t be implemented, no matter how fancy we get.</p>\n<p>There are many reasons why we wouldn\u2019t want to use a direct mechanism in practice: full types might be too complex to communicate easily, agents might be worried about privacy, or agents might not trust the mechanism operator to use the information like promised (changing the rules of the game after the fact, despite a claimed commitment to a particular outcome)<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup> <sup>,</sup> <sup><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\">3</a></sup>. Still, direct mechanisms are very straightforward for people to use. Rather than investing time to figure out how to play the game, the participants only have to consider what their true preferences are.</p>\n<p>In any case, direct mechanisms are very handy theoretically. From this perspective, all incentive problems boil down to encouraging agents to be honest about their private information. The only leeway we have is whether being honest should be a dominant strategy, a best response to the honesty of others, minimize the agent\u2019s maximimum regret, etc.</p>\n<h2 id=\"Returning_to_the_painting_puzzle\">Returning to the painting puzzle</h2>\n<p>In our housemate story, we don\u2019t need to consider all possible mechanisms; we only need to consider procedures where they write down their valuation and both have an incentive to be honest about their true preferences. This is still a big space of possible mechanisms, but is much more manageable. Think for a moment about what direct mechanism you\u2019d recommend to the housemates.</p>\n<p>Here is one procedure that always makes the efficient decision, though it\u2019s less than ideal: Collect valuations simultaneously. If the sum of the two reports <span class=\"math\"><em>\u03b8</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> +\u2005<em>\u03b8</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub></span> is at least $300, the room will be painted. Jack\u2019s payment will be <span class=\"math\"><em>p</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> =\u2004300\u2005\u2212\u2005<em>\u03b8</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub></span> if the room is painted and zero otherwise. Similarly, Jill\u2019s payment will be <span class=\"math\"><em>p</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> =\u2004300\u2005\u2212\u2005<em>\u03b8</em><sub><em>Jack</em></sub></span>&nbsp;when the room is painted and zero otherwise.</p>\n<p>Take a moment to convince yourself that honesty is (weakly) dominant for each person under this mechanism. For instance, suppose Jack\u2019s value is $120. What would happen if he reports above this, say at $150? If Jill reports something less than $150, then the room isn\u2019t painted and Jack\u2019s utility is zero, the same as when he is honest. If Jill reports between $150 and $180, the room is painted and Jack gets a payoff of <span class=\"math\">120\u2005\u2212\u2005(300\u2005\u2212\u2005<em>\u03b8</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub>)\u2004=\u2004<em>\u03b8</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> \u2212\u2005180\u2004&lt;\u20040</span>, less than his payoff from honesty where the room isn\u2019t painted. If Jill reports above $180, the room is painted whether or not Jack reports honestly at $120 or dishonestly at $150, so the payoff is the same. Similar reasoning goes through for any report Jack considers below $120.</p>\n<p>So what\u2019s the issue here? Look at the sum of the payments. When the room is painted, Jack and Jill have a $300 bill, but the mechanism only collects <span class=\"math\">300\u2005\u2212\u2005<em>\u03b8</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> +\u2005300\u2005\u2212\u2005<em>\u03b8</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> &lt;\u2004300</span>. While the mechanism is efficient, it assumes extra money is coming from somewhere! If the individual values could be anywhere between $0 and $300, then the deficit could be up to $300.</p>\n<p>In an attempt to solve this budget issue, we could tweak the payments to the following: <span class=\"math\"><em>p</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub> =\u2004450\u2005\u2212\u2005<em>\u03b8</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub></span> and <span class=\"math\"><em>p</em><sub><em>J</em><em>i</em><em>l</em><em>l</em></sub> =\u2004450\u2005\u2212\u2005<em>\u03b8</em><sub><em>J</em><em>a</em><em>c</em><em>k</em></sub></span> when the room is painted and both <span class=\"math\">150</span> when it isn\u2019t. Honest reporting is still a dominant strategy and there is never a budget shortfall, but now the two are paying out $300 whether or not the room is painted. Hard to imagine them agreeing to use a procedure that could leave them worse off than never having considered the renovation.</p>\n<p>Is there a mechanism that makes the efficient decision but avoids both these pitfalls, never forcing someone to pay more than their valuation while still covering the bill? Alas, this turns out to be impossible.</p>\n<h2 id=\"Second_best_mechanisms_and_the_Myerson_Satterthwaite_impossibility\">Second-best mechanisms and the Myerson-Satterthwaite impossibility</h2>\n<p><a href=\"http://www.econ.yale.edu/~dirkb/teach/521b-08-09/reading/1983-bilateral-trade.pdf\">Myerson and Satterthwaite (1983)</a> prove no possible mechanism exists that paints the room exactly when efficient without requiring an outside subsidy or making one agent worse off for having participated. The original paper was framed in terms of a single buyer and seller considering whether to trade an item, showing perfectly efficient trade is impossible in the presence of incomplete information when types are independently distributed and trade isn\u2019t a foregone conclusion<sup><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\">4</a></sup>.</p>\n<p>Rather than looking for the \u201cfirst-best\u201d mechanism that always makes the efficient decision, we\u2019re going to be forced to find a \u201csecond-best\u201d mechanism that maximizes welfare while still satisfying our constraints. Surprisingly, the naive \u201cvote and split the cost\u201d mechanism is the best feasible procedure in dominant strategies. Restated in direct mechanism terms, if both submit a valuation greater than $150, the room is painted and they split the cost equally. Jeff Ely provides a very nice graphical proof of this fact <a href=\"http://cheeptalk.files.wordpress.com/2009/07/second-best-mechanisms.pdf\">here</a>.</p>\n<p>Honesty as a dominant strategy is a compelling feature of a mechanism\u2014agents don\u2019t have to put any thought into what others might do. Requiring honesty to be dominant might be overly strict though. Not all games have dominant strategies. On the other hand, we expect all (well-behaved) games to have a Nash equilibrium. If we weaken dominant-strategy IC to Bayes-Nash IC, we can do slightly better, but full efficiency is still impossible. The best feasible direct mechanism\u2014assuming values are uniform between $0 and $300\u2014is to paint the room if and only if the values sum to more than $375, with each paying $150 and the person with the high valuation giving the other one-third of the difference in their valuations, i.e.</p>\n<p style=\"padding-left: 120px;\"><img src=\"http://latex.codecogs.com/gif.latex?p_{Jack}=\\left\\{\\begin{aligned}&amp;150+(\\theta_{Jack}-\\theta_{Jill})/3,&amp;\\mbox{&amp;space;if&amp;space;}\\theta_{Jack}+\\theta_{Jill}\\ge375\\\\&amp;0,&amp;\\mbox{&amp;space;if&amp;space;}\\theta_{Jack}+\\theta_{Jill}<375\\end{aligned}\\right.\" alt=\"\"></p>\n<p>and similarly for Jill. This direct mechanism corresponds to a Bayes-Nash equilibrium of the following, more intuitive mechanism: each writes down a bid and the room is painted if the bids sum to more than $300, with excess total payments over $300 split equally between them (so if Jack bids $200 and Jill bids $150, Jack pays $175 and Jill pays $125).</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>By reducing the design problem to encouraging the roommates to be honest via the Revelation Principle, we can identify the best feasible mechanism and end up uncovering a general impossibility about bargaining along the way. In the next post, I\u2019ll delve further into what is possible under dominant-strategy incentive compatibility.</p>\n<p>&nbsp;</p>\n<p><em>Previously on:</em> <a href=\"/lw/k5r/mechanism_design_constructing_algorithms_for/\">Mechanism Design: Constructing Algorithms for Strategic Agents</a></p>\n<p><em>Next up: </em><a href=\"/lw/k8r/strategyproof_mechanisms_impossibilities/\">Strategyproof Mechanisms: Impossibilities</a></p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p><em>Question 1:</em> Yes, it is still incentive compatible since the constraint still holds for all types kept, with the constraints involving all types thrown out being no longer relevant. <em>Question 2:</em> Bayesian incentive compatibility is the weaker condition, implied by dominant-strategy incentive compatibility, since if the constraint holds for every possible realization, then it must hold on average for any distribution across possible types.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>Although cryptography could help solve the privacy and commitment problems of direct mechanisms. See <a href=\"http://pdf.aminer.org/000/592/908/privacy_preserving_auctions_and_mechanism_design.pdf\">Naor et al 1999, \u201cPrivacy preserving auctions and mechanism design\u201d</a>.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n<li id=\"fn3\">\n<p>I\u2019ve also glossed over the issue of <em>full</em> vs <em>partial</em> implementation. The revelation principle guarantees some equilibrium of the direct mechanism coincides with our social choice function. However, there might be other \u201cbad\u201d equilibria in the direct mechanism, whil an indirect mechanism might have a unique good equilibrium.<a href=\"#fnref3\">\u21a9</a></p>\n</li>\n<li id=\"fn4\">\n<p>Luckily, the welfare loss disappears quickly as the size of the market grows. Once there are at least six people on each side of the market, the overall welfare loss due to incomplete info is less than 1% (<a href=\"http://faculty.las.illinois.edu/swillia3/www/research/2013-03-12optimalityvspracticality.pdf\">Satterthwaite et al 2014, \u201cOptimality vs Practicality in Market Design</a>)<a href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Returning to the painting puzzle", "anchor": "Returning_to_the_painting_puzzle", "level": 1}, {"title": "Second-best mechanisms and the Myerson-Satterthwaite impossibility", "anchor": "Second_best_mechanisms_and_the_Myerson_Satterthwaite_impossibility", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xTvdaCwaeZnePMuX5", "wE3CRBTpSSBXf9EHK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-03T19:05:43.491Z", "modifiedAt": null, "url": null, "title": "Email tone and status: !s, friendliness, 'please', etc.", "slug": "email-tone-and-status-s-friendliness-please-etc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tog", "createdAt": "2011-10-04T12:54:07.164Z", "isAdmin": false, "displayName": "tog"}, "userId": "b4f6teTtsKfegjTaH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZeuHMfqfpaxfXtaeE/email-tone-and-status-s-friendliness-please-etc", "pageUrlRelative": "/posts/ZeuHMfqfpaxfXtaeE/email-tone-and-status-s-friendliness-please-etc", "linkUrl": "https://www.lesswrong.com/posts/ZeuHMfqfpaxfXtaeE/email-tone-and-status-s-friendliness-please-etc", "postedAtFormatted": "Saturday, May 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Email%20tone%20and%20status%3A%20!s%2C%20friendliness%2C%20'please'%2C%20etc.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmail%20tone%20and%20status%3A%20!s%2C%20friendliness%2C%20'please'%2C%20etc.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZeuHMfqfpaxfXtaeE%2Femail-tone-and-status-s-friendliness-please-etc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Email%20tone%20and%20status%3A%20!s%2C%20friendliness%2C%20'please'%2C%20etc.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZeuHMfqfpaxfXtaeE%2Femail-tone-and-status-s-friendliness-please-etc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZeuHMfqfpaxfXtaeE%2Femail-tone-and-status-s-friendliness-please-etc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>Do the following things in email tone lower status?:</p>\n<p>\n<ul>\n<li>Exclamation marks</li>\n<li>Friendliness</li>\n<li>Saying 'please', or 'it'd be great if'</li>\n<li>Saying 'you don't have to do this, but...'</li>\n<li>Saying 'sorry' (e.g. 'sorry to bother you')</li>\n<li>Signing of with 'Thanks!'</li>\n</ul>\n<div>My impression is that they don't, because I haven't seen people who do this as low status. But they've all been people who are clearly high status anyway, due to their professional positions.</div>\n<div><br /></div>\n<div>It'd be great to get any pointers on this, as I worry I do all these things. Sorry if there's already a discussion of this, I did try looking! Thanks!</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZeuHMfqfpaxfXtaeE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -1, "extendedScore": null, "score": 1.7043822487680195e-06, "legacy": true, "legacyId": "26146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-04T08:21:17.681Z", "modifiedAt": null, "url": null, "title": "May Monthly Bragging Thread", "slug": "may-monthly-bragging-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LSKzYshdk3xWP44ya/may-monthly-bragging-thread", "pageUrlRelative": "/posts/LSKzYshdk3xWP44ya/may-monthly-bragging-thread", "linkUrl": "https://www.lesswrong.com/posts/LSKzYshdk3xWP44ya/may-monthly-bragging-thread", "postedAtFormatted": "Sunday, May 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20May%20Monthly%20Bragging%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMay%20Monthly%20Bragging%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLSKzYshdk3xWP44ya%2Fmay-monthly-bragging-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=May%20Monthly%20Bragging%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLSKzYshdk3xWP44ya%2Fmay-monthly-bragging-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLSKzYshdk3xWP44ya%2Fmay-monthly-bragging-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p>Your job, should you choose to accept it, is to comment on this thread explaining <strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself <em>the coolest freaking person ever</em> because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p>Remember, however, that this <strong>isn't</strong> any kind of progress thread. Nor is it any kind of proposal thread. <em>This thread is solely for people to talk about the awesome things they have done. Not \"will do\". Not \"are working on\"</em>. <strong>Have already done.</strong> This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p>So, what's the coolest thing you've done this month?</p>\n<p>(I think <a href=\"/lw/jfw/january_monthly_bragging_thread/\">this</a> is the most recent thread, so anything since January is fair game, assuming you haven't already mentioned it.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LSKzYshdk3xWP44ya", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "26154", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YRvDZCoktaN5vMpck"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-04T14:57:59.623Z", "modifiedAt": null, "url": null, "title": "Hawking on AI", "slug": "hawking-on-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:06.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Brillyant", "createdAt": "2013-03-20T05:15:44.974Z", "isAdmin": false, "displayName": "Brillyant"}, "userId": "yu8w4FXdhFcBnKLAk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bFj42MriLF5dqzkNg/hawking-on-ai", "pageUrlRelative": "/posts/bFj42MriLF5dqzkNg/hawking-on-ai", "linkUrl": "https://www.lesswrong.com/posts/bFj42MriLF5dqzkNg/hawking-on-ai", "postedAtFormatted": "Sunday, May 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hawking%20on%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHawking%20on%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFj42MriLF5dqzkNg%2Fhawking-on-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hawking%20on%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFj42MriLF5dqzkNg%2Fhawking-on-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFj42MriLF5dqzkNg%2Fhawking-on-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>[This](http://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bFj42MriLF5dqzkNg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -6, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "26156", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-04T19:52:33.779Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt: Presentation about Operant Conditioning", "slug": "meetup-frankfurt-presentation-about-operant-conditioning", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ijhKXGxwo9FX76hk8/meetup-frankfurt-presentation-about-operant-conditioning", "pageUrlRelative": "/posts/ijhKXGxwo9FX76hk8/meetup-frankfurt-presentation-about-operant-conditioning", "linkUrl": "https://www.lesswrong.com/posts/ijhKXGxwo9FX76hk8/meetup-frankfurt-presentation-about-operant-conditioning", "postedAtFormatted": "Sunday, May 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt%3A%20Presentation%20about%20Operant%20Conditioning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%3A%20Presentation%20about%20Operant%20Conditioning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FijhKXGxwo9FX76hk8%2Fmeetup-frankfurt-presentation-about-operant-conditioning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%3A%20Presentation%20about%20Operant%20Conditioning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FijhKXGxwo9FX76hk8%2Fmeetup-frankfurt-presentation-about-operant-conditioning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FijhKXGxwo9FX76hk8%2Fmeetup-frankfurt-presentation-about-operant-conditioning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zw'>Frankfurt: Presentation about Operant Conditioning</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 May 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting again and you are welcome to join! Please contact me under 0176 34 095 760 to get the precise location.\nIf you have any special needs for attending, please tell us in advance, we'll try at best to accommodate your needs.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zw'>Frankfurt: Presentation about Operant Conditioning</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ijhKXGxwo9FX76hk8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7063905043994958e-06, "legacy": true, "legacyId": "26157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt__Presentation_about_Operant_Conditioning\">Discussion article for the meetup : <a href=\"/meetups/zw\">Frankfurt: Presentation about Operant Conditioning</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 May 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting again and you are welcome to join! Please contact me under 0176 34 095 760 to get the precise location.\nIf you have any special needs for attending, please tell us in advance, we'll try at best to accommodate your needs.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt__Presentation_about_Operant_Conditioning1\">Discussion article for the meetup : <a href=\"/meetups/zw\">Frankfurt: Presentation about Operant Conditioning</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt: Presentation about Operant Conditioning", "anchor": "Discussion_article_for_the_meetup___Frankfurt__Presentation_about_Operant_Conditioning", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt: Presentation about Operant Conditioning", "anchor": "Discussion_article_for_the_meetup___Frankfurt__Presentation_about_Operant_Conditioning1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-04T22:07:54.354Z", "modifiedAt": null, "url": null, "title": "Truth: It's Not That Great", "slug": "truth-it-s-not-that-great", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.166Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bH2N59ovSiFTJvdZM/truth-it-s-not-that-great", "pageUrlRelative": "/posts/bH2N59ovSiFTJvdZM/truth-it-s-not-that-great", "linkUrl": "https://www.lesswrong.com/posts/bH2N59ovSiFTJvdZM/truth-it-s-not-that-great", "postedAtFormatted": "Sunday, May 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Truth%3A%20It's%20Not%20That%20Great&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATruth%3A%20It's%20Not%20That%20Great%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbH2N59ovSiFTJvdZM%2Ftruth-it-s-not-that-great%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Truth%3A%20It's%20Not%20That%20Great%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbH2N59ovSiFTJvdZM%2Ftruth-it-s-not-that-great", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbH2N59ovSiFTJvdZM%2Ftruth-it-s-not-that-great", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1141, "htmlBody": "<blockquote>\n<p>Rationality is pretty great. Just not quite as great as everyone here seems to think it is.</p>\n<p>-Yvain, <a href=\"/lw/9p/extreme_rationality_its_not_that_great/\">\"Extreme Rationality: It's Not That Great\"</a></p>\n</blockquote>\n<blockquote>\n<p>The folks most vocal about loving \"truth\" are usually selling something. For preachers, demagogues, and salesmen of all sorts, the wilder their story, the more they go on about how they love truth...</p>\n<p>The people who just want to know things because they need to make important decisions, in contrast, usually say little about their love of truth; they are too busy trying to figure stuff out.</p>\n<p>-Robin Hanson, <a href=\"http://www.overcomingbias.com/2009/04/who-loves-truth-most.html\">\"Who Loves Truth Most?\"</a></p>\n</blockquote>\n<p>A couple weeks ago, Brienne made a post on Facebook that included this remark: \"I've also gained a lot of reverence for the truth, in virtue of the centrality of truth-seeking to the fate of the galaxy.\" But then she edited to add a footnote to this sentence: \"That was the justification my brain originally threw at me, but it doesn't actually quite feel true. There's something more directly responsible for the motivation that I haven't yet identified.\"</p>\n<p>I saw this, and commented:</p>\n<blockquote>\n<p>&lt;puts rubber Robin Hanson mask on&gt;</p>\n<p>What we have here is a case of subcultural in-group signaling masquerading as something else. In this case, proclaiming how vitally important truth-seeking is is a mark of your subculture. In reality, the truth is sometimes really important, but sometimes it isn't.</p>\n<p>&lt;/rubber Robin Hanson mask&gt;</p>\n</blockquote>\n<p>In spite of the distancing pseudo-HTML tags, I actually believe this. When I read some of the more extreme proclamations of the value of truth that float around the rationalist community, I suspect people are doing in-group signaling&mdash;or perhaps conflating their own idiosyncratic preferences with rationality. As a mild antidote to this, when you hear someone talking about the value of the truth, try seeing if the statement still makes sense if you replace \"truth\" with \"information.\"</p>\n<p>This standard gives many statements about the value of truth its stamp of approval. After all, <em>information </em>is pretty damn valuable. But statements like \"truth seeking is central to the fate of the galaxy\" look a bit suspicious. Is information-gathering central to the fate of the galaxy? You could argue that statement is <em>kinda </em>true if you squint at it right, but really it's too general. Surely it's not just any information that's central to shaping the fate of the galaxy, but information about specific subjects, and even then there are tradeoffs to make.</p>\n<p>This is an example of why I suspect \"effective altruism\" may be better branding for a movement than \"rationalism.\" The \"rationalism\" branding encourages the meme that truth-seeking is great we should do lots and lots of it because truth is so great. The effective altruism movement, on the other hand, recognizes that while gathering information about the effectiveness of various interventions is important, there are tradeoffs to be made between spending time and money on gathering information vs. just doing whatever currently seems likely to have the greatest direct impact. Recognize information is valuable, but avoid <a href=\"/en.wikipedia.org/wiki/Analysis_paralysis\">analysis paralysis</a>.</p>\n<p>Or, consider statements like:</p>\n<ul>\n<li>Some truths don't matter much.</li>\n<li>People often have legitimate reasons for not wanting others to have certain truths.</li>\n<li>The value of truth often has to be weighed against other goals.</li>\n</ul>\n<p>Do these statements sound heretical to you? But what about:</p>\n<ul>\n<li>Information can be perfectly accurate and also worthless.&nbsp;</li>\n<li>People often have legitimate reasons for not wanting other people to gain access to their private information.&nbsp;</li>\n<li>A desire for more information often has to be weighed against other goals.&nbsp;</li>\n</ul>\n<p>I struggled to write the first set of statements, though I think they're right on reflection. Why do they sound so much worse than the second set? Because the word \"truth\" carries powerful emotional connotations that go beyond its literal meaning. This isn't just true for rationalists&mdash;there's a reason religions have sayings like, \"God is Truth\" or \"I am the way, the truth, and the life.\" \"God is Facts\" or \"God is Information\" don't work so well.</p>\n<p>There's something about \"truth\"&mdash;how it readily acts as an <a href=\"/lw/jb/applause_lights/\">applause light</a>, a <a href=\"/lw/lm/affective_death_spirals/\">sacred value</a> which must not be traded off against anything else. As I type that, a little voice in me protests \"but truth <em>really is </em>sacred\"... but once we can't say there's some limit to how great truth is, hello affective <a href=\"/lw/lm/affective_death_spirals/\">death spiral</a>.</p>\n<p>Consider another quote, from <a href=\"http://www.acceleratingfuture.com/steven/?p=124\">Steven Kaas</a>, that I see frequently referenced on LessWrong: \"Promoting less than maximally accurate beliefs is an act of sabotage. Don&rsquo;t do it to anyone unless you&rsquo;d also slash their tires, because they&rsquo;re Nazis or whatever.\" Interestingly, the original blog included a caveat&mdash;\"we may have to count everyday social interactions as a partial exception\"&mdash;which I never see quoted. That aside, the quote has always bugged me. I've never had my tires slashed, but I imagine it ruins your whole day. On the other hand, having less than maximally accurate beliefs about something <em>could </em>ruin your whole day, but it could very easily not, depending on the topic.</p>\n<p>Furthermore, sometimes sharing certain information doesn't just have little benefit, it can have substantial costs, or at least substantial risks. It would seriously trivialize Nazi Germany's crimes to compare it to the current US government, but I don't think that means we have to promote maximally accurate beliefs about ourselves to the folks at the NSA. Or, when negotiating over the price of something, are you required to promote maximally accurate beliefs about the highest price you'd be willing to pay, even if the other party isn't willing to reciprocate and may respond by demanding that price?</p>\n<p>Private information is usually considered private precisely because it has limited benefit to most people, but sharing it could significantly harm the person whose private information it is. A sensible ethic around information needs to be able to deal with issues like that. It needs to be able to deal with questions like: is this information that is in the public interest to know? And is there a power imbalance involved? My rule of thumb is: secrets kept by the powerful deserve extra scrutiny, but so conversely do their attempts to gather <em>other people's </em>private information.&nbsp;</p>\n<p><a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">\"Corrupted hardware\"</a>-type arguments can suggest you should doubt your own justifications for deceiving others. But parallel arguments suggest you should doubt your own justifications for feeling entitled to information others might have legitimate reasons for keeping private. Arguments like, \"well truth is supremely valuable,\" \"it's extremely important for me to have accurate beliefs,\" or \"I'm highly rational so people should trust me\" just don't cut it.</p>\n<p>Finally, being rational in the sense of being <a href=\"http://en.wikipedia.org/wiki/Calibrated_probability_assessment\">well-calibrated</a> doesn't necessarily require making truth-seeking a major priority. Using the evidence you have well doesn't necessarily mean gathering lots of new evidence. Often, the alternative to knowing the truth is not believing falsehood, but admitting you don't know and living with the uncertainty.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 1, "9YFoDPFwMoWthzgkY": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bH2N59ovSiFTJvdZM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 50, "extendedScore": null, "score": 0.00026, "legacy": true, "legacyId": "26105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LgavAYtzFQZKg95WC", "dLbkrPu5STNCBLRjr", "XrzQW69HpidzvBxGr", "K9ZaZXDnL3SEmYZqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-04T23:45:25.469Z", "modifiedAt": null, "url": null, "title": "Calling all MIRI supporters for unique May 6 giving opportunity!", "slug": "calling-all-miri-supporters-for-unique-may-6-giving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:23.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FuoPkThduHrRgxRSR/calling-all-miri-supporters-for-unique-may-6-giving", "pageUrlRelative": "/posts/FuoPkThduHrRgxRSR/calling-all-miri-supporters-for-unique-may-6-giving", "linkUrl": "https://www.lesswrong.com/posts/FuoPkThduHrRgxRSR/calling-all-miri-supporters-for-unique-may-6-giving", "postedAtFormatted": "Sunday, May 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calling%20all%20MIRI%20supporters%20for%20unique%20May%206%20giving%20opportunity!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalling%20all%20MIRI%20supporters%20for%20unique%20May%206%20giving%20opportunity!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFuoPkThduHrRgxRSR%2Fcalling-all-miri-supporters-for-unique-may-6-giving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calling%20all%20MIRI%20supporters%20for%20unique%20May%206%20giving%20opportunity!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFuoPkThduHrRgxRSR%2Fcalling-all-miri-supporters-for-unique-may-6-giving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFuoPkThduHrRgxRSR%2Fcalling-all-miri-supporters-for-unique-may-6-giving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 589, "htmlBody": "<p><small>(Cross-posted from <a href=\"http://intelligence.org/2014/05/04/calling-all-miri-supporters/\">MIRI's blog</a>. <a href=\"http://intelligence.org/\">MIRI</a> maintains Less Wrong, with generous help from <a href=\"http://trikeapps.com/\">Trike Apps</a>, and much of the core content is written by salaried MIRI staff members.)</small></p>\n<p>Update: I'm liveblogging the fundraiser <a href=\"http://intelligence.org/2014/05/06/liveblogging-the-svgives-fundraiser/\">here</a>.</p>\n<h2 style=\"text-align:center\">Read our strategy below, then <a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\">give here</a>!</h2>\n<p><a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\"><img style=\"float: right; padding: 10px;\" src=\"https://intelligence.org/wp-content/uploads/2014/05/SVGives-logo-lrg.jpg\" alt=\"SVGives logo lrg\" width=\"230\" height=\"178\" /></a>As previously <a title=\"Help MIRI in a Massive 24-Hour Fundraiser on May 6th\" href=\"http://intelligence.org/2014/04/25/may-6th-miri-participating-in-massive-24-hour-online-fundraiser/\">announced</a>,&nbsp;MIRI is participating in a massive 24-hour fundraiser on May 6th, called <a href=\"http://svgives.razoo.com/giving_events/svg14/home\">SV Gives</a>. This is a unique opportunity for all MIRI supporters to increase the impact of their donations. To be successful we'll need to pre-commit to a strategy and see it through. <strong>If you plan to give at least $10 to MIRI sometime this year, during this event would be the best time to do it!</strong></p>\n<h2><br /></h2>\n<h2>The plan</h2>\n<p>We need all hands on deck to help us win the following prize as many times as possible:</p>\n<blockquote>$2,000 prize for the nonprofit that has the most individual donors in an hour, every hour for 24 hours.</blockquote>\n<p>To paraphrase, <em>every hour</em>, there is a $2,000 prize for the organization that has the most individual donors during that hour. <strong>That's a total of $48,000 in prizes, from sources that wouldn't normally give to MIRI.&nbsp;</strong> The minimum donation is $10, and an individual donor can give as many times as they want. Therefore we ask our supporters to:</p>\n<ol>\n<li><strong><a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\">give</a> $10 an hour, during <em>every hour</em> of the fundraiser that they are awake (I'll be up and donating for all 24 hours!)</strong>;</li>\n<li>for those whose giving budgets won't cover all those hours, see below for list of which hours you should privilege; and</li>\n<li>publicize this effort as widely as possible.</li>\n</ol>\n<h3 style=\"text-align: center;\">International donors, we especially need your help!</h3>\n<p>MIRI has a strong community of international supporters, and this gives us a distinct advantage! While North America sleeps, you'll be awake, ready to target all of the overnight $2,000 hourly prizes.</p>\n<p><a id=\"more\"></a></p>\n<h2>Hours to target in order of importance</h2>\n<p>To increase our chances of winning these prizes we want to preferentially target the hours that will see the least donation traffic from donors of other participating organizations. Below are the top 12 hours we'd like to target in order of importance. Remember that all times are in Pacific Time. (Click on an hour to see what time it is in your timezone.)</p>\n<ul>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T01&amp;p1=224&amp;ah=1\">1 am hour</a>&nbsp;&nbsp;(01:00&ndash;01:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T02&amp;p1=224&amp;ah=1\">2 am hour</a>&nbsp;(02:00&ndash;02:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T03&amp;p1=224&amp;ah=1\">3 am hour</a>&nbsp;(03:00&ndash;03:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T04&amp;p1=224&amp;ah=1\">4 am hour</a>&nbsp;(04:00&ndash;04:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T05&amp;p1=224&amp;ah=1\">5 am hour</a>&nbsp;(05:00&ndash;05:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T06&amp;p1=224&amp;ah=1\">6 am hour</a>&nbsp;(06:00&ndash;06:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T23&amp;p1=224&amp;ah=1\">11 pm hour</a>&nbsp;(23:00&ndash;23:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T07&amp;p1=224&amp;ah=1\">7 am hour</a> (07:00&ndash;07:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T22&amp;p1=224&amp;ah=1\">10 pm hour</a> (22:00&ndash;22:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T08&amp;p1=224&amp;ah=1\">8 am hour</a>&nbsp;(08:00&ndash;08:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T17&amp;p1=224&amp;ah=1\">5 pm hour</a> (17:00&ndash;17:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T21&amp;p1=224&amp;ah=1\">9 pm hour</a> (21:00&ndash;21:59 PT)</li>\n</ul>\n<p>For the 5 pm hour there is an additional prize I think we can win:</p>\n<blockquote>$1,000 golden ticket added to the first 50 organizations receiving gifts in the 5 pm hour.</blockquote>\n<p><strong>So if you are giving in the 5 pm hour try and give right at the beginning of the hour.</strong></p>\n<h3 style=\"text-align: center;\">Bottom line, for every hour you are awake, <a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\">give</a> $10 an hour.</h3>\n<h3 style=\"text-align: center;\">&nbsp;Give preferentially to the hours above, if unable to give during all waking hours.</h3>\n<p>We also have plans to target the $300,000 in matching funds up for grabs during the event. If you would like to contribute $500 or more to this effort, shoot Malo an email at <a href=\"mailto:malo@intelligence.org\">malo@intelligence.org</a>. &nbsp;</p>\n<p>For those who want to follow along and contribute to the last minute planning, as well as receive updates and giving reminders during the event, <strong><a href=\"https://docs.google.com/forms/d/1kldd0puHup8t1lnnWGVCA0IXz_rnEU1vwcqfmTeEXKc/viewform\">sign up here</a>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FuoPkThduHrRgxRSR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 34, "extendedScore": null, "score": 1.70670540701498e-06, "legacy": true, "legacyId": "26158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>(Cross-posted from <a href=\"http://intelligence.org/2014/05/04/calling-all-miri-supporters/\">MIRI's blog</a>. <a href=\"http://intelligence.org/\">MIRI</a> maintains Less Wrong, with generous help from <a href=\"http://trikeapps.com/\">Trike Apps</a>, and much of the core content is written by salaried MIRI staff members.)</small></p>\n<p>Update: I'm liveblogging the fundraiser <a href=\"http://intelligence.org/2014/05/06/liveblogging-the-svgives-fundraiser/\">here</a>.</p>\n<h2 style=\"text-align:center\" id=\"Read_our_strategy_below__then_give_here_\">Read our strategy below, then <a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\">give here</a>!</h2>\n<p><a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\"><img style=\"float: right; padding: 10px;\" src=\"https://intelligence.org/wp-content/uploads/2014/05/SVGives-logo-lrg.jpg\" alt=\"SVGives logo lrg\" width=\"230\" height=\"178\"></a>As previously <a title=\"Help MIRI in a Massive 24-Hour Fundraiser on May 6th\" href=\"http://intelligence.org/2014/04/25/may-6th-miri-participating-in-massive-24-hour-online-fundraiser/\">announced</a>,&nbsp;MIRI is participating in a massive 24-hour fundraiser on May 6th, called <a href=\"http://svgives.razoo.com/giving_events/svg14/home\">SV Gives</a>. This is a unique opportunity for all MIRI supporters to increase the impact of their donations. To be successful we'll need to pre-commit to a strategy and see it through. <strong>If you plan to give at least $10 to MIRI sometime this year, during this event would be the best time to do it!</strong></p>\n<h2><br></h2>\n<h2 id=\"The_plan\">The plan</h2>\n<p>We need all hands on deck to help us win the following prize as many times as possible:</p>\n<blockquote>$2,000 prize for the nonprofit that has the most individual donors in an hour, every hour for 24 hours.</blockquote>\n<p>To paraphrase, <em>every hour</em>, there is a $2,000 prize for the organization that has the most individual donors during that hour. <strong>That's a total of $48,000 in prizes, from sources that wouldn't normally give to MIRI.&nbsp;</strong> The minimum donation is $10, and an individual donor can give as many times as they want. Therefore we ask our supporters to:</p>\n<ol>\n<li><strong><a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\">give</a> $10 an hour, during <em>every hour</em> of the fundraiser that they are awake (I'll be up and donating for all 24 hours!)</strong>;</li>\n<li>for those whose giving budgets won't cover all those hours, see below for list of which hours you should privilege; and</li>\n<li>publicize this effort as widely as possible.</li>\n</ol>\n<h3 style=\"text-align: center;\" id=\"International_donors__we_especially_need_your_help_\">International donors, we especially need your help!</h3>\n<p>MIRI has a strong community of international supporters, and this gives us a distinct advantage! While North America sleeps, you'll be awake, ready to target all of the overnight $2,000 hourly prizes.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Hours_to_target_in_order_of_importance\">Hours to target in order of importance</h2>\n<p>To increase our chances of winning these prizes we want to preferentially target the hours that will see the least donation traffic from donors of other participating organizations. Below are the top 12 hours we'd like to target in order of importance. Remember that all times are in Pacific Time. (Click on an hour to see what time it is in your timezone.)</p>\n<ul>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T01&amp;p1=224&amp;ah=1\">1 am hour</a>&nbsp;&nbsp;(01:00\u201301:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T02&amp;p1=224&amp;ah=1\">2 am hour</a>&nbsp;(02:00\u201302:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T03&amp;p1=224&amp;ah=1\">3 am hour</a>&nbsp;(03:00\u201303:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T04&amp;p1=224&amp;ah=1\">4 am hour</a>&nbsp;(04:00\u201304:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T05&amp;p1=224&amp;ah=1\">5 am hour</a>&nbsp;(05:00\u201305:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T06&amp;p1=224&amp;ah=1\">6 am hour</a>&nbsp;(06:00\u201306:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T23&amp;p1=224&amp;ah=1\">11 pm hour</a>&nbsp;(23:00\u201323:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T07&amp;p1=224&amp;ah=1\">7 am hour</a> (07:00\u201307:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T22&amp;p1=224&amp;ah=1\">10 pm hour</a> (22:00\u201322:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T08&amp;p1=224&amp;ah=1\">8 am hour</a>&nbsp;(08:00\u201308:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T17&amp;p1=224&amp;ah=1\">5 pm hour</a> (17:00\u201317:59 PT)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?iso=20140506T21&amp;p1=224&amp;ah=1\">9 pm hour</a> (21:00\u201321:59 PT)</li>\n</ul>\n<p>For the 5 pm hour there is an additional prize I think we can win:</p>\n<blockquote>$1,000 golden ticket added to the first 50 organizations receiving gifts in the 5 pm hour.</blockquote>\n<p><strong id=\"So_if_you_are_giving_in_the_5_pm_hour_try_and_give_right_at_the_beginning_of_the_hour_\">So if you are giving in the 5 pm hour try and give right at the beginning of the hour.</strong></p>\n<h3 style=\"text-align: center;\" id=\"Bottom_line__for_every_hour_you_are_awake__give__10_an_hour_\">Bottom line, for every hour you are awake, <a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\">give</a> $10 an hour.</h3>\n<h3 style=\"text-align: center;\" id=\"_Give_preferentially_to_the_hours_above__if_unable_to_give_during_all_waking_hours_\">&nbsp;Give preferentially to the hours above, if unable to give during all waking hours.</h3>\n<p>We also have plans to target the $300,000 in matching funds up for grabs during the event. If you would like to contribute $500 or more to this effort, shoot Malo an email at <a href=\"mailto:malo@intelligence.org\">malo@intelligence.org</a>. &nbsp;</p>\n<p>For those who want to follow along and contribute to the last minute planning, as well as receive updates and giving reminders during the event, <strong><a href=\"https://docs.google.com/forms/d/1kldd0puHup8t1lnnWGVCA0IXz_rnEU1vwcqfmTeEXKc/viewform\">sign up here</a>.</strong></p>", "sections": [{"title": "Read our strategy below, then give here!", "anchor": "Read_our_strategy_below__then_give_here_", "level": 1}, {"title": "The plan", "anchor": "The_plan", "level": 1}, {"title": "International donors, we especially need your help!", "anchor": "International_donors__we_especially_need_your_help_", "level": 2}, {"title": "Hours to target in order of importance", "anchor": "Hours_to_target_in_order_of_importance", "level": 1}, {"title": "So if you are giving in the 5 pm hour try and give right at the beginning of the hour.", "anchor": "So_if_you_are_giving_in_the_5_pm_hour_try_and_give_right_at_the_beginning_of_the_hour_", "level": 3}, {"title": "Bottom line, for every hour you are awake, give $10 an hour.", "anchor": "Bottom_line__for_every_hour_you_are_awake__give__10_an_hour_", "level": 2}, {"title": "\u00a0Give preferentially to the hours above, if unable to give during all waking hours.", "anchor": "_Give_preferentially_to_the_hours_above__if_unable_to_give_during_all_waking_hours_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "48 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-05T02:32:28.735Z", "modifiedAt": null, "url": null, "title": "2014 Survey of Effective Altruists", "slug": "2014-survey-of-effective-altruists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:38.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tog", "createdAt": "2011-10-04T12:54:07.164Z", "isAdmin": false, "displayName": "tog"}, "userId": "b4f6teTtsKfegjTaH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b3okErfPtzHoHvgYp/2014-survey-of-effective-altruists", "pageUrlRelative": "/posts/b3okErfPtzHoHvgYp/2014-survey-of-effective-altruists", "linkUrl": "https://www.lesswrong.com/posts/b3okErfPtzHoHvgYp/2014-survey-of-effective-altruists", "postedAtFormatted": "Monday, May 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202014%20Survey%20of%20Effective%20Altruists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2014%20Survey%20of%20Effective%20Altruists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3okErfPtzHoHvgYp%2F2014-survey-of-effective-altruists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2014%20Survey%20of%20Effective%20Altruists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3okErfPtzHoHvgYp%2F2014-survey-of-effective-altruists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3okErfPtzHoHvgYp%2F2014-survey-of-effective-altruists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 355, "htmlBody": "<p>I'm pleased to announce the <a href=\"http://bit.ly/1jjBWT9\">first annual survey of effective altruists</a>. This is a short survey of around 40 questions (generally multiple choice), which several collaborators and I have put a great deal of work into and would be very grateful if you took. I'll offer $250 of my own money to one participant.</p>\n<p><em>Take the survey at <a href=\"http://bit.ly/1jjBWT9\">http://survey.effectivealtruismhub.com/</a></em></p>\n<p>The survey should yield some interesting results such as EAs' political and religious views, what actions they take, and the causes they favour and donate to. It will also enable useful applications which will be launched immediately afterwards, such as a map of EAs with contact details and a cause-neutral register of planned donations or pledges which can be verified each year. I'll also provide an open platform for followup surveys and other actions people can take. If you'd like to suggest questions, <a href=\"mailto:survey@effectivealtruismhub.com\">email me</a> or comment.</p>\n<p>Anonymised results will be shared publicly and not belong to any individual or organisation. The most robust <a href=\"http://effectivealtruismhub.com/survey/privacy\">privacy practices</a> will be followed, with clear opt-ins and opt-outs.</p>\n<p>I'd like to thank Jacy Anthis, Ben Landau-Taylor, David Moss and Peter Hurford for their help.</p>\n<h2>Other surveys' results, and predictions for this one</h2>\n<p>Other surveys have had intriguing results. For example, Joey Savoie and Xio Kikauka's interviewed 42 often highly active EAs over Skype, and found that they generally had left-leaning parents, donated on average 10%, and were altruistic before becoming EAs. The time they spent on EA activities was correlated with the percentage they donated (0.4), the time their parents spend volunteering (0.3), and the percentage of their friends who were EAs (0.3).</p>\n<p>80,000 Hours also released a questionnaire and, while this was mainly focused on their impact, it yielded a list of which careers people plan to pursue: 16% for academia, &nbsp;9% for both finance and software engineering, and 8% for both medicine and non-profits. &nbsp;</p>\n<p>I'd be curious to hear people's predictions as to what the results of this survey will be. You might enjoy <a href=\"http://bit.ly/1fwrush\">reading or sharing them here</a>. For my part, I'd imagine we have few conservatives or even libertarians, are over 70% male, and have directed most of our donations to poverty charities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b3okErfPtzHoHvgYp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 35, "extendedScore": null, "score": 1.7069313809707822e-06, "legacy": true, "legacyId": "26136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 148, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-05T10:35:45.563Z", "modifiedAt": "2022-03-23T06:48:28.263Z", "url": null, "title": "Open Thread, May 5 - 11, 2014", "slug": "open-thread-may-5-11-2014", "viewCount": null, "lastCommentedAt": "2014-08-08T02:18:17.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cti7vfwZGgeK6wKL6/open-thread-may-5-11-2014", "pageUrlRelative": "/posts/Cti7vfwZGgeK6wKL6/open-thread-may-5-11-2014", "linkUrl": "https://www.lesswrong.com/posts/Cti7vfwZGgeK6wKL6/open-thread-may-5-11-2014", "postedAtFormatted": "Monday, May 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20May%205%20-%2011%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20May%205%20-%2011%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCti7vfwZGgeK6wKL6%2Fopen-thread-may-5-11-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20May%205%20-%2011%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCti7vfwZGgeK6wKL6%2Fopen-thread-may-5-11-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCti7vfwZGgeK6wKL6%2Fopen-thread-may-5-11-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p><a href=\"/r/discussion/lw/k4u/open_thread_april_27may_4_2014/\">Previous Open Thread</a></p>\n<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">You know the drill - If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000; font-size: small; line-height: 19px;\">Notes for future OT posters:</span></p>\n<p><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p><span style=\"color: #000000; font-size: small; line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p><span style=\"color: #000000; font-size: small; line-height: 19px;\">3. Open Threads should start on Monday, and end on Sunday.</span></p>\n<p><span style=\"color: #000000; font-size: small; line-height: 19px;\">4. Open Threads should be posted in Discussion, and not Main.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cti7vfwZGgeK6wKL6", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "26163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 286, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wR3xa3AgwRWFoat89"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-05-05T10:35:45.563Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-05T15:48:31.307Z", "modifiedAt": null, "url": null, "title": "Ottawa meetup: Applied Rationality Series, Value of Information", "slug": "ottawa-meetup-applied-rationality-series-value-of", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7XdLhhYaYhcs8mmCb/ottawa-meetup-applied-rationality-series-value-of", "pageUrlRelative": "/posts/7XdLhhYaYhcs8mmCb/ottawa-meetup-applied-rationality-series-value-of", "linkUrl": "https://www.lesswrong.com/posts/7XdLhhYaYhcs8mmCb/ottawa-meetup-applied-rationality-series-value-of", "postedAtFormatted": "Monday, May 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ottawa%20meetup%3A%20Applied%20Rationality%20Series%2C%20Value%20of%20Information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOttawa%20meetup%3A%20Applied%20Rationality%20Series%2C%20Value%20of%20Information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XdLhhYaYhcs8mmCb%2Fottawa-meetup-applied-rationality-series-value-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ottawa%20meetup%3A%20Applied%20Rationality%20Series%2C%20Value%20of%20Information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XdLhhYaYhcs8mmCb%2Fottawa-meetup-applied-rationality-series-value-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XdLhhYaYhcs8mmCb%2Fottawa-meetup-applied-rationality-series-value-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>The sixth talk in the Ottawa Applied Rationality series will take place on Tuesday, May 20th at 7:00 pm, at the Canal Royal Oak in Ottawa, Canada. These events are run through the Ottawa Skeptics meetup group. See link here: http://www.meetup.com/Ottawa-Skeptics/events/181263842/</p>\n<p>The usual format consists of an approximately 15 minute talk on the topic of the day, followed by semi-structured exercises, followed by beers and unstructured discussion. Previous topics have included \"Rational Debating\", \"Bayes\", \"Calibration\", \"Rationality Dojo\" (a review session), and \"Goal Factoring.\"&nbsp;</p>\n<p>If you are not from Ottawa, but are interested in running meetups in your area, send me a PM and I can give you the PowerPoints that I use for these talks.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7XdLhhYaYhcs8mmCb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.7080089128342766e-06, "legacy": true, "legacyId": "26164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-05T16:49:46.192Z", "modifiedAt": null, "url": null, "title": "Arguments and relevance claims", "slug": "arguments-and-relevance-claims", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:33.289Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t6ih3RYxKfRm6GCHz/arguments-and-relevance-claims", "pageUrlRelative": "/posts/t6ih3RYxKfRm6GCHz/arguments-and-relevance-claims", "linkUrl": "https://www.lesswrong.com/posts/t6ih3RYxKfRm6GCHz/arguments-and-relevance-claims", "postedAtFormatted": "Monday, May 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arguments%20and%20relevance%20claims&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArguments%20and%20relevance%20claims%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft6ih3RYxKfRm6GCHz%2Farguments-and-relevance-claims%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arguments%20and%20relevance%20claims%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft6ih3RYxKfRm6GCHz%2Farguments-and-relevance-claims", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft6ih3RYxKfRm6GCHz%2Farguments-and-relevance-claims", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<div>The following once happened: I posted a link to some article on an IRC channel. A friend of mine read the article in question and brought up several criticisms. I felt that her criticisms were mostly correct though not very serious, so I indicated agreement with them.</div>\n<div><br /></div>\n<div>Later on the same link was posted again. My friend commented something along the lines of \"that was already posted before, we discussed this with Kaj and we found that the article was complete rubbish\". I was surprised - I had thought that I had only agreed to some minor criticisms that didn't affect the main point of the article. But my friend had clearly thought that the criticisms were decisive and had made the article impossible to salvage.</div>\n<div><br /></div>\n<div>--</div>\n<div><br /></div>\n<div>Every argument actually has two parts, even if people often only state the first part. There's the argument itself, and an implied claim of why the argument would matter if it were true. Call this implied part the <em>relevance claim</em>.</div>\n<div><br /></div>\n<div>Suppose that I say \"Martians are green\". Someone else says, \"I have seen a blue Martian\", and means \"I have seen a blue Martian (argument), therefore your claim of <em>all</em> Martians being green is false (relevance claim)\". But I might interpret this as them saying, \"I have seen a blue Martian (argument), therefore your claim of <em>most</em> Martians being green is less likely (relevance claim)\". I then indicate agreement. Now I will be left with the impression that the other person made a true-but-not-very-powerful claim that left my argument mostly intact, whereas the other person is left with the impression that they made a very powerful claim that I agreed with, and therefore I admitted that I was wrong.</div>\n<div><br /></div>\n<div>We could also say that the relevance claim is a claim of how much the probability of the original statement would be affected if the argument in question were true. So, for example \"I have seen a blue martian (argument), therefore the probability of 'Martians are green' is less than .01 (relevance claim)\", or equivalently, \"I have seen a blue martian\" &amp; \"P(martians are green|I have seen a blue martian) &lt; .01\".</div>\n<div><br /></div>\n<div>If someone says something that I feel is entirely irrelevant to the whole topic, <a href=\"/lw/iq6/inferential_silence/\">inferential silence</a> may follow.</div>\n<div><br /></div>\n<div>Therefore, if someone makes an argument that I agree with, but I suspect that we might disagree about its relevance, I now try to explicitly comment on what my view of the relevance is. <a href=\"/lw/k60/2014_survey_of_effective_altruists/avry\">Example</a>.</div>\n<div><br /></div>\n<div>Notice that people who are treating <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">arguments as soldiers</a> are more likely to do this automatically, without needing to explicitly remind themselves of it. In fact, for every argument that their opponent makes that they're forced to concede, they're likely to immediately say \"but that doesn't matter because X!\". Because we like to think that we're not treating arguments as soldiers, we also try to avoid automatically objecting \"but that doesn't matter because X\" whenever our favored position gets weakened. This is a good thing, but it also means that we're probably less likely than average to comment about an argument's relevance even in cases where we <em>should</em> comment on it.</div>\n<div><br /></div>\n<div><em>(Cross-posted from <a href=\"http://kajsotala.fi/2014/05/arguments-and-relevance-claims/\">my blog</a>.)</em><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t6ih3RYxKfRm6GCHz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 39, "extendedScore": null, "score": 0.000155, "legacy": true, "legacyId": "26165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZfWLsDD8KKSFR3MLB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-06T05:37:07.030Z", "modifiedAt": null, "url": null, "title": "Find a study partner - May 2014 Thread", "slug": "find-a-study-partner-may-2014-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.415Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MathieuRoy", "createdAt": "2013-06-16T02:41:27.071Z", "isAdmin": false, "displayName": "Mati_Roy"}, "userId": "Tw9etd8rMnHLeSQ9q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R9dxPDzRafKYqQzYt/find-a-study-partner-may-2014-thread", "pageUrlRelative": "/posts/R9dxPDzRafKYqQzYt/find-a-study-partner-may-2014-thread", "linkUrl": "https://www.lesswrong.com/posts/R9dxPDzRafKYqQzYt/find-a-study-partner-may-2014-thread", "postedAtFormatted": "Tuesday, May 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Find%20a%20study%20partner%20-%20May%202014%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFind%20a%20study%20partner%20-%20May%202014%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR9dxPDzRafKYqQzYt%2Ffind-a-study-partner-may-2014-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Find%20a%20study%20partner%20-%20May%202014%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR9dxPDzRafKYqQzYt%2Ffind-a-study-partner-may-2014-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR9dxPDzRafKYqQzYt%2Ffind-a-study-partner-may-2014-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>For reasons mentioned in <a href=\"/lw/j10/on_learning_difficult_things/\">So8res article</a> as well as for other reasons: studying with a partner can be very good.</p>\n<p>So if you're looking for a study partner for an online course, reading a manual or else (whether it's in the <a href=\"http://intelligence.org/courses/\">MIRI course list</a> or not) tell others in the comment section.</p>\n<p>The past threads about finding a study partner can be found under the tag <a href=\"/r/discussion/tag/study_thread/\">study_thread</a>. However, you have higher probability of finding a study partner in the most recent thread. If you haven't found a study partner last month, you are welcome to post the same comment again here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R9dxPDzRafKYqQzYt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 1.7091317870771104e-06, "legacy": true, "legacyId": "26169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w5F4w8tNZc6LcBKRP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-06T05:47:40.852Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Museums", "slug": "meetup-washington-dc-museums", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XzcJkaEywDEDG3mqM/meetup-washington-dc-museums", "pageUrlRelative": "/posts/XzcJkaEywDEDG3mqM/meetup-washington-dc-museums", "linkUrl": "https://www.lesswrong.com/posts/XzcJkaEywDEDG3mqM/meetup-washington-dc-museums", "postedAtFormatted": "Tuesday, May 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Museums&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Museums%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXzcJkaEywDEDG3mqM%2Fmeetup-washington-dc-museums%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Museums%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXzcJkaEywDEDG3mqM%2Fmeetup-washington-dc-museums", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXzcJkaEywDEDG3mqM%2Fmeetup-washington-dc-museums", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zx'>Washington DC: Museums</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to go look at museums. We'll rendezvous in the usual place then head out.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zx'>Washington DC: Museums</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XzcJkaEywDEDG3mqM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.7091461109265728e-06, "legacy": true, "legacyId": "26170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Museums\">Discussion article for the meetup : <a href=\"/meetups/zx\">Washington DC: Museums</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to go look at museums. We'll rendezvous in the usual place then head out.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Museums1\">Discussion article for the meetup : <a href=\"/meetups/zx\">Washington DC: Museums</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Museums", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Museums", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Museums", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Museums1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-06T06:10:04.713Z", "modifiedAt": null, "url": null, "title": "Sortition - Hacking Government To Avoid Cognitive Biases And Corruption", "slug": "sortition-hacking-government-to-avoid-cognitive-biases-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aussiekas", "createdAt": "2014-05-02T11:33:59.753Z", "isAdmin": false, "displayName": "Aussiekas"}, "userId": "3xTiHjnPFeaavSvLs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ej3p7FxahyJHW5uqS/sortition-hacking-government-to-avoid-cognitive-biases-and", "pageUrlRelative": "/posts/Ej3p7FxahyJHW5uqS/sortition-hacking-government-to-avoid-cognitive-biases-and", "linkUrl": "https://www.lesswrong.com/posts/Ej3p7FxahyJHW5uqS/sortition-hacking-government-to-avoid-cognitive-biases-and", "postedAtFormatted": "Tuesday, May 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sortition%20-%20Hacking%20Government%20To%20Avoid%20Cognitive%20Biases%20And%20Corruption&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASortition%20-%20Hacking%20Government%20To%20Avoid%20Cognitive%20Biases%20And%20Corruption%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEj3p7FxahyJHW5uqS%2Fsortition-hacking-government-to-avoid-cognitive-biases-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sortition%20-%20Hacking%20Government%20To%20Avoid%20Cognitive%20Biases%20And%20Corruption%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEj3p7FxahyJHW5uqS%2Fsortition-hacking-government-to-avoid-cognitive-biases-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEj3p7FxahyJHW5uqS%2Fsortition-hacking-government-to-avoid-cognitive-biases-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1123, "htmlBody": "<p><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">I've elaborated on this form of government I have proposed in great detail on my blog here</span></p>\n<p>&nbsp;</p>\n<div style=\"text-align: justify;\"><span style=\"font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #333333;\"><span style=\"font-size: 14px; line-height: 19.600000381469727px;\">http://ecophilosophylife.blogspot.com.au/2014/01/a-thoughtful-constitution-by-sortition.html</span></span></div>\n<p><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">The purpose of this post is to be a persuasive argument for my proposed system of democracy. &nbsp;I am arguing along the lines that my legislature by sortition, random selection, is superior to electoral systems. &nbsp;It also mirrors the advances in overcoming bias which are currently being pioneered in the Sciences.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">I. The Problem</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">It is insane that we allow the same people who are elected to cast their eye on society to identify problems, write up the solutions to those problems, and then also vote to approve those solutions. &nbsp;This triple function of government by elected officials isn't simply corruptible, but is inherently flawed in its decision making process.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">II. The Central Committee, overcoming bias, electoral shenanigans, and demographics bias</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">In my system of sortition election there is a mini-referendum done by a huge sampling of 1,000-5,000 representatives at the highest level. &nbsp;They vote everything up or down and cannot change anything about a bill themselves. &nbsp;They are not congregated into one place and there is no politics between them. &nbsp;They don't even need to know, nor could they know each other. &nbsp;Perhaps they could be part of political parties, but there is no need or money behind this as the members of what I'm calling the Central Committee (C2) are never candidates and can individually never serve more than once per lifetime or perhaps per decade in 3 year terms.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">Contentious issues can be moved to a general referendum. &nbsp;In the 1,000 member C2, any law in the margins of 550-450 can have a special second vote proposed by the disagreeing side such that if more than 600 agree then the item is added to the general monthly or quarterly referendum conducted electronically with the entire population. &nbsp;In this way the average person participates and feels heard by their government on a regular basis.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">The major advantage of this C2 is that it is representative. It will have people from all areas, be 50% male and 50% female and will include all minorities. &nbsp;There can be no great misrepresentation or capture of the legislature by a powerful group. &nbsp;This overcome many of the inherent biases of an electoral system which in almost every democracy today routinely under represents minorities.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">III. The Issue Committees (IC)</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">The IC is a totally separate body whose sole job is to identify areas of the law which need updating. &nbsp;They are comprised of 100 citizens and are a split between 51 Regular Citizens (RCs) and 49 Expert Citizens (EC) serving single 3 year terms. &nbsp;There are around 30 ICs and they each serve an area such as defence, environment, food safety, drug safety, telecommunications, changes to government, finance sector, banking sector, etc.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">These committees will meet in person and discuss what needs exist which the government can address. &nbsp;They do not get to write any laws, nor do they get to vote on any laws. &nbsp;There are in fact more of these than there are members of the C2 and they will be the primary face of government where the average citizen can send in requests or communicate needs. &nbsp;The IC shines a spotlight on the issues facing the country. &nbsp;They also form the law writing bodies</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">IV. The Sub Committee (SC)</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">These are temporary parts of the legislature who write the laws. &nbsp;They have no authority over what topic area they get to write laws about, that is determined by the IC and then voted upon by the C2. &nbsp;They are composed of 10 RCs and 10 ECs with the support of 10 Lawyer Citizens (LC). The LCs do not participate to vote when the draft law can be moved up to the C2 for consideration, they simply help draft reasonable laws.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">These SC's form and dissolved quickly, lasting no more than 3-6 months before a proposed law is made. &nbsp;Being called up to the SC is a lot more akin to being drafted for Jury Duty than the IC or C2 level of government as it is a short term of service.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">V. Conclusions</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /></p>\n<ul style=\"margin: 0.5em 0px; outline: none; padding: 0px 0px 0px 2em; list-style-position: initial; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">\n<li style=\"margin: 0px; outline: none; padding: 0px;\">This system is indeed more democratic and more representative than current electoral democracies. &nbsp;It is less prone to corruption and electioneering is impossible as there are no elections.&nbsp;</li>\n</ul>\n<p><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /></p>\n<ul style=\"margin: 0.5em 0px; outline: none; padding: 0px 0px 0px 2em; list-style-position: initial; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">\n<li style=\"margin: 0px; outline: none; padding: 0px;\">Members of the C2, IC, and SC parts of intentionally split in their duties so no conflict of interest can arise and there is no legislator bias where they have pet bills and issues to push through for benefits to specific parts of the country.</li>\n</ul>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\"><br /></div>\n<ul style=\"margin: 0.5em 0px; outline: none; padding: 0px 0px 0px 2em; list-style-position: initial; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">\n<li style=\"margin: 0px; outline: none; padding: 0px;\">This system is also less influenced by the views an opinions of the very wealthy and the demographic and economic makeup of the people involved.</li>\n</ul>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\"><br /></div>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">And that's it. &nbsp;Could it work? &nbsp;Would it work? &nbsp;I'd like to think it has some advantages over the current and outdated mechanisms of democracy in terms of new knowledge about how the human mind works.</div>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\"><br /></div>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">EDIT: &nbsp;moved notes to bottom of post</div>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\"><br /></div>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\"><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">NOTE 1: &nbsp;I anticipate this objection. &nbsp;Random Citizens (RC) and Expert Citizens (EC) have various stipulations on their service and on how often they can serve, check out my linked post at the top for &nbsp;details. &nbsp;Suffice to say, the RCs must have completed high school and cannot be intellectually disabled. &nbsp;Whatever you can think of that might disqualify someone for a jury, think of something along those lines.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">NOTE 2: As for the nature of this being different, look at juries. &nbsp;We already use a process of sortition, though heavily and perhaps unfairly constrained in its current form, to determine if people are guilty or innocent and what sort of punishment they might receive. &nbsp;We even use sortition in committees of experts in various forms form peer reviewed journals with somewhat random selection from a pool of qualified individuals or ECs in my system.</span><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><br style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\" /><span style=\"color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\">NOTE 3: &nbsp;This is not about politics. &nbsp;I often say I am interested in government, but not politics. &nbsp;This confuses a lot of people. &nbsp;If anything, this system would lessen or (too optimistically) eliminate politics. &nbsp;I know there is a general ban on discussion of politics and this is not that. &nbsp;I am trying to modify government and democratic systems to reflect advances in cognitive bias, decision theory, and computer technology to modernize and further democratize the practice of government.</span></div>\n<div style=\"margin: 0px; outline: none; padding: 0px; color: #333333; font-family: 'Helvetica Neue Light', HelveticaNeue-Light, 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 14px; line-height: 19.600000381469727px; text-align: justify;\"><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ej3p7FxahyJHW5uqS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -4, "extendedScore": null, "score": 1.7091764817489103e-06, "legacy": true, "legacyId": "26171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-06T14:57:06.846Z", "modifiedAt": null, "url": null, "title": "LW Meetup in Nuremberg (Germany)", "slug": "lw-meetup-in-nuremberg-germany", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MartinB", "createdAt": "2009-04-20T11:11:22.800Z", "isAdmin": false, "displayName": "MartinB"}, "userId": "2BGK5dWpTXzCE7iwF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WcWLNLzr6onZQ8jjh/lw-meetup-in-nuremberg-germany", "pageUrlRelative": "/posts/WcWLNLzr6onZQ8jjh/lw-meetup-in-nuremberg-germany", "linkUrl": "https://www.lesswrong.com/posts/WcWLNLzr6onZQ8jjh/lw-meetup-in-nuremberg-germany", "postedAtFormatted": "Tuesday, May 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Meetup%20in%20Nuremberg%20(Germany)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Meetup%20in%20Nuremberg%20(Germany)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcWLNLzr6onZQ8jjh%2Flw-meetup-in-nuremberg-germany%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Meetup%20in%20Nuremberg%20(Germany)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcWLNLzr6onZQ8jjh%2Flw-meetup-in-nuremberg-germany", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcWLNLzr6onZQ8jjh%2Flw-meetup-in-nuremberg-germany", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>Hi everyone,</p>\n<p>&nbsp;</p>\n<p>we just reached the staggering number of four interested parties to meetup in Nuremberg. If anyone else is curious just mail me. We are so far working out the time. So no specific plans, just a get to know.</p>\n<p>&nbsp;</p>\n<p>Martin</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WcWLNLzr6onZQ8jjh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 1.709891408888224e-06, "legacy": true, "legacyId": "26175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-06T18:11:44.980Z", "modifiedAt": null, "url": null, "title": "Meetup : Rescheduled: Chicago Calibration Game", "slug": "meetup-rescheduled-chicago-calibration-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:13.167Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YasTMdEHwjappLkQG/meetup-rescheduled-chicago-calibration-game", "pageUrlRelative": "/posts/YasTMdEHwjappLkQG/meetup-rescheduled-chicago-calibration-game", "linkUrl": "https://www.lesswrong.com/posts/YasTMdEHwjappLkQG/meetup-rescheduled-chicago-calibration-game", "postedAtFormatted": "Tuesday, May 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rescheduled%3A%20Chicago%20Calibration%20Game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rescheduled%3A%20Chicago%20Calibration%20Game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYasTMdEHwjappLkQG%2Fmeetup-rescheduled-chicago-calibration-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rescheduled%3A%20Chicago%20Calibration%20Game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYasTMdEHwjappLkQG%2Fmeetup-rescheduled-chicago-calibration-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYasTMdEHwjappLkQG%2Fmeetup-rescheduled-chicago-calibration-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zy'>Rescheduled: Chicago Calibration Game</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 May 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Corner Bakery, 360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Saturday, we'll be playing a personal calibration game. From the LW wiki: \"One person reads the question aloud, and everyone writes down their 50% and 90% confidence intervals. For example, if you\u2019re 50% sure that 20% - 40% of the world\u2019s countries are landlocked, write that down as your 50% confidence interval.\" After everyone has written down confidence intervals, the correct answer is revealed. If we run out of questions, we can spend the rest of the time discussing PredictionBook, idea futures, and similar.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zy'>Rescheduled: Chicago Calibration Game</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YasTMdEHwjappLkQG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.7101555670405778e-06, "legacy": true, "legacyId": "26177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rescheduled__Chicago_Calibration_Game\">Discussion article for the meetup : <a href=\"/meetups/zy\">Rescheduled: Chicago Calibration Game</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 May 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Corner Bakery, 360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Saturday, we'll be playing a personal calibration game. From the LW wiki: \"One person reads the question aloud, and everyone writes down their 50% and 90% confidence intervals. For example, if you\u2019re 50% sure that 20% - 40% of the world\u2019s countries are landlocked, write that down as your 50% confidence interval.\" After everyone has written down confidence intervals, the correct answer is revealed. If we run out of questions, we can spend the rest of the time discussing PredictionBook, idea futures, and similar.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rescheduled__Chicago_Calibration_Game1\">Discussion article for the meetup : <a href=\"/meetups/zy\">Rescheduled: Chicago Calibration Game</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rescheduled: Chicago Calibration Game", "anchor": "Discussion_article_for_the_meetup___Rescheduled__Chicago_Calibration_Game", "level": 1}, {"title": "Discussion article for the meetup : Rescheduled: Chicago Calibration Game", "anchor": "Discussion_article_for_the_meetup___Rescheduled__Chicago_Calibration_Game1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-07T02:42:56.582Z", "modifiedAt": null, "url": null, "title": "Some historical evaluations of forecasting", "slug": "some-historical-evaluations-of-forecasting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Edtd2SPAEHMzzA3Wi/some-historical-evaluations-of-forecasting", "pageUrlRelative": "/posts/Edtd2SPAEHMzzA3Wi/some-historical-evaluations-of-forecasting", "linkUrl": "https://www.lesswrong.com/posts/Edtd2SPAEHMzzA3Wi/some-historical-evaluations-of-forecasting", "postedAtFormatted": "Wednesday, May 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20historical%20evaluations%20of%20forecasting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20historical%20evaluations%20of%20forecasting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdtd2SPAEHMzzA3Wi%2Fsome-historical-evaluations-of-forecasting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20historical%20evaluations%20of%20forecasting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdtd2SPAEHMzzA3Wi%2Fsome-historical-evaluations-of-forecasting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdtd2SPAEHMzzA3Wi%2Fsome-historical-evaluations-of-forecasting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1816, "htmlBody": "<p><em>This post has been written in relation with work I'm doing for the <a href=\"http://intellignece.org\">Machine Intelligence Research Institute (MIRI)</a>, but hasn't been formally vetted by MIRI. I'm posting this to LessWrong because of its potential interest to a segment of the LessWrong readership</em>.</p>\n<p>In order to assess the quality of current and future forecasts, it's important to consider the historical record of forecasting. Doing such a historical evaluation typically requires a systematic record of past forecasts. The set of forecasts may be:</p>\n<ul>\n<li>collected explicitly for the purpose of an experiment to test the quality of forecasts,</li>\n<li>collected implicitly in the form of a market-clearing price or other indicator, or</li>\n<li>collected for the purpose of directly creating useful forecasts. In this case, the evaluation that happens later is incidental and is not the main goal of generating the forecasts.</li>\n</ul>\n<p>Forecasts may be evaluated for any of these (for a longer discussion, see <a href=\"/r/discussion/lw/k2a/the_usefulness_of_forecasts_and_the_rationality/\">here</a>):</p>\n<ul>\n<li>Accuracy of forecasts: How close are the forecasts to what actually transpires?</li>\n<li>Rationality or efficiency of forecasts: Are the forecasters ignoring obvious ways to improve the quality of their forecasts? Forecasts can rarely be proved to be efficient or rational, but we can test for the presence of some specific forms of irrationality or inefficiency (an example is where the forecasts have systematic bias, so that applying a Theil's correction for the bias would yield a better forecast).</li>\n<li>Utility of forecasts: How much more valuable is it to have correct forecasts than incorrect forecasts?</li>\n</ul>\n<ul>\n</ul>\n<p>The forecasts could also take at least two different perspectives:</p>\n<ul>\n<li>They could put the focus on the individuals or institutions making the forecasts, and consider the quality of the forecasts along various dimensions in relation with the incentives and conditions facing these individuals and institutions. This is a <em>black box </em>approach with respect to the forecasting <em>method</em> used. The focus is on explaining why some people or institutions have better incentives or informational advantages that allow them to come up with better forecasts, rather than on what methods work better.</li>\n<li>They could put the focus on the methods used for the forecasts. This is best seen in cases where we compare the performance of programs or softwares that use different forecasting algorithms to guess unknown points in a time series from known points.</li>\n</ul>\n<p><strong>A quick list of forecast evaluations so far</strong></p>\n<p>See also the detailed discussion of each of the evaluations later in the post.</p>\n<p>\n<table border=\"1\">\n<tbody>\n<tr>\n<th>Name of forecast evaluation or forecast data referenced in evaluations</th><th>How were the forecasts collected? </th><th>Evaluation primarily for accuracy or efficiency?</th><th>Focus of evaluation (forecasters or forecasting methods)?</th>\n</tr>\n<tr>\n<td><a href=\"http://en.wikipedia.org/wiki/Makridakis_Competitions\">Makridakis Competitions</a></td>\n<td>Explicitly collected for the forecast evaluation (in the form of a competition).</td>\n<td>Primarily accuracy</td>\n<td>Forecasting methods</td>\n</tr>\n<tr>\n<td><a href=\"/lw/k2z/the_track_record_of_surveybased_macroeconomic/\">Survey-based macroeconomic forecasts</a></td>\n<td>Collected for direct utility of forecasts</td>\n<td>The literature includes evaluations of both accuracy and efficiency</td>\n<td>Forecasters</td>\n</tr>\n<tr>\n<td><a href=\"http://press.princeton.edu/titles/7959.html\">Tetlock study of expert political judgment</a></td>\n<td>Explicitly collected for the forecast evaluation</td>\n<td>Focused on accuracy, using basic computer algorithms as benchmark</td>\n<td>Forecasters (but also discussion of the overall philosophies used)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.dtic.mil/get-tr-doc/pdf?AD=ADA568107\">Tauri Group Retrospective</a></td>\n<td>Collected for direct utility of forecasts, retrospectively used for study<br /></td>\n<td>Focused an accuracy and also systematic bias (a form of irrationality)</td>\n<td>Forecasting methods as well as nature of the items being forecast</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p><strong>Makridakis Competitions</strong></p>\n<p>In this post, I describe existing evaluations and what we can learn from them.</p>\n<p>The Makridakis Competitions (<a href=\"http://en.wikipedia.org/wiki/Makridakis_Competitions\">Wikipedia</a>), known in the forecasting literature as the M-Competitions, are three competitions organized by teams led by forecasting researcher <a href=\"http://en.wikipedia.org/wiki/Spyros_Makridakis\">Spyros Makridakis</a>. Here's a quick listing and summary of the competitions (table from Wikipedia):</p>\n<table class=\"sortable jquery-tablesorter\" border=\"1\">\n<thead> \n<tr>\n<th class=\"headerSort\" title=\"Sort ascending\">No.</th> <th class=\"headerSort\" title=\"Sort ascending\">Informal name for competition</th> <th class=\"headerSort\" title=\"Sort ascending\">Year of publication of results</th> <th class=\"headerSort\" title=\"Sort ascending\">Number of time series used</th> <th class=\"headerSort\" title=\"Sort ascending\">Number of methods tested</th> <th class=\"headerSort\" title=\"Sort ascending\">Other features</th>\n</tr>\n</thead> \n<tbody>\n<tr>\n<td>1</td>\n<td>M Competition or M-Competition<sup id=\"cite_ref-m_5-0\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m-5\"><span>[</span>5<span>]</span></a></sup><sup id=\"cite_ref-m3m_1-1\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m3m-1\"><span>[</span>1<span>]</span></a></sup></td>\n<td>1982</td>\n<td>1001 (used a subsample of 111 for the methods where it was too difficult to run all 1001)</td>\n<td>15 (plus 9 variations)</td>\n<td>Not real-time</td>\n</tr>\n<tr>\n<td>2</td>\n<td>M-2 Competition or M2-Competition<sup id=\"cite_ref-m2_6-0\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m2-6\"><span>[</span>6<span>]</span></a></sup><sup id=\"cite_ref-m3m_1-2\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m3m-1\"><span>[</span>1<span>]</span></a></sup></td>\n<td>1993</td>\n<td>29 (23 from collaborating companies, 6 from macroeconomic indicators)</td>\n<td>16 (including 5 human forecasters and 11 automatic trend-based methods) plus 2 combined forecasts and 1 overall average</td>\n<td>Real-time, many collaborating organizations, competition announced in advance</td>\n</tr>\n<tr>\n<td>3</td>\n<td>M-3 Competition or M3-Competition<sup id=\"cite_ref-m3m_1-3\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m3m-1\"><span>[</span>1<span>]</span></a></sup></td>\n<td>2000</td>\n<td>3003</td>\n<td>24</td>\n<td>&nbsp;</td>\n</tr>\n</tbody>\n<tfoot></tfoot>\n</table>\n<p>According to the authors, the following main conclusions held in all three competitions:</p>\n<ol>\n<li>Statistically sophisticated or complex methods do not necessarily provide more accurate forecasts than simpler ones.</li>\n<li>The relative ranking of the performance of the various methods varies according to the accuracy measure being used.</li>\n<li>The accuracy when various methods are combined outperforms, on average, the individual methods being combined and does very well in comparison to other methods.</li>\n<li>The accuracy of the various methods depends on the length of the forecasting horizon involved.</li>\n</ol>\n<p>Although the organizers of the M3-Competition did contact researchers in the area of <a title=\"Artificial neural network\" href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">artificial neural networks</a> to seek their participation in the competition, only one researcher participated, and that researcher's forecasts fared poorly. The reluctance of most ANN researchers to participate at the time was due to the computationally intensive nature of ANN-based forecasting and the huge time series used for the competition. In 2005, Crone, Nikolopoulos and Hibon organized the NN-3 Competition, using 111 of the time series from the M3-Competition (not the same data, because it was shifted in time, but the same sources). The NN-3 Competition found that the best ANN-based forecasts performed comparably with the best known forecasting methods, but were far more computationally intensive. It was also noted that many ANN-based techniques fared considerably worse than simple forecasting methods, despite greater <em>theoretical</em> potential for good performance. More on the NN-3 Competition <a href=\"http://forecasters.org/pdfs/IIF-SAS-Report-Apr2008.pdf\">here</a>. It's quite possible that if the competition were rerun a few years out from now, the neural network methods would outperform the best simple methods. We'll talk more about simple versus complicated methods in a <a href=\"/lw/k6j/paradigm_shifts_in_forecasting/\">later post</a>.</p>\n<p><strong>Survey-based macroeconomic forecasts</strong></p>\n<p>More details are available in my post reviewing <a href=\"/lw/k2z/the_track_record_of_surveybased_macroeconomic/\">the track record of survey-based macroeconomic forecasting</a>. The following overall conclusions seem to emerge:</p>\n<ol>\n<li>For mature and well-understood economics such as that of the United States, consensus forecasts are not notably biased or inefficient. In cases where they miss the mark, this can usually be attributed to issues of insufficient information or shocks to the economy.</li>\n<li>There may however be some countries. particularly those whose economies are not sufficiently well-understood, where the consensus forecasts are more biased.</li>\n<li>The evidence on whether individual forecasts are biased or inefficient is more murky, but the research generally points in the direction of some individual forecasts being biased. Some people have posited a \"rational bias\" theory where forecasters have incentives to choose a value that is plausible but not the most likely in order to maximize their chances of getting a successful unexpected prediction. We can think of this as an example of <a href=\"http://en.wikipedia.org/wiki/Product_differentiation\">product differentiation</a>. Other sources and theories of rational bias have also been posited, but there is no consensus in the literature on whether and how these are sufficient to explain observed individual bias.</li>\n</ol>\n<p><strong>Tetlock study of expert political judgment</strong></p>\n<p>For his book <a href=\"http://press.princeton.edu/titles/7959.html\"><em>Expert Political Judgment</em></a>, Tetlock surveyed 284 experts and collected a total of 28,000 predictions. His findings, as described in the book and in an <a href=\"http://www.cato-unbound.org/2011/07/11/dan-gardner-philip-tetlock/overcoming-our-aversion-acknowledging-our-ignorance\">article</a> for <em>Cato Unbound</em> co-authored with Dan Gardner, are as follows (note that the language is copy-pasted from the <em>Cato Unbound </em>article but restructured somewhat for sentence flow):</p>\n<ol>\n<li>The average expert&rsquo;s forecasts were revealed to be only slightly more accurate than random guessing&mdash;or, to put more harshly, only a bit better than the proverbial dart-throwing chimpanzee. And the average expert performed slightly worse than a still more mindless competition: simple extrapolation algorithms that automatically predicted more of the same.</li>\n<li>The experts could be divided roughly into two overlapping yet statistically distinguishable groups. One group (the <em>hedgehogs</em>) would actually have been beaten rather soundly even by the chimp, not to mention the more formidable extrapolation algorithm. The other (the <em>foxes</em>) would have beaten the chimp and sometimes even the extrapolation algorithm, although not by a wide margin. </li>\n<li>The hedgehogs tended to use one analytical tool in many different domains; they preferred keeping their analysis simple and elegant by minimizing &ldquo;distractions.&rdquo; These experts zeroed in on only essential information, and they were unusually confident&mdash;they were far more likely to say something is &ldquo;certain&rdquo; or &ldquo;impossible.&rdquo; In explaining their forecasts, they often built up a lot of intellectual momentum in favor of their preferred conclusions. For instance, they were more likely to say &ldquo;moreover&rdquo; than &ldquo;however.&rdquo; </li>\n<li>The foxes used a wide assortment of analytical tools, sought out information from diverse sources, were comfortable with complexity and uncertainty, and were much less sure of themselves&mdash;they tended to talk in terms of possibilities and probabilities and were often happy to say &ldquo;maybe.&rdquo; In explaining their forecasts, they frequently shifted intellectual gears, sprinkling their speech with transition markers such as &ldquo;although,&rdquo; &ldquo;but,&rdquo; and &ldquo;however.&rdquo; </li>\n<li>It's unclear whether the performance of the best forecasters is the best that is in principle possible.</li>\n<li>This widespread lack of curiosity&mdash;lack of interest in thinking about how we think about possible futures&mdash;is a phenomenon worthy of investigation in its own right. </li>\n</ol>\n<p>Tetlock followed up the research in the project with co-creating <a href=\"http://goodjudgmentproject.com\">The Good Judgment Project</a> (<a href=\"http://en.wikipedia.org/wiki/The_Good_Judgment_Project\">Wikipedia</a>), that used aggregation of information from large numbers of participants who had access to Google search and the Internet but didn't necessarily have prior subject matter expertise. The Good Judgment Project produced better forecasts than other contestants in the IARPA <a href=\"http://en.wikipedia.org/wiki/Aggregative_Contingent_Estimation\">Aggregative Contingent Estimation</a> contest. This finding combines the idea that foxes have advantages over hedgehogs (Google searches by people without much prior knowledge resembles fox-like thinking) and the miracle of aggregation.</p>\n<p><strong>Tauri Group Retrospective</strong></p>\n<p>The report titled <a href=\"http://www.dtic.mil/get-tr-doc/pdf?AD=ADA568107\"><em>Retrospective Analysis of Technology Forecasting: InScope Extension</em></a> by Carie Mullins for the Tauri Group often goes by the name of the Tauri Group Retrospective. The report was published on August 13, 2012 and includes 2,092 forecasts that were found to be timely, specific, complete, and relevant enough to be further verified and assessed for accuracy. The following were the main findings (from Table ES-2 of the paper, Page 3):</p>\n<ol>\n<li>In general, forecasts provide more accurate predictions than uninformed guesses. Six of the eight methodologies statistically are more accurate than a theoretical probability of success (random guess). Although qualitative trend analysis and gaming and scenarios methods have observed accuracies better than a random guess, at a 95% confidence interval there is no statistical evidence that these methods would perform better than a guess.</li>\n<li>Forecasts based on numeric trends are more accurate than forecasts based on opinion. Forecasts generated from quantitative trend analyses have statistically higher success rates than do forecasts generated from other methodologies.</li>\n<li>Forecasts are more likely to overestimate the event date. This is a change from our previous study, which indicated that there was a balance between pessimistic and optimistic forecasts.</li>\n<li>Short -term forecasts are more accurate than medium- and long-term forecasts.</li>\n<li>A predictive model of forecast accuracy could not be developed. Forecast accuracy appears to be influenced by a random component or some other attribute not captured in the study.</li>\n<li>Forecasts that clearly describe timeframe, technology, predicted event, and associated performance metrics are more informative.</li>\n</ol>\n<p>&nbsp;</p>\n<ol> </ol> \n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Edtd2SPAEHMzzA3Wi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 1.7108496995324531e-06, "legacy": true, "legacyId": "26060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This post has been written in relation with work I'm doing for the <a href=\"http://intellignece.org\">Machine Intelligence Research Institute (MIRI)</a>, but hasn't been formally vetted by MIRI. I'm posting this to LessWrong because of its potential interest to a segment of the LessWrong readership</em>.</p>\n<p>In order to assess the quality of current and future forecasts, it's important to consider the historical record of forecasting. Doing such a historical evaluation typically requires a systematic record of past forecasts. The set of forecasts may be:</p>\n<ul>\n<li>collected explicitly for the purpose of an experiment to test the quality of forecasts,</li>\n<li>collected implicitly in the form of a market-clearing price or other indicator, or</li>\n<li>collected for the purpose of directly creating useful forecasts. In this case, the evaluation that happens later is incidental and is not the main goal of generating the forecasts.</li>\n</ul>\n<p>Forecasts may be evaluated for any of these (for a longer discussion, see <a href=\"/r/discussion/lw/k2a/the_usefulness_of_forecasts_and_the_rationality/\">here</a>):</p>\n<ul>\n<li>Accuracy of forecasts: How close are the forecasts to what actually transpires?</li>\n<li>Rationality or efficiency of forecasts: Are the forecasters ignoring obvious ways to improve the quality of their forecasts? Forecasts can rarely be proved to be efficient or rational, but we can test for the presence of some specific forms of irrationality or inefficiency (an example is where the forecasts have systematic bias, so that applying a Theil's correction for the bias would yield a better forecast).</li>\n<li>Utility of forecasts: How much more valuable is it to have correct forecasts than incorrect forecasts?</li>\n</ul>\n<ul>\n</ul>\n<p>The forecasts could also take at least two different perspectives:</p>\n<ul>\n<li>They could put the focus on the individuals or institutions making the forecasts, and consider the quality of the forecasts along various dimensions in relation with the incentives and conditions facing these individuals and institutions. This is a <em>black box </em>approach with respect to the forecasting <em>method</em> used. The focus is on explaining why some people or institutions have better incentives or informational advantages that allow them to come up with better forecasts, rather than on what methods work better.</li>\n<li>They could put the focus on the methods used for the forecasts. This is best seen in cases where we compare the performance of programs or softwares that use different forecasting algorithms to guess unknown points in a time series from known points.</li>\n</ul>\n<p><strong id=\"A_quick_list_of_forecast_evaluations_so_far\">A quick list of forecast evaluations so far</strong></p>\n<p>See also the detailed discussion of each of the evaluations later in the post.</p>\n<p>\n</p><table border=\"1\">\n<tbody>\n<tr>\n<th>Name of forecast evaluation or forecast data referenced in evaluations</th><th>How were the forecasts collected? </th><th>Evaluation primarily for accuracy or efficiency?</th><th>Focus of evaluation (forecasters or forecasting methods)?</th>\n</tr>\n<tr>\n<td><a href=\"http://en.wikipedia.org/wiki/Makridakis_Competitions\">Makridakis Competitions</a></td>\n<td>Explicitly collected for the forecast evaluation (in the form of a competition).</td>\n<td>Primarily accuracy</td>\n<td>Forecasting methods</td>\n</tr>\n<tr>\n<td><a href=\"/lw/k2z/the_track_record_of_surveybased_macroeconomic/\">Survey-based macroeconomic forecasts</a></td>\n<td>Collected for direct utility of forecasts</td>\n<td>The literature includes evaluations of both accuracy and efficiency</td>\n<td>Forecasters</td>\n</tr>\n<tr>\n<td><a href=\"http://press.princeton.edu/titles/7959.html\">Tetlock study of expert political judgment</a></td>\n<td>Explicitly collected for the forecast evaluation</td>\n<td>Focused on accuracy, using basic computer algorithms as benchmark</td>\n<td>Forecasters (but also discussion of the overall philosophies used)</td>\n</tr>\n<tr>\n<td><a href=\"http://www.dtic.mil/get-tr-doc/pdf?AD=ADA568107\">Tauri Group Retrospective</a></td>\n<td>Collected for direct utility of forecasts, retrospectively used for study<br></td>\n<td>Focused an accuracy and also systematic bias (a form of irrationality)</td>\n<td>Forecasting methods as well as nature of the items being forecast</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p><strong id=\"Makridakis_Competitions\">Makridakis Competitions</strong></p>\n<p>In this post, I describe existing evaluations and what we can learn from them.</p>\n<p>The Makridakis Competitions (<a href=\"http://en.wikipedia.org/wiki/Makridakis_Competitions\">Wikipedia</a>), known in the forecasting literature as the M-Competitions, are three competitions organized by teams led by forecasting researcher <a href=\"http://en.wikipedia.org/wiki/Spyros_Makridakis\">Spyros Makridakis</a>. Here's a quick listing and summary of the competitions (table from Wikipedia):</p>\n<table class=\"sortable jquery-tablesorter\" border=\"1\">\n<thead> \n<tr>\n<th class=\"headerSort\" title=\"Sort ascending\">No.</th> <th class=\"headerSort\" title=\"Sort ascending\">Informal name for competition</th> <th class=\"headerSort\" title=\"Sort ascending\">Year of publication of results</th> <th class=\"headerSort\" title=\"Sort ascending\">Number of time series used</th> <th class=\"headerSort\" title=\"Sort ascending\">Number of methods tested</th> <th class=\"headerSort\" title=\"Sort ascending\">Other features</th>\n</tr>\n</thead> \n<tbody>\n<tr>\n<td>1</td>\n<td>M Competition or M-Competition<sup id=\"cite_ref-m_5-0\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m-5\"><span>[</span>5<span>]</span></a></sup><sup id=\"cite_ref-m3m_1-1\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m3m-1\"><span>[</span>1<span>]</span></a></sup></td>\n<td>1982</td>\n<td>1001 (used a subsample of 111 for the methods where it was too difficult to run all 1001)</td>\n<td>15 (plus 9 variations)</td>\n<td>Not real-time</td>\n</tr>\n<tr>\n<td>2</td>\n<td>M-2 Competition or M2-Competition<sup id=\"cite_ref-m2_6-0\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m2-6\"><span>[</span>6<span>]</span></a></sup><sup id=\"cite_ref-m3m_1-2\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m3m-1\"><span>[</span>1<span>]</span></a></sup></td>\n<td>1993</td>\n<td>29 (23 from collaborating companies, 6 from macroeconomic indicators)</td>\n<td>16 (including 5 human forecasters and 11 automatic trend-based methods) plus 2 combined forecasts and 1 overall average</td>\n<td>Real-time, many collaborating organizations, competition announced in advance</td>\n</tr>\n<tr>\n<td>3</td>\n<td>M-3 Competition or M3-Competition<sup id=\"cite_ref-m3m_1-3\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions#cite_note-m3m-1\"><span>[</span>1<span>]</span></a></sup></td>\n<td>2000</td>\n<td>3003</td>\n<td>24</td>\n<td>&nbsp;</td>\n</tr>\n</tbody>\n<tfoot></tfoot>\n</table>\n<p>According to the authors, the following main conclusions held in all three competitions:</p>\n<ol>\n<li>Statistically sophisticated or complex methods do not necessarily provide more accurate forecasts than simpler ones.</li>\n<li>The relative ranking of the performance of the various methods varies according to the accuracy measure being used.</li>\n<li>The accuracy when various methods are combined outperforms, on average, the individual methods being combined and does very well in comparison to other methods.</li>\n<li>The accuracy of the various methods depends on the length of the forecasting horizon involved.</li>\n</ol>\n<p>Although the organizers of the M3-Competition did contact researchers in the area of <a title=\"Artificial neural network\" href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">artificial neural networks</a> to seek their participation in the competition, only one researcher participated, and that researcher's forecasts fared poorly. The reluctance of most ANN researchers to participate at the time was due to the computationally intensive nature of ANN-based forecasting and the huge time series used for the competition. In 2005, Crone, Nikolopoulos and Hibon organized the NN-3 Competition, using 111 of the time series from the M3-Competition (not the same data, because it was shifted in time, but the same sources). The NN-3 Competition found that the best ANN-based forecasts performed comparably with the best known forecasting methods, but were far more computationally intensive. It was also noted that many ANN-based techniques fared considerably worse than simple forecasting methods, despite greater <em>theoretical</em> potential for good performance. More on the NN-3 Competition <a href=\"http://forecasters.org/pdfs/IIF-SAS-Report-Apr2008.pdf\">here</a>. It's quite possible that if the competition were rerun a few years out from now, the neural network methods would outperform the best simple methods. We'll talk more about simple versus complicated methods in a <a href=\"/lw/k6j/paradigm_shifts_in_forecasting/\">later post</a>.</p>\n<p><strong id=\"Survey_based_macroeconomic_forecasts\">Survey-based macroeconomic forecasts</strong></p>\n<p>More details are available in my post reviewing <a href=\"/lw/k2z/the_track_record_of_surveybased_macroeconomic/\">the track record of survey-based macroeconomic forecasting</a>. The following overall conclusions seem to emerge:</p>\n<ol>\n<li>For mature and well-understood economics such as that of the United States, consensus forecasts are not notably biased or inefficient. In cases where they miss the mark, this can usually be attributed to issues of insufficient information or shocks to the economy.</li>\n<li>There may however be some countries. particularly those whose economies are not sufficiently well-understood, where the consensus forecasts are more biased.</li>\n<li>The evidence on whether individual forecasts are biased or inefficient is more murky, but the research generally points in the direction of some individual forecasts being biased. Some people have posited a \"rational bias\" theory where forecasters have incentives to choose a value that is plausible but not the most likely in order to maximize their chances of getting a successful unexpected prediction. We can think of this as an example of <a href=\"http://en.wikipedia.org/wiki/Product_differentiation\">product differentiation</a>. Other sources and theories of rational bias have also been posited, but there is no consensus in the literature on whether and how these are sufficient to explain observed individual bias.</li>\n</ol>\n<p><strong id=\"Tetlock_study_of_expert_political_judgment\">Tetlock study of expert political judgment</strong></p>\n<p>For his book <a href=\"http://press.princeton.edu/titles/7959.html\"><em>Expert Political Judgment</em></a>, Tetlock surveyed 284 experts and collected a total of 28,000 predictions. His findings, as described in the book and in an <a href=\"http://www.cato-unbound.org/2011/07/11/dan-gardner-philip-tetlock/overcoming-our-aversion-acknowledging-our-ignorance\">article</a> for <em>Cato Unbound</em> co-authored with Dan Gardner, are as follows (note that the language is copy-pasted from the <em>Cato Unbound </em>article but restructured somewhat for sentence flow):</p>\n<ol>\n<li>The average expert\u2019s forecasts were revealed to be only slightly more accurate than random guessing\u2014or, to put more harshly, only a bit better than the proverbial dart-throwing chimpanzee. And the average expert performed slightly worse than a still more mindless competition: simple extrapolation algorithms that automatically predicted more of the same.</li>\n<li>The experts could be divided roughly into two overlapping yet statistically distinguishable groups. One group (the <em>hedgehogs</em>) would actually have been beaten rather soundly even by the chimp, not to mention the more formidable extrapolation algorithm. The other (the <em>foxes</em>) would have beaten the chimp and sometimes even the extrapolation algorithm, although not by a wide margin. </li>\n<li>The hedgehogs tended to use one analytical tool in many different domains; they preferred keeping their analysis simple and elegant by minimizing \u201cdistractions.\u201d These experts zeroed in on only essential information, and they were unusually confident\u2014they were far more likely to say something is \u201ccertain\u201d or \u201cimpossible.\u201d In explaining their forecasts, they often built up a lot of intellectual momentum in favor of their preferred conclusions. For instance, they were more likely to say \u201cmoreover\u201d than \u201chowever.\u201d </li>\n<li>The foxes used a wide assortment of analytical tools, sought out information from diverse sources, were comfortable with complexity and uncertainty, and were much less sure of themselves\u2014they tended to talk in terms of possibilities and probabilities and were often happy to say \u201cmaybe.\u201d In explaining their forecasts, they frequently shifted intellectual gears, sprinkling their speech with transition markers such as \u201calthough,\u201d \u201cbut,\u201d and \u201chowever.\u201d </li>\n<li>It's unclear whether the performance of the best forecasters is the best that is in principle possible.</li>\n<li>This widespread lack of curiosity\u2014lack of interest in thinking about how we think about possible futures\u2014is a phenomenon worthy of investigation in its own right. </li>\n</ol>\n<p>Tetlock followed up the research in the project with co-creating <a href=\"http://goodjudgmentproject.com\">The Good Judgment Project</a> (<a href=\"http://en.wikipedia.org/wiki/The_Good_Judgment_Project\">Wikipedia</a>), that used aggregation of information from large numbers of participants who had access to Google search and the Internet but didn't necessarily have prior subject matter expertise. The Good Judgment Project produced better forecasts than other contestants in the IARPA <a href=\"http://en.wikipedia.org/wiki/Aggregative_Contingent_Estimation\">Aggregative Contingent Estimation</a> contest. This finding combines the idea that foxes have advantages over hedgehogs (Google searches by people without much prior knowledge resembles fox-like thinking) and the miracle of aggregation.</p>\n<p><strong id=\"Tauri_Group_Retrospective\">Tauri Group Retrospective</strong></p>\n<p>The report titled <a href=\"http://www.dtic.mil/get-tr-doc/pdf?AD=ADA568107\"><em>Retrospective Analysis of Technology Forecasting: InScope Extension</em></a> by Carie Mullins for the Tauri Group often goes by the name of the Tauri Group Retrospective. The report was published on August 13, 2012 and includes 2,092 forecasts that were found to be timely, specific, complete, and relevant enough to be further verified and assessed for accuracy. The following were the main findings (from Table ES-2 of the paper, Page 3):</p>\n<ol>\n<li>In general, forecasts provide more accurate predictions than uninformed guesses. Six of the eight methodologies statistically are more accurate than a theoretical probability of success (random guess). Although qualitative trend analysis and gaming and scenarios methods have observed accuracies better than a random guess, at a 95% confidence interval there is no statistical evidence that these methods would perform better than a guess.</li>\n<li>Forecasts based on numeric trends are more accurate than forecasts based on opinion. Forecasts generated from quantitative trend analyses have statistically higher success rates than do forecasts generated from other methodologies.</li>\n<li>Forecasts are more likely to overestimate the event date. This is a change from our previous study, which indicated that there was a balance between pessimistic and optimistic forecasts.</li>\n<li>Short -term forecasts are more accurate than medium- and long-term forecasts.</li>\n<li>A predictive model of forecast accuracy could not be developed. Forecast accuracy appears to be influenced by a random component or some other attribute not captured in the study.</li>\n<li>Forecasts that clearly describe timeframe, technology, predicted event, and associated performance metrics are more informative.</li>\n</ol>\n<p>&nbsp;</p>\n<ol> </ol> \n<ul>\n</ul>", "sections": [{"title": "A quick list of forecast evaluations so far", "anchor": "A_quick_list_of_forecast_evaluations_so_far", "level": 1}, {"title": "Makridakis Competitions", "anchor": "Makridakis_Competitions", "level": 1}, {"title": "Survey-based macroeconomic forecasts", "anchor": "Survey_based_macroeconomic_forecasts", "level": 1}, {"title": "Tetlock study of expert political judgment", "anchor": "Tetlock_study_of_expert_political_judgment", "level": 1}, {"title": "Tauri Group Retrospective", "anchor": "Tauri_Group_Retrospective", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EkpP6BweXi7zpdDKX", "9FsaWyQXSPErJj34K", "wsM7wpEs9jRsQLuEK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-07T06:12:52.690Z", "modifiedAt": null, "url": null, "title": "MIRI Donation Collaboration Station Redux: The Final Push (IMPORTANT)", "slug": "miri-donation-collaboration-station-redux-the-final-push", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Skeptityke", "createdAt": "2013-05-06T21:18:20.431Z", "isAdmin": false, "displayName": "Skeptityke"}, "userId": "H5HExNf3BsmBRvKJG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2K754Kn85EiRpcr2L/miri-donation-collaboration-station-redux-the-final-push", "pageUrlRelative": "/posts/2K754Kn85EiRpcr2L/miri-donation-collaboration-station-redux-the-final-push", "linkUrl": "https://www.lesswrong.com/posts/2K754Kn85EiRpcr2L/miri-donation-collaboration-station-redux-the-final-push", "postedAtFormatted": "Wednesday, May 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI%20Donation%20Collaboration%20Station%20Redux%3A%20The%20Final%20Push%20(IMPORTANT)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI%20Donation%20Collaboration%20Station%20Redux%3A%20The%20Final%20Push%20(IMPORTANT)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2K754Kn85EiRpcr2L%2Fmiri-donation-collaboration-station-redux-the-final-push%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI%20Donation%20Collaboration%20Station%20Redux%3A%20The%20Final%20Push%20(IMPORTANT)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2K754Kn85EiRpcr2L%2Fmiri-donation-collaboration-station-redux-the-final-push", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2K754Kn85EiRpcr2L%2Fmiri-donation-collaboration-station-redux-the-final-push", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>As you likely already know, there is a one-day only fundraiser for MIRI going on tonight. 1 hour left, or less on the clock.</p>\n<p>Donation page is <a href=\"http://svgives.razoo.com/story/Machine-Intelligence-Research-Institute\" target=\"_blank\">HERE</a></p>\n<p>And, \"$250K in Microsoft in-kind giving is available for the organization which gets the most unique donors, which won't literally be worth $250K to us but includes hardware, software and consulting services; plus the publicity of winning this major competition among charities. This is one of the most leveraged chances to give $10 that you'll probably ever encounter ever.\" -Yudkowsky</p>\n<p>So, a 250k opportunity is on the line, under an hour is left, we are in second place, and it only takes a 10 dollar donation to be registered as a unique donor (if you haven't donated already).</p>\n<p>So let's do this. In the last 10 minutes before midnight, donate once. And if possible, persuade people around you to donate once.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2K754Kn85EiRpcr2L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "26181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-07T18:48:32.833Z", "modifiedAt": null, "url": null, "title": "Proposal: Community Curated Anki Decks For MIRI Recommended Courses", "slug": "proposal-community-curated-anki-decks-for-miri-recommended", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:36.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "iconreforged", "createdAt": "2013-03-29T17:05:43.614Z", "isAdmin": false, "displayName": "iconreforged"}, "userId": "6pY4H6nEqYTBvKrvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fHgLKnvCBm3KvoRiK/proposal-community-curated-anki-decks-for-miri-recommended", "pageUrlRelative": "/posts/fHgLKnvCBm3KvoRiK/proposal-community-curated-anki-decks-for-miri-recommended", "linkUrl": "https://www.lesswrong.com/posts/fHgLKnvCBm3KvoRiK/proposal-community-curated-anki-decks-for-miri-recommended", "postedAtFormatted": "Wednesday, May 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20Community%20Curated%20Anki%20Decks%20For%20MIRI%20Recommended%20Courses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20Community%20Curated%20Anki%20Decks%20For%20MIRI%20Recommended%20Courses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHgLKnvCBm3KvoRiK%2Fproposal-community-curated-anki-decks-for-miri-recommended%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20Community%20Curated%20Anki%20Decks%20For%20MIRI%20Recommended%20Courses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHgLKnvCBm3KvoRiK%2Fproposal-community-curated-anki-decks-for-miri-recommended", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHgLKnvCBm3KvoRiK%2Fproposal-community-curated-anki-decks-for-miri-recommended", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 800, "htmlBody": "<p>Spaced repetition is optimal for recalling factual information. It won't necessarily teach you anything that you haven't already learned. It helps you retain knowledge, and won't necessarily help you develop skills. But, within the domain of factual information that you can already comprehend, spaced repetition systems are pretty optimal. So, if you want to train your brain on a bunch of Spanish-to-English sentence translations, or stock market tickers, or definitions, or sample questions, you should use something like Anki.&nbsp;</p>\n<p>Once you start using spaced repetition, you learn that one of the biggest limits is the card-making process. Making your own cards is time-consuming, although experience will make you much faster. Experience will also teach you what makes a better card. The <a href=\"http://www.supermemo.com/articles/20rules.htm\">20 Rules of Formatting Knowledge</a>&nbsp;pretty much spells it out for you, but I still had to make my own cards, find the sticking points, and edit them until I got a good sense for proper context and suitably short, distinct answers.&nbsp;</p>\n<p>You can find other people's shared decks and skip the card making process yourself, but not without some new problems. First, you are going to learn more making the cards yourself than studying someone else's. If it's subtle material, you can make cards that fill in the gaps in your particular understanding. But if someone else studies something and makes cards to fill in their own gaps, that means that what you're studying may not cover material that you don't know you don't know. It would be nice if everyone's shared decks were a completely thorough treatment of the material, but, alas, it is not so. And the only way that you can tell is by comparing the deck and the material during your own studying.</p>\n<p>Shared decks also just aren't all that good sometimes. Someone, I can't recall who, wrote a script to scrape the entire LW wiki and then cloze-delete the title from the article. I appreciate the idea of SRSing the LW wiki, and scripting the whole thing was undoubtedly really efficient. However, the result was usually question and answer text hundreds of words long, with tables of content in the middle, and probably too many cards of insufficient value.&nbsp;</p>\n<p>Despite their problems, I think that shared decks have way more potential than their current use suggests. A well-crafted deck that gives its subject matter a thorough treatment could be more valuable than a textbook, and about as difficult to compose. But, looking at some of the best Anki decks I've come across, it will likely take more than one person to get such a deck off the ground.&nbsp;</p>\n<p>Anki's .apkg files are sorta unwieldy to edit collaboratively, because there's not really a way to merge edits from multiple contributors. Luckily, we can export and import decks as text, and use version control like GitHub to do the same thing. With a GitHub-hosted collaborative deck, a team of people studying a textbook, like Thinking and Deciding, could all make flashcards as they go, add them all to the same deck, remove redundant cards, standardize the layout, tag cards appropriately, and share them with whomever else comes along. Then, anyone else who wants to study the textbook has a high-quality Anki deck to use in conjunction, and if they know how a question can be asked better, or if they find an error, or if the seventh chapter didn't really get much coverage, they can contribute to the deck, too.&nbsp;</p>\n<p><a href=\"http://intelligence.org/courses/\">This huge list of material</a>&nbsp;put together by Louie Helm should be Anki-fied. Hopefully we can unite the efforts of many autodidacts and start to curate decks for each of the areas covered. Maybe a group of friends is about to work through a course on Quantum Computing or Set Theory. The rest of LW would benefit from their work making flashcards, but especially so if they leave the project open to collaboration.&nbsp;</p>\n<p>So, the things needed to move forward:</p>\n<p>\n<ul>\n<li>Someone learned in IP tell me what kind of licensing or copyright applies here. Should people post these with a Creative Commons or a GPL? Obviously we don't want to start plagiarizing or copyright-violating in the process of making this work. We don't want to abscond with other people's decks and start building on them, I think.</li>\n<li>If you're about to tackle an area of study on the MIRI courses list, make a GitHub repo for it.</li>\n<li>If this interests you in the slightest way, please contact me. iconreforged@gmail.com&nbsp;</li>\n<li>I'm working through the dull details of hosting an Anki deck in text form on GitHub myself with the copious number of Russian flashcards I made in three semesters of Russian classes. Hopefully that can provide some kind of template. If I'm feeling ambitious, I might start a Heuristics and Biases deck based on Thinking and Deciding.</li>\n</ul>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fHgLKnvCBm3KvoRiK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-07T21:47:37.752Z", "modifiedAt": null, "url": null, "title": "[LINK] Sean Carroll Against Afterlife", "slug": "link-sean-carroll-against-afterlife", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3B4j6Xjku3is9JBtb/link-sean-carroll-against-afterlife", "pageUrlRelative": "/posts/3B4j6Xjku3is9JBtb/link-sean-carroll-against-afterlife", "linkUrl": "https://www.lesswrong.com/posts/3B4j6Xjku3is9JBtb/link-sean-carroll-against-afterlife", "postedAtFormatted": "Wednesday, May 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Sean%20Carroll%20Against%20Afterlife&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Sean%20Carroll%20Against%20Afterlife%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3B4j6Xjku3is9JBtb%2Flink-sean-carroll-against-afterlife%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Sean%20Carroll%20Against%20Afterlife%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3B4j6Xjku3is9JBtb%2Flink-sean-carroll-against-afterlife", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3B4j6Xjku3is9JBtb%2Flink-sean-carroll-against-afterlife", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>Well, <a href=\"http://www.preposterousuniverse.com/blog/2014/05/07/is-there-life-after-death-a-debate/\">not quite, but close</a>. The debate starts 1 hour after this post is up. From Sean's blog post:</p>\n<p style=\"padding-left: 30px;\">Is There Life After Death? A Debate</p>\n<p style=\"padding-left: 30px;\">No, there&rsquo;s not. In order to believe otherwise, you would have to be willing to radically alter our fundamental understanding of physics on the basis of almost no evidence. Which I&rsquo;m not willing to do. But others feel differently! So we&rsquo;re going to have a debate about it tonight &mdash; to be live-streamed.</p>\n<p>Note that Sean <a href=\"http://www.preposterousuniverse.com/blog/2014/02/24/post-debate-reflections/\">did extremely well against W.L. Craig</a>&nbsp;(<a href=\"/lw/jr7/link_sean_carrols_reflections_on_his_debate_with/\">LW discussion</a>), so this should be interesting. His co-debater&nbsp;<a href=\"http://en.wikipedia.org/wiki/Steven_Novella\">Steven Novella</a>&nbsp;runs&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Skeptics%27_Guide_to_the_Universe\">The Skeptics' Guide to the Universe</a>&nbsp;podcast, well worth listening to.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3B4j6Xjku3is9JBtb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gemsMw4acvMWrg6wW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-08T00:26:33.998Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal - How to be charismatic", "slug": "meetup-montreal-how-to-be-charismatic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dr5Zh5bwJgNvHzScm/meetup-montreal-how-to-be-charismatic", "pageUrlRelative": "/posts/Dr5Zh5bwJgNvHzScm/meetup-montreal-how-to-be-charismatic", "linkUrl": "https://www.lesswrong.com/posts/Dr5Zh5bwJgNvHzScm/meetup-montreal-how-to-be-charismatic", "postedAtFormatted": "Thursday, May 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20-%20How%20to%20be%20charismatic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20-%20How%20to%20be%20charismatic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDr5Zh5bwJgNvHzScm%2Fmeetup-montreal-how-to-be-charismatic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20-%20How%20to%20be%20charismatic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDr5Zh5bwJgNvHzScm%2Fmeetup-montreal-how-to-be-charismatic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDr5Zh5bwJgNvHzScm%2Fmeetup-montreal-how-to-be-charismatic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/zz'>Montreal - How to be charismatic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3459 McTavish, montreal</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Charisma isn't just \"something you're born with\"; it's a specific set of behaviors that can be learned by anyone.  You can practice these skills, and work on the three main areas of charisma: presence, power and warmth.</p>\n\n<p>I'll present some of the exercises found in The Charisma Myth: How Anyone Can Master the Art and Science of Personal Magnetism (by Olivia Fox Cabane), and we'll see if we can find some fun ways of practicing!</p>\n\n<p>If anyone has any good ideas for locations, please let me know; ideally, we would want a location where we can talk loudly without fear of bothering anyone.  If the weather permits, we could do it outside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/zz'>Montreal - How to be charismatic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dr5Zh5bwJgNvHzScm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.7126221048649503e-06, "legacy": true, "legacyId": "26185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal___How_to_be_charismatic\">Discussion article for the meetup : <a href=\"/meetups/zz\">Montreal - How to be charismatic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3459 McTavish, montreal</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Charisma isn't just \"something you're born with\"; it's a specific set of behaviors that can be learned by anyone.  You can practice these skills, and work on the three main areas of charisma: presence, power and warmth.</p>\n\n<p>I'll present some of the exercises found in The Charisma Myth: How Anyone Can Master the Art and Science of Personal Magnetism (by Olivia Fox Cabane), and we'll see if we can find some fun ways of practicing!</p>\n\n<p>If anyone has any good ideas for locations, please let me know; ideally, we would want a location where we can talk loudly without fear of bothering anyone.  If the weather permits, we could do it outside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal___How_to_be_charismatic1\">Discussion article for the meetup : <a href=\"/meetups/zz\">Montreal - How to be charismatic</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal - How to be charismatic", "anchor": "Discussion_article_for_the_meetup___Montreal___How_to_be_charismatic", "level": 1}, {"title": "Discussion article for the meetup : Montreal - How to be charismatic", "anchor": "Discussion_article_for_the_meetup___Montreal___How_to_be_charismatic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-08T00:37:38.525Z", "modifiedAt": null, "url": null, "title": "[LINK] No Boltzmann Brains in an Empty Expanding Universe", "slug": "link-no-boltzmann-brains-in-an-empty-expanding-universe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:19.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BgXHNa8uEfMKarYG5/link-no-boltzmann-brains-in-an-empty-expanding-universe", "pageUrlRelative": "/posts/BgXHNa8uEfMKarYG5/link-no-boltzmann-brains-in-an-empty-expanding-universe", "linkUrl": "https://www.lesswrong.com/posts/BgXHNa8uEfMKarYG5/link-no-boltzmann-brains-in-an-empty-expanding-universe", "postedAtFormatted": "Thursday, May 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20No%20Boltzmann%20Brains%20in%20an%20Empty%20Expanding%20Universe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20No%20Boltzmann%20Brains%20in%20an%20Empty%20Expanding%20Universe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgXHNa8uEfMKarYG5%2Flink-no-boltzmann-brains-in-an-empty-expanding-universe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20No%20Boltzmann%20Brains%20in%20an%20Empty%20Expanding%20Universe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgXHNa8uEfMKarYG5%2Flink-no-boltzmann-brains-in-an-empty-expanding-universe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgXHNa8uEfMKarYG5%2Flink-no-boltzmann-brains-in-an-empty-expanding-universe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 261, "htmlBody": "<p>Another link to Sean Carroll's blog:&nbsp;<a href=\"http://www.preposterousuniverse.com/blog/2014/05/05/squelching-boltzmann-brains-and-maybe-eternal-inflation/\">Squelching Boltzmann Brains (And Maybe Eternal Inflation)</a>. The discussion of Boltzmann brains has come up many times on LW, starting from <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">this</a> Eliezer's post. Now Sean and his collaborators <a href=\"http://arxiv.org/abs/1405.0298\">argue</a> that in an empty expanding universe:</p>\n<p style=\"padding-left: 30px;\"><strong>Quantum fluctuations are not dynamical processes inherent to a system, but instead reflect the statistical nature of measurement outcomes.</strong> Making a de\fnite measurement requires an out-of-equilibrium, low-entropy detection apparatus that interacts with an environment to induce decoherence. Quantum variables are not equivalent to classical stochastic variables. They may behave similarly when measured repeatedly over time, in which case it is sensible to identify the nonzero variance of a quantum-mechanical observable with the physical fluctuations of a classical variable. <strong>In a truly stationary state, however, there are no fluctuations that decohere.</strong> We conclude that systems in such a state|including, in particular, the Hartle-Hawking vacuum never fluctuate into lower-entropy states, including false vacua or con\fgurations with Boltzmann brains.</p>\n<p style=\"padding-left: 30px;\">Although our universe, today or during inflation, is of course not in the vacuum, the cosmic no-hair theorem implies that any patch in an expanding universe with a positive cosmological constant will asymptote to the vacuum. Within QFT in curved spacetime, the Boltzmann brain problem is thus eliminated: a patch in eternal de Sitter can form only a finite (and small) number of brains on its way to the vacuum.</p>\n<p>In other words, in an empty universe no macroscopic areas of low entropy can form. And a non-vacuum expanding universe like ours becomes vacuum after a time too short to form more than a few Boltzmann brains.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BgXHNa8uEfMKarYG5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "26186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LubwxZHKKvCivYGzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-08T11:18:52.888Z", "modifiedAt": null, "url": null, "title": "[Link] Quantum theory as the most robust description of reproducible experiments", "slug": "link-quantum-theory-as-the-most-robust-description-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:29.804Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gRR", "createdAt": "2012-02-02T12:11:00.628Z", "isAdmin": false, "displayName": "gRR"}, "userId": "LPBRzHQvMP9chLNWH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rok6ZgZ6L85dpBfH7/link-quantum-theory-as-the-most-robust-description-of", "pageUrlRelative": "/posts/rok6ZgZ6L85dpBfH7/link-quantum-theory-as-the-most-robust-description-of", "linkUrl": "https://www.lesswrong.com/posts/rok6ZgZ6L85dpBfH7/link-quantum-theory-as-the-most-robust-description-of", "postedAtFormatted": "Thursday, May 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Quantum%20theory%20as%20the%20most%20robust%20description%20of%20reproducible%20experiments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Quantum%20theory%20as%20the%20most%20robust%20description%20of%20reproducible%20experiments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frok6ZgZ6L85dpBfH7%2Flink-quantum-theory-as-the-most-robust-description-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Quantum%20theory%20as%20the%20most%20robust%20description%20of%20reproducible%20experiments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frok6ZgZ6L85dpBfH7%2Flink-quantum-theory-as-the-most-robust-description-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frok6ZgZ6L85dpBfH7%2Flink-quantum-theory-as-the-most-robust-description-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>The paper: <a href=\"http://www.sciencedirect.com/science/article/pii/S000349161400102X\">http://www.sciencedirect.com/science/article/pii/S000349161400102X</a></p>\n<p>Authors: Hans De Raedt, Mikhail I. Katsnelson, Kristel Michielsen</p>\n<h4>Abstract</h4>\n<p>It is shown that the basic equations of quantum theory can be obtained from a straightforward application of logical inference to experiments for which there is uncertainty about individual events and for which the frequencies of the observed events are robust with respect to small changes in the conditions under which the experiments are carried out.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rok6ZgZ6L85dpBfH7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 0, "extendedScore": null, "score": 1.7135102113104478e-06, "legacy": true, "legacyId": "26190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-08T13:52:35.938Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Recreation", "slug": "meetup-urbana-champaign-recreation", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/funNavdrwn52Taffz/meetup-urbana-champaign-recreation", "pageUrlRelative": "/posts/funNavdrwn52Taffz/meetup-urbana-champaign-recreation", "linkUrl": "https://www.lesswrong.com/posts/funNavdrwn52Taffz/meetup-urbana-champaign-recreation", "postedAtFormatted": "Thursday, May 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Recreation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Recreation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfunNavdrwn52Taffz%2Fmeetup-urbana-champaign-recreation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Recreation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfunNavdrwn52Taffz%2Fmeetup-urbana-champaign-recreation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfunNavdrwn52Taffz%2Fmeetup-urbana-champaign-recreation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/100'>Urbana-Champaign: Recreation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 May 2014 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.1112,-88.2274</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Since the weather is nice, let's meet outside, on the benches south of the engineering quad. I'll bring kites - coincidentally, the engineering quad is a great place to fly kites.</p>\n\n<p>We may talk about mechanism design (<a href=\"http://lesswrong.com/lw/k5r/mechanism_design_constructing_algorithms_for/\">reading</a>).</p>\n\n<p>Bonus topic: what do debates like <a href=\"http://lesswrong.com/r/discussion/lw/k7b/link_sean_carroll_against_afterlife/\">this</a> recent one tell us about what's going on in peoples' heads?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/100'>Urbana-Champaign: Recreation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "funNavdrwn52Taffz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.713719611558684e-06, "legacy": true, "legacyId": "26191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Recreation\">Discussion article for the meetup : <a href=\"/meetups/100\">Urbana-Champaign: Recreation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 May 2014 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.1112,-88.2274</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Since the weather is nice, let's meet outside, on the benches south of the engineering quad. I'll bring kites - coincidentally, the engineering quad is a great place to fly kites.</p>\n\n<p>We may talk about mechanism design (<a href=\"http://lesswrong.com/lw/k5r/mechanism_design_constructing_algorithms_for/\">reading</a>).</p>\n\n<p>Bonus topic: what do debates like <a href=\"http://lesswrong.com/r/discussion/lw/k7b/link_sean_carroll_against_afterlife/\">this</a> recent one tell us about what's going on in peoples' heads?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Recreation1\">Discussion article for the meetup : <a href=\"/meetups/100\">Urbana-Champaign: Recreation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Recreation", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Recreation", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Recreation", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Recreation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xTvdaCwaeZnePMuX5", "3B4j6Xjku3is9JBtb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-08T15:43:40.096Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes May 2014", "slug": "rationality-quotes-may-2014-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Username", "createdAt": "2010-09-04T16:10:50.550Z", "isAdmin": false, "displayName": "Username"}, "userId": "8iY88evtFECPgCs3s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8so6cR2p4Y3DYzsYN/rationality-quotes-may-2014-0", "pageUrlRelative": "/posts/8so6cR2p4Y3DYzsYN/rationality-quotes-may-2014-0", "linkUrl": "https://www.lesswrong.com/posts/8so6cR2p4Y3DYzsYN/rationality-quotes-may-2014-0", "postedAtFormatted": "Thursday, May 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20May%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20May%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8so6cR2p4Y3DYzsYN%2Frationality-quotes-may-2014-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20May%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8so6cR2p4Y3DYzsYN%2Frationality-quotes-may-2014-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8so6cR2p4Y3DYzsYN%2Frationality-quotes-may-2014-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<div style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em; line-height: 15.600000381469727px;\">Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul style=\"padding: 0px; line-height: 15.600000381469727px;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n<li><span style=\"line-height: 19.5px;\">Provide sufficient information (URL, title, date, page number, etc.) to enable a reader to find the place where you read the quote, or its original source if available. Do not quote with only a name.</span></li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8so6cR2p4Y3DYzsYN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -23, "extendedScore": null, "score": -2.2e-05, "legacy": true, "legacyId": "26184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-08T19:38:11.822Z", "modifiedAt": null, "url": null, "title": "Paradigm shifts in forecasting", "slug": "paradigm-shifts-in-forecasting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:29.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wsM7wpEs9jRsQLuEK/paradigm-shifts-in-forecasting", "pageUrlRelative": "/posts/wsM7wpEs9jRsQLuEK/paradigm-shifts-in-forecasting", "linkUrl": "https://www.lesswrong.com/posts/wsM7wpEs9jRsQLuEK/paradigm-shifts-in-forecasting", "postedAtFormatted": "Thursday, May 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paradigm%20shifts%20in%20forecasting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParadigm%20shifts%20in%20forecasting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsM7wpEs9jRsQLuEK%2Fparadigm-shifts-in-forecasting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paradigm%20shifts%20in%20forecasting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsM7wpEs9jRsQLuEK%2Fparadigm-shifts-in-forecasting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsM7wpEs9jRsQLuEK%2Fparadigm-shifts-in-forecasting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2535, "htmlBody": "<p><em>This post has been written in relation with work I'm doing for the <a href=\"http://intellignece.org\">Machine Intelligence Research Institute (MIRI)</a>, but hasn't been formally vetted by MIRI. I'm posting this to LessWrong because of its potential interest to a segment of the LessWrong readership. As always, all thoughts are appreciated.</em></p>\n<p>In this post, I'll try to apply some of the scientific theory of paradigm shifts to the domain of forecasting. In a sense, all of science is about making (conditional) predictions about the behavior of systems. Forecasting simply refers to the act of making predictions about the real-world future rather than about a specific controlled experimental setup. So while the domain of forecasting is far more restricted than the domain of science, we can still apply the conceptual framework of paradigm shifts in science to forecasting.</p>\n<p><strong>Thomas Kuhn and paradigm shifts</strong></p>\n<p>Thomas Kuhn's book <em>The Structure of Scientific Revolutions</em> (<a href=\"http://www.amazon.com/Structure-Scientific-Revolutions-50th-Anniversary-ebook/dp/B007USH7J2/\">Amazon</a>, <a href=\"https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\">Wikipedia</a>) provides a detailed descriptive theory of the nature of paradigm shifts in science. Quoting from the <a href=\"https://en.wikipedia.org/wiki/Paradigm_shift\">Wikipedia page on paradigm shifts</a>:</p>\n<blockquote>\n<p>An <a title=\"Epistemology\" href=\"https://en.wikipedia.org/wiki/Epistemology\">epistemological</a> <strong>paradigm shift</strong> was called a \"scientific revolution\" by epistemologist and <a title=\"History of science\" href=\"https://en.wikipedia.org/wiki/History_of_science\">historian of science</a> Thomas Kuhn in his book <em>The Structure of Scientific Revolutions</em>.</p>\n<p>A scientific revolution occurs, according to Kuhn, when scientists encounter anomalies that cannot be explained by the universally accepted paradigm within which scientific progress has thereto been made. The paradigm, in Kuhn's view, is not simply the current theory, but the entire <a title=\"World view\" href=\"https://en.wikipedia.org/wiki/World_view\">worldview</a> in which it exists, and all of the implications which come with it. This is based on features of landscape of knowledge that scientists can identify around them.</p>\n<p>There are anomalies for all paradigms, Kuhn maintained, that are brushed away as acceptable levels of error, or simply ignored and not dealt with (a principal argument Kuhn uses to reject <a title=\"Karl Popper\" href=\"https://en.wikipedia.org/wiki/Karl_Popper\">Karl Popper</a>'s model of <a title=\"Falsifiability\" href=\"https://en.wikipedia.org/wiki/Falsifiability\">falsifiability</a> as the key force involved in scientific change). Rather, according to Kuhn, anomalies have various levels of significance to the practitioners of science at the time. To put it in the context of early 20th century physics, some scientists found the problems with calculating <a title=\"Tests of general relativity\" href=\"https://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury\">Mercury's perihelion</a> more troubling than the <a class=\"mw-redirect\" title=\"Michelson-Morley experiment\" href=\"https://en.wikipedia.org/wiki/Michelson-Morley_experiment\">Michelson-Morley experiment</a> results, and some the other way around. Kuhn's model of scientific change differs here, and in many places, from that of the <a class=\"mw-redirect\" title=\"Logical positivists\" href=\"https://en.wikipedia.org/wiki/Logical_positivists\">logical positivists</a> in that it puts an enhanced emphasis on the individual humans involved as scientists, rather than abstracting science into a purely logical or philosophical venture.</p>\n<p>When enough significant anomalies have accrued against a current paradigm, the scientific discipline is thrown into a state of <em>crisis,</em> according to Kuhn. During this crisis, new ideas, perhaps ones previously discarded, are tried. Eventually a <em>new</em> paradigm is formed, which gains its own new followers, and an intellectual \"battle\" takes place between the followers of the new paradigm and the hold-outs of the old paradigm. Again, for early 20th century physics, the transition between the <a title=\"James Clerk Maxwell\" href=\"https://en.wikipedia.org/wiki/James_Clerk_Maxwell\">Maxwellian</a> <a title=\"Maxwell's equations\" href=\"https://en.wikipedia.org/wiki/Maxwell%27s_equations\">electromagnetic worldview</a> and the <a title=\"Albert Einstein\" href=\"https://en.wikipedia.org/wiki/Albert_Einstein\">Einsteinian</a> <a title=\"Theory of relativity\" href=\"https://en.wikipedia.org/wiki/Theory_of_relativity\">Relativistic</a> worldview was neither instantaneous nor calm, and instead involved a protracted set of \"attacks,\" both with empirical data as well as rhetorical or philosophical arguments, by both sides, with the Einsteinian theory winning out in the long run. Again, the weighing of evidence and importance of new data was fit through the human sieve: some scientists found the simplicity of Einstein's equations to be most compelling, while some found them more complicated than the notion of Maxwell's aether which they banished. Some found <a title=\"Arthur Eddington\" href=\"https://en.wikipedia.org/wiki/Arthur_Eddington\">Eddington's</a> photographs of light bending around the sun to be compelling, while some questioned their accuracy and meaning. Sometimes the convincing force is just time itself and the human toll it takes, Kuhn said, using a quote from <a title=\"Max Planck\" href=\"https://en.wikipedia.org/wiki/Max_Planck\">Max Planck</a>: \"a new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Paradigm_shift#cite_note-1\"><span>[</span>1<span>]</span></a></sup></p>\n<p>After a given discipline has changed from one paradigm to another, this is called, in Kuhn's terminology, a <em>scientific revolution</em> or a <em>paradigm shift</em>. It is often this final conclusion, the result of the long process, that is meant when the term <em>paradigm shift</em> is used colloquially: simply the (often radical) change of worldview, without reference to the specificities of Kuhn's historical argument.</p>\n</blockquote>\n<p><strong>Simple methods in science are often \"good enough\" until you want a much higher \"resolution\"</strong></p>\n<p>It's worth noting that most paradigm shifts move from simpler, more tractable models to more complicated ones. Initially, the scientific theory is not trying to explain the real world at too fine a resolution, and therefore it is tolerant of large errors. The theories in vogue initially are the simplest among those that can explain the world within the generous margin of error. Over time, as measurement becomes more precise and accurate, and the desire for understanding or engineering at stronger precision levels becomes more important, the focus shifts to finding a model where error rates are lower, accepting a possible increase in the model complexity.</p>\n<p>Consider the following examples:</p>\n<ul>\n<li>Classical mechanics is an inferior paradigm to relativistic mechanics, and the distinction begins to matter once we are operating at high speeds or large length and time scales. But for many practical purposes, classical mechanics is still good enough, and the relativistic correction terms don't improve accuracy much. Historically, the accuracy and precision of measurement, and the required accuracy and precision for technological and engineering purposes, were not high enough to give relativity much of an added advantage in predictive power relative to classical mechanics. This is what started changing in the late 19th century, as the framework for electromagnetism was laid out and the inconsistencies in the Mercury perihelion came to be seen as too huge to be explicable by measurement error. In today's world, classical mechanics suffices for most purposes, but relativity is crucial in some cases: it's important in space travel, satellite launches, and <a href=\"http://www.astronomy.ohio-state.edu/~pogge/Ast162/Unit5/gps.html\">global positioning system (GPS)-based navigation</a>. The GPS is absolutely essential to transportation and communication in today's world. And of course, the whole idea of nuclear energy and nuclear bombs was an offshoot of Einstein's equation <em>E = mc<sup>2</sup></em>, part of the theory of relativity.</li>\n<li>Classical mechanics is an inferior paradigm to quantum mechanics, and the distinction begins to matter once we are operating at sufficiently small length scales. But for many practical purposes, classical mechanics is still good enough. In some cases, particularly for the behavior of atomic particles, we create classical mechanics-like theories that predict phenomena fairly similar to the actual ones predicted by quantum mechanics, but are more analytically tractable. Again, historically, classical mechanics has been good enough at the macro scale that people have dealt with. But once people wanted a stronger foundation for behavior at the atomic and subatomic scale that could be used in understading chemistry properly, the deficiencies of classical mechanics became clear. In today's world, quantum mechanics underlies quantum chemistry, which in turn is crucial for understanding biochemistry and other small-scale phenomena. It also forms the basis of quantum computing, though the commercial feasibility of the quantum computing paradigm is still uncertain.</li>\n<li>For predicting the structure of molecules, VSEPR theory in chemistry is simple and easy to work with, but far less correct than molecular orbital theory. In most simple contexts, VSEPR theory is a great place to start. But molecular orbital theory is what's needed to get completely correct answers. Often, intermediate theories are used to incorporate some aspects from molecular orbital theory without getting the whole package.</li>\n</ul>\n<p><strong>Complexity and paradigms in the context of forecasting</strong></p>\n<p>There are often competing methods for forecasting a given indicator. The methods vary considerably in complexity. For instance, <em>persistence</em> is one of the simplest forecasting methods: persistence of levels means that tomorrow will be the same as today, whereas persistence of trends means that the difference between tomorrow and today equals the difference between today and yesterday. Somewhat more sophisticated than simple persistence is various variations of linear regression that are well-suited to time series and tackle the problems both of periodic fluctuation and noise. More sophisticated methods allow for functional forms obtained by additive or multiplicative combination, or composition, of the functional forms used in simpler methods.</p>\n<p>Here are some measures of complexity for forecasting methods:</p>\n<ul>\n<li>Complexity measures for computer code executing the method, including length and complexity of the code, as well as time requirements and memory requirements for execution,</li>\n<li>Complexity measures for the underlying mathematical or statistical structure. This could be measured as the amount of mathematical or statistical sophistication needed to understand or implement the model, or the amount of sophistication needed to come up with the model or understand what's going on underneath and why the method works. Or the length of the description of the relevant theorems their proofs, and supporting definitions.</li>\n<li>The diversity of trend types that the model can describe. Some models are capable of only capturing a very restricted class of trends, such as linear trends only or exponential trends only. Others can capture any trend of a general functional form. Yet others can capture practically any continuous function given enough data. In the langauge of the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma\">bias-variance dilemma</a>, simple models tend to have higher bias and lower variance and complex models tend to have lower bias and higher variance.</li>\n<li>The minimum amount of data needed for the model to start outperforming other competing models.</li>\n</ul>\n<p><strong>Do complicated methods beat simpler methods?</strong></p>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions\">Makridakis Competitions</a> are often cited as canonical sources of information for how different types of quantitative trend forecasting compare. Makridakis and Hibon draw four conclusions (listed on the linked page and in their papers) of which Finding 1 is most relevant to us: \"<strong>Statistically sophisticated or complex methods do not necessarily provide more accurate forecasts than simpler ones.</strong>\" Some people (such as Nassim Nicholas Taleb) have used this to argue that sophisticated methods are useless.</p>\n<ol> </ol>\n<p>The conclusion drawn by Makridakis and Hibon is supported by the data, but there is less to it than meets the eye. As noted earlier in the post, even the most revolutionary and impressive complicated scientific paradigms (such as relativity and quantum mechanics) only rarely outperform the simpler, more widely known paradigms (such as classical mechanics) except in cases that are designed to draw on the strength of the new paradigm (such as high speed or small length scales). And yet, in the cases where those slight improvements matter, we may be able to improve a lot by using the more sophisticated model. Just as knowledge of relativity makes possible a high-precision GPS that would have been impossible otherwise, new forecasting paradigms may make possible things (such as just-in-time inventory management) that would not have been possible at anywhere near that level of quality otherwise.</p>\n<p>Of course, the selection of the sophisticated method matters: some sophisticated methods are simply wrong-headed and will therefore underperform simpler methods except in tailor-rigged situations. But the key point here is that an <em>appropriately selected sophisticated model with access to adequate data and computational resources can systematically outperform simpler models</em>. Finding (2) for the Makridakis Competitions is \"<strong>The relative ranking of the performance of the various methods varies according to the accuracy measure being used.</strong>\" Finding 4 says \"<strong>The accuracy of the various methods depends on the length of the forecasting horizon involved.</strong>\" The choice of best method also varies across types of time series (so the best method for macroeconomic time series could differ from the best method for time series provided by industries for their production or sales data).</p>\n<p>Duncan Watts makes a similar point in his book <a href=\"http://www.amazon.com/Everything-Obvious-Once-Know-Answer-ebook/dp/B004DEPHGQ/\"><em>Everything is Obvious: One You Know The Answer</em></a> (paraphrased): sophisticated methods don't offer a huge advantage over simpler methods. But the best sophisticated methods <em>are</em> modestly better. And if you're operating at a huge scale (for instance, if you're running an electrical utility that needs to forecast consumer demand, or you're WalMart and you need to manage inventory to minimize waste, or if you're Google or Facebook and need to forecast the amount of traffic in order to budget appropriately for servers), even modest proportional improvements to accuracy can translate to huge absolute reductions in waste and increase in profits.</p>\n<p><strong>The evolution of complicated methods</strong></p>\n<p>Complicated methods can start off as performing a lot worse than simpler methods, and therefore be deemed useless. But then, at some point, they could start overtaking simpler methods, and once they overtake, they could rapidly gain on the simpler methods. What might change in the process? It could be any of these three, or some combination thereof.</p>\n<ol>\n<li><strong>More data becomes available</strong>. This could arise because new measurement setups get deployed, or because existing measurement setups get a longer time series or get refined to a higher resolution. We can argue that with the advent of the Internet, it's much easier to collect a large amount of data, making it possible to use more complicated methods whose relative success depends on having more data available.</li>\n<li><strong>More computational power becomes available</strong>. This could arise due to improvements in computing technology, or the building out of more computers. For instance, weather simulations today can use thousands of times as much computing power as weather simulations 30 years ago. Therefore, they can work with finer divisions of the grid on which forecasting is being done, allowing for more accurate weather simulation.</li>\n<li><strong>The method itself, and/or the code to implement it, improve</strong>. Tweaks and edge case improvements to existing algorithms can improve them enough that they perform better, even holding data and computational power constant. Sometimes, the improvements require investing in customized hardware or backend software, which take some time to develop after the method is first released. In other cases, it's just about people coming up with new incremental improvements over the idea.</li>\n</ol>\n<p><strong>How do we judge the potential and promise of the new complicated forecasting method?<br /></strong></p>\n<p>Given a complicated method that people claim could work given sufficient data or computing power that we don't yet have access to, how are we to judge the plausibility of the claims? The question is similar to the general question of whether a new proclaimed model or theory is the harbinger of a paradigm shift in a scientific discipline. I don't have satisfactory answers. In a subsequent post, I'll look at a few historical and current examples of changes of paradigm shifts in forecasting. The examples that I currently plan to cover are:</p>\n<ul>\n<li>The paradigm shift in weather forecasting from the situation where persistence and climatology were the most effective methods to the situation where numerical weather prediction became the most reliable.</li>\n<li>The ongoing potential paradigm shift in the direction of using neural nets for a wide range of forecasting and prediction problems.</li>\n</ul>\n<p>Any thoughts on the post as a whole would be appreciated, but <em>I'm particularly interested in thoughts on this last topic in the post.</em></p>\n<p><em>Thanks to Luke Muehlhauser for helpful early discussions that led to this post and to Jonah Sinick for his thoughts on an early draft of the post.</em></p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wsM7wpEs9jRsQLuEK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 1.7141905645716957e-06, "legacy": true, "legacyId": "26155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This post has been written in relation with work I'm doing for the <a href=\"http://intellignece.org\">Machine Intelligence Research Institute (MIRI)</a>, but hasn't been formally vetted by MIRI. I'm posting this to LessWrong because of its potential interest to a segment of the LessWrong readership. As always, all thoughts are appreciated.</em></p>\n<p>In this post, I'll try to apply some of the scientific theory of paradigm shifts to the domain of forecasting. In a sense, all of science is about making (conditional) predictions about the behavior of systems. Forecasting simply refers to the act of making predictions about the real-world future rather than about a specific controlled experimental setup. So while the domain of forecasting is far more restricted than the domain of science, we can still apply the conceptual framework of paradigm shifts in science to forecasting.</p>\n<p><strong id=\"Thomas_Kuhn_and_paradigm_shifts\">Thomas Kuhn and paradigm shifts</strong></p>\n<p>Thomas Kuhn's book <em>The Structure of Scientific Revolutions</em> (<a href=\"http://www.amazon.com/Structure-Scientific-Revolutions-50th-Anniversary-ebook/dp/B007USH7J2/\">Amazon</a>, <a href=\"https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\">Wikipedia</a>) provides a detailed descriptive theory of the nature of paradigm shifts in science. Quoting from the <a href=\"https://en.wikipedia.org/wiki/Paradigm_shift\">Wikipedia page on paradigm shifts</a>:</p>\n<blockquote>\n<p>An <a title=\"Epistemology\" href=\"https://en.wikipedia.org/wiki/Epistemology\">epistemological</a> <strong>paradigm shift</strong> was called a \"scientific revolution\" by epistemologist and <a title=\"History of science\" href=\"https://en.wikipedia.org/wiki/History_of_science\">historian of science</a> Thomas Kuhn in his book <em>The Structure of Scientific Revolutions</em>.</p>\n<p>A scientific revolution occurs, according to Kuhn, when scientists encounter anomalies that cannot be explained by the universally accepted paradigm within which scientific progress has thereto been made. The paradigm, in Kuhn's view, is not simply the current theory, but the entire <a title=\"World view\" href=\"https://en.wikipedia.org/wiki/World_view\">worldview</a> in which it exists, and all of the implications which come with it. This is based on features of landscape of knowledge that scientists can identify around them.</p>\n<p>There are anomalies for all paradigms, Kuhn maintained, that are brushed away as acceptable levels of error, or simply ignored and not dealt with (a principal argument Kuhn uses to reject <a title=\"Karl Popper\" href=\"https://en.wikipedia.org/wiki/Karl_Popper\">Karl Popper</a>'s model of <a title=\"Falsifiability\" href=\"https://en.wikipedia.org/wiki/Falsifiability\">falsifiability</a> as the key force involved in scientific change). Rather, according to Kuhn, anomalies have various levels of significance to the practitioners of science at the time. To put it in the context of early 20th century physics, some scientists found the problems with calculating <a title=\"Tests of general relativity\" href=\"https://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury\">Mercury's perihelion</a> more troubling than the <a class=\"mw-redirect\" title=\"Michelson-Morley experiment\" href=\"https://en.wikipedia.org/wiki/Michelson-Morley_experiment\">Michelson-Morley experiment</a> results, and some the other way around. Kuhn's model of scientific change differs here, and in many places, from that of the <a class=\"mw-redirect\" title=\"Logical positivists\" href=\"https://en.wikipedia.org/wiki/Logical_positivists\">logical positivists</a> in that it puts an enhanced emphasis on the individual humans involved as scientists, rather than abstracting science into a purely logical or philosophical venture.</p>\n<p>When enough significant anomalies have accrued against a current paradigm, the scientific discipline is thrown into a state of <em>crisis,</em> according to Kuhn. During this crisis, new ideas, perhaps ones previously discarded, are tried. Eventually a <em>new</em> paradigm is formed, which gains its own new followers, and an intellectual \"battle\" takes place between the followers of the new paradigm and the hold-outs of the old paradigm. Again, for early 20th century physics, the transition between the <a title=\"James Clerk Maxwell\" href=\"https://en.wikipedia.org/wiki/James_Clerk_Maxwell\">Maxwellian</a> <a title=\"Maxwell's equations\" href=\"https://en.wikipedia.org/wiki/Maxwell%27s_equations\">electromagnetic worldview</a> and the <a title=\"Albert Einstein\" href=\"https://en.wikipedia.org/wiki/Albert_Einstein\">Einsteinian</a> <a title=\"Theory of relativity\" href=\"https://en.wikipedia.org/wiki/Theory_of_relativity\">Relativistic</a> worldview was neither instantaneous nor calm, and instead involved a protracted set of \"attacks,\" both with empirical data as well as rhetorical or philosophical arguments, by both sides, with the Einsteinian theory winning out in the long run. Again, the weighing of evidence and importance of new data was fit through the human sieve: some scientists found the simplicity of Einstein's equations to be most compelling, while some found them more complicated than the notion of Maxwell's aether which they banished. Some found <a title=\"Arthur Eddington\" href=\"https://en.wikipedia.org/wiki/Arthur_Eddington\">Eddington's</a> photographs of light bending around the sun to be compelling, while some questioned their accuracy and meaning. Sometimes the convincing force is just time itself and the human toll it takes, Kuhn said, using a quote from <a title=\"Max Planck\" href=\"https://en.wikipedia.org/wiki/Max_Planck\">Max Planck</a>: \"a new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"https://en.wikipedia.org/wiki/Paradigm_shift#cite_note-1\"><span>[</span>1<span>]</span></a></sup></p>\n<p>After a given discipline has changed from one paradigm to another, this is called, in Kuhn's terminology, a <em>scientific revolution</em> or a <em>paradigm shift</em>. It is often this final conclusion, the result of the long process, that is meant when the term <em>paradigm shift</em> is used colloquially: simply the (often radical) change of worldview, without reference to the specificities of Kuhn's historical argument.</p>\n</blockquote>\n<p><strong id=\"Simple_methods_in_science_are_often__good_enough__until_you_want_a_much_higher__resolution_\">Simple methods in science are often \"good enough\" until you want a much higher \"resolution\"</strong></p>\n<p>It's worth noting that most paradigm shifts move from simpler, more tractable models to more complicated ones. Initially, the scientific theory is not trying to explain the real world at too fine a resolution, and therefore it is tolerant of large errors. The theories in vogue initially are the simplest among those that can explain the world within the generous margin of error. Over time, as measurement becomes more precise and accurate, and the desire for understanding or engineering at stronger precision levels becomes more important, the focus shifts to finding a model where error rates are lower, accepting a possible increase in the model complexity.</p>\n<p>Consider the following examples:</p>\n<ul>\n<li>Classical mechanics is an inferior paradigm to relativistic mechanics, and the distinction begins to matter once we are operating at high speeds or large length and time scales. But for many practical purposes, classical mechanics is still good enough, and the relativistic correction terms don't improve accuracy much. Historically, the accuracy and precision of measurement, and the required accuracy and precision for technological and engineering purposes, were not high enough to give relativity much of an added advantage in predictive power relative to classical mechanics. This is what started changing in the late 19th century, as the framework for electromagnetism was laid out and the inconsistencies in the Mercury perihelion came to be seen as too huge to be explicable by measurement error. In today's world, classical mechanics suffices for most purposes, but relativity is crucial in some cases: it's important in space travel, satellite launches, and <a href=\"http://www.astronomy.ohio-state.edu/~pogge/Ast162/Unit5/gps.html\">global positioning system (GPS)-based navigation</a>. The GPS is absolutely essential to transportation and communication in today's world. And of course, the whole idea of nuclear energy and nuclear bombs was an offshoot of Einstein's equation <em>E = mc<sup>2</sup></em>, part of the theory of relativity.</li>\n<li>Classical mechanics is an inferior paradigm to quantum mechanics, and the distinction begins to matter once we are operating at sufficiently small length scales. But for many practical purposes, classical mechanics is still good enough. In some cases, particularly for the behavior of atomic particles, we create classical mechanics-like theories that predict phenomena fairly similar to the actual ones predicted by quantum mechanics, but are more analytically tractable. Again, historically, classical mechanics has been good enough at the macro scale that people have dealt with. But once people wanted a stronger foundation for behavior at the atomic and subatomic scale that could be used in understading chemistry properly, the deficiencies of classical mechanics became clear. In today's world, quantum mechanics underlies quantum chemistry, which in turn is crucial for understanding biochemistry and other small-scale phenomena. It also forms the basis of quantum computing, though the commercial feasibility of the quantum computing paradigm is still uncertain.</li>\n<li>For predicting the structure of molecules, VSEPR theory in chemistry is simple and easy to work with, but far less correct than molecular orbital theory. In most simple contexts, VSEPR theory is a great place to start. But molecular orbital theory is what's needed to get completely correct answers. Often, intermediate theories are used to incorporate some aspects from molecular orbital theory without getting the whole package.</li>\n</ul>\n<p><strong id=\"Complexity_and_paradigms_in_the_context_of_forecasting\">Complexity and paradigms in the context of forecasting</strong></p>\n<p>There are often competing methods for forecasting a given indicator. The methods vary considerably in complexity. For instance, <em>persistence</em> is one of the simplest forecasting methods: persistence of levels means that tomorrow will be the same as today, whereas persistence of trends means that the difference between tomorrow and today equals the difference between today and yesterday. Somewhat more sophisticated than simple persistence is various variations of linear regression that are well-suited to time series and tackle the problems both of periodic fluctuation and noise. More sophisticated methods allow for functional forms obtained by additive or multiplicative combination, or composition, of the functional forms used in simpler methods.</p>\n<p>Here are some measures of complexity for forecasting methods:</p>\n<ul>\n<li>Complexity measures for computer code executing the method, including length and complexity of the code, as well as time requirements and memory requirements for execution,</li>\n<li>Complexity measures for the underlying mathematical or statistical structure. This could be measured as the amount of mathematical or statistical sophistication needed to understand or implement the model, or the amount of sophistication needed to come up with the model or understand what's going on underneath and why the method works. Or the length of the description of the relevant theorems their proofs, and supporting definitions.</li>\n<li>The diversity of trend types that the model can describe. Some models are capable of only capturing a very restricted class of trends, such as linear trends only or exponential trends only. Others can capture any trend of a general functional form. Yet others can capture practically any continuous function given enough data. In the langauge of the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma\">bias-variance dilemma</a>, simple models tend to have higher bias and lower variance and complex models tend to have lower bias and higher variance.</li>\n<li>The minimum amount of data needed for the model to start outperforming other competing models.</li>\n</ul>\n<p><strong id=\"Do_complicated_methods_beat_simpler_methods_\">Do complicated methods beat simpler methods?</strong></p>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Makridakis_Competitions\">Makridakis Competitions</a> are often cited as canonical sources of information for how different types of quantitative trend forecasting compare. Makridakis and Hibon draw four conclusions (listed on the linked page and in their papers) of which Finding 1 is most relevant to us: \"<strong>Statistically sophisticated or complex methods do not necessarily provide more accurate forecasts than simpler ones.</strong>\" Some people (such as Nassim Nicholas Taleb) have used this to argue that sophisticated methods are useless.</p>\n<ol> </ol>\n<p>The conclusion drawn by Makridakis and Hibon is supported by the data, but there is less to it than meets the eye. As noted earlier in the post, even the most revolutionary and impressive complicated scientific paradigms (such as relativity and quantum mechanics) only rarely outperform the simpler, more widely known paradigms (such as classical mechanics) except in cases that are designed to draw on the strength of the new paradigm (such as high speed or small length scales). And yet, in the cases where those slight improvements matter, we may be able to improve a lot by using the more sophisticated model. Just as knowledge of relativity makes possible a high-precision GPS that would have been impossible otherwise, new forecasting paradigms may make possible things (such as just-in-time inventory management) that would not have been possible at anywhere near that level of quality otherwise.</p>\n<p>Of course, the selection of the sophisticated method matters: some sophisticated methods are simply wrong-headed and will therefore underperform simpler methods except in tailor-rigged situations. But the key point here is that an <em>appropriately selected sophisticated model with access to adequate data and computational resources can systematically outperform simpler models</em>. Finding (2) for the Makridakis Competitions is \"<strong>The relative ranking of the performance of the various methods varies according to the accuracy measure being used.</strong>\" Finding 4 says \"<strong>The accuracy of the various methods depends on the length of the forecasting horizon involved.</strong>\" The choice of best method also varies across types of time series (so the best method for macroeconomic time series could differ from the best method for time series provided by industries for their production or sales data).</p>\n<p>Duncan Watts makes a similar point in his book <a href=\"http://www.amazon.com/Everything-Obvious-Once-Know-Answer-ebook/dp/B004DEPHGQ/\"><em>Everything is Obvious: One You Know The Answer</em></a> (paraphrased): sophisticated methods don't offer a huge advantage over simpler methods. But the best sophisticated methods <em>are</em> modestly better. And if you're operating at a huge scale (for instance, if you're running an electrical utility that needs to forecast consumer demand, or you're WalMart and you need to manage inventory to minimize waste, or if you're Google or Facebook and need to forecast the amount of traffic in order to budget appropriately for servers), even modest proportional improvements to accuracy can translate to huge absolute reductions in waste and increase in profits.</p>\n<p><strong id=\"The_evolution_of_complicated_methods\">The evolution of complicated methods</strong></p>\n<p>Complicated methods can start off as performing a lot worse than simpler methods, and therefore be deemed useless. But then, at some point, they could start overtaking simpler methods, and once they overtake, they could rapidly gain on the simpler methods. What might change in the process? It could be any of these three, or some combination thereof.</p>\n<ol>\n<li><strong>More data becomes available</strong>. This could arise because new measurement setups get deployed, or because existing measurement setups get a longer time series or get refined to a higher resolution. We can argue that with the advent of the Internet, it's much easier to collect a large amount of data, making it possible to use more complicated methods whose relative success depends on having more data available.</li>\n<li><strong>More computational power becomes available</strong>. This could arise due to improvements in computing technology, or the building out of more computers. For instance, weather simulations today can use thousands of times as much computing power as weather simulations 30 years ago. Therefore, they can work with finer divisions of the grid on which forecasting is being done, allowing for more accurate weather simulation.</li>\n<li><strong>The method itself, and/or the code to implement it, improve</strong>. Tweaks and edge case improvements to existing algorithms can improve them enough that they perform better, even holding data and computational power constant. Sometimes, the improvements require investing in customized hardware or backend software, which take some time to develop after the method is first released. In other cases, it's just about people coming up with new incremental improvements over the idea.</li>\n</ol>\n<p><strong id=\"How_do_we_judge_the_potential_and_promise_of_the_new_complicated_forecasting_method_\">How do we judge the potential and promise of the new complicated forecasting method?<br></strong></p>\n<p>Given a complicated method that people claim could work given sufficient data or computing power that we don't yet have access to, how are we to judge the plausibility of the claims? The question is similar to the general question of whether a new proclaimed model or theory is the harbinger of a paradigm shift in a scientific discipline. I don't have satisfactory answers. In a subsequent post, I'll look at a few historical and current examples of changes of paradigm shifts in forecasting. The examples that I currently plan to cover are:</p>\n<ul>\n<li>The paradigm shift in weather forecasting from the situation where persistence and climatology were the most effective methods to the situation where numerical weather prediction became the most reliable.</li>\n<li>The ongoing potential paradigm shift in the direction of using neural nets for a wide range of forecasting and prediction problems.</li>\n</ul>\n<p>Any thoughts on the post as a whole would be appreciated, but <em>I'm particularly interested in thoughts on this last topic in the post.</em></p>\n<p><em>Thanks to Luke Muehlhauser for helpful early discussions that led to this post and to Jonah Sinick for his thoughts on an early draft of the post.</em></p>\n<ul>\n</ul>", "sections": [{"title": "Thomas Kuhn and paradigm shifts", "anchor": "Thomas_Kuhn_and_paradigm_shifts", "level": 1}, {"title": "Simple methods in science are often \"good enough\" until you want a much higher \"resolution\"", "anchor": "Simple_methods_in_science_are_often__good_enough__until_you_want_a_much_higher__resolution_", "level": 1}, {"title": "Complexity and paradigms in the context of forecasting", "anchor": "Complexity_and_paradigms_in_the_context_of_forecasting", "level": 1}, {"title": "Do complicated methods beat simpler methods?", "anchor": "Do_complicated_methods_beat_simpler_methods_", "level": 1}, {"title": "The evolution of complicated methods", "anchor": "The_evolution_of_complicated_methods", "level": 1}, {"title": "How do we judge the potential and promise of the new complicated forecasting method?", "anchor": "How_do_we_judge_the_potential_and_promise_of_the_new_complicated_forecasting_method_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-09T13:37:19.129Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup - possibly in a park", "slug": "meetup-london-social-meetup-possibly-in-a-park", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/stNjNZYHhE3nQCi3G/meetup-london-social-meetup-possibly-in-a-park", "pageUrlRelative": "/posts/stNjNZYHhE3nQCi3G/meetup-london-social-meetup-possibly-in-a-park", "linkUrl": "https://www.lesswrong.com/posts/stNjNZYHhE3nQCi3G/meetup-london-social-meetup-possibly-in-a-park", "postedAtFormatted": "Friday, May 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstNjNZYHhE3nQCi3G%2Fmeetup-london-social-meetup-possibly-in-a-park%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstNjNZYHhE3nQCi3G%2Fmeetup-london-social-meetup-possibly-in-a-park", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FstNjNZYHhE3nQCi3G%2Fmeetup-london-social-meetup-possibly-in-a-park", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/101'>London social meetup - possibly in a park</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>If the weather is nice, we'll head to <a href=\"https://goo.gl/maps/Tdy1r\" rel=\"nofollow\">Lincoln&#39;s Inn Fields</a>, probably somewhere in the northwest quadrant. If not, we'll be in the usual Shakespeare's Head. If the weather is variable, we might move from one to the other - give me a call or text if you're not sure. My number is 07792009646.</p>\n\n<p><strong>Edit:</strong> It seems a little cold and windy right now. Let's start out in the pub.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/101'>London social meetup - possibly in a park</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "stNjNZYHhE3nQCi3G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7156625793100315e-06, "legacy": true, "legacyId": "26198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park\">Discussion article for the meetup : <a href=\"/meetups/101\">London social meetup - possibly in a park</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>If the weather is nice, we'll head to <a href=\"https://goo.gl/maps/Tdy1r\" rel=\"nofollow\">Lincoln's Inn Fields</a>, probably somewhere in the northwest quadrant. If not, we'll be in the usual Shakespeare's Head. If the weather is variable, we might move from one to the other - give me a call or text if you're not sure. My number is 07792009646.</p>\n\n<p><strong>Edit:</strong> It seems a little cold and windy right now. Let's start out in the pub.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park1\">Discussion article for the meetup : <a href=\"/meetups/101\">London social meetup - possibly in a park</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup - possibly in a park", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park", "level": 1}, {"title": "Discussion article for the meetup : London social meetup - possibly in a park", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-09T16:07:29.845Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Houston", "slug": "new-lw-meetup-houston", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d78neHtwpKSbHbc6J/new-lw-meetup-houston", "pageUrlRelative": "/posts/d78neHtwpKSbHbc6J/new-lw-meetup-houston", "linkUrl": "https://www.lesswrong.com/posts/d78neHtwpKSbHbc6J/new-lw-meetup-houston", "postedAtFormatted": "Friday, May 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Houston&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Houston%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd78neHtwpKSbHbc6J%2Fnew-lw-meetup-houston%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Houston%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd78neHtwpKSbHbc6J%2Fnew-lw-meetup-houston", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd78neHtwpKSbHbc6J%2Fnew-lw-meetup-houston", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 508, "htmlBody": "<p><strong>This summary was posted to LW main on May 2nd. The following week's summary is <a href=\"/lw/k7s/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">03 May 2014 02:00PM</span></a></li>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/z8\">Australia Mega-Meetup:&nbsp;<span class=\"date\">09 May 2014 05:00PM</span></a></li>\n<li><a href=\"/meetups/zm\">(Buffalo NY) Sunday Meetup:&nbsp;<span class=\"date\">04 May 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/zp\">Helsinki Meetup - Effective Altruism:&nbsp;<span class=\"date\">10 May 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/zh\">Munich Meetup:&nbsp;<span class=\"date\">11 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zg\">Utrecht:&nbsp;<span class=\"date\">03 May 2014 05:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/zv\">Canberra: Rationalist Fun and Games!:&nbsp;<span class=\"date\">24 May 2014 06:00PM</span></a> </li>\n<li><a href=\"/meetups/zt\">London social meetup:&nbsp;<span class=\"date\">04 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zu\">Washington DC Games meetup:&nbsp;<span class=\"date\">06 May 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d78neHtwpKSbHbc6J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7158676131025688e-06, "legacy": true, "legacyId": "26143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RmtGzCFk37gy3LtHd", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-09T17:32:41.137Z", "modifiedAt": null, "url": null, "title": "Tiling agents with transfinite parametric polymorphism", "slug": "tiling-agents-with-transfinite-parametric-polymorphism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:34.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x4eMMWgPWAqqxHyr8/tiling-agents-with-transfinite-parametric-polymorphism", "pageUrlRelative": "/posts/x4eMMWgPWAqqxHyr8/tiling-agents-with-transfinite-parametric-polymorphism", "linkUrl": "https://www.lesswrong.com/posts/x4eMMWgPWAqqxHyr8/tiling-agents-with-transfinite-parametric-polymorphism", "postedAtFormatted": "Friday, May 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tiling%20agents%20with%20transfinite%20parametric%20polymorphism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATiling%20agents%20with%20transfinite%20parametric%20polymorphism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4eMMWgPWAqqxHyr8%2Ftiling-agents-with-transfinite-parametric-polymorphism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tiling%20agents%20with%20transfinite%20parametric%20polymorphism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4eMMWgPWAqqxHyr8%2Ftiling-agents-with-transfinite-parametric-polymorphism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4eMMWgPWAqqxHyr8%2Ftiling-agents-with-transfinite-parametric-polymorphism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 618, "htmlBody": "<p><em>The formalism presented in this post turned out to be erroneous (as opposed to the formalism in the previous post). The problem is that the step in the proof of the main proposition in which the soundness schema is applied cannot be generalized to the ordinal setting since we don't know whether&nbsp;&alpha;<sub>&kappa;</sub>&nbsp;is a successor ordinal so we can't replace it by&nbsp;</em><em>&alpha;<sub>&kappa;'</sub>=</em><em>&alpha;<sub>&kappa;</sub>-1. I'm not deleting this post primarily to preserve the useful discussion in the comments.</em></p>\n<p><em><br /></em></p>\n<p>Followup to: <a href=\"/lw/k4g/parametric_polymorphism_in_updateless/\">Parametric polymorphism in updateless intelligence metric</a></p>\n<p>In the previous post, I formulated a variant of Benja's parametric polymorphism suitable for constructing updateless intelligence metrics. More generally, this variants admits agents which are utility maximizers (in the informal sense of trying their best to maximize a utility function, not in the formal sense of finding the absolutely optimal solution; for example they might be \"meliorizers\" to use the terminology of <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky and Herreshoff</a>) rather than satisficers. The agents using this formalism labor under a certain \"delusion\", namely, since they believe that&nbsp;&kappa; (\"the number of ineffable mystery\") is an actual finite number (whereas it is secretly infinite, in a sense), they think that the chain of tiling agents is effectively finite as well (since at some point agent #n in the chain will discover that &kappa; &gt; n is false and will fail to construct a subsequent agent of the same \"epistemic strength\"). The same problem exists in <a href=\"http://arxiv.org/abs/1312.3626\">Weaver's</a> intuitionistic assertability predicate formalism.</p>\n<p>To overcome this limitation, I suggest to extend&nbsp;&kappa;'s semantics from natural numbers to elements of a certain recursive ordinal (at least morally; technically it is done a bit differently, see below).&nbsp;In Benja's original formulation this doesn't appear a valid option, since &kappa; is interpreted as a time interval. However, in my variant &kappa; is just an abstract parameter queries about which can be directed to a special \"oracle\", so there is no such limitation.</p>\n<h1>Formalism</h1>\n<p>Fix&nbsp;&alpha; a recursive ordinal and&nbsp;&alpha;<sub>i</sub>&nbsp;a system of notations for ordinals smaller than&nbsp;&alpha; such that &alpha;<sub>0</sub>=0, the function f defined by&nbsp;&alpha;<sub>f(i)</sub>=&alpha;<sub>i</sub>+1 is recursive and the bit-valued function g(i,j) which equals 1 iff &alpha;<sub>i</sub>&nbsp;&lt; &alpha;<sub>j</sub>&nbsp;is also recursive. We also assume that</p>\n<p>(0) T |- For all i: g(i,f(i))=1</p>\n<p>where T is our starting theory. We define T<sub>&kappa;</sub>(&alpha;) to be T extended by the symbol &kappa; (which stands for the index of the ordinal, not the ordinal itself) and the following self-referential soundness schema:</p>\n<p>(1) For all formulae &phi;: For all n, i: ((@_T<sub>&kappa;</sub>(&alpha;)&nbsp;\"&phi;(n)\" and g(i,&kappa;)=1) -&gt; &phi;(n)[&kappa;\\i])</p>\n<p>T<sub>&kappa;</sub>(&alpha;) is proven to be sound analogously to&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky and Herreshoff</a>, with transfinite induction over ordinals below &alpha; replacing the ordinary induction.</p>\n<p>The licensing condition for agent A<sub>0</sub>&nbsp;to perform action&nbsp;<strong>b<sub>0</sub></strong>&nbsp;is</p>\n<p>(2) Exists n: T<sub>&kappa;</sub>(&alpha;) |- (g(n,&kappa;)=1 -&gt; (A<sub>0</sub>()=<strong>b<sub>0&nbsp;</sub></strong>-&gt; G))</p>\n<p>The property which is sufficient for agent B<sub>1</sub>&nbsp;to be creatable by agent A<sub>0</sub>&nbsp;is</p>\n<p>(3)&nbsp;T<sub>&kappa;</sub>(&alpha;)&nbsp;|- For all&nbsp;<strong>b<sub>1</sub></strong>: (B<sub>1</sub>()=<strong>b<sub>1</sub></strong>&nbsp;-&gt; (<strong>b<sub>1</sub></strong>=<strong>0</strong>&nbsp;or&nbsp;Exists n: (g(n,&kappa;)=1&nbsp;and @_T<sub>&kappa;</sub>(&alpha;)&nbsp;\"(g(n,f(&kappa;))=1 and B<sub>1</sub>()[&kappa;\\f(&kappa;)]=<strong>b<sub>1</sub></strong>) -&gt; G[&kappa;\\f(&kappa;)]\")))</p>\n<p>The proof that (3) indeed enables tiling is analogous to the previous post (but requires using (0)).</p>\n<h1>Discussion</h1>\n<p>In order for A<sub>0</sub>&nbsp;to know that the tiling chain is infinite, it is enough for her to verify &alpha;<sub>&kappa;</sub>&nbsp;&gt;=&nbsp;&omega;. In order for her to know that A<sub>1</sub>&nbsp;is going to know the&nbsp;tiling chain is infinite, she needs to&nbsp;verify &alpha;<sub>&kappa;</sub>&nbsp;&gt;=&nbsp;&omega;+1. In order for her to know that <em>all</em>&nbsp;agents are going to know the tiling chain is infinite, she needs to verify &alpha;<sub>&kappa;</sub>&nbsp;&gt;= 2&omega;. In order for her to know that all agents are going to know <em>that</em>, she needs to verify&nbsp;&alpha;<sub>&kappa;</sub>&nbsp;&gt;= 3&omega; et cetera.</p>\n<p>It remains to decide which ordinal should we actually use. My intuition is that the correct ordinal is the least&nbsp;&alpha; with the property that &alpha; is the proof-theoretic ordinal of T<sub>&kappa;</sub>(&alpha;) extended by the axiom schema {g(i,&kappa;)=1}. This seems right since the agent shouldn't get much from &alpha;<sub>&kappa;</sub>&nbsp;&gt;&nbsp;&beta; for &beta; above the proof theoretic ordinal. However, a more formal justification is probably in order.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x4eMMWgPWAqqxHyr8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.7159839369386566e-06, "legacy": true, "legacyId": "26199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>The formalism presented in this post turned out to be erroneous (as opposed to the formalism in the previous post). The problem is that the step in the proof of the main proposition in which the soundness schema is applied cannot be generalized to the ordinal setting since we don't know whether&nbsp;\u03b1<sub>\u03ba</sub>&nbsp;is a successor ordinal so we can't replace it by&nbsp;</em><em>\u03b1<sub>\u03ba'</sub>=</em><em>\u03b1<sub>\u03ba</sub>-1. I'm not deleting this post primarily to preserve the useful discussion in the comments.</em></p>\n<p><em><br></em></p>\n<p>Followup to: <a href=\"/lw/k4g/parametric_polymorphism_in_updateless/\">Parametric polymorphism in updateless intelligence metric</a></p>\n<p>In the previous post, I formulated a variant of Benja's parametric polymorphism suitable for constructing updateless intelligence metrics. More generally, this variants admits agents which are utility maximizers (in the informal sense of trying their best to maximize a utility function, not in the formal sense of finding the absolutely optimal solution; for example they might be \"meliorizers\" to use the terminology of <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky and Herreshoff</a>) rather than satisficers. The agents using this formalism labor under a certain \"delusion\", namely, since they believe that&nbsp;\u03ba (\"the number of ineffable mystery\") is an actual finite number (whereas it is secretly infinite, in a sense), they think that the chain of tiling agents is effectively finite as well (since at some point agent #n in the chain will discover that \u03ba &gt; n is false and will fail to construct a subsequent agent of the same \"epistemic strength\"). The same problem exists in <a href=\"http://arxiv.org/abs/1312.3626\">Weaver's</a> intuitionistic assertability predicate formalism.</p>\n<p>To overcome this limitation, I suggest to extend&nbsp;\u03ba's semantics from natural numbers to elements of a certain recursive ordinal (at least morally; technically it is done a bit differently, see below).&nbsp;In Benja's original formulation this doesn't appear a valid option, since \u03ba is interpreted as a time interval. However, in my variant \u03ba is just an abstract parameter queries about which can be directed to a special \"oracle\", so there is no such limitation.</p>\n<h1 id=\"Formalism\">Formalism</h1>\n<p>Fix&nbsp;\u03b1 a recursive ordinal and&nbsp;\u03b1<sub>i</sub>&nbsp;a system of notations for ordinals smaller than&nbsp;\u03b1 such that \u03b1<sub>0</sub>=0, the function f defined by&nbsp;\u03b1<sub>f(i)</sub>=\u03b1<sub>i</sub>+1 is recursive and the bit-valued function g(i,j) which equals 1 iff \u03b1<sub>i</sub>&nbsp;&lt; \u03b1<sub>j</sub>&nbsp;is also recursive. We also assume that</p>\n<p>(0) T |- For all i: g(i,f(i))=1</p>\n<p>where T is our starting theory. We define T<sub>\u03ba</sub>(\u03b1) to be T extended by the symbol \u03ba (which stands for the index of the ordinal, not the ordinal itself) and the following self-referential soundness schema:</p>\n<p>(1) For all formulae \u03c6: For all n, i: ((@_T<sub>\u03ba</sub>(\u03b1)&nbsp;\"\u03c6(n)\" and g(i,\u03ba)=1) -&gt; \u03c6(n)[\u03ba\\i])</p>\n<p>T<sub>\u03ba</sub>(\u03b1) is proven to be sound analogously to&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky and Herreshoff</a>, with transfinite induction over ordinals below \u03b1 replacing the ordinary induction.</p>\n<p>The licensing condition for agent A<sub>0</sub>&nbsp;to perform action&nbsp;<strong>b<sub>0</sub></strong>&nbsp;is</p>\n<p>(2) Exists n: T<sub>\u03ba</sub>(\u03b1) |- (g(n,\u03ba)=1 -&gt; (A<sub>0</sub>()=<strong>b<sub>0&nbsp;</sub></strong>-&gt; G))</p>\n<p>The property which is sufficient for agent B<sub>1</sub>&nbsp;to be creatable by agent A<sub>0</sub>&nbsp;is</p>\n<p>(3)&nbsp;T<sub>\u03ba</sub>(\u03b1)&nbsp;|- For all&nbsp;<strong>b<sub>1</sub></strong>: (B<sub>1</sub>()=<strong>b<sub>1</sub></strong>&nbsp;-&gt; (<strong>b<sub>1</sub></strong>=<strong>0</strong>&nbsp;or&nbsp;Exists n: (g(n,\u03ba)=1&nbsp;and @_T<sub>\u03ba</sub>(\u03b1)&nbsp;\"(g(n,f(\u03ba))=1 and B<sub>1</sub>()[\u03ba\\f(\u03ba)]=<strong>b<sub>1</sub></strong>) -&gt; G[\u03ba\\f(\u03ba)]\")))</p>\n<p>The proof that (3) indeed enables tiling is analogous to the previous post (but requires using (0)).</p>\n<h1 id=\"Discussion\">Discussion</h1>\n<p>In order for A<sub>0</sub>&nbsp;to know that the tiling chain is infinite, it is enough for her to verify \u03b1<sub>\u03ba</sub>&nbsp;&gt;=&nbsp;\u03c9. In order for her to know that A<sub>1</sub>&nbsp;is going to know the&nbsp;tiling chain is infinite, she needs to&nbsp;verify \u03b1<sub>\u03ba</sub>&nbsp;&gt;=&nbsp;\u03c9+1. In order for her to know that <em>all</em>&nbsp;agents are going to know the tiling chain is infinite, she needs to verify \u03b1<sub>\u03ba</sub>&nbsp;&gt;= 2\u03c9. In order for her to know that all agents are going to know <em>that</em>, she needs to verify&nbsp;\u03b1<sub>\u03ba</sub>&nbsp;&gt;= 3\u03c9 et cetera.</p>\n<p>It remains to decide which ordinal should we actually use. My intuition is that the correct ordinal is the least&nbsp;\u03b1 with the property that \u03b1 is the proof-theoretic ordinal of T<sub>\u03ba</sub>(\u03b1) extended by the axiom schema {g(i,\u03ba)=1}. This seems right since the agent shouldn't get much from \u03b1<sub>\u03ba</sub>&nbsp;&gt;&nbsp;\u03b2 for \u03b2 above the proof theoretic ordinal. However, a more formal justification is probably in order.</p>", "sections": [{"title": "Formalism", "anchor": "Formalism", "level": 1}, {"title": "Discussion", "anchor": "Discussion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wpAbd75s5pcbyTz4Y"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-09T18:18:23.666Z", "modifiedAt": null, "url": null, "title": "Three Parables of Microeconomics", "slug": "three-parables-of-microeconomics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.813Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5XDuE9BEiRZcbKZhW/three-parables-of-microeconomics", "pageUrlRelative": "/posts/5XDuE9BEiRZcbKZhW/three-parables-of-microeconomics", "linkUrl": "https://www.lesswrong.com/posts/5XDuE9BEiRZcbKZhW/three-parables-of-microeconomics", "postedAtFormatted": "Friday, May 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20Parables%20of%20Microeconomics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20Parables%20of%20Microeconomics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5XDuE9BEiRZcbKZhW%2Fthree-parables-of-microeconomics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20Parables%20of%20Microeconomics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5XDuE9BEiRZcbKZhW%2Fthree-parables-of-microeconomics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5XDuE9BEiRZcbKZhW%2Fthree-parables-of-microeconomics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 945, "htmlBody": "<p><em>(Epistemic status: Satire.)</em></p>\n<p>&nbsp;</p>\n<p><strong>First Parable: Equilibrium Pricing</strong></p>\n<p>Highway Offramp 72 leads to the isolated town of Townton. Visitors are greeted by two fuel stations, Carbonaceous Fossils (CF) and Hydrogenated Chains (HC), on opposite sides of the main road. There are no other gas stations for many miles. Together, these two stations sell 1000 gallons per day. Since their products are indistinguishable, and they have prominently posted prices, every driver will choose the cheaper one; or if the prices are the same, they will split half and half. &nbsp;Both pay $1.50/gal for their stock and charge $2/gal to drivers, so half the drivers stop at each.</p>\n<p>The owner of CF reasons as follows: If I keep my current price of $2, I will make 500*(2-1.5)=$250 of profit. But if I lower my price to $1.99, I will get twice as much business and make 1000*(1.99-1.5)=$490 of profit. The next morning, he updates his price.</p>\n<p>Across the street, the owner of HC (who is having a bad day, due to the complete lack of customers), reasons the same way. The next morning HC has updated its price to $1.98; the morning after that CF lowers its price to $1.97; and so on.</p>\n<p>Because CF and HC's owners are law-abiding model citizens, they never talk to each other about prices. That would be collusion, which is illegal. Later that month, with CF's price down to $1.52 and HC's price at $1.51, the local community center holds Game Theory night, where both owners attend a local economist's presentation on the Iterated Prisoner's Dilemma.</p>\n<p>The next morning, both stations charge $1.52. The morning after that, $1.53. The morning after that, $1.54, and so on. Later that year, CF reasons as follows: If I keep my current price of $20...</p>\n<p><em>(Moral: Gas station attendants should study game theory.)</em></p>\n<p>&nbsp;</p>\n<p><strong>Second Parable: Comparative Advantage</strong></p>\n<p>Two farmers, Alex and Bertha, grow potatoes and carrots. In one year, Alex can either grow 4 barrels of potatoes or 10 barrels of carrots, or some linear combination of the two, such as 2 barrels of potatoes and 5 barrels of carrots. Bertha is better at farming, and can produce 15 barrels of potatoes or 20 barrels of carrots, or some combination of the two. Doctors agree that everyone should eat exactly equal numbers of potatoes and carrots - an excess of one over the other would be unacceptable. So in the first year, having just settled a new frontier and not having met their neighbors, Alex plants 2.9 barrels' worth of each, and Bertha plants 8.6 barrels of each.</p>\n<p><a id=\"more\"></a></p>\n<p>During the next year's spring festival, Alex and Bertha meet, and Alex suggests arranging a trade: he will focus on carrots, and Bertha will focus on potatoes. At first, Bertha is skeptical; Alex is worse at farming, so how could trade be beneficial?</p>\n<p>At that very moment, they are overhead by a passing economist, who explains the Principle of Comparative Advantage, which says that as long as their ability to produce crops comes in different <em>ratios</em>, they can profit from trade. With the help of a passing algebra teacher, they determine that Alex will plant 10 barrels' worth of carrots, and Bertha will plant 2.9 barrels of carrots and 12.9 barrels of potatoes. That fall, they trade.</p>\n<p>That winter, a travelling wizard casts a spell on Bertha, enchanting her voice, and explains that if she sings opera to her crops, they'll grow twice as fast, doubling her yield. When Alex and Bertha meet and plan their trade, they agree that Alex will plant 10 barrels' worth of carrots and Bertha will plant 12.9 barrels of carrots, 22.9 barrels of potatoes. Alex, unfortunately, missed the wizard because he had the flu, but thanks to comparative advantage and trade, he is nevertheless slightly better off than before.</p>\n<p>The next year, the wizard returns, and casts another spell on Bertha, enchanting her hands. This, he explains, will cause any vegetable she plants with them, to taste slightly better. Bertha decides that she doesn't want Alex's less-tasty vegetables, and stops trading.</p>\n<p>That winter, Alex starves to death.</p>\n<p><em>(Moral: Don't be a subsistence farmer.)</em></p>\n<p>&nbsp;</p>\n<p><strong>Third Parable: Regulatory Capture</strong></p>\n<p>Fleem production is a highly regulated industry. There are only three firms that make fleem, and prices are high. Investigation into the laws regarding fleem production reveals that, in order to make fleem, you must:</p>\n<li>Not dump toxic waste into rivers</li>\n<li>Have an outside accountant look at your books once in awhile</li>\n<li>Fill out lots of paperwork</li>\n<li>Do the secret regulator handshake, and</li>\n<li>Perform the regulator dance</li>\n<p>Buyers are complaining about prices, and producers are complaining about the burdensome regulations. The legislature decides that, in order to deal with the problem, they need to clean up and simplify the regulations. After extensively debating the issue and consulting with the fleem industry, they agree that, to be a fleem producer, you should:</p>\n<li>Not dump toxic waste into rivers</li>\n<li>Fill out lots of paperwork</li>\n<li>Do the secret regulator handshake, and</li>\n<li>Perform the regulator dance in under a minute</li>\n<p>Several years later, one of the major fleem producers is caught defrauding its customers and contractors out of huge sums of money. The enraged public demands solutions to ensure that this never happens again. After extensively debating the issue and consulting with the fleem industry, the legislature decides to form a new agency, the Office of Fleem Scandal Prevention. The office contains dozens of workers, who spend their days:</p>\n<li>Checking fleem producers' paperwork for stray marks</li>\n<li>Updating the secret regulator handshake, and</li>\n<li>Issuing fines for missteps in the regulator dance</li>\n<p>Several years later, there are two firms that make fleem.</p>\n<p><em>(Moral: Learn to live without fleem.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1, "PDJ6KqJBRzvKPfuS3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5XDuE9BEiRZcbKZhW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 34, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "26201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(Epistemic status: Satire.)</em></p>\n<p>&nbsp;</p>\n<p><strong id=\"First_Parable__Equilibrium_Pricing\">First Parable: Equilibrium Pricing</strong></p>\n<p>Highway Offramp 72 leads to the isolated town of Townton. Visitors are greeted by two fuel stations, Carbonaceous Fossils (CF) and Hydrogenated Chains (HC), on opposite sides of the main road. There are no other gas stations for many miles. Together, these two stations sell 1000 gallons per day. Since their products are indistinguishable, and they have prominently posted prices, every driver will choose the cheaper one; or if the prices are the same, they will split half and half. &nbsp;Both pay $1.50/gal for their stock and charge $2/gal to drivers, so half the drivers stop at each.</p>\n<p>The owner of CF reasons as follows: If I keep my current price of $2, I will make 500*(2-1.5)=$250 of profit. But if I lower my price to $1.99, I will get twice as much business and make 1000*(1.99-1.5)=$490 of profit. The next morning, he updates his price.</p>\n<p>Across the street, the owner of HC (who is having a bad day, due to the complete lack of customers), reasons the same way. The next morning HC has updated its price to $1.98; the morning after that CF lowers its price to $1.97; and so on.</p>\n<p>Because CF and HC's owners are law-abiding model citizens, they never talk to each other about prices. That would be collusion, which is illegal. Later that month, with CF's price down to $1.52 and HC's price at $1.51, the local community center holds Game Theory night, where both owners attend a local economist's presentation on the Iterated Prisoner's Dilemma.</p>\n<p>The next morning, both stations charge $1.52. The morning after that, $1.53. The morning after that, $1.54, and so on. Later that year, CF reasons as follows: If I keep my current price of $20...</p>\n<p><em>(Moral: Gas station attendants should study game theory.)</em></p>\n<p>&nbsp;</p>\n<p><strong id=\"Second_Parable__Comparative_Advantage\">Second Parable: Comparative Advantage</strong></p>\n<p>Two farmers, Alex and Bertha, grow potatoes and carrots. In one year, Alex can either grow 4 barrels of potatoes or 10 barrels of carrots, or some linear combination of the two, such as 2 barrels of potatoes and 5 barrels of carrots. Bertha is better at farming, and can produce 15 barrels of potatoes or 20 barrels of carrots, or some combination of the two. Doctors agree that everyone should eat exactly equal numbers of potatoes and carrots - an excess of one over the other would be unacceptable. So in the first year, having just settled a new frontier and not having met their neighbors, Alex plants 2.9 barrels' worth of each, and Bertha plants 8.6 barrels of each.</p>\n<p><a id=\"more\"></a></p>\n<p>During the next year's spring festival, Alex and Bertha meet, and Alex suggests arranging a trade: he will focus on carrots, and Bertha will focus on potatoes. At first, Bertha is skeptical; Alex is worse at farming, so how could trade be beneficial?</p>\n<p>At that very moment, they are overhead by a passing economist, who explains the Principle of Comparative Advantage, which says that as long as their ability to produce crops comes in different <em>ratios</em>, they can profit from trade. With the help of a passing algebra teacher, they determine that Alex will plant 10 barrels' worth of carrots, and Bertha will plant 2.9 barrels of carrots and 12.9 barrels of potatoes. That fall, they trade.</p>\n<p>That winter, a travelling wizard casts a spell on Bertha, enchanting her voice, and explains that if she sings opera to her crops, they'll grow twice as fast, doubling her yield. When Alex and Bertha meet and plan their trade, they agree that Alex will plant 10 barrels' worth of carrots and Bertha will plant 12.9 barrels of carrots, 22.9 barrels of potatoes. Alex, unfortunately, missed the wizard because he had the flu, but thanks to comparative advantage and trade, he is nevertheless slightly better off than before.</p>\n<p>The next year, the wizard returns, and casts another spell on Bertha, enchanting her hands. This, he explains, will cause any vegetable she plants with them, to taste slightly better. Bertha decides that she doesn't want Alex's less-tasty vegetables, and stops trading.</p>\n<p>That winter, Alex starves to death.</p>\n<p><em>(Moral: Don't be a subsistence farmer.)</em></p>\n<p>&nbsp;</p>\n<p><strong id=\"Third_Parable__Regulatory_Capture\">Third Parable: Regulatory Capture</strong></p>\n<p>Fleem production is a highly regulated industry. There are only three firms that make fleem, and prices are high. Investigation into the laws regarding fleem production reveals that, in order to make fleem, you must:</p>\n<li>Not dump toxic waste into rivers</li>\n<li>Have an outside accountant look at your books once in awhile</li>\n<li>Fill out lots of paperwork</li>\n<li>Do the secret regulator handshake, and</li>\n<li>Perform the regulator dance</li>\n<p>Buyers are complaining about prices, and producers are complaining about the burdensome regulations. The legislature decides that, in order to deal with the problem, they need to clean up and simplify the regulations. After extensively debating the issue and consulting with the fleem industry, they agree that, to be a fleem producer, you should:</p>\n<li>Not dump toxic waste into rivers</li>\n<li>Fill out lots of paperwork</li>\n<li>Do the secret regulator handshake, and</li>\n<li>Perform the regulator dance in under a minute</li>\n<p>Several years later, one of the major fleem producers is caught defrauding its customers and contractors out of huge sums of money. The enraged public demands solutions to ensure that this never happens again. After extensively debating the issue and consulting with the fleem industry, the legislature decides to form a new agency, the Office of Fleem Scandal Prevention. The office contains dozens of workers, who spend their days:</p>\n<li>Checking fleem producers' paperwork for stray marks</li>\n<li>Updating the secret regulator handshake, and</li>\n<li>Issuing fines for missteps in the regulator dance</li>\n<p>Several years later, there are two firms that make fleem.</p>\n<p><em>(Moral: Learn to live without fleem.)</em></p>", "sections": [{"title": "First Parable: Equilibrium Pricing", "anchor": "First_Parable__Equilibrium_Pricing", "level": 1}, {"title": "Second Parable: Comparative Advantage", "anchor": "Second_Parable__Comparative_Advantage", "level": 1}, {"title": "Third Parable: Regulatory Capture", "anchor": "Third_Parable__Regulatory_Capture", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "40 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-10T13:36:58.759Z", "modifiedAt": null, "url": null, "title": "Announcing a google group for technical discussion of FAI", "slug": "announcing-a-google-group-for-technical-discussion-of-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.908Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v8LhggAmF4RkoW4ua/announcing-a-google-group-for-technical-discussion-of-fai", "pageUrlRelative": "/posts/v8LhggAmF4RkoW4ua/announcing-a-google-group-for-technical-discussion-of-fai", "linkUrl": "https://www.lesswrong.com/posts/v8LhggAmF4RkoW4ua/announcing-a-google-group-for-technical-discussion-of-fai", "postedAtFormatted": "Saturday, May 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Announcing%20a%20google%20group%20for%20technical%20discussion%20of%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnnouncing%20a%20google%20group%20for%20technical%20discussion%20of%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8LhggAmF4RkoW4ua%2Fannouncing-a-google-group-for-technical-discussion-of-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Announcing%20a%20google%20group%20for%20technical%20discussion%20of%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8LhggAmF4RkoW4ua%2Fannouncing-a-google-group-for-technical-discussion-of-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8LhggAmF4RkoW4ua%2Fannouncing-a-google-group-for-technical-discussion-of-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>I'm pleased to announce&nbsp;<a href=\"https://groups.google.com/forum/#!forum/friendly-artificial-intelligence\">friendly-artificial-intelligence</a>, a google group intended for research-level discussion of problems in FAI and AGI, in particular for discussions that are highly technical and/or math intensive.</p>\n<p>Some examples of possible discussion topics: naturalized induction, decision theory, tiling agents / Loebian obstacle, logical uncertainty...</p>\n<p>I invite everyone who want to take part in FAI research to participate in the group. This obviously includes people affiliated with MIRI, FHI and CSER, people who attend MIRI workshops and participants of the <a href=\"/lw/k3h/southern_california_fai_workshop/\">southern california FAI workshop</a>.</p>\n<p>Please, come in and share your discoveries, ideas, thoughts, questions et cetera. See you there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v8LhggAmF4RkoW4ua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.7176298853464843e-06, "legacy": true, "legacyId": "26202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nqgY7mNy8JMN99SHA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-11T19:38:47.821Z", "modifiedAt": null, "url": null, "title": "A Dialogue On Doublethink", "slug": "a-dialogue-on-doublethink", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:39.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gRBbFTh6e3MzyojTf/a-dialogue-on-doublethink", "pageUrlRelative": "/posts/gRBbFTh6e3MzyojTf/a-dialogue-on-doublethink", "linkUrl": "https://www.lesswrong.com/posts/gRBbFTh6e3MzyojTf/a-dialogue-on-doublethink", "postedAtFormatted": "Sunday, May 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Dialogue%20On%20Doublethink&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Dialogue%20On%20Doublethink%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgRBbFTh6e3MzyojTf%2Fa-dialogue-on-doublethink%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Dialogue%20On%20Doublethink%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgRBbFTh6e3MzyojTf%2Fa-dialogue-on-doublethink", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgRBbFTh6e3MzyojTf%2Fa-dialogue-on-doublethink", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3219, "htmlBody": "<p>Followup to: <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Doublethink\">Against Doublethink (sequence)</a>, <a href=\"/lw/jhs/dark_arts_of_rationality/\">Dark Arts of Rationality</a>, <a href=\"/lw/if/your_strength_as_a_rationalist/\">Your Strength as a Rationalist</a></p>\n<hr />\n<h2>Doublethink</h2>\n<blockquote>\n<p>It is obvious that the same thing will not be willing to do or undergo opposites in the same part of itself, in relation to the same thing, at the same time. --Book IV of Plato's <em>Republic</em></p>\n</blockquote>\n<p>Can you simultaneously want sex and not want it? Can you believe in God and not believe in Him at the same time? Can you be fearless while frightened?</p>\n<p>To be fair to Plato, this was meant not as an assertion that such contradictions are impossible, but as an argument that the soul has multiple parts. It seems we can, in fact, want something while also not wanting it. This is awfully strange, and it led Plato to conclude the soul must have multiple parts, for surely no one part could contain both sides of the contradiction.</p>\n<p>Often, when we attempt to accept contradictory statements as correct, it causes cognitive dissonance--that nagging, itchy feeling in your brain that won't leave you alone until you admit that something is wrong. Like when you try to convince yourself that staying up just a little longer playing 2048 won't have adverse effects on the presentation you're giving tomorrow, when you know full well that's exactly what's going to happen.</p>\n<p>But it may be that cognitive dissonance is the exception in the face of contradictions, rather than the rule. How would you know? If it doesn't cause any emotional friction, the two propositions will just sit quietly together in your brain, never mentioning that it's logically impossible for both of them to be true. When we accept a contradiction wholesale without cognitive dissonance, it's what Orwell called \"doublethink\".</p>\n<p>When you're a mere mortal trying to get by in a complex universe, doublethink may be adaptive. If you want to be completely free of contradictory beliefs without spending your whole life alone in a cave, you'll likely waste a lot of your precious time working through conundrums, which will often produce even more conundrums.</p>\n<p>Suppose I believe that my husband is faithful, and I also believe that the unfamiliar perfume on his collar indicates he's sleeping with other women without my permission. I could let that pesky little contradiction turn into an extended investigation that may ultimately ruin my marriage. Or I could get on with my day and leave my marriage intact.</p>\n<p>It's better to just leave those kinds of thoughts alone, isn't it? It probably makes for a happier life.</p>\n<h2>Against Doublethink</h2>\n<p>Suppose you believe that driving is dangerous, and also that, while you are driving, you're completely safe. As established in Doublethink, there may be some benefits to letting that mental configuration be.</p>\n<p>There are also some life-shattering downsides. One of the things you believe is false, you see, by the law of the excluded middle. In point of fact, it's the one that goes \"I'm completely safe while driving\". Believing false things has consequences.</p>\n<blockquote>\n<p>Be irrationally optimistic about your driving skills, and you will be happily unconcerned where others sweat and fear. You won't have to put up with the inconvenience of a seatbelt. You will be happily unconcerned for a day, a week, a year. Then CRASH, and spend the rest of your life wishing you could scratch the itch in your phantom limb. Or paralyzed from the neck down. Or dead. It's not inevitable, but it's possible; how probable is it? You can't make that tradeoff rationally unless you know your real driving skills, so you can figure out how much danger you're placing yourself in. --Eliezer Yudkowsky, <a href=\"/lw/je/doublethink_choosing_to_be_biased/\">Doublethink (Choosing to be Biased) </a></p>\n</blockquote>\n<p>What are beliefs for? Please pause for ten seconds and come up with your own answer.</p>\n<p>Ultimately, I think beliefs are inputs for predictions. We're basically very complicated simulators that try to guess which actions will cause desired outcomes, like survival or reproduction or chocolate. We input beliefs about how the world behaves, make inferences from them to which experiences we should anticipate given various changes we might make to the world, and output behaviors that get us what we want, provided our simulations are good enough.</p>\n<p>My car is making a mysterious ticking sound. I have many beliefs about cars, and one of them is that if my car makes noises it shouldn't, it will probably stop working eventually, and possibly explode. I can use this input to simulate the future. Since I've observed my car making a noise it shouldn't, I predict that my car will stop working. I also believe that there is something causing the ticking. So I predict that if I intervene and stop the ticking (in non-ridiculous ways), my car will keep working. My belief has thus led to the action of researching the ticking noise, planning some simple tests, and will probably lead to cleaning the sticky lifters.</p>\n<p>If it's true that solving the ticking noise will keep my car running, then my beliefs will cash out in correctly anticipated experiences, and my actions will cause desired outcomes. If it's false, perhaps because the ticking can be solved without addressing a larger underlying problem, then the experiences I anticipate will not occur, and my actions may lead to my car exploding.</p>\n<p>Doublethink guarantees that you believe falsehoods. Some of the time you'll call upon the true belief (\"driving is dangerous\"), anticipate future experiences accurately, and get the results you want from your chosen actions (\"don't drive three times the speed limit at night while it's raining\"). But some of the time, if you actually believe the false thing as well, you'll call upon the opposite belief, anticipate inaccurately, and choose the last action you'll ever take.</p>\n<p>Without any principled algorithm determining which of the contradictory propositions to use as an input for the simulation at hand, you'll fail as often as you succeed. So it makes no sense to anticipate more positive outcomes from believing contradictions.</p>\n<p>Contradictions may keep you happy as long as you never need to use them. Should you call upon them, though, to guide your actions, the debt on false beliefs will come due. You will drive too fast at night in the rain, you will crash, you will fly out of the car with no seat belt to restrain you, you will die, and it will be your fault.</p>\n<h2>Against Against Doublethink</h2>\n<p>What if Plato was pretty much right, and we sometimes believe contradictions because we're sort of not actually one single person?</p>\n<p>It is not literally true that Systems 1 and 2 are separate individuals the way you and I are. But the idea of Systems 1 and 2 suggests to me something quite interesting with respect to the relationship between beliefs and their role in decision making, and modeling them as separate people with very different personalities seems to work pretty darn well when I test my suspicions.</p>\n<blockquote>\n<p>I read Atlas Shrugged probably about a decade ago. I was impressed with its defense of capitalism, which really hammers home the reasons it&rsquo;s good and important on a gut level. But I was equally turned off by its promotion of selfishness as a moral ideal. I thought that was *basically* just being a jerk. After all, if there&rsquo;s one thing the world doesn&rsquo;t need (I thought) it&rsquo;s more selfishness.</p>\n<p>Then I talked to a friend who told me Atlas Shrugged had changed his life. That he&rsquo;d been raised in a really strict family that had told him that ever enjoying himself was selfish and made him a bad person, that he had to be working at every moment to make his family and other people happy or else let them shame him to pieces. And the revelation that it was sometimes okay to consider your own happiness gave him the strength to stand up to them and turn his life around, while still keeping the basic human instinct of helping others when he wanted to and he felt they deserved it (as, indeed, do Rand characters). --Scott of Slate Star Codex in <a href=\"http://slatestarcodex.com/2013/06/09/all-debates-are-bravery-debates/\">All Debates Are Bravery Debates </a></p>\n</blockquote>\n<p>If you're generous to a fault, \"I should be more selfish\" is probably a belief that will pay off in positive outcomes should you install it for future use. If you're selfish to a fault, the same belief will be harmful. So what if you were too generous half of the time and too selfish the other half? Well, then you would want to believe \"I should be more selfish\" with only the generous half, while disbelieving it with the selfish half.</p>\n<p>Systems 1 and 2 need to hear different things. System 2 might be able to understand the reality of biases and make appropriate adjustments that would work if System 1 were on board, but System 1 isn't so great at being reasonable. And it's not System 2 that's in charge of most of your actions. If you want your beliefs to positively influence your actions (which is the point of beliefs, after all), you need to tailor your beliefs to System 1's needs.</p>\n<p>For example: The planning fallacy is nearly ubiquitous. I know this because for the past three years or so, I've gotten everywhere five to fifteen minutes early. Almost every single person I meet with arrives five to fifteen minutes late. It is very rare for someone to be on time, and only twice in three years have I encountered the (rather awkward) circumstance of meeting with someone who also arrived early.</p>\n<p>Before three years ago, I was also usually late, and I far underestimated how long my projects would take. I knew, abstractly and intellectually, about the planning fallacy, but that didn't stop System 1 from thinking things would go implausibly quickly. System 1's just optimistic like that. It responds to, \"Dude, that is not going to work, and I have a twelve point argument supporting my position and suggesting alternative plans,\" with \"Naaaaw, it'll be fine! We can totally make that deadline.\"</p>\n<p>At some point (I don't remember when or exactly how), I gained the ability to look at the true due date, shift my System 1 beliefs to make up for the planning fallacy, and then hide my memory that I'd ever seen the original due date. I would see that my flight left at 2:30, and be surprised to discover on travel day that I was not late for my 2:00 flight, but a little early for my 2:30 one. I consistently finished projects on time, and only disasters caused me to be late for meetings. It took me about three months before I noticed the pattern and realized what must be going on.</p>\n<p>I got a little worried I might make a mistake, such as leaving a meeting thinking the other person just wasn't going to show when the actual meeting time hadn't arrived. I did have a couple close calls along those lines. But it was easy enough to fix; in important cases, I started receiving Boomeranged notes from past-me around the time present-me expected things to start that said, \"Surprise! You've still got ten minutes!\"</p>\n<p>This unquestionably improved my life. You don't realize just how inconvenient the planning fallacy is until you've left it behind. Clearly, considered in isolation, the action of believing falsely in this domain was instrumentally rational.</p>\n<p>Doublethink, and the <a href=\"/lw/jhs/dark_arts_of_rationality/\">Dark Arts</a>&nbsp;generally, applied to carefully chosen domains is a powerful tool. It's dumb to believe false things about really dangerous stuff like driving, obviously. But you don't have to doublethink indiscriminately. As long as you're careful, as long as you suspend epistemic rationality only when it's clearly beneficial to do so, employing doublethink at will is a great idea.</p>\n<p>Instrumental rationality is what really matters. Epistemic rationality is useful, but what use is holding accurate beliefs in situations where that won't get you what you want?</p>\n<h2>Against Against Against Doublethink</h2>\n<p>There are indeed epistemically irrational actions that are instrumentally rational, and instrumental rationality is what really matters. It is pointless to believing true things if it doesn't get you what you want. This has always been very obvious to me, and it remains so.</p>\n<p>There is a bigger picture.</p>\n<p>Certain epistemic rationality techniques are not compatible with dark side epistemology. Most importantly, the Dark Arts do not play nicely with \"notice your confusion\", which is essentially <a href=\"/lw/if/your_strength_as_a_rationalist/\">your strength as a rationalist</a>. If you use doublethink on purpose, confusion doesn't always indicate that you need to find out what false thing you believe so you can fix it. Sometimes you have to bury your confusion. There's an itsy bitsy pause where you try to predict whether it's useful to bury.</p>\n<p>As soon as I finally decided to abandon the Dark Arts, I began to sweep out corners I'd allowed myself to neglect before. They were mainly corners I didn't know I'd neglected.</p>\n<p>The first one I noticed was the way I responded to requests from my boyfriend. He'd mentioned before that I often seemed resentful when he made requests of me, and I'd insisted that he was wrong, that I was actually happy all the while. (Notice that in the short term, since I was probably going to do as he asked anyway, attending to the resentment would probably have made things more difficult for me.) This self-deception went on for months.</p>\n<p>Shortly after I gave up doublethink, he made a request, and I felt a little stab of dissonance. Something I might have swept away before, because it seemed more immediately useful to bury the confusion than to notice it. But I thought (wordlessly and with my emotions), \"No, look at it. This is exactly what I've decided to watch for. I have noticed confusion, and I will attend to it.\"</p>\n<p>It was very upsetting at first to learn that he'd been right. I feared the implications for our relationship. But that fear didn't last, because we both knew the only problems you can solve are the ones you acknowledge, so it is a comfort to know the truth.</p>\n<p>I was far more shaken by the realization that I really, truly was ignorant that this had been happening. Not because the consequences of this one bit of ignorance were so important, but because who knows what other epistemic curses have hidden themselves in the shadows? I realized that I had not been in control of my doublethink, that I couldn't have been.</p>\n<p>Pinning down that one tiny little stab of dissonance took great preparation and effort, and there's no way I'd been working fast enough before. \"How often,\" I wondered, \"does this kind of thing happen?\"</p>\n<p>Very often, it turns out. I began noticing and acting on confusion several times a day, where before I'd been doing it a couple times a week. I wasn't just noticing things that I'd have ignored on purpose before; I was noticing things that would have slipped by because my reflexes slowed as I weighed the benefit of paying attention. \"Ignore it\" was not an available action in the face of confusion anymore, and that was a dramatic change. Because there are no disruptions, acting on confusion is becoming automatic.</p>\n<p>I can't know for sure which bits of confusion I've noticed since the change would otherwise have slipped by unseen. But here's a plausible instance. Tonight I was having dinner with a friend I've met very recently. I was feeling s little bit tired and nervous, so I wasn't putting as much effort as usual into directing the conversation. At one point I realized we had stopped making making any progress toward my goals, since it was clear we were drifting toward small talk. In a tired and slightly nervous state, I imagine that I might have buried that bit of information and abdicated responsibility for the conversation--not by means of considering whether allowing small talk to happen was actually a good idea, but by not pouncing on the dissonance aggressively, and thereby letting it get away. Instead, I directed my attention at the feeling (without effort this time!), inquired of myself what precisely was causing it, identified the prediction that the current course of conversation was leading away from my goals, listed potential interventions, weighed their costs and benefits against my simulation of small talk, and said, \"What are your terminal values?\"</p>\n<p>(I know that sounds like a lot of work, but it took at most three seconds. The hard part was building the pouncing reflex.)</p>\n<p>When you know that some of your beliefs are false, and you know that leaving them be is instrumentally rational, you do not develop the automatic reflex of interrogating every suspicion of confusion. You might think you can do this selectively, but if you do, I strongly suspect you're wrong in exactly the way I was.</p>\n<p>I have long been more viscerally motivated by things that are interesting or beautiful than by things that correspond to the territory. So it's not too surprising that toward the beginning of my rationality training, I went through a long period of being so enamored with a-veridical instrumental techniques--things like willful doublethink--that I double-thought myself into believing accuracy was not so great.</p>\n<p>But I was wrong. And that mattered. Having accurate beliefs is a ridiculously convergent incentive. Every utility function that involves interaction with the territory--interaction of just about any kind!--benefits from a sound map. Even if \"beauty\" is a terminal value, \"being viscerally motivated to increase your ability to make predictions that lead to greater beauty\" increases your odds of success.</p>\n<p>Dark side epistemology prevents total dedication to continuous improvement in epistemic rationality. Though individual dark side actions may be instrumentally rational, the patterns of thought required to allow them are not. Though instrumental rationality is ultimately the goal, your instrumental rationality will always be limited by your epistemic rationality.</p>\n<p>That was important enough to say again: Your instrumental rationality will always be limited by your epistemic rationality.</p>\n<p>It only takes a fraction of a second to sweep an observation into the corner. You don't have time to decide whether looking at it might prove problematic. If you take the time to protect your compartments, false beliefs you don't endorse will slide in from everywhere through those split-second cracks in your art. You must attend to your confusion the very moment you notice it. You must be relentless an unmerciful toward your own beliefs.</p>\n<p>Excellent epistemology is not the natural state of a human brain. Rationality is hard. Without extreme dedication and advanced training, without reliable automatic reflexes of rational thought, your belief structure will be a mess. You can't have totally automatic anti-rationalization reflexes if you use doublethink as a technique of instrumental rationality.</p>\n<p>This has been a difficult lesson for me. I have lost some benefits I'd gained from the Dark Arts. I'm late now, sometimes. And painful truths are painful, though now they are sharp and fast instead of dull and damaging.</p>\n<p>And it is so worth it! I have much more work to do before I can move on to the next thing. But whatever the next thing is, I'll tackle it with far more predictive power than I otherwise would have--though I doubt I'd have noticed the difference.</p>\n<p>So when I say that I'm against against against doublethink--that dark side epistemology is bad--I mean that there is more potential on the light side, not that the dark side has no redeeming features. Its fruits hang low, and they are delicious.</p>\n<p>But the fruits of the light side are worth the climb. You'll never even know they're there if you gorge yourself in the dark forever.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YTCrHWYHAsAD74EHo": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gRBbFTh6e3MzyojTf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 81, "extendedScore": null, "score": 0.000248, "legacy": true, "legacyId": "26189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Followup to: <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Doublethink\">Against Doublethink (sequence)</a>, <a href=\"/lw/jhs/dark_arts_of_rationality/\">Dark Arts of Rationality</a>, <a href=\"/lw/if/your_strength_as_a_rationalist/\">Your Strength as a Rationalist</a></p>\n<hr>\n<h2 id=\"Doublethink\">Doublethink</h2>\n<blockquote>\n<p>It is obvious that the same thing will not be willing to do or undergo opposites in the same part of itself, in relation to the same thing, at the same time. --Book IV of Plato's <em>Republic</em></p>\n</blockquote>\n<p>Can you simultaneously want sex and not want it? Can you believe in God and not believe in Him at the same time? Can you be fearless while frightened?</p>\n<p>To be fair to Plato, this was meant not as an assertion that such contradictions are impossible, but as an argument that the soul has multiple parts. It seems we can, in fact, want something while also not wanting it. This is awfully strange, and it led Plato to conclude the soul must have multiple parts, for surely no one part could contain both sides of the contradiction.</p>\n<p>Often, when we attempt to accept contradictory statements as correct, it causes cognitive dissonance--that nagging, itchy feeling in your brain that won't leave you alone until you admit that something is wrong. Like when you try to convince yourself that staying up just a little longer playing 2048 won't have adverse effects on the presentation you're giving tomorrow, when you know full well that's exactly what's going to happen.</p>\n<p>But it may be that cognitive dissonance is the exception in the face of contradictions, rather than the rule. How would you know? If it doesn't cause any emotional friction, the two propositions will just sit quietly together in your brain, never mentioning that it's logically impossible for both of them to be true. When we accept a contradiction wholesale without cognitive dissonance, it's what Orwell called \"doublethink\".</p>\n<p>When you're a mere mortal trying to get by in a complex universe, doublethink may be adaptive. If you want to be completely free of contradictory beliefs without spending your whole life alone in a cave, you'll likely waste a lot of your precious time working through conundrums, which will often produce even more conundrums.</p>\n<p>Suppose I believe that my husband is faithful, and I also believe that the unfamiliar perfume on his collar indicates he's sleeping with other women without my permission. I could let that pesky little contradiction turn into an extended investigation that may ultimately ruin my marriage. Or I could get on with my day and leave my marriage intact.</p>\n<p>It's better to just leave those kinds of thoughts alone, isn't it? It probably makes for a happier life.</p>\n<h2 id=\"Against_Doublethink\">Against Doublethink</h2>\n<p>Suppose you believe that driving is dangerous, and also that, while you are driving, you're completely safe. As established in Doublethink, there may be some benefits to letting that mental configuration be.</p>\n<p>There are also some life-shattering downsides. One of the things you believe is false, you see, by the law of the excluded middle. In point of fact, it's the one that goes \"I'm completely safe while driving\". Believing false things has consequences.</p>\n<blockquote>\n<p>Be irrationally optimistic about your driving skills, and you will be happily unconcerned where others sweat and fear. You won't have to put up with the inconvenience of a seatbelt. You will be happily unconcerned for a day, a week, a year. Then CRASH, and spend the rest of your life wishing you could scratch the itch in your phantom limb. Or paralyzed from the neck down. Or dead. It's not inevitable, but it's possible; how probable is it? You can't make that tradeoff rationally unless you know your real driving skills, so you can figure out how much danger you're placing yourself in. --Eliezer Yudkowsky, <a href=\"/lw/je/doublethink_choosing_to_be_biased/\">Doublethink (Choosing to be Biased) </a></p>\n</blockquote>\n<p>What are beliefs for? Please pause for ten seconds and come up with your own answer.</p>\n<p>Ultimately, I think beliefs are inputs for predictions. We're basically very complicated simulators that try to guess which actions will cause desired outcomes, like survival or reproduction or chocolate. We input beliefs about how the world behaves, make inferences from them to which experiences we should anticipate given various changes we might make to the world, and output behaviors that get us what we want, provided our simulations are good enough.</p>\n<p>My car is making a mysterious ticking sound. I have many beliefs about cars, and one of them is that if my car makes noises it shouldn't, it will probably stop working eventually, and possibly explode. I can use this input to simulate the future. Since I've observed my car making a noise it shouldn't, I predict that my car will stop working. I also believe that there is something causing the ticking. So I predict that if I intervene and stop the ticking (in non-ridiculous ways), my car will keep working. My belief has thus led to the action of researching the ticking noise, planning some simple tests, and will probably lead to cleaning the sticky lifters.</p>\n<p>If it's true that solving the ticking noise will keep my car running, then my beliefs will cash out in correctly anticipated experiences, and my actions will cause desired outcomes. If it's false, perhaps because the ticking can be solved without addressing a larger underlying problem, then the experiences I anticipate will not occur, and my actions may lead to my car exploding.</p>\n<p>Doublethink guarantees that you believe falsehoods. Some of the time you'll call upon the true belief (\"driving is dangerous\"), anticipate future experiences accurately, and get the results you want from your chosen actions (\"don't drive three times the speed limit at night while it's raining\"). But some of the time, if you actually believe the false thing as well, you'll call upon the opposite belief, anticipate inaccurately, and choose the last action you'll ever take.</p>\n<p>Without any principled algorithm determining which of the contradictory propositions to use as an input for the simulation at hand, you'll fail as often as you succeed. So it makes no sense to anticipate more positive outcomes from believing contradictions.</p>\n<p>Contradictions may keep you happy as long as you never need to use them. Should you call upon them, though, to guide your actions, the debt on false beliefs will come due. You will drive too fast at night in the rain, you will crash, you will fly out of the car with no seat belt to restrain you, you will die, and it will be your fault.</p>\n<h2 id=\"Against_Against_Doublethink\">Against Against Doublethink</h2>\n<p>What if Plato was pretty much right, and we sometimes believe contradictions because we're sort of not actually one single person?</p>\n<p>It is not literally true that Systems 1 and 2 are separate individuals the way you and I are. But the idea of Systems 1 and 2 suggests to me something quite interesting with respect to the relationship between beliefs and their role in decision making, and modeling them as separate people with very different personalities seems to work pretty darn well when I test my suspicions.</p>\n<blockquote>\n<p>I read Atlas Shrugged probably about a decade ago. I was impressed with its defense of capitalism, which really hammers home the reasons it\u2019s good and important on a gut level. But I was equally turned off by its promotion of selfishness as a moral ideal. I thought that was *basically* just being a jerk. After all, if there\u2019s one thing the world doesn\u2019t need (I thought) it\u2019s more selfishness.</p>\n<p>Then I talked to a friend who told me Atlas Shrugged had changed his life. That he\u2019d been raised in a really strict family that had told him that ever enjoying himself was selfish and made him a bad person, that he had to be working at every moment to make his family and other people happy or else let them shame him to pieces. And the revelation that it was sometimes okay to consider your own happiness gave him the strength to stand up to them and turn his life around, while still keeping the basic human instinct of helping others when he wanted to and he felt they deserved it (as, indeed, do Rand characters). --Scott of Slate Star Codex in <a href=\"http://slatestarcodex.com/2013/06/09/all-debates-are-bravery-debates/\">All Debates Are Bravery Debates </a></p>\n</blockquote>\n<p>If you're generous to a fault, \"I should be more selfish\" is probably a belief that will pay off in positive outcomes should you install it for future use. If you're selfish to a fault, the same belief will be harmful. So what if you were too generous half of the time and too selfish the other half? Well, then you would want to believe \"I should be more selfish\" with only the generous half, while disbelieving it with the selfish half.</p>\n<p>Systems 1 and 2 need to hear different things. System 2 might be able to understand the reality of biases and make appropriate adjustments that would work if System 1 were on board, but System 1 isn't so great at being reasonable. And it's not System 2 that's in charge of most of your actions. If you want your beliefs to positively influence your actions (which is the point of beliefs, after all), you need to tailor your beliefs to System 1's needs.</p>\n<p>For example: The planning fallacy is nearly ubiquitous. I know this because for the past three years or so, I've gotten everywhere five to fifteen minutes early. Almost every single person I meet with arrives five to fifteen minutes late. It is very rare for someone to be on time, and only twice in three years have I encountered the (rather awkward) circumstance of meeting with someone who also arrived early.</p>\n<p>Before three years ago, I was also usually late, and I far underestimated how long my projects would take. I knew, abstractly and intellectually, about the planning fallacy, but that didn't stop System 1 from thinking things would go implausibly quickly. System 1's just optimistic like that. It responds to, \"Dude, that is not going to work, and I have a twelve point argument supporting my position and suggesting alternative plans,\" with \"Naaaaw, it'll be fine! We can totally make that deadline.\"</p>\n<p>At some point (I don't remember when or exactly how), I gained the ability to look at the true due date, shift my System 1 beliefs to make up for the planning fallacy, and then hide my memory that I'd ever seen the original due date. I would see that my flight left at 2:30, and be surprised to discover on travel day that I was not late for my 2:00 flight, but a little early for my 2:30 one. I consistently finished projects on time, and only disasters caused me to be late for meetings. It took me about three months before I noticed the pattern and realized what must be going on.</p>\n<p>I got a little worried I might make a mistake, such as leaving a meeting thinking the other person just wasn't going to show when the actual meeting time hadn't arrived. I did have a couple close calls along those lines. But it was easy enough to fix; in important cases, I started receiving Boomeranged notes from past-me around the time present-me expected things to start that said, \"Surprise! You've still got ten minutes!\"</p>\n<p>This unquestionably improved my life. You don't realize just how inconvenient the planning fallacy is until you've left it behind. Clearly, considered in isolation, the action of believing falsely in this domain was instrumentally rational.</p>\n<p>Doublethink, and the <a href=\"/lw/jhs/dark_arts_of_rationality/\">Dark Arts</a>&nbsp;generally, applied to carefully chosen domains is a powerful tool. It's dumb to believe false things about really dangerous stuff like driving, obviously. But you don't have to doublethink indiscriminately. As long as you're careful, as long as you suspend epistemic rationality only when it's clearly beneficial to do so, employing doublethink at will is a great idea.</p>\n<p>Instrumental rationality is what really matters. Epistemic rationality is useful, but what use is holding accurate beliefs in situations where that won't get you what you want?</p>\n<h2 id=\"Against_Against_Against_Doublethink\">Against Against Against Doublethink</h2>\n<p>There are indeed epistemically irrational actions that are instrumentally rational, and instrumental rationality is what really matters. It is pointless to believing true things if it doesn't get you what you want. This has always been very obvious to me, and it remains so.</p>\n<p>There is a bigger picture.</p>\n<p>Certain epistemic rationality techniques are not compatible with dark side epistemology. Most importantly, the Dark Arts do not play nicely with \"notice your confusion\", which is essentially <a href=\"/lw/if/your_strength_as_a_rationalist/\">your strength as a rationalist</a>. If you use doublethink on purpose, confusion doesn't always indicate that you need to find out what false thing you believe so you can fix it. Sometimes you have to bury your confusion. There's an itsy bitsy pause where you try to predict whether it's useful to bury.</p>\n<p>As soon as I finally decided to abandon the Dark Arts, I began to sweep out corners I'd allowed myself to neglect before. They were mainly corners I didn't know I'd neglected.</p>\n<p>The first one I noticed was the way I responded to requests from my boyfriend. He'd mentioned before that I often seemed resentful when he made requests of me, and I'd insisted that he was wrong, that I was actually happy all the while. (Notice that in the short term, since I was probably going to do as he asked anyway, attending to the resentment would probably have made things more difficult for me.) This self-deception went on for months.</p>\n<p>Shortly after I gave up doublethink, he made a request, and I felt a little stab of dissonance. Something I might have swept away before, because it seemed more immediately useful to bury the confusion than to notice it. But I thought (wordlessly and with my emotions), \"No, look at it. This is exactly what I've decided to watch for. I have noticed confusion, and I will attend to it.\"</p>\n<p>It was very upsetting at first to learn that he'd been right. I feared the implications for our relationship. But that fear didn't last, because we both knew the only problems you can solve are the ones you acknowledge, so it is a comfort to know the truth.</p>\n<p>I was far more shaken by the realization that I really, truly was ignorant that this had been happening. Not because the consequences of this one bit of ignorance were so important, but because who knows what other epistemic curses have hidden themselves in the shadows? I realized that I had not been in control of my doublethink, that I couldn't have been.</p>\n<p>Pinning down that one tiny little stab of dissonance took great preparation and effort, and there's no way I'd been working fast enough before. \"How often,\" I wondered, \"does this kind of thing happen?\"</p>\n<p>Very often, it turns out. I began noticing and acting on confusion several times a day, where before I'd been doing it a couple times a week. I wasn't just noticing things that I'd have ignored on purpose before; I was noticing things that would have slipped by because my reflexes slowed as I weighed the benefit of paying attention. \"Ignore it\" was not an available action in the face of confusion anymore, and that was a dramatic change. Because there are no disruptions, acting on confusion is becoming automatic.</p>\n<p>I can't know for sure which bits of confusion I've noticed since the change would otherwise have slipped by unseen. But here's a plausible instance. Tonight I was having dinner with a friend I've met very recently. I was feeling s little bit tired and nervous, so I wasn't putting as much effort as usual into directing the conversation. At one point I realized we had stopped making making any progress toward my goals, since it was clear we were drifting toward small talk. In a tired and slightly nervous state, I imagine that I might have buried that bit of information and abdicated responsibility for the conversation--not by means of considering whether allowing small talk to happen was actually a good idea, but by not pouncing on the dissonance aggressively, and thereby letting it get away. Instead, I directed my attention at the feeling (without effort this time!), inquired of myself what precisely was causing it, identified the prediction that the current course of conversation was leading away from my goals, listed potential interventions, weighed their costs and benefits against my simulation of small talk, and said, \"What are your terminal values?\"</p>\n<p>(I know that sounds like a lot of work, but it took at most three seconds. The hard part was building the pouncing reflex.)</p>\n<p>When you know that some of your beliefs are false, and you know that leaving them be is instrumentally rational, you do not develop the automatic reflex of interrogating every suspicion of confusion. You might think you can do this selectively, but if you do, I strongly suspect you're wrong in exactly the way I was.</p>\n<p>I have long been more viscerally motivated by things that are interesting or beautiful than by things that correspond to the territory. So it's not too surprising that toward the beginning of my rationality training, I went through a long period of being so enamored with a-veridical instrumental techniques--things like willful doublethink--that I double-thought myself into believing accuracy was not so great.</p>\n<p>But I was wrong. And that mattered. Having accurate beliefs is a ridiculously convergent incentive. Every utility function that involves interaction with the territory--interaction of just about any kind!--benefits from a sound map. Even if \"beauty\" is a terminal value, \"being viscerally motivated to increase your ability to make predictions that lead to greater beauty\" increases your odds of success.</p>\n<p>Dark side epistemology prevents total dedication to continuous improvement in epistemic rationality. Though individual dark side actions may be instrumentally rational, the patterns of thought required to allow them are not. Though instrumental rationality is ultimately the goal, your instrumental rationality will always be limited by your epistemic rationality.</p>\n<p>That was important enough to say again: Your instrumental rationality will always be limited by your epistemic rationality.</p>\n<p>It only takes a fraction of a second to sweep an observation into the corner. You don't have time to decide whether looking at it might prove problematic. If you take the time to protect your compartments, false beliefs you don't endorse will slide in from everywhere through those split-second cracks in your art. You must attend to your confusion the very moment you notice it. You must be relentless an unmerciful toward your own beliefs.</p>\n<p>Excellent epistemology is not the natural state of a human brain. Rationality is hard. Without extreme dedication and advanced training, without reliable automatic reflexes of rational thought, your belief structure will be a mess. You can't have totally automatic anti-rationalization reflexes if you use doublethink as a technique of instrumental rationality.</p>\n<p>This has been a difficult lesson for me. I have lost some benefits I'd gained from the Dark Arts. I'm late now, sometimes. And painful truths are painful, though now they are sharp and fast instead of dull and damaging.</p>\n<p>And it is so worth it! I have much more work to do before I can move on to the next thing. But whatever the next thing is, I'll tackle it with far more predictive power than I otherwise would have--though I doubt I'd have noticed the difference.</p>\n<p>So when I say that I'm against against against doublethink--that dark side epistemology is bad--I mean that there is more potential on the light side, not that the dark side has no redeeming features. Its fruits hang low, and they are delicious.</p>\n<p>But the fruits of the light side are worth the climb. You'll never even know they're there if you gorge yourself in the dark forever.</p>", "sections": [{"title": "Doublethink", "anchor": "Doublethink", "level": 1}, {"title": "Against Doublethink", "anchor": "Against_Doublethink", "level": 1}, {"title": "Against Against Doublethink", "anchor": "Against_Against_Doublethink", "level": 1}, {"title": "Against Against Against Doublethink", "anchor": "Against_Against_Against_Doublethink", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "108 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4DBBQkEQvNEWafkek", "5JDkW4MYXit2CquLs", "Hs3ymqypvhgFMkgLb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-12T08:16:58.489Z", "modifiedAt": null, "url": null, "title": "Open Thread, May 12 - 18, 2014", "slug": "open-thread-may-12-18-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EsRTvwRnXjA44K2Tr/open-thread-may-12-18-2014", "pageUrlRelative": "/posts/EsRTvwRnXjA44K2Tr/open-thread-may-12-18-2014", "linkUrl": "https://www.lesswrong.com/posts/EsRTvwRnXjA44K2Tr/open-thread-may-12-18-2014", "postedAtFormatted": "Monday, May 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20May%2012%20-%2018%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20May%2012%20-%2018%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsRTvwRnXjA44K2Tr%2Fopen-thread-may-12-18-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20May%2012%20-%2018%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsRTvwRnXjA44K2Tr%2Fopen-thread-may-12-18-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsRTvwRnXjA44K2Tr%2Fopen-thread-may-12-18-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><a style=\"font-size: small; line-height: 19px;\" title=\"Previous Open Thread\" href=\"/lw/k6r/open_thread_may_5_11_2014/\">Previous Open Thread</a></p>\n<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\"><br />You know the drill - If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. Open Threads should start on Monday, and end on Sunday.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. Open Threads should be posted in Discussion, and not Main.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EsRTvwRnXjA44K2Tr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.7211380116416665e-06, "legacy": true, "legacyId": "26206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 201, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cti7vfwZGgeK6wKL6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-12T13:10:07.746Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht - Social discussion at the Film Caf\u00e9", "slug": "meetup-utrecht-social-discussion-at-the-film-cafe", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoerenMind", "createdAt": "2013-07-16T18:10:55.180Z", "isAdmin": false, "displayName": "SoerenMind"}, "userId": "DGetADxtea2LRL946", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YmJjPpxZnyj5JkcMK/meetup-utrecht-social-discussion-at-the-film-cafe", "pageUrlRelative": "/posts/YmJjPpxZnyj5JkcMK/meetup-utrecht-social-discussion-at-the-film-cafe", "linkUrl": "https://www.lesswrong.com/posts/YmJjPpxZnyj5JkcMK/meetup-utrecht-social-discussion-at-the-film-cafe", "postedAtFormatted": "Monday, May 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht%20-%20Social%20discussion%20at%20the%20Film%20Caf%C3%A9&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht%20-%20Social%20discussion%20at%20the%20Film%20Caf%C3%A9%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmJjPpxZnyj5JkcMK%2Fmeetup-utrecht-social-discussion-at-the-film-cafe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht%20-%20Social%20discussion%20at%20the%20Film%20Caf%C3%A9%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmJjPpxZnyj5JkcMK%2Fmeetup-utrecht-social-discussion-at-the-film-cafe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmJjPpxZnyj5JkcMK%2Fmeetup-utrecht-social-discussion-at-the-film-cafe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/102'>Utrecht - Social discussion at the Film Caf\u00e9</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 May 2014 05:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Slachtstraat 5</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone, This meetup will be an informal social discussion. The topics could be anything concerning effective altruism and rationality. There will be plenty of time for your personal questions and topics.</p>\n\n<p>Imma is going to share some thoughts and videos about GiveWell and effective giving. We have a <a href=\"https://docs.google.com/document/d/16bBtla1iVzkJjie-JK7Ozb9Ao8SbyJ9U924XyaEXTqY/edit?usp=sharing\" rel=\"nofollow\">Google Doc</a> where anyone can make topic suggestions, also for the coming meetups.</p>\n\n<p>This time we will meet at Film Caf\u00e9 Oscar, which is just around the corner from De Winkel van Sinkel. The kitchen there is open until 21:00 and is a little bit cheaper.\nLooking forward to meeting you again! New people will be warmly welcomed!</p>\n\n<p>I will be holding a sign that says 'LW' on it. If you have trouble finding us you can reach me at 0684140766.\nAnd feel free to join our <a href=\"https://www.facebook.com/groups/262932060523750/?ref=br_tf\" rel=\"nofollow\">facebook group</a> and/or <a href=\"http://www.meetup.com/LWEANL/events/182597422/\" rel=\"nofollow\">meetup.com group</a> as well.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/102'>Utrecht - Social discussion at the Film Caf\u00e9</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YmJjPpxZnyj5JkcMK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7215405487603962e-06, "legacy": true, "legacyId": "26207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht___Social_discussion_at_the_Film_Caf_\">Discussion article for the meetup : <a href=\"/meetups/102\">Utrecht - Social discussion at the Film Caf\u00e9</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 May 2014 05:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Slachtstraat 5</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everyone, This meetup will be an informal social discussion. The topics could be anything concerning effective altruism and rationality. There will be plenty of time for your personal questions and topics.</p>\n\n<p>Imma is going to share some thoughts and videos about GiveWell and effective giving. We have a <a href=\"https://docs.google.com/document/d/16bBtla1iVzkJjie-JK7Ozb9Ao8SbyJ9U924XyaEXTqY/edit?usp=sharing\" rel=\"nofollow\">Google Doc</a> where anyone can make topic suggestions, also for the coming meetups.</p>\n\n<p>This time we will meet at Film Caf\u00e9 Oscar, which is just around the corner from De Winkel van Sinkel. The kitchen there is open until 21:00 and is a little bit cheaper.\nLooking forward to meeting you again! New people will be warmly welcomed!</p>\n\n<p>I will be holding a sign that says 'LW' on it. If you have trouble finding us you can reach me at 0684140766.\nAnd feel free to join our <a href=\"https://www.facebook.com/groups/262932060523750/?ref=br_tf\" rel=\"nofollow\">facebook group</a> and/or <a href=\"http://www.meetup.com/LWEANL/events/182597422/\" rel=\"nofollow\">meetup.com group</a> as well.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht___Social_discussion_at_the_Film_Caf_1\">Discussion article for the meetup : <a href=\"/meetups/102\">Utrecht - Social discussion at the Film Caf\u00e9</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht - Social discussion at the Film Caf\u00e9", "anchor": "Discussion_article_for_the_meetup___Utrecht___Social_discussion_at_the_Film_Caf_", "level": 1}, {"title": "Discussion article for the meetup : Utrecht - Social discussion at the Film Caf\u00e9", "anchor": "Discussion_article_for_the_meetup___Utrecht___Social_discussion_at_the_Film_Caf_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-12T21:49:11.133Z", "modifiedAt": null, "url": null, "title": "Quantum Decisions", "slug": "quantum-decisions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:20.941Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DhRn3pfE3mkWkByei/quantum-decisions", "pageUrlRelative": "/posts/DhRn3pfE3mkWkByei/quantum-decisions", "linkUrl": "https://www.lesswrong.com/posts/DhRn3pfE3mkWkByei/quantum-decisions", "postedAtFormatted": "Monday, May 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantum%20Decisions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantum%20Decisions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhRn3pfE3mkWkByei%2Fquantum-decisions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantum%20Decisions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhRn3pfE3mkWkByei%2Fquantum-decisions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhRn3pfE3mkWkByei%2Fquantum-decisions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>CFAR sometimes plays a <a href=\"/lw/bc3/sotw_be_specific/681c\">Monday / Tuesday game</a>&nbsp;(invented by&nbsp;<a href=\"/user/palladias/\">palladias</a>). Copying from the URL:</p>\n<blockquote>\n<p>On Monday, your proposition is true. On Tuesday, your proposition is false. Tell me a story about each of the days so I can see how they are different. Don't just list the differences (because you're already not doing that well). Start with \"I wake up\" so you start concrete and move on in that vein, naming the parts of your day that are identical as well as those that are different.</p>\n</blockquote>\n<p>So my question is (edited on 2014/05/13):</p>\n<p>On Monday, I make my decisions by rolling a normal die. Example: should I eat vanilla or chocolate ice-cream? I then decide that if I roll 4 or higher on a 6-sided die, I'll pick vanilla. I roll the die, get a 3, and so proceed to eat chocolate ice-cream.</p>\n<p>On Tuesday, I use the same procedure, but use a quantum random number generator instead. (For the purpose of this discussion, let's assume that I can actually find a true/reliable generator. May be I'm shooting a photon through a half-silvered mirror.)</p>\n<p>What's the difference? (<a href=\"/lw/ixr/quantum_versus_logical_bombs/\">Relevant discussion</a> pointed out Pfft.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DhRn3pfE3mkWkByei", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 1.7222536861711362e-06, "legacy": true, "legacyId": "26210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JGHQPybvjLAgimXae"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-13T11:46:41.284Z", "modifiedAt": null, "url": null, "title": "Finding LessWrongers on LinkedIn", "slug": "finding-lesswrongers-on-linkedin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:22.115Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KoWjSM32AMN6pKDRX/finding-lesswrongers-on-linkedin", "pageUrlRelative": "/posts/KoWjSM32AMN6pKDRX/finding-lesswrongers-on-linkedin", "linkUrl": "https://www.lesswrong.com/posts/KoWjSM32AMN6pKDRX/finding-lesswrongers-on-linkedin", "postedAtFormatted": "Tuesday, May 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Finding%20LessWrongers%20on%20LinkedIn&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFinding%20LessWrongers%20on%20LinkedIn%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKoWjSM32AMN6pKDRX%2Ffinding-lesswrongers-on-linkedin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Finding%20LessWrongers%20on%20LinkedIn%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKoWjSM32AMN6pKDRX%2Ffinding-lesswrongers-on-linkedin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKoWjSM32AMN6pKDRX%2Ffinding-lesswrongers-on-linkedin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>We've had a few initiatives recently to connect LessWrongers for business networking.</p>\n<p>Here is another one: <a href=\"http://www.linkedin.com/groups/LessWrong-8108647 \">A&nbsp;</a><span style=\"; ; ;\"><a href=\"http://www.linkedin.com/groups/LessWrong-8108647 \">LW group at LinkedIn</a>.&nbsp;</span>Think of it not as a discussion group or online community, but as a \"tag\" on your&nbsp;LinkedIn&nbsp;profile, to help other&nbsp;LWers&nbsp;find you.</p>\n<p style=\"; ; ;\">(I've turned off the discussion functionality on the LI group, since the&nbsp;<a style=\"; ;\" href=\"/r/discussion/lw/k45/less_wrong_business_networking_google_group/\" target=\"_blank\">Google Group</a>&nbsp;or&nbsp;<a style=\";\" href=\"/\" target=\"_blank\">LessWrong.com</a>&nbsp;are better for that purpose.)</p>\n<p style=\"; ; ;\">Join here: <a href=\"http://www.linkedin.com/groups/LessWrong-8108647\">http://www.linkedin.com/groups/LessWrong-8108647</a>&nbsp;</p>\n<p style=\"; ; ;\">Another idea: Invite other LessWrongers to connect on LinkedIn, including not only those on the LW LI group, but any you know from the online community. It's a good way to get them in your Rolodex.* (You may have to dig up their email first. Then, add them at the LI menu&nbsp;<em>Connections-&gt;Add Connections-&gt;Any Email-&gt;Add by individual Email</em>.)&nbsp;</p>\n<p style=\"; ; ;\">For reference, here are recent posts on connecting between LWers:</p>\n<ol>\n<li><a style=\"; ;\" href=\"/r/discussion/lw/k45/less_wrong_business_networking_google_group/\" target=\"_blank\">A Google Group</a>&nbsp;for networking</li>\n<li><a style=\"; ;\" href=\"/r/discussion/lw/k43/lesswrongwiki_user_pages_underutilized_tag/\" target=\"_blank\">Updating your LW profile page</a><span style=\"; ;\">&nbsp;&nbsp;</span></li>\n<li><span style=\"; ;\">A </span><a style=\"; ;\" href=\"/lw/jzp/business_networking_through_lesswrong/\" target=\"_blank\">survey</a><span style=\"; ;\">, and&nbsp;</span><span style=\";\">reports about&nbsp;</span><a style=\"; ;\" href=\"/lw/k4o/lesswrong_as_social_catalyst/\" target=\"_blank\">LessWrong as social catalyst</a>.</li>\n</ol>\n<p><br /><span style=\";\">I'm interested in seeing which of these initiatives actually helps people, so please let us know.</span></p>\n<div>*&nbsp;<span style=\"font-color: gray; font-size:8px\">Is &nbsp;the Rolodex dead enough that we can use that metaphor again?&nbsp;</span></div>\n<div><br /></div>\n<div><span style=\"font-color: gray; font-size:8px\"><br /></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KoWjSM32AMN6pKDRX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 1.723405442810768e-06, "legacy": true, "legacyId": "26203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9saspMPtf4CTcKWcL", "PSiSpbcNETi66fQNt", "AXsew2jCcHRFTmW55", "Bq2jNNrk8JaZoD5jW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-13T15:06:18.994Z", "modifiedAt": null, "url": null, "title": "Meetup : Israel Less Wrong Meetup - Social and Board Games", "slug": "meetup-israel-less-wrong-meetup-social-and-board-games-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kZJtAhocN3XJeR2gK/meetup-israel-less-wrong-meetup-social-and-board-games-4", "pageUrlRelative": "/posts/kZJtAhocN3XJeR2gK/meetup-israel-less-wrong-meetup-social-and-board-games-4", "linkUrl": "https://www.lesswrong.com/posts/kZJtAhocN3XJeR2gK/meetup-israel-less-wrong-meetup-social-and-board-games-4", "postedAtFormatted": "Tuesday, May 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkZJtAhocN3XJeR2gK%2Fmeetup-israel-less-wrong-meetup-social-and-board-games-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkZJtAhocN3XJeR2gK%2Fmeetup-israel-less-wrong-meetup-social-and-board-games-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkZJtAhocN3XJeR2gK%2Fmeetup-israel-less-wrong-meetup-social-and-board-games-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 322, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/103'>Israel Less Wrong Meetup - Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 May 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, Mat 15th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>IMPORTANT NOTE: The time above might say 6pm or 7pm or 8pm depending on how daylight savings time is processed. The meetup is at 7pm Israel Local Time (Which is DST right now).</p>\n\n<p>This time we're going to have a social meetup! We'll be socializing and playing games.</p>\n\n<p>Specifically, we look forward to playing any cool board or card game anyone will bring.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda. (We've decided to start slightly earlier this time to give us more time and accommodate people with different schedules).</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is). If you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>Things that might happen: - You'll trade cool ideas with cool people from the Israel LW community. - You'll discover kindred spirits who agree with you about one/two boxing. - You'll kick someone's ass (and teach them how you did it) at some awesome boardgame. - You'll discover how to build a friendly AGI running on cold fusion (well probably not)</p>\n\n<p>Things that will happen for sure: - You'll get to hang out with awesome people and have fun!</p>\n\n<p>There is also talk of food and beers, and if you'd like to bring some too - that would be great. (But you don't have to).</p>\n\n<p>If you have any question feel free to email Anatoly at avorobey@gmail.com or call him at 054-245-1060. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/103'>Israel Less Wrong Meetup - Social and Board Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kZJtAhocN3XJeR2gK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.723680178531247e-06, "legacy": true, "legacyId": "26213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games\">Discussion article for the meetup : <a href=\"/meetups/103\">Israel Less Wrong Meetup - Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 May 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, Mat 15th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>IMPORTANT NOTE: The time above might say 6pm or 7pm or 8pm depending on how daylight savings time is processed. The meetup is at 7pm Israel Local Time (Which is DST right now).</p>\n\n<p>This time we're going to have a social meetup! We'll be socializing and playing games.</p>\n\n<p>Specifically, we look forward to playing any cool board or card game anyone will bring.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda. (We've decided to start slightly earlier this time to give us more time and accommodate people with different schedules).</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is). If you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>Things that might happen: - You'll trade cool ideas with cool people from the Israel LW community. - You'll discover kindred spirits who agree with you about one/two boxing. - You'll kick someone's ass (and teach them how you did it) at some awesome boardgame. - You'll discover how to build a friendly AGI running on cold fusion (well probably not)</p>\n\n<p>Things that will happen for sure: - You'll get to hang out with awesome people and have fun!</p>\n\n<p>There is also talk of food and beers, and if you'd like to bring some too - that would be great. (But you don't have to).</p>\n\n<p>If you have any question feel free to email Anatoly at avorobey@gmail.com or call him at 054-245-1060. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games1\">Discussion article for the meetup : <a href=\"/meetups/103\">Israel Less Wrong Meetup - Social and Board Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games", "level": 1}, {"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-13T18:04:19.023Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - An introduction to digital cryptography", "slug": "meetup-boston-an-introduction-to-digital-cryptography", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hPm6wcDz72kCGxqgq/meetup-boston-an-introduction-to-digital-cryptography", "pageUrlRelative": "/posts/hPm6wcDz72kCGxqgq/meetup-boston-an-introduction-to-digital-cryptography", "linkUrl": "https://www.lesswrong.com/posts/hPm6wcDz72kCGxqgq/meetup-boston-an-introduction-to-digital-cryptography", "postedAtFormatted": "Tuesday, May 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20An%20introduction%20to%20digital%20cryptography&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20An%20introduction%20to%20digital%20cryptography%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPm6wcDz72kCGxqgq%2Fmeetup-boston-an-introduction-to-digital-cryptography%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20An%20introduction%20to%20digital%20cryptography%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPm6wcDz72kCGxqgq%2Fmeetup-boston-an-introduction-to-digital-cryptography", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPm6wcDz72kCGxqgq%2Fmeetup-boston-an-introduction-to-digital-cryptography", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/104'>Boston - An introduction to digital cryptography</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">MIT, 25 Ames St, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Joe Schneider will be presenting an introduction to digital cryptography, starting at 4pm.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/104'>Boston - An introduction to digital cryptography</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hPm6wcDz72kCGxqgq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7239252143447165e-06, "legacy": true, "legacyId": "26215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___An_introduction_to_digital_cryptography\">Discussion article for the meetup : <a href=\"/meetups/104\">Boston - An introduction to digital cryptography</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">MIT, 25 Ames St, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Joe Schneider will be presenting an introduction to digital cryptography, starting at 4pm.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___An_introduction_to_digital_cryptography1\">Discussion article for the meetup : <a href=\"/meetups/104\">Boston - An introduction to digital cryptography</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - An introduction to digital cryptography", "anchor": "Discussion_article_for_the_meetup___Boston___An_introduction_to_digital_cryptography", "level": 1}, {"title": "Discussion article for the meetup : Boston - An introduction to digital cryptography", "anchor": "Discussion_article_for_the_meetup___Boston___An_introduction_to_digital_cryptography1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-13T21:28:21.409Z", "modifiedAt": null, "url": null, "title": "Meetup : RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? ", "slug": "meetup-rtlw-durham-location-changed-cognitive-load-and", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MSjBNcQRD3CeHuCWg/meetup-rtlw-durham-location-changed-cognitive-load-and", "pageUrlRelative": "/posts/MSjBNcQRD3CeHuCWg/meetup-rtlw-durham-location-changed-cognitive-load-and", "linkUrl": "https://www.lesswrong.com/posts/MSjBNcQRD3CeHuCWg/meetup-rtlw-durham-location-changed-cognitive-load-and", "postedAtFormatted": "Tuesday, May 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20RTLW%20(Durham)%2C%20LOCATION%20CHANGED%3A%20Cognitive%20Load%20and%20Decision%20Fatigue%2C%20or%2C%20Can%20You%20Defrag%20Your%20Brain%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20RTLW%20(Durham)%2C%20LOCATION%20CHANGED%3A%20Cognitive%20Load%20and%20Decision%20Fatigue%2C%20or%2C%20Can%20You%20Defrag%20Your%20Brain%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMSjBNcQRD3CeHuCWg%2Fmeetup-rtlw-durham-location-changed-cognitive-load-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20RTLW%20(Durham)%2C%20LOCATION%20CHANGED%3A%20Cognitive%20Load%20and%20Decision%20Fatigue%2C%20or%2C%20Can%20You%20Defrag%20Your%20Brain%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMSjBNcQRD3CeHuCWg%2Fmeetup-rtlw-durham-location-changed-cognitive-load-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMSjBNcQRD3CeHuCWg%2Fmeetup-rtlw-durham-location-changed-cognitive-load-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/105'>RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2411 N. Roxboro St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion: What do we know about cognitive load? How does it affect us? What can we do to mitigate it?</p>\n\n<p>7:00 gather, procure beverages <br />\n7:30 discuss <br />\n9 or 9:30 adjourn, possibly to Fullsteam</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/105'>RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MSjBNcQRD3CeHuCWg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "26217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___RTLW__Durham___LOCATION_CHANGED__Cognitive_Load_and_Decision_Fatigue__or__Can_You_Defrag_Your_Brain__\">Discussion article for the meetup : <a href=\"/meetups/105\">RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2411 N. Roxboro St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion: What do we know about cognitive load? How does it affect us? What can we do to mitigate it?</p>\n\n<p>7:00 gather, procure beverages <br>\n7:30 discuss <br>\n9 or 9:30 adjourn, possibly to Fullsteam</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___RTLW__Durham___LOCATION_CHANGED__Cognitive_Load_and_Decision_Fatigue__or__Can_You_Defrag_Your_Brain__1\">Discussion article for the meetup : <a href=\"/meetups/105\">RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? </a></h2>", "sections": [{"title": "Discussion article for the meetup : RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? ", "anchor": "Discussion_article_for_the_meetup___RTLW__Durham___LOCATION_CHANGED__Cognitive_Load_and_Decision_Fatigue__or__Can_You_Defrag_Your_Brain__", "level": 1}, {"title": "Discussion article for the meetup : RTLW (Durham), LOCATION CHANGED: Cognitive Load and Decision Fatigue, or, Can You Defrag Your Brain? ", "anchor": "Discussion_article_for_the_meetup___RTLW__Durham___LOCATION_CHANGED__Cognitive_Load_and_Decision_Fatigue__or__Can_You_Defrag_Your_Brain__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-13T21:46:48.131Z", "modifiedAt": null, "url": null, "title": "What do rationalists think about the afterlife?", "slug": "what-do-rationalists-think-about-the-afterlife", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.038Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CNqB5sH3ASa2cRCpi/what-do-rationalists-think-about-the-afterlife", "pageUrlRelative": "/posts/CNqB5sH3ASa2cRCpi/what-do-rationalists-think-about-the-afterlife", "linkUrl": "https://www.lesswrong.com/posts/CNqB5sH3ASa2cRCpi/what-do-rationalists-think-about-the-afterlife", "postedAtFormatted": "Tuesday, May 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20do%20rationalists%20think%20about%20the%20afterlife%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20do%20rationalists%20think%20about%20the%20afterlife%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNqB5sH3ASa2cRCpi%2Fwhat-do-rationalists-think-about-the-afterlife%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20do%20rationalists%20think%20about%20the%20afterlife%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNqB5sH3ASa2cRCpi%2Fwhat-do-rationalists-think-about-the-afterlife", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNqB5sH3ASa2cRCpi%2Fwhat-do-rationalists-think-about-the-afterlife", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 331, "htmlBody": "<p>I've read a fair amount on Less Wrong and can't recall much said about the plausibility of some sort of afterlife. What do you guys think about it? Is there some sort of consensus?</p>\n<p>Here's my take:</p>\n<ul>\n<li>Rationality is all about using the past to make predictions about the future.</li>\n<li>\"What happens to our consciousness when we die?\" (may not be worded precisely, but hopefully you know what I mean).</li>\n<li>We have some data on what preconditions seem to produce consciousness (ie. neuronal firing). However, this is just data on the preconditions that seem to produce consciousness <em>that can/do communicate/demonstrate its consciousness to us.</em></li>\n<li>Can we say that a different set of preconditions <em>doesn't</em>&nbsp;produce consciousness? I personally don't see reason to believe this. I see 3 possibilities that we don't have reason to reject, because we have no data on them. I'm still confused and not too confident in this belief though.</li>\n<li>Possibility 1) Maybe the 'other' conscious beings <em>don't want to</em> communicate their consciousness to us.</li>\n<li>Possibility 2) Maybe the 'other' conscious beings <em>can't </em>communicate their consciousness to us ever.</li>\n<li>Possibility 3) Maybe the 'other' conscious beings can't communicate their consciousness to us <em>given our level of technology.</em></li>\n<li>And finally, since we have no data, what can we say about the <em>likelihood </em>of our consciousness returning/remaining after we die? I would say the chances are 50/50. For something you have no data on, any outcome is equally likely (This feels like something that must have been talked about before. So side-question: is this logic sound?).</li>\n</ul>\n<p>Edit: People in the comments have just taken it as a given that consciousness resides solely in the brain without explaining why they think this. My point in this post is that I don't see why we have reason to reject the 3 possibilities above. If you reject the idea that consciousness could reside outside of the brain, please explain why.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CNqB5sH3ASa2cRCpi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": -28, "extendedScore": null, "score": -9.5e-05, "legacy": true, "legacyId": "26218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-13T23:05:39.642Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal - Fun and games", "slug": "meetup-montreal-fun-and-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:10.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GLtXwhDnZFTTXRvep/meetup-montreal-fun-and-games", "pageUrlRelative": "/posts/GLtXwhDnZFTTXRvep/meetup-montreal-fun-and-games", "linkUrl": "https://www.lesswrong.com/posts/GLtXwhDnZFTTXRvep/meetup-montreal-fun-and-games", "postedAtFormatted": "Tuesday, May 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20-%20Fun%20and%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20-%20Fun%20and%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGLtXwhDnZFTTXRvep%2Fmeetup-montreal-fun-and-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20-%20Fun%20and%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGLtXwhDnZFTTXRvep%2Fmeetup-montreal-fun-and-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGLtXwhDnZFTTXRvep%2Fmeetup-montreal-fun-and-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/106'>Montreal - Fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 May 2014 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4109 Ch. C\u00f4te des Neiges Apt 14, Montr\u00e9al, QC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come relax on a Saturday afternoon/evening, there's bound to be some great conversation, fun games and new friends! I'm thinking of doing something like playing the game Werewolf, or maybe we can play the Clicker game (if you don't know either of these games, you'll see, they're really fun).</p>\n\n<p>If you have any trouble finding the place, send me a PM and I'll send you my phone number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/106'>Montreal - Fun and games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GLtXwhDnZFTTXRvep", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7243401841204492e-06, "legacy": true, "legacyId": "26219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal___Fun_and_games\">Discussion article for the meetup : <a href=\"/meetups/106\">Montreal - Fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 May 2014 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4109 Ch. C\u00f4te des Neiges Apt 14, Montr\u00e9al, QC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come relax on a Saturday afternoon/evening, there's bound to be some great conversation, fun games and new friends! I'm thinking of doing something like playing the game Werewolf, or maybe we can play the Clicker game (if you don't know either of these games, you'll see, they're really fun).</p>\n\n<p>If you have any trouble finding the place, send me a PM and I'll send you my phone number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal___Fun_and_games1\">Discussion article for the meetup : <a href=\"/meetups/106\">Montreal - Fun and games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal - Fun and games", "anchor": "Discussion_article_for_the_meetup___Montreal___Fun_and_games", "level": 1}, {"title": "Discussion article for the meetup : Montreal - Fun and games", "anchor": "Discussion_article_for_the_meetup___Montreal___Fun_and_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T06:53:38.656Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Vienna Meetup", "slug": "meetup-lw-vienna-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RationalityVienna", "createdAt": "2014-01-27T13:42:25.040Z", "isAdmin": false, "displayName": "RationalityVienna"}, "userId": "mquibPrcrRF7TByie", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JFriMgoajwNGqDzpn/meetup-lw-vienna-meetup", "pageUrlRelative": "/posts/JFriMgoajwNGqDzpn/meetup-lw-vienna-meetup", "linkUrl": "https://www.lesswrong.com/posts/JFriMgoajwNGqDzpn/meetup-lw-vienna-meetup", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Vienna%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Vienna%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJFriMgoajwNGqDzpn%2Fmeetup-lw-vienna-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Vienna%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJFriMgoajwNGqDzpn%2Fmeetup-lw-vienna-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJFriMgoajwNGqDzpn%2Fmeetup-lw-vienna-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/107'>LW Vienna Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 May 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Reichsratsstrasse 17, 1010 Vienna, Austria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup at Cafe Votiv. \nThe progression of the Rationality movement in Vienna, and the GBS.\nThinking Fast and Slow Part 2 by Andreas, followed by a discussion.\nNewcomers welcome. \nPlease register for the FB event: <a href=\"https://www.facebook.com/events/272454069604212/\" rel=\"nofollow\">https://www.facebook.com/events/272454069604212/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/107'>LW Vienna Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JFriMgoajwNGqDzpn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7249849800628703e-06, "legacy": true, "legacyId": "26220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Vienna_Meetup\">Discussion article for the meetup : <a href=\"/meetups/107\">LW Vienna Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 May 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Reichsratsstrasse 17, 1010 Vienna, Austria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup at Cafe Votiv. \nThe progression of the Rationality movement in Vienna, and the GBS.\nThinking Fast and Slow Part 2 by Andreas, followed by a discussion.\nNewcomers welcome. \nPlease register for the FB event: <a href=\"https://www.facebook.com/events/272454069604212/\" rel=\"nofollow\">https://www.facebook.com/events/272454069604212/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Vienna_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/107\">LW Vienna Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Vienna Meetup", "anchor": "Discussion_article_for_the_meetup___LW_Vienna_Meetup", "level": 1}, {"title": "Discussion article for the meetup : LW Vienna Meetup", "anchor": "Discussion_article_for_the_meetup___LW_Vienna_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T10:13:19.826Z", "modifiedAt": null, "url": null, "title": "Book Review: The Reputation Society. Part I", "slug": "book-review-the-reputation-society-part-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:28.919Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4Q9cnMiBbQniy3ceW/book-review-the-reputation-society-part-i", "pageUrlRelative": "/posts/4Q9cnMiBbQniy3ceW/book-review-the-reputation-society-part-i", "linkUrl": "https://www.lesswrong.com/posts/4Q9cnMiBbQniy3ceW/book-review-the-reputation-society-part-i", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20The%20Reputation%20Society.%20Part%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20The%20Reputation%20Society.%20Part%20I%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Q9cnMiBbQniy3ceW%2Fbook-review-the-reputation-society-part-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20The%20Reputation%20Society.%20Part%20I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Q9cnMiBbQniy3ceW%2Fbook-review-the-reputation-society-part-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Q9cnMiBbQniy3ceW%2Fbook-review-the-reputation-society-part-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1363, "htmlBody": "<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><em><a href=\"https://mitpress.mit.edu/books/reputation-society\">The Reputation Society</a></em>&nbsp;(MIT Press, 2012), edited by Hassan Masum and Mark Tovey, is an anthology on&nbsp;the possibilities of using online rating and reputation systems to systematically disseminate information about virtually everything - people, goods and services, ideas, etc., etc. Even though the use of online rating systems is an overarching theme, the book is, however, quite heterogeneous (like many anthologies). I have therefore chosen to structure the material in a somewhat different way. This post consists of a short introduction to the book, while in the <a href=\"/lw/k86/book_review_the_reputation_society_part_ii/\">next, far longer post</a>, I list a number of concepts and distinctions commented on by the authors (either explicitly or implicitly) and briefly summarize their take on them.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">My hope is that this Wiki-style approach maximizes the amount of information per line of text. Also, though these concepts and distinctions are arguably the most useful stuff in the book, they are </span>unfortunately not gathered in any one place in the book. Hence I think that my list should be of use for those that go on to read the book, or parts of it. I also hope that this list of entries could be a start to a series of <a href=\"http://wiki.lesswrong.com/wiki/LessWrong_Wiki\">Less Wrong Wiki</a>&nbsp;entries on reputation systems. Moreover, it could be a good point of departure for general discussions on rating and reputation systems. I would be happy to receive feedback on this choice of presentation form (as well as on the content, of course).</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">A <a href=\"http://ernestgellner.wordpress.com/2014/05/13/the-reputation-society/\">chapter-by-chapter review</a></span><span lang=\"EN-US\">&nbsp;(more of a guide to what chapters to read, really) can be found on my blog. (This review is already too long which is why I put the chapter-by-chapter overview there rather than here at Less Wrong.)&nbsp;<a href=\"http://www.suffolk.edu/documents/jhtl_book_reviews/Sadarangani11.pdf\">Monique Sadarangani</a></span><span lang=\"EN-US\">&nbsp;has also written a review (which focuses on various legal aspects of online rating systems). Another associated text you might consider reading is Masum's and Yi-Cheng Zhang's <a href=\"http://firstmonday.org/article/view/1158/1078\">\"Manifesto for the Reputation Society\"</a> (2004).</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> <strong>Introduction</strong><strong><br /> <!--[if !supportLineBreakNewLine]--><br /> <!--[endif]--></strong></span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">People have of course always relied on others' recommendations on a massive scale. We often don't have time to figure out who is reliable and who is not, what goods are worth buying and what are not, which university education is valued by employers and which is not, etc. Instead we look to the testimonies and recommendations of others.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> As pointed out by several of the authors, these recommendations have, however, often been given in a quite unsystematic fashion. In small societies, this lack of systematicity and structure was, though, to some extent outweighed by the wealth of information you would obtain about any individual person or item. Everybody knew everybody, which meant that a crook would sooner or later typically be identified as such, even though information about people&rsquo;s trustworthiness was not being spread in an organized, rational fashion.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> However, when people moved into cities, it became easier for dishonest people to hide in the crowds. One-off encounters with strangers became much more common, and with them the incentives to cheat increased: these strangers could typically not identify you, which meant that your reputation was not damaged by dishonorable behavior (see chs. 4, 6).</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> The inhabitants of cities, particularly those working in professions such as trade, tried to counter these problems by forming associations which guaranteed that their members conducted themselves properly (or else they would be thrown out). As the complexity of society has increased, so has the number and efficiency of these <em>recommendation and reputation systems </em>(italicized terms appear as entries in the next post). Today there are countless organizations that keep track of the creditworthiness of individuals (e.g., &nbsp;<a href=\"http://en.wikipedia.org/wiki/FICO\">FICO</a>), companies and countries (e.g., <span style=\"text-decoration: underline;\">Standard &amp; Poor and Moody</span>), the quality of education (e.g.,<span>&nbsp;</span><a href=\"http://www.theguardian.com/education/table/2012/may/21/university-league-table-2013\">The Guardian's University League Table</a>, which provides an influential&nbsp;annual university ranking in the UK), the quality of restaurants (<a href=\"http://en.wikipedia.org/wiki/Michelin_Guide\">Guide Michelin</a>), etc.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> As virtually all of the authors argue, the Internet offers, however, spectacular opportunities for constructing rating systems that are more reliable and vastly much more pervasive than anything yet seen. The editors sum up this optimism in their introduction&nbsp;</span>(Location 182, 2<sup>nd</sup>&nbsp;page of introduction):</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\">&nbsp;</p>\n<blockquote>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">In today&rsquo;s world, reliable advice from others&rsquo; experience is often unavailable, whether for buying products, choosing a service, or judging a policy. One promising solution is to bring to reputation a similar kind of systematization as currencies, laws, and accounting brought to primitive barter economies. Properly designed reputation systems have the potential to reshape society for the better by shining the light of accountability into dark places, through the mediated judgments of billions of people worldwide, to create what we call the<span>&nbsp;</span><em>Reputation Society</em>.&nbsp;</span></p>\n</blockquote>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> There are of course already a great number of Internet rating systems, including those used by Google, Facebook (the like system), Amazon, eBay, Slashdot, Yelp, Netflix, Reddit, and, not to forget, Less Wrong. Many of these systems are discussed in the book (not Less Wrong, though). In particular, the authors try to assess what we can learn from the successes and failures of these rating sites. </span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">There is a general (though seldom explicitly stated) sentiment in the book that the existing rating systems do not nearly exhaust the opportunities that the Internet provides us with. I certainly share this sentiment. As pointed out in ch. 5 (see the entry <em>Underutilization of reputational information</em>), people make a great number of &ldquo;private judgments&rdquo; in their heads which it would be very useful for others to learn about, but which they do not share. If they could be persuaded to share them to a greater extent, the social gains would be huge. If consumers got more reliable information about the quality of different goods and services (via consumer rating systems), the providers of those items would be forced to increase the quality of their products. In some areas, this is already happening, but other areas are lagging. The potential gains stretch far beyond goods and services, though: public debates would be conducted more rationally if rating systems penalized bullshitters (ch. 15), government would be better run if its actions and policies were rated in a rational way (ch. 13), science could improve if peers rated others&rsquo; work in a more rational way than they do at present (chs. 10-12). You would even imagine people rating your life plans, your behavior, and other stuff that is primarily interesting to yourself. Only imagination puts a limit to the potential uses of rating systems.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\"><br /> Even though there is some research on rating systems - on which<span>&nbsp;</span><a href=\"http://dellarocas.com/\">Chrysantos (Chris) Dellarocas</a>, the author of the first chapter, is an expert - most rating systems seem to be created in a quite unsystematic, trial-and-error fashion. Instead we should draw from the full range of the social sciences &ndash; e.g., from psychology, sociology, economics, law, history, anthropology and political sciences &ndash; when constructing such systems. I am convinced that we could benefit greatly as a society if we spent more time and resources on the construction of efficient rating systems. I also think that the Less Wrong community, with its combination of an intellectually curious and rational attitude and strong programming skills, potentially has a lot to contribute here.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\">At the same time, one shouldn't get over-optimistic. There are lots of hurdles to pass. I certainly do not share the wild optimism of Craig Newmark, the founder of <a href=\"http://en.wikipedia.org/wiki/Craigslist\">Craigslist</a>, who writes as follows in the foreword (Location 70, 1<sup>st</sup>&nbsp;page of Foreword):&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span lang=\"EN-US\">&nbsp;</span></p>\n<blockquote>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\">By the end of this decade, power and influence will have shifted largely to those people with the best reputations and trust networks and away from people with money and nominal power. That is, peer networks will confer legitimacy on people emerging from the grassroots.</p>\n</blockquote>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\">These bold words remind me of some similarly bold predictions of how prediction markets, Wikis, and other forms of collective enterprises in the spirit of the <a href=\"http://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">Wisdom of the crowd</a>&nbsp;will transform society and especially human knowledge, made in Cass Sunstein&rsquo;s <a href=\"http://www.amazon.com/Infotopia-Many-Minds-Produce-Knowledge/dp/B007SRWJB6\"><em>Infotopia</em></a><em> &nbsp;</em>(2006). So far, Sunstein&rsquo;s predictions haven&rsquo;t been borne out and the odds don&rsquo;t look too good for Newmark, either. I do agree with Newmark that there is a huge potential in rating systems, but realizing that potential is not going to happen by itself. It will take lots of testing, lots of ingenuity, lots of hard work,&nbsp;and certainly a considerably greater amount of time than Newmark believes, to do that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4Q9cnMiBbQniy3ceW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 1.7252602409417933e-06, "legacy": true, "legacyId": "26194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["X2c7LoX36pKwW9ic9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T10:16:34.380Z", "modifiedAt": null, "url": null, "title": "Book review: The Reputation Society. Part II", "slug": "book-review-the-reputation-society-part-ii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X2c7LoX36pKwW9ic9/book-review-the-reputation-society-part-ii", "pageUrlRelative": "/posts/X2c7LoX36pKwW9ic9/book-review-the-reputation-society-part-ii", "linkUrl": "https://www.lesswrong.com/posts/X2c7LoX36pKwW9ic9/book-review-the-reputation-society-part-ii", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20review%3A%20The%20Reputation%20Society.%20Part%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20review%3A%20The%20Reputation%20Society.%20Part%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2c7LoX36pKwW9ic9%2Fbook-review-the-reputation-society-part-ii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20review%3A%20The%20Reputation%20Society.%20Part%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2c7LoX36pKwW9ic9%2Fbook-review-the-reputation-society-part-ii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2c7LoX36pKwW9ic9%2Fbook-review-the-reputation-society-part-ii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2471, "htmlBody": "<p style=\"margin: 0cm; margin-bottom: .0001pt;\">This is the second part of my book review of<span>&nbsp;</span><em><a href=\"http://www.amazon.co.uk/The-Reputation-Society-Reshaping-Information/dp/0262016648/\">The Reputation Society</a></em>. See the first part for an overview of the structure of the review.&nbsp;</p>\n<p style=\"margin: 0cm 0cm 0.0001pt;\">&nbsp;</p>\n<p style=\"margin: 0cm 0cm 0.0001pt;\"><span style=\"text-align: justify; text-indent: -18pt;\"><span style=\"font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><strong><em>Central concepts of </em>The Reputation Society</strong></span></span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt;\">&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt;\"><em style=\"text-align: justify; text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Aggregation of reputational information. </strong></span></em><span style=\"text-align: justify; text-indent: -18pt; font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">Since the book is entirely untechnical, and since aggregation rules by their nature are mathematical formulae, there isn&rsquo;t much on aggregation rules (i.e., on how we are to aggregate individuals' ranking of, e.g., a person or a product, into one overall rating) in the book. The choice of aggregation rules is, however, obviously very important to optimize the different <em>functions of reputation systems</em>.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt;\"><span style=\"text-align: justify; text-indent: -18pt; font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">One problem that is discussed, though, is whether the aggregation rules should be transparent or not (e.g., in chs. 1 and 3). Concealing them makes it harder for participants to game the system, but on the other hand it makes it easier for the system providers to game the system (for instance, Google has famously been accused of manipulating search results for money). Hence concealment of the aggregation rules can damage the credibility of the site. (See also </span><em style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">Display of reputational information</em><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">.)</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em></em><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Altruism vs self-interest</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong> <em>as incentives in rating systems</em>.</strong> An important question for any rating system is whether it should appeal to people&rsquo;s altruism (their community spirit) or to their self-interest. Craig Newmark (foreword) seems to take the former route, arguing that &ldquo;people are normally trustworthy&rdquo;, whereas the authors of ch. 11 argue that scientists need to be given incentives that appeal to their self-interest to take part in their reputation system.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">It could be argued that the success of Wikipedia shows that appealing to people&rsquo;s self-interest is not necessary to get them to contribute. On the other hand, it could also be argued that the notion that Wikipedia has been successful is due to a lack of imagination concerning the potential of sites with user-generated content. Perhaps Wikipedia would have been still more successful if they had given contributors stronger incentives.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Anonymity in online systems</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>. </strong>Dellarocas (ch. 1) emphasizes that letting social network users remain anonymous while failing to guard against the creation of multiple identities facilitates <em>gaming </em>greatly. On the other hand, prohibitions against remaining anonymous might raise <em>privacy </em>concerns.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Display of reputational information.</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong> </strong>Dellarocas (ch. 1, Location 439, p. 7) discusses a number of ways of displaying reputational information:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<ol>\n<li><span style=\"text-indent: -18pt; font-size: 12pt; line-height: 115%; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">Simple statistics (number of transactions, etc.</span></li>\n<li><span style=\"font-size: 10pt; text-indent: -18pt; line-height: 115%; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><span style=\"font-size: 7pt; line-height: normal; font-family: 'Times New Roman';\">&nbsp;</span></span><span style=\"text-indent: -18pt; font-size: 12pt; line-height: 115%; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">Star ratings (e.g. Amazon reviews)</span></li>\n<li><span style=\"text-indent: -18pt; font-size: 12pt; line-height: 115%; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">Numerical scores (e.g., eBay&rsquo;s reputation score)</span></li>\n<li><span style=\"text-indent: -18pt; font-size: 12pt; line-height: 115%; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">Numbered tiers (e.g., World of Warcraft player levels)</span></li>\n<li><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt; line-height: 115%; text-indent: -18pt;\">Achievement badges (e.g., Yelp elite reviewer)</span></li>\n<li><span style=\"text-indent: -18pt; font-size: 12pt; line-height: 115%; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">Leaderboards (lists where users are ranked relative to other users; e.g. list of Amazon top reviewers.</span></li>\n</ol>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><span style=\"font-size: 12.0pt; line-height: 115%; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">See <em>gaming</em> for a brief discussion of the advantages and disadvantages of comparative (e.g., 6) and non-comparative systems (e.g., 5).<br />&nbsp;&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><em style=\"text-align: justify; text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Expert</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong> <em>vs peer rating systems</em>. </strong>Most pre-Internet rating systems were ran by experts (e.g., movie guides, restaurant guides, etc.) Internet has created huge opportunities for rating systems where large number of non-expert ratings and votes are aggregated into an overall rating. Proponents of the <a href=\"http://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">Wisdom of the crowd</a>&nbsp;argue that even though many non-experts are not very reliable, the noise tends to even out as the number of rater grows, and we are left with an aggregated judgment which can beat that of experienced experts.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">However, the Internet also offers new ways of identifying experts (emphasized, e.g., in ch. 8). People whose written recommendations are popular, or whose ratings are reliable as measured against some objective standard (if such a standard can be constructed &ndash; that obviously depends on context) can be given a special status. For instance, their recommendations can become more visible, and their ratings more heavily weighted. It could be argued that such systems are more meritocratic ways of identifying the experts than the ones that dominate society today (see, e.g., ch. 8).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Explicit</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><em><strong> vs implicit reputation systems.</strong></em> In the former, your reputation is a function of other users&rsquo; votes, whereas in the latter, your reputation is derived from other forms of behavior (e.g., the number of readers of your posts, your number of successful transactions, etc.). This is a distinction made by several authors, but unfortunately they use different terms for it, something which is never acknowledged. Here the editors should have done a better job.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">In the language of economics, the implicit reputation systems (such as Google&rsquo;s page rank system) are, by and large, based on people's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Revealed_preference\">revealed preferences</a></span><span style=\"font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\">&nbsp;- by their actions &ndash; whereas explicit reputation systems are built on their stated preferences. Two main advantages of revealed preferences are that we typically get them for free (since we infer them from publically observable behavior that people do for other reasons &ndash; e.g., &nbsp;making a link to a page &ndash; whereas we need to ask people if we want their stated preferences) and that they typically express people&rsquo;s true preferences (whereas their stated preferences might be false &ndash; see <em>untruthful reporting</em>). On the other hand, we typically only get quite coarse-grained information about people&rsquo;s preferences by observing their behavior (e.g., observing that John chose a Toyota over a Ford does not tell us whether he did that because it was cheaper, or because of a preference for Japanese cars, or because of its lower fuel consumption, etc.), whereas we can get more fine-grained information about their preferences by asking them to state them.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Functions</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong> <em>of reputation systems.</em></strong> Dellarocas (ch. 1, Location 364, p. 4) argue that online reputation systems have the following functions (to varying degrees, depending on the system):</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">a) a socializing function (rewarding desired behavior and punish undesired one; build trust). As pointed in chs. 6 and 7, this makes reputation systems an alternative to other systems intended to socialize people; in particular government regulation (backed by the threat of force). This should make reputation systems especially interesting to those opposed to the latter (e.g., libertarians).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">b) an information-filtering function (makes reliable information more visible).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">c) a matching function (matching users with similar interests and tastes in, e.g., restaurants or films &ndash; this is similar to b) with the difference that it is not assumed that some users are more reliable than others).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">d) a user lock-in function &ndash; users who have spent considerable amounts of time creating a good reputation on one site are unlikely to change to another site where they have to start from scratch.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Gaming</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>. </strong>Gaming has been a massive problem at many sites making use of reputation systems. In general, more competitive/comparative <em>displays of reputational information </em>exacerbate gaming problems (as pointed out in ch. 2). On the other hand, strong incentives to gain a good reputation are to some extent necessary to solve the <em>undersupply of reputational information</em> problem.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">Dellarocas (ch. 1) emphasizes that it is impossible to create a system that is totally secure from manipulation. Manipulators will continuously come up with new gaming strategies, and therefore the site&rsquo;s providers constantly have to update its rules. The situation is, however, quite analogous to the interplay between tax evaders and legislators and hence these problems are not unique to online rating systems by any means.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><strong>Global vs personalized/local trust metrics</strong></span></em><span style=\"font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><strong> </strong>(Massa, ch. 14). While the former gives the same assessments of the trustworthiness of person <em>X</em> to each other person <em>Y</em>, the latter gives different assessments of the trustworthiness of <em>X</em> to different people. Thus, the former are comprised of statements such as </span><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&ldquo;the reputation of Carol is .4&rdquo;, the latter of statements such as &ldquo;Alice should trust Carol to degree .9&rdquo; and &ldquo;Bob should trust Carol to degree .1&rdquo; (Location 3619, p. 155). Different people may trust others to different degrees based on their beliefs and preferences, and this is reflected in the personalized trust metrics. Massa argues that a major problem with global rating systems is that they lead to &ldquo;the tyranny of the majority&rdquo;, where original views are unfairly down-voted. At the same time, he also argues that the use of personalized trust metrics may lead to the formation of echo chambers, where people only listen to those who agree with them.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Immune system disorders of reputation systems</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"> (Foreword). Rating systems can be seen as &ldquo;immune systems&rdquo; intended to give protection against undesirable behavior and unreliable information. However, they can also give rise to diseases of their own. For instance, the academic &ldquo;rating systems&rdquo; based mainly on number of articles and numbers of citations famously give rise to all sorts of undesirable behavior (see section IV, chs. 10-12, on the use of rating/reputation systems in science). An optimal rating system would of course minimize these immune system disorders.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Karma as currency</strong></span></em><span style=\"font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><strong>. </strong>This idea is developed in several chapters (e.g., 1 and 2) but especially in the last chapter (18) by Madeline Ashby and Cory Doctorow, two science fiction writers. They envision a reputation-based future society where people earn <a href=\"http://en.wikipedia.org/wiki/Whuffie\">&ldquo;Whuffie&rdquo;</a> &nbsp;&ndash; Karma or reputation &ndash; when they are talked about, and spend it when they talk about others. You can also exchange Whuffie for goods and services, effectively making it a currency.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Moderation</strong></span></em><span style=\"text-indent: -18pt; font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><strong>.</strong> Moderation is to some extent an alternative to ratings in online forums. Moderators could either be paid professionals, or picked from the community of users (the latter arguably being more cost-efficient; ch. 2). The moderators can in turn be moderated in a <em>meta-moderation</em> system used, e.g., by Slashdot (their system is discussed by several of the authors).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"text-indent: -18pt; font-size: 12pt; font-family: 'Times New Roman', serif;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">Yet another system which in effect is a version of the meta-moderation system is the <em><a href=\"http://www.hks.harvard.edu/fs/rzeckhau/elicit.pdf\">peer-prediction model</a></em> (see ch. 1), in which your ratings are assessed on the basis of whether they manage to predict subsequent ratings. These later ratings then in effect function as meta-ratings of your ratings. &nbsp;&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Privacy</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"> &ndash; several authors raise concerns over privacy (in particular chs. 16-18). In a fully-fledged reputation society, everything you did would be recorded and counted either for or against you. (Such a society would thus be very much like life according to many religions &ndash; the vicious would get punished, the virtuous rewarded &ndash; with the crucial difference that the punishments and rewards would be given in this life rather than in the after-life.) While this certainly could improve behavior (see <em>Functions of reputation systems</em>) it could also make society hard and unforgiving (or so several authors argue; see especially ch. 17). People have argued that it therefore should be possible to undergo &ldquo;reputational bankruptcy&rdquo; (cf. forgiveness of sins in, e.g., Catholicism), to escape one&rsquo;s past, as it were, but as Eric Goldman points out (ch. 5, Location 1573, p. 59), this would allow people to get away with anti-social behavior without any reputational consequences, and hence make the reputation system&rsquo;s socializing effects much weaker.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">As stated in the introduction, in small villages people often have more reliable information about others&rsquo; past behavior and their general trustworthiness. This makes the villages&rsquo; informal reputation systems very powerful, but it is also to some extent detrimental to privacy. The story of the free-thinker who leaves the village where everyone knows everything about everyone for the freedom of the anonymous city is a perennial one in literature.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">It could be argued, thus, that there is necessarily a trade-off between the efficiency of a reputation system, and the degree to which it protects people&rsquo;s privacy. (See also </span><em style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">Anonymity in online systems</em><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\"> for more on this.) According to this line of reasoning, privacy encroachments are </span><em style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">immune system disorders of reputation systems</em><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">. It is a challenger for the architect of reputation systems to minimize this, and other, immune system disorders.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Referees</strong> </span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&ndash; all rating systems need to be overlooked by referees. The received view seems to be that they need to be independent and impartial, and the question is raised whether private companies such as Google can function as such trustworthy and impartial referees (ch. 3). An important problem with regards to this is who &ldquo;guards the guards?&rdquo;. In ch. 3, John Henry Clippinger argues that this problem, which &ldquo;has been the Achilles heel of human institutions since times immemorial&rdquo; (Location 1046, p. 33) can be overcome in online reputation systems. The key, he argues, is transparency:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><br /></span></p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">In situations in which both activities and their associated reputation systems become fully digital, they can in principle be made fully transparent and auditable. Hence the activities of interested parties to subvert or game policies or reputation metrics can themselves be monitored, flagged, and defended against.</span></p>\n</blockquote>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></em><em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em></em><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Reporting bias</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong> </strong>(ch. 1) &ndash; e.g., that people refrain from giving negative votes for</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">&nbsp;fear of retaliation. Obviously this is more likely to happen in systems where it is publically visible how you have voted. Another form of reporting bias is due to a certain good or service only being consumed by fans, who tend to give high ratings.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Reputation systems vs recommendation systems.</strong>&nbsp;</span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US; mso-bidi-font-style: italic;\" lang=\"EN-US\">This is a simple terminological distinction: reputation systems are ratings of people, recommendations systems are ratings of goods and services. I use &ldquo;rating systems&rdquo; as a general term covering both reputation and recommendation system.</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US; mso-bidi-font-style: italic;\" lang=\"EN-US\"><br /></span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Undersupply of reputational information</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>;</strong> i.e. that people don&rsquo;t rate as much as is socially optimal. This is also a concept mentioned by several authors, but in most detail in ch. 5 (Location 1520, p. 57):</span></p>\n<p style=\"margin: 0cm; margin-bottom: .0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></em></p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"font-size: 12.0pt; line-height: 115%; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">Much reputational information starts out as non-public (i.e. &ldquo;private&rdquo;) information in the form of a customer&rsquo;s subjective impressions about his or her interactions with a vendor. To the extent that this information remains private, it does not help other consumers make marketplace decisions. These collective mental impressions represent a vital but potentially underutilized social resource.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">The fact that private information remains locked in consumers&rsquo; head could represent a marketplace failure. If the social benefit from making reputational information public exceeds the private benefit, public reputational information will be undersupplied.</span></p>\n</blockquote>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt;\">Personally I think this is a massively underappreciated problem. People get countless such subjective impressions every day. At present we harvest but a tiny portion of these subjective impressions, or judgments, as a community. If the authors&rsquo; vision is to stand a chance of getting realized, we need to make people share these judgements to a much greater extent than they do today. (It goes without saying that we also need to distinguish the reliable ones from the unreliable ones).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Universal</strong></span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong> vs. <em>constrained</em> <em>(or contextual) reputation systems</em>. </strong>(ch. 17)<strong>&nbsp;</strong>The former are a function of your behavior across all contexts, and influences your reputation in all contexts, whereas the latter are rather constrained to a particular context (say selling and buying stuff on eBay).&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-align: justify;\"><em style=\"text-indent: -18pt;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\"><strong>Untruthful reporting</strong> </span></em><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-language: EN-US;\" lang=\"EN-US\">(ch. 1). This can happen either because raters try to <em>game </em>the system (e.g., in order to benefit themselves, their restaurant, or what not) or because of vandalism/trolling. Taking a leaf out of Bryan Caplan&rsquo;s \"<a href=\"http://www.cato.org/sites/cato.org/files/pubs/pdf/pa594.pdf\">The Myth of the Rational Voter\",</a> I&rsquo;d like to add that even people who are neither gaming or trolling&nbsp;</span><span style=\"font-family: 'Times New Roman', serif; font-size: 12pt; text-indent: -18pt;\">typically spend less time and effort giving accurate ratings for others&rsquo; benefit than they do when they make decisions that influence their own pockets. Presumably this will decrease the level of accuracy of their ratings.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X2c7LoX36pKwW9ic9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 1.7252647113110966e-06, "legacy": true, "legacyId": "26214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T10:28:55.354Z", "modifiedAt": null, "url": null, "title": "Want to work on \"strong AI\" topic in my bachelor thesis ", "slug": "want-to-work-on-strong-ai-topic-in-my-bachelor-thesis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.161Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kotrfa", "createdAt": "2014-01-09T19:06:28.770Z", "isAdmin": false, "displayName": "kotrfa"}, "userId": "PNgajXg435nt8gCWo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7qC6iHW8m9hcyQbGt/want-to-work-on-strong-ai-topic-in-my-bachelor-thesis", "pageUrlRelative": "/posts/7qC6iHW8m9hcyQbGt/want-to-work-on-strong-ai-topic-in-my-bachelor-thesis", "linkUrl": "https://www.lesswrong.com/posts/7qC6iHW8m9hcyQbGt/want-to-work-on-strong-ai-topic-in-my-bachelor-thesis", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Want%20to%20work%20on%20%22strong%20AI%22%20topic%20in%20my%20bachelor%20thesis%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWant%20to%20work%20on%20%22strong%20AI%22%20topic%20in%20my%20bachelor%20thesis%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7qC6iHW8m9hcyQbGt%2Fwant-to-work-on-strong-ai-topic-in-my-bachelor-thesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Want%20to%20work%20on%20%22strong%20AI%22%20topic%20in%20my%20bachelor%20thesis%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7qC6iHW8m9hcyQbGt%2Fwant-to-work-on-strong-ai-topic-in-my-bachelor-thesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7qC6iHW8m9hcyQbGt%2Fwant-to-work-on-strong-ai-topic-in-my-bachelor-thesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p>Hello,</p>\n<p>I currently study maths, physics and programming (general course) on CVUT at Prague (CZE). I'm <strong>finishing second year</strong> and I'm really into AI. The <strong>most interesting questions</strong> for me are:</p>\n<ul>\n<li>what formalism to use for connecting epistemology questions (about knowledge, memory...) and cognitive sciences with maths and how to formulate them</li>\n<li>find principles of those and trying to \"materialize\" them into new models</li>\n<li>I'm also kind of philosophy-like questions about AI</li>\n</ul>\n<div>It is clear to me, that <strong>I'm not able to work on these problems fully</strong>, because of my lack of knowledge. Despite that, I'd like to <strong>find a field</strong>, where I could work on at least similar topics. Currently, I'm working on datamining project, but for last few months I don't find it fulfilling as I'd expected. On my university there is plenty of possibilities in multi-agent systems, \"weak AI\" (e.g well-known drone navigation), brain simulations and so on. As it seems to me, no one is really seriously <strong>maintaining with something like MIRI</strong>, nor they are presenting something what has as least same direction.&nbsp;</div>\n<div><br /></div>\n<div>The only group which is working on \"strong AI\", is kind of closed (it is sponsored by philanthropist Marek Rosa) and they are not interested in students as I am (partly understandable).</div>\n<div><a id=\"more\"></a></div>\n<div><strong>My questions are:</strong></div>\n<div>\n<ul>\n<li>Am I naive to hope that I can do anything useful and fulfilling (based on the given data) in this area (\"strong AI\")?</li>\n<li>Where (and for what) should I try to find someone (something) who could help me in the next progress in my studies in this area?&nbsp;</li>\n<li>Do you think my enthusiasm is going in good direction? Haven't I bitten off more than I could chew? Where should I go then?</li>\n</ul>\n<div>Thank you very much for any advice.</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7qC6iHW8m9hcyQbGt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 1.7252817373760737e-06, "legacy": true, "legacyId": "26221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T13:00:02.670Z", "modifiedAt": null, "url": null, "title": "[LINK] Utilitarian self-driving cars?", "slug": "link-utilitarian-self-driving-cars", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "V_V", "createdAt": "2012-08-19T23:47:08.798Z", "isAdmin": false, "displayName": "V_V"}, "userId": "4sqF3byZGkwwXmCt4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yrctFompYpisvGwmv/link-utilitarian-self-driving-cars", "pageUrlRelative": "/posts/yrctFompYpisvGwmv/link-utilitarian-self-driving-cars", "linkUrl": "https://www.lesswrong.com/posts/yrctFompYpisvGwmv/link-utilitarian-self-driving-cars", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Utilitarian%20self-driving%20cars%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Utilitarian%20self-driving%20cars%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrctFompYpisvGwmv%2Flink-utilitarian-self-driving-cars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Utilitarian%20self-driving%20cars%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrctFompYpisvGwmv%2Flink-utilitarian-self-driving-cars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrctFompYpisvGwmv%2Flink-utilitarian-self-driving-cars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>When a collision is unavoidable, should a self-driving car try to maximize the survival chances of its occupants, or of all people involved?</p>\n<p><a href=\"http://www.wired.com/2014/05/the-robot-car-of-tomorrow-might-just-be-programmed-to-hit-you/\">http://www.wired.com/2014/05/the-robot-car-of-tomorrow-might-just-be-programmed-to-hit-you/<br /><br /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z5A4c4kjTgLSFEr3h": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yrctFompYpisvGwmv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 1.7254901103230151e-06, "legacy": true, "legacyId": "26222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T13:27:30.586Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup - possibly in a park", "slug": "meetup-london-social-meetup-possibly-in-a-park-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qXs9gHYtZYR8v8dt3/meetup-london-social-meetup-possibly-in-a-park-2", "pageUrlRelative": "/posts/qXs9gHYtZYR8v8dt3/meetup-london-social-meetup-possibly-in-a-park-2", "linkUrl": "https://www.lesswrong.com/posts/qXs9gHYtZYR8v8dt3/meetup-london-social-meetup-possibly-in-a-park-2", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqXs9gHYtZYR8v8dt3%2Fmeetup-london-social-meetup-possibly-in-a-park-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqXs9gHYtZYR8v8dt3%2Fmeetup-london-social-meetup-possibly-in-a-park-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqXs9gHYtZYR8v8dt3%2Fmeetup-london-social-meetup-possibly-in-a-park-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/108'>London social meetup - possibly in a park</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>If the weather is nice, we'll head to <a href=\"https://goo.gl/maps/Tdy1r\" rel=\"nofollow\">Lincoln&#39;s Inn Fields</a>, probably somewhere in the northwest quadrant. If not, we'll be in the usual Shakespeare's Head. If the weather is variable, we might move from one to the other - give me a call or text if you're not sure. My number is 07792009646.</p>\n\n<p><strong>Update</strong>: Weather's nice, the park it is!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/108'>London social meetup - possibly in a park</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qXs9gHYtZYR8v8dt3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "26223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park\">Discussion article for the meetup : <a href=\"/meetups/108\">London social meetup - possibly in a park</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>If the weather is nice, we'll head to <a href=\"https://goo.gl/maps/Tdy1r\" rel=\"nofollow\">Lincoln's Inn Fields</a>, probably somewhere in the northwest quadrant. If not, we'll be in the usual Shakespeare's Head. If the weather is variable, we might move from one to the other - give me a call or text if you're not sure. My number is 07792009646.</p>\n\n<p><strong>Update</strong>: Weather's nice, the park it is!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park1\">Discussion article for the meetup : <a href=\"/meetups/108\">London social meetup - possibly in a park</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup - possibly in a park", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park", "level": 1}, {"title": "Discussion article for the meetup : London social meetup - possibly in a park", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T18:19:59.555Z", "modifiedAt": null, "url": null, "title": "[LINK] Sentient Robots Not Possible According To Math Proof", "slug": "link-sentient-robots-not-possible-according-to-math-proof", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:25.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Darklight", "createdAt": "2013-09-01T21:44:31.447Z", "isAdmin": false, "displayName": "Darklight"}, "userId": "3ovWJeXAjCSj9Lwg6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8T9ZCR35zxx6nPLfx/link-sentient-robots-not-possible-according-to-math-proof", "pageUrlRelative": "/posts/8T9ZCR35zxx6nPLfx/link-sentient-robots-not-possible-according-to-math-proof", "linkUrl": "https://www.lesswrong.com/posts/8T9ZCR35zxx6nPLfx/link-sentient-robots-not-possible-according-to-math-proof", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Sentient%20Robots%20Not%20Possible%20According%20To%20Math%20Proof&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Sentient%20Robots%20Not%20Possible%20According%20To%20Math%20Proof%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8T9ZCR35zxx6nPLfx%2Flink-sentient-robots-not-possible-according-to-math-proof%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Sentient%20Robots%20Not%20Possible%20According%20To%20Math%20Proof%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8T9ZCR35zxx6nPLfx%2Flink-sentient-robots-not-possible-according-to-math-proof", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8T9ZCR35zxx6nPLfx%2Flink-sentient-robots-not-possible-according-to-math-proof", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p>Based on Tononi's earlier work on Integrated Information Theory, apparently, Maguire et al. have come up with a formulation of consciousness as a lossless integration of information that requires noncomputable functions, which implies that consciousness cannot be modeled computationally.</p>\n<p>http://www.newscientist.com/article/dn25560-sentient-robots-not-possible-if-you-do-the-maths.html</p>\n<p>I'm personally skeptical of this, but their paper (seen here:&nbsp;http://arxiv.org/abs/1405.0126v1) has some impressive looking formal mathematical proofs that I will admit I lack the mathematical competence to judge the veracity of. &nbsp;Anyone with greater mathematical acumen want to take a look?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8T9ZCR35zxx6nPLfx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -6, "extendedScore": null, "score": 1.725931414454618e-06, "legacy": true, "legacyId": "26224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-14T21:15:16.524Z", "modifiedAt": null, "url": null, "title": "Meetup : Christchurch, New Zealand Meetup", "slug": "meetup-christchurch-new-zealand-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "free_rip", "createdAt": "2010-04-04T09:56:57.893Z", "isAdmin": false, "displayName": "free_rip"}, "userId": "q8zJHtRt24KhQ6fSs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hJD9Pw8ixQZ8BYYz6/meetup-christchurch-new-zealand-meetup", "pageUrlRelative": "/posts/hJD9Pw8ixQZ8BYYz6/meetup-christchurch-new-zealand-meetup", "linkUrl": "https://www.lesswrong.com/posts/hJD9Pw8ixQZ8BYYz6/meetup-christchurch-new-zealand-meetup", "postedAtFormatted": "Wednesday, May 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Christchurch%2C%20New%20Zealand%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Christchurch%2C%20New%20Zealand%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJD9Pw8ixQZ8BYYz6%2Fmeetup-christchurch-new-zealand-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Christchurch%2C%20New%20Zealand%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJD9Pw8ixQZ8BYYz6%2Fmeetup-christchurch-new-zealand-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhJD9Pw8ixQZ8BYYz6%2Fmeetup-christchurch-new-zealand-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 231, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/109'>Christchurch, New Zealand Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2014 04:30:00PM (+1200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Room 901, James Hight, University of Canterbury, Christchurch</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second Chch meetup is about to begin! 4.30pm this Sunday. We already have three confirmed attendees so if you live in Chch and would like to connect with the Less Wrong community here it's well worth the effort.</p>\n\n<p>It will be held in James Hight library, discussion room 901, at the University of Canterbury. You do not need to be a student or in any way affiliated with UC to come along, the discussion rooms are open to everyone.</p>\n\n<p>The way to get there is to go through the front (and only) doors of the library, walk forwards until you see elevators to your right, and take one of those elevators to floor 9. Then walk around the edges of the floor until you see room 901. Don't take the stairs, they only lead up one floor! If you need clearer directions or anticipate getting lost, PM or post below and I can clear it up and give you a contact cell number.</p>\n\n<p>Last time was a lot of fun, spanning topics from self-enhancing drugs,  what the bayesian/frequentist debate is all about, cool blogs to follow and the ways in which a Giant might attempt to dismember a Sky Whale.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/109'>Christchurch, New Zealand Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hJD9Pw8ixQZ8BYYz6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.726173266579541e-06, "legacy": true, "legacyId": "26225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Christchurch__New_Zealand_Meetup\">Discussion article for the meetup : <a href=\"/meetups/109\">Christchurch, New Zealand Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2014 04:30:00PM (+1200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Room 901, James Hight, University of Canterbury, Christchurch</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second Chch meetup is about to begin! 4.30pm this Sunday. We already have three confirmed attendees so if you live in Chch and would like to connect with the Less Wrong community here it's well worth the effort.</p>\n\n<p>It will be held in James Hight library, discussion room 901, at the University of Canterbury. You do not need to be a student or in any way affiliated with UC to come along, the discussion rooms are open to everyone.</p>\n\n<p>The way to get there is to go through the front (and only) doors of the library, walk forwards until you see elevators to your right, and take one of those elevators to floor 9. Then walk around the edges of the floor until you see room 901. Don't take the stairs, they only lead up one floor! If you need clearer directions or anticipate getting lost, PM or post below and I can clear it up and give you a contact cell number.</p>\n\n<p>Last time was a lot of fun, spanning topics from self-enhancing drugs,  what the bayesian/frequentist debate is all about, cool blogs to follow and the ways in which a Giant might attempt to dismember a Sky Whale.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Christchurch__New_Zealand_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/109\">Christchurch, New Zealand Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Christchurch, New Zealand Meetup", "anchor": "Discussion_article_for_the_meetup___Christchurch__New_Zealand_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Christchurch, New Zealand Meetup", "anchor": "Discussion_article_for_the_meetup___Christchurch__New_Zealand_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T00:05:41.059Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Short Talks Meetup", "slug": "meetup-washington-dc-short-talks-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pz4Ki2Z673dSANDst/meetup-washington-dc-short-talks-meetup", "pageUrlRelative": "/posts/pz4Ki2Z673dSANDst/meetup-washington-dc-short-talks-meetup", "linkUrl": "https://www.lesswrong.com/posts/pz4Ki2Z673dSANDst/meetup-washington-dc-short-talks-meetup", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Short%20Talks%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Short%20Talks%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpz4Ki2Z673dSANDst%2Fmeetup-washington-dc-short-talks-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Short%20Talks%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpz4Ki2Z673dSANDst%2Fmeetup-washington-dc-short-talks-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpz4Ki2Z673dSANDst%2Fmeetup-washington-dc-short-talks-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10a'>Washington DC Short Talks Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to listen to/give short talks (again)!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10a'>Washington DC Short Talks Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pz4Ki2Z673dSANDst", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.726408451270525e-06, "legacy": true, "legacyId": "26226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Short_Talks_Meetup\">Discussion article for the meetup : <a href=\"/meetups/10a\">Washington DC Short Talks Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to listen to/give short talks (again)!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Short_Talks_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/10a\">Washington DC Short Talks Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Short Talks Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Short_Talks_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Short Talks Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Short_Talks_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T01:41:06.197Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Meetup - May", "slug": "meetup-sydney-meetup-may-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zF4yrKFwFW4fF6Dh6/meetup-sydney-meetup-may-0", "pageUrlRelative": "/posts/zF4yrKFwFW4fF6Dh6/meetup-sydney-meetup-may-0", "linkUrl": "https://www.lesswrong.com/posts/zF4yrKFwFW4fF6Dh6/meetup-sydney-meetup-may-0", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Meetup%20-%20May&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Meetup%20-%20May%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzF4yrKFwFW4fF6Dh6%2Fmeetup-sydney-meetup-may-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Meetup%20-%20May%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzF4yrKFwFW4fF6Dh6%2Fmeetup-sydney-meetup-may-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzF4yrKFwFW4fF6Dh6%2Fmeetup-sydney-meetup-may-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10b'>Sydney Meetup - May</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 May 2014 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>I'll book another table under the name \"less wrong\". Last meetup we were in the restaurant on level 2. When I arrive I'll facebook about where exactly the table is located.</p>\n\n<p>We'll have general discussion over dinner, followed by a rationality exercise and more specific discussion-topic.</p>\n\n<p>The theme this month is: \"errors of social human interactions and our solutions\"</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10b'>Sydney Meetup - May</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zF4yrKFwFW4fF6Dh6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7265401656228994e-06, "legacy": true, "legacyId": "26227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___May\">Discussion article for the meetup : <a href=\"/meetups/10b\">Sydney Meetup - May</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 May 2014 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>I'll book another table under the name \"less wrong\". Last meetup we were in the restaurant on level 2. When I arrive I'll facebook about where exactly the table is located.</p>\n\n<p>We'll have general discussion over dinner, followed by a rationality exercise and more specific discussion-topic.</p>\n\n<p>The theme this month is: \"errors of social human interactions and our solutions\"</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___May1\">Discussion article for the meetup : <a href=\"/meetups/10b\">Sydney Meetup - May</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Meetup - May", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___May", "level": 1}, {"title": "Discussion article for the meetup : Sydney Meetup - May", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___May1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T05:55:29.536Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt", "slug": "meetup-frankfurt-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LrE4S7tQfvCyA7C2E/meetup-frankfurt-2", "pageUrlRelative": "/posts/LrE4S7tQfvCyA7C2E/meetup-frankfurt-2", "linkUrl": "https://www.lesswrong.com/posts/LrE4S7tQfvCyA7C2E/meetup-frankfurt-2", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrE4S7tQfvCyA7C2E%2Fmeetup-frankfurt-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrE4S7tQfvCyA7C2E%2Fmeetup-frankfurt-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrE4S7tQfvCyA7C2E%2Fmeetup-frankfurt-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10c'>Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 May 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup! Location still unclear, contact our mailing list for more information. You can find our mailing list on the list of LW meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10c'>Frankfurt</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LrE4S7tQfvCyA7C2E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7268914057851623e-06, "legacy": true, "legacyId": "26228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt\">Discussion article for the meetup : <a href=\"/meetups/10c\">Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 May 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another meetup! Location still unclear, contact our mailing list for more information. You can find our mailing list on the list of LW meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt1\">Discussion article for the meetup : <a href=\"/meetups/10c\">Frankfurt</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T09:11:15.791Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow meet up", "slug": "meetup-moscow-meet-up", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x3S4YnE6tQjc7gEXd/meetup-moscow-meet-up", "pageUrlRelative": "/posts/x3S4YnE6tQjc7gEXd/meetup-moscow-meet-up", "linkUrl": "https://www.lesswrong.com/posts/x3S4YnE6tQjc7gEXd/meetup-moscow-meet-up", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20meet%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20meet%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3S4YnE6tQjc7gEXd%2Fmeetup-moscow-meet-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20meet%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3S4YnE6tQjc7gEXd%2Fmeetup-moscow-meet-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3S4YnE6tQjc7gEXd%2Fmeetup-moscow-meet-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10d'>Moscow meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have:</p>\n\n<ul>\n<li><p>Probe phenomenon and limits of corporeality, report.</p></li>\n<li><p>Small reports about cognitive biases.</p></li>\n<li><p>What is pragmatics and semantic solutions of some of its issues, report.</p></li>\n<li><p>Unicorns: false but useful beliefs, report.</p></li>\n<li><p>Belief investigation, exercise.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the first revolving door under the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10d'>Moscow meet up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x3S4YnE6tQjc7gEXd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.7271617968660527e-06, "legacy": true, "legacyId": "26229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_meet_up\">Discussion article for the meetup : <a href=\"/meetups/10d\">Moscow meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have:</p>\n\n<ul>\n<li><p>Probe phenomenon and limits of corporeality, report.</p></li>\n<li><p>Small reports about cognitive biases.</p></li>\n<li><p>What is pragmatics and semantic solutions of some of its issues, report.</p></li>\n<li><p>Unicorns: false but useful beliefs, report.</p></li>\n<li><p>Belief investigation, exercise.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the first revolving door under the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_meet_up1\">Discussion article for the meetup : <a href=\"/meetups/10d\">Moscow meet up</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow meet up", "anchor": "Discussion_article_for_the_meetup___Moscow_meet_up", "level": 1}, {"title": "Discussion article for the meetup : Moscow meet up", "anchor": "Discussion_article_for_the_meetup___Moscow_meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T10:31:12.910Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-17", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jJGnaxSWbhD83MEHa/meetup-melbourne-social-meetup-17", "pageUrlRelative": "/posts/jJGnaxSWbhD83MEHa/meetup-melbourne-social-meetup-17", "linkUrl": "https://www.lesswrong.com/posts/jJGnaxSWbhD83MEHa/meetup-melbourne-social-meetup-17", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjJGnaxSWbhD83MEHa%2Fmeetup-melbourne-social-meetup-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjJGnaxSWbhD83MEHa%2Fmeetup-melbourne-social-meetup-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjJGnaxSWbhD83MEHa%2Fmeetup-melbourne-social-meetup-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10e'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 May 2014 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7/24 Green St, Windsor 3181 VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our social meetup is on as regularly scheduled! This month we're meeting in Windsor.</p>\n\n<p>Come along for a friendly chat and maybe a game or two. We'll organise something for dinner if people are interested.</p>\n\n<p>You can buzz in when you get there, or call Ryan on 0438869257 or me on 0421231789.</p>\n\n<p>Ryan notes: parking can get pretty full out the front but you might be able to find spots in nearby streets.</p>\n\n<p>(Apologies for the late notice)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10e'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jJGnaxSWbhD83MEHa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7272722449260074e-06, "legacy": true, "legacyId": "26230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/10e\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 May 2014 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7/24 Green St, Windsor 3181 VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our social meetup is on as regularly scheduled! This month we're meeting in Windsor.</p>\n\n<p>Come along for a friendly chat and maybe a game or two. We'll organise something for dinner if people are interested.</p>\n\n<p>You can buzz in when you get there, or call Ryan on 0438869257 or me on 0421231789.</p>\n\n<p>Ryan notes: parking can get pretty full out the front but you might be able to find spots in nearby streets.</p>\n\n<p>(Apologies for the late notice)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/10e\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T12:53:45.469Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne June Rationality Dojo: Memory", "slug": "meetup-melbourne-june-rationality-dojo-memory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MqnqrA2ffGQsCf3A9/meetup-melbourne-june-rationality-dojo-memory", "pageUrlRelative": "/posts/MqnqrA2ffGQsCf3A9/meetup-melbourne-june-rationality-dojo-memory", "linkUrl": "https://www.lesswrong.com/posts/MqnqrA2ffGQsCf3A9/meetup-melbourne-june-rationality-dojo-memory", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20June%20Rationality%20Dojo%3A%20Memory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20June%20Rationality%20Dojo%3A%20Memory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMqnqrA2ffGQsCf3A9%2Fmeetup-melbourne-june-rationality-dojo-memory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20June%20Rationality%20Dojo%3A%20Memory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMqnqrA2ffGQsCf3A9%2Fmeetup-melbourne-june-rationality-dojo-memory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMqnqrA2ffGQsCf3A9%2Fmeetup-melbourne-june-rationality-dojo-memory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10f'>Melbourne June Rationality Dojo: Memory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 King Street, West Melbourne, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Continuing the succession of immensely successful dojos, Megan will present in June on memory.</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo ('I will have done X by the next Dojo'). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call me on 0425-855-124.\nIf you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10f'>Melbourne June Rationality Dojo: Memory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MqnqrA2ffGQsCf3A9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.727469188600594e-06, "legacy": true, "legacyId": "26231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_June_Rationality_Dojo__Memory\">Discussion article for the meetup : <a href=\"/meetups/10f\">Melbourne June Rationality Dojo: Memory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 King Street, West Melbourne, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Continuing the succession of immensely successful dojos, Megan will present in June on memory.</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo ('I will have done X by the next Dojo'). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call me on 0425-855-124.\nIf you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_June_Rationality_Dojo__Memory1\">Discussion article for the meetup : <a href=\"/meetups/10f\">Melbourne June Rationality Dojo: Memory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne June Rationality Dojo: Memory", "anchor": "Discussion_article_for_the_meetup___Melbourne_June_Rationality_Dojo__Memory", "level": 1}, {"title": "Discussion article for the meetup : Melbourne June Rationality Dojo: Memory", "anchor": "Discussion_article_for_the_meetup___Melbourne_June_Rationality_Dojo__Memory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-15T20:10:11.534Z", "modifiedAt": null, "url": null, "title": "Common sense quantum mechanics", "slug": "common-sense-quantum-mechanics", "viewCount": null, "lastCommentedAt": "2019-05-04T00:27:01.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dvasya", "createdAt": "2011-03-08T00:30:12.369Z", "isAdmin": false, "displayName": "dvasya"}, "userId": "2484AHxytrNyQXajh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5zgHkio95otxnjsWY/common-sense-quantum-mechanics", "pageUrlRelative": "/posts/5zgHkio95otxnjsWY/common-sense-quantum-mechanics", "linkUrl": "https://www.lesswrong.com/posts/5zgHkio95otxnjsWY/common-sense-quantum-mechanics", "postedAtFormatted": "Thursday, May 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Common%20sense%20quantum%20mechanics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommon%20sense%20quantum%20mechanics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zgHkio95otxnjsWY%2Fcommon-sense-quantum-mechanics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Common%20sense%20quantum%20mechanics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zgHkio95otxnjsWY%2Fcommon-sense-quantum-mechanics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zgHkio95otxnjsWY%2Fcommon-sense-quantum-mechanics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1758, "htmlBody": "<p><strong>Related to: <a href=\"/lw/r5/the_quantum_physics_sequence/\" target=\"_blank\">Quantum physics sequence</a>.</strong></p>\n<p><em>TLDR: Quantum mechanics can be derived from the rules of probabilistic reasoning. The wavefunction is a mathematical vehicle to transform a nonlinear problem into a linear one. The Born rule that is so puzzling for MWI results from the particular mathematical form of this functional substitution.</em></p>\n<p>This is a brief overview a recent paper in <em>Annals of Physics</em>&nbsp;(recently mentioned in <a href=\"/r/discussion/lw/k7i/link_quantum_theory_as_the_most_robust/\" target=\"_self\">Discussion</a>):</p>\n<p><a href=\"http://dx.doi.org/10.1016/j.aop.2014.04.021\" target=\"_blank\">Quantum theory as the most robust description of reproducible experiments</a>&nbsp;(<a href=\"http://arxiv.org/abs/1303.4574\" target=\"_blank\">arXiv</a>)</p>\n<p>by&nbsp;<span>Hans De Raedt</span><span>,&nbsp;</span><span>Mikhail I. Katsnelson</span><span>, and&nbsp;</span><span>Kristel Michielsen. Abstract:</span></p>\n<div>\n<blockquote>\n<p>It is shown that the basic equations of quantum theory can be obtained from a straightforward application of logical inference to experiments for which there is uncertainty about individual events and for which the frequencies of the observed events are robust with respect to small changes in the conditions under which the experiments are carried out.</p>\n</blockquote>\n<p><span>In a nutshell, the authors use the \"plausible reasoning\" rules (as in, e.g., Jaynes' <em>Probability Theory</em>) to recover the quantum-physical results for the EPR and Stern</span>&ndash;<span>Gerlach experiments by adding a notion of experimental reproducibility in a mathematically well-formulated way and <em>without any \"quantum\" assumptions</em>. Then they show how the Schrodinger equation (SE) can be obtained from the nonlinear variational problem on the probability <em>P</em> for the particle-in-a-potential problem when the classical Hamilton-Jacobi equation holds \"on average\". The SE allows to transform the nonlinear variational problem into a linear one, and in the course of said transformation, the (real-valued) probability <em>P</em>&nbsp;and the action <em>S</em>&nbsp;are combined in a single complex-valued function ~<em>P</em><sup>1/2</sup>exp(<em>iS</em>) which becomes the argument of SE (the wavefunction).</span></p>\n<p>This casts the <a href=\"/lw/py/the_born_probabilities/\" target=\"_blank\">\"serious mystery\" of Born probabilities</a> in a new light. Instead of the observed frequency being the square(d amplitude) of the \"physically fundamental\" wavefunction, the wavefunction is seen as a mathematical vehicle to convert a difficult nonlinear variational problem for inferential probability into a manageable linear PDE, where it so happens that the probability enters the wavefunction under a square root.</p>\n<p>Below I will excerpt some math from the paper, mainly to show that the approach actually works, but outlining just the key steps. This will be followed by some general discussion and reflection.</p>\n<p><strong>1. Plausible reasoning and reproducibility</strong></p>\n<p>The authors start from the usual desiderata that are well laid out in Jaynes' <em>Probability Theory</em>&nbsp;and elsewhere, and add to them another condition:</p>\n<blockquote>\n<p><span>There may be uncertainty about each event. The conditions under which the experiment is carried out may be uncertain. The frequencies with which events are observed are reproducible and robust against small changes in the conditions.</span></p>\n</blockquote>\n<p>Mathematically, this is a requirement that the probability&nbsp;<em>P</em>(<em>x</em>|<em>&theta;</em>,<em>Z</em>) of observation <em>x</em>&nbsp;given an uncertain experimental parameter <em>&theta;</em>&nbsp;and the rest of out knowledge <em>Z</em>, is maximally&nbsp;<em>robust</em> to small changes in&nbsp;<em>&theta;</em>&nbsp;and independent of&nbsp;<em>&theta;</em>. Using log-probabilities, this amounts to minimizing the \"evidence\"</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\mathrm{Ev}=\\ln\\frac{P\\left(x,y|\\theta+\\epsilon,Z\\right)}{P\\left(x,y|\\theta,Z\\right)}\" alt=\"\" width=\"190\" height=\"44\" /></p>\n<p>for any small <em>&epsilon;</em>&nbsp;so that |Ev| is not a function of&nbsp;<em>&theta;</em>&nbsp;(but the probability is).</p>\n<p><strong>2. The Einstein</strong><strong>&ndash;</strong><strong>Podolsky</strong><strong>&ndash;</strong><strong>Rosen</strong><strong>&ndash;</strong><strong>Bohm experiment</strong></p>\n<p>There is a source S that, when activated, sends a pair of signals to two routers R<sub>1,2</sub>. Each router then sends the signal to one of its two detectors <em>D<sub>i</sub></em><sub>+,&ndash;</sub>&nbsp;(<em>i</em>=1,2). Each router can be rotated and we denote as&nbsp;<em>&theta;</em>&nbsp;the angle between them. The experiment is repeated <em>N</em>&nbsp;times yielding the data set {<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>}, {<em>x</em><sub>2</sub>,<em>y</em><sub>2</sub>}, ... {<em>x<sub>N</sub></em>,<em>y<sub>N</sub></em>} where&nbsp;<em>x</em>&nbsp;and&nbsp;<em>y</em>&nbsp;are the outcomes from the two detectors (+1 or&nbsp;&ndash;1). We want to find the probability <em>P</em>(<em>x</em>,<em>y</em>|<em>&theta;</em>,<em>Z</em>).</p>\n<p>After some calculations it is found that the single-trial probability can be expressed as&nbsp;<em>P</em>(<em>x</em>,<em>y</em>|<em>&theta;</em>,<em>Z</em>)&nbsp;= (1 + <em>xyE</em><sub>12</sub>(<em>&theta;</em>)&nbsp;) / 4, where <em>E</em><sub>12</sub>(<em>&theta;</em>) = &Sigma;<sub>x,y=+&ndash;1</sub> <em>xy</em><em>P</em>(<em>x</em>,<em>y</em>|<em>&theta;</em>,<em>Z</em>)&nbsp;is a periodic function.</p>\n<p>From the properties of Bernoulli trials it follows that, for a data set of <em>N</em>&nbsp;trials with <em>n<sub>xy</sub></em>&nbsp;total outcomes of each type {<em>x</em>,<em>y</em>},</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\mathrm{Ev} = \\sum_{x,y=\\pm1} n_{xy} \\ln\\frac{P\\left(x,y|\\theta+\\epsilon,Z\\right)}{P\\left(x,y|\\theta,Z\\right)}\" alt=\"\" width=\"266\" height=\"50\" /></p>\n<p>and expanding this in a Taylor series it is found that</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\mathrm{Ev} = -\\frac{N\\epsilon^{2}}{2} \\sum_{x,y=\\pm1} \\frac{1}{P\\left(x,y|\\theta,Z\\right)} \\left(\\frac{\\partial P\\left(x,y|\\theta,Z\\right)}{\\partial\\theta}\\right)^{2} + O(\\epsilon^{3})\" alt=\"\" width=\"452\" height=\"55\" /></p>\n<p>The expression in the sum is the Fisher information <em>I<sub>F</sub></em> for <em>P</em>. The maximum robustness requirement means it must be minimized. Writing it down as <em>I<sub>F</sub></em> = 1/(1 &ndash;&nbsp;<em>E</em><sub>12</sub>(<em>&theta;</em>)<sup>2</sup>) (<em>dE</em><sub>12</sub>(<em>&theta;</em>)/<em>d</em><em>&theta;</em>)<sup>2</sup> one finds that <em>E</em><sub>12</sub>(<em>&theta;</em>) = cos(<em>&theta;</em><em>I<sub>F</sub></em><sup>1/2</sup> + <em>&phi;</em>), and since <em>E</em><sub>12</sub> must be periodic in angle, <em>I<sub>F</sub></em><sup>1/2</sup> is a natural number, so the smallest possible value is <em>I<sub>F</sub></em> = 1. Choosing <em>&phi;&nbsp;</em>=&nbsp;<em>&pi;</em> it is found that <em>E</em><sub>12</sub>(<em>&theta;</em>) =&nbsp;&ndash;cos(<em>&theta;</em>), and we obtain the result that</p>\n</div>\n<p style=\"padding-left: 30px;\"><em></em><img src=\"http://www.codecogs.com/png.latex?P\\left(x,y|\\theta,Z\\right) = \\frac{1 - \\cos\\theta}{4}\" alt=\"\" width=\"188\" height=\"38\" /></p>\n<p>which is the well-known correlation of two spin-1/2 particles in the singlet state.</p>\n<blockquote>\n<div>\n<p>Needless to say, our derivation did not use any concepts of quantum theory. Only plain, rational reasoning strictly complying with the rules of logical inference and some elementary facts about the experiment were used</p>\n</div>\n</blockquote>\n<p><strong>3. The Stern</strong><strong>&ndash;</strong><strong>Gerlach experiment</strong></p>\n<p>This case is analogous and simpler than the previous one. The setup contains a source emitting a particle with magnetic moment <strong>S</strong>, a magnet with field in the direction <strong>a</strong>, and two detectors <em>D</em><sub>+</sub> and <em>D</em><sub>&ndash;</sub><em>.</em></p>\n<p>Similarly to the previous section,&nbsp;<em>P</em>(<em>x</em>|<em>&theta;</em>,<em>Z</em>)&nbsp;= (1 +&nbsp;<em>xE</em>(<em>&theta;</em>)&nbsp;) / 2, where&nbsp;<em>E</em>(<em>&theta;</em>) = <em>P</em>(+|<em>&theta;</em>,<em>Z</em>)&nbsp;&ndash;&nbsp;<em>P</em>(&ndash;|<em>&theta;</em>,<em>Z</em>)&nbsp;is an unknown periodic function. By complete analogy we seek the minimum of <em>I<sub>F</sub></em>&nbsp;and find that <em>E</em>(<em>&theta;</em>) = +&ndash;cos(<em>&theta;</em>), so that</p>\n<p style=\"padding-left: 30px;\"><em></em></p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?P\\left(x|\\theta,Z\\right) = \\frac{\\left(1 + x\\mathbf{a}\\cdot\\mathbf{S}\\right)}{2}\" alt=\"\" width=\"193\" height=\"39\" /></p>\n<blockquote>\n<p>In quantum theory, <em>[this]</em> equation is in essence just the postulate (Born&rsquo;s rule) that the probability to observe the particle with spin up is given by the square of the absolute value of the amplitude of the wavefunction projected onto the spin-up state. Obviously, the variability of the conditions under which an experiment is carried out is not included in the quantum theoretical description. In contrast, in the logical inference approach, <em>[equation]</em> is not postulated but follows from the assumption that the (thought) experiment that is being performed yields the most reproducible results, revealing the conditions for an experiment to produce data which is described by quantum theory.</p>\n</blockquote>\n<p>To repeat: there are no wavefunctions in the present approach. The only assumption is that a dependence of outcome on particle/magnet orientation is observed with robustness/reproducibility.</p>\n<p><strong>4.&nbsp;Schrodinger equation</strong></p>\n<p>A particle is located in unknown position&nbsp;<em>&theta;</em>&nbsp;on a line segment [&ndash;<em>L</em>, <em>L</em>]. Another line segment [&ndash;<em>L</em>, <em>L</em>] is uniformly covered with detectors. A source emits a signal and the particle's response is detected by one of the detectors.</p>\n<p>After going to the continuum limit of infinitely many infinitely small detectors and accounting for translational invariance it is possible to show that the position of the particle&nbsp;<em>&theta;</em>&nbsp;and of the detector <em>x</em> can be interchanged so that <em>dP</em>(<em>x</em>|<em>&theta;</em>,<em>Z</em>)/<em>d</em><em>&theta;</em>&nbsp;= &ndash;<em>dP</em>(<em>x</em>|<em>&theta;</em>,<em>Z</em>)/<em>dx</em>.</p>\n<p>In exactly the same way as before we need to minimize Ev by minimizing the Fisher information, which is now</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: monospace; font-size: medium;\"><img src=\"http://www.codecogs.com/png.latex?I_F=\\int{\\frac{1}{P\\left(x|\\theta,Z\\right)}\\left(\\frac{\\partial P(x|\\theta,Z)}{\\partial x\\right)^{2}}dx\" alt=\"\" width=\"288\" height=\"49\" /></span></p>\n<p>However, simply solving this minimization problem will not give us anything new because nothing so far accounted for the fact that the particle moves in a potential. This needs to be built into the problem. This can be done by requiring that the classical Hamilton-Jacobi equation holds <em>on average</em>. Using the Lagrange multiplier method, we now need to minimize the functional</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?F(\\theta)=\\int{\\left\\{\\frac{1}{P\\left(x|\\theta,Z\\right)}\\left(\\frac{\\partial P(x|\\theta,Z)}{\\partial x\\right)^{2}+\\lambda\\left[\\left(\\frac{\\partial S(x)}{\\partial x}}\\right)^{2}+2m\\left[V\\left(x\\right)-E\\right]\\right]P\\left(x|\\theta,Z\\right)}\\right\\}dx\" alt=\"\" width=\"697\" height=\"55\" /></p>\n<p>Here <em>S</em>(<em>x</em>) is the action (Hamilton's principal function). This minimization yields solutions for the two functions&nbsp;<em>P</em>(<em>x|</em><em>&theta;</em>,<em>Z</em>) and <em>S</em>(<em>x</em>). It is a difficult nonlinear minimization problem, but it is possible to find a matching solution in a tractable way using a mathematical \"trick\". It is known that standard variational minimization of the functional</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?Q\\left(\\theta\\right)=\\int{\\left\\{4\\frac{\\partial\\psi^{*}\\left(x|\\theta,Z\\right)}{\\partial x}\\frac{\\partial\\psi\\left(x|\\theta,Z\\right)}{\\partial x}+2m\\lambda\\left[V\\left(x\\right)-E\\right]\\psi^{*}\\left(x|\\theta,Z\\right)\\psi\\left(x|\\theta,Z\\right)\\right\\}dx}\" alt=\"\" width=\"640\" height=\"45\" /></p>\n<p>yields the Schrodinger equation for its extrema. On the other hand, if one makes the substitution combining two real-valued functions <em>P</em>&nbsp;and <em>S</em>&nbsp;into a single complex-valued <em>&psi;</em>,</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\psi\\left(x|\\theta,Z\\right)=\\sqrt{P\\left(x|\\theta,Z\\right)}e^{iS\\left(x\\right)\\sqrt{\\lambda}/2}\" alt=\"\" width=\"263\" height=\"25\" /></p>\n<p><em>Q</em> is immediately transformed into <em>F</em>, concluding the derivation of the Schrodinger equation. Incidentally,&nbsp;<em>&psi;</em>&nbsp;is constructed so that&nbsp;<em>P</em>(<em>x|</em><em>&theta;</em>,<em>Z</em>) = |<em>&psi;</em>(<em>x</em>|<em>&theta;</em>,<em>Z</em>)|<sup>2</sup>, which is the Born rule.</p>\n<p>Summing up the meaning of Schrodinger equation in the present context:</p>\n<blockquote>\n<p>Of course, a priori there is no good reason to assume that on average there is agreement with Newtonian mechanics ...&nbsp;In other words, the time-independent Schrodinger equation describes the collective of repeated experiments ... subject to the condition that the averaged observations comply with Newtonian mechanics.</p>\n</blockquote>\n<p>The authors then proceed to derive the time-dependent SE (independently from the stationary SE) in a largely similar fashion.</p>\n<p><strong>5. What it all means</strong></p>\n<p>Classical mechanics assumes that everything about the system's state and dynamics can be known (at least in principle). It starts from axioms and proceeds to derive its conclusions deductively (as opposed to inductive reasoning). In this respect quantum mechanics is to classical mechanics what probabilistic logic is to classical logic.</p>\n<p>Quantum theory is viewed here not as a description of what really goes on at the microscopic level, but <em>as an instance of logical inference</em>:</p>\n<blockquote>\n<p>in the logical inference approach, we take the point of view that a description of our knowledge of the phenomena at a certain level is independent of the description at a more detailed level.</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>quantum theory does not provide any insight into the motion of a particle but instead describes all what can be inferred (within the framework of logical inference) from or, using Bohr&rsquo;s words, <em>said</em> about the observed data</p>\n</blockquote>\n<p>Such a treatment of QM is similar in spirit to Jaynes' <em>Information Theory and Statistical Mechanics</em>&nbsp;papers (<a href=\"http://dx.doi.org/10.1103/PhysRev.106.620\" target=\"_blank\">I</a>, <a href=\"http://dx.doi.org/10.1103/PhysRev.108.171\" target=\"_blank\">II</a>). Traditionally statistical mechanics/thermodynamics is derived bottom-up from the microscopic mechanics and a series of postulates (such as ergodicity) that allow us to progressively ignore microscopic details under strictly defined conditions. In contrast, Jaynes starts with minimum possible assumptions:</p>\n<blockquote>\n<p>\"The quantity&nbsp;<em>x</em>&nbsp;is capable of assuming the discrete values&nbsp;<em>x<sub>i</sub></em>&nbsp;... all we know is the expectation value of the function&nbsp;<em>f</em>(<em>x</em>) ... On the basis of this information, what is the expectation value of the function&nbsp;<em>g</em>(<em>x</em>)?\"</p>\n</blockquote>\n<p>and proceeds to derive the foundations of statistical physics from the maximum entropy principle. Of course, these papers deserve a separate post.</p>\n<p>This community should be particularly interested in how this all aligns with the many-worlds interpretation. Obviously, any conclusions drawn from this work can only apply to the \"quantum multiverse\" level and cannot rule out or support any other many-worlds proposals.</p>\n<p>In quantum physics, MWI does quite naturally resolve some difficult issues in the \"wavefunction-centristic\" view. However, we see that the concept wavefunction is not really central for quantum mechanics. This removes the whole problem of wavefunction collapse that MWI seeks to resolve.</p>\n<p>The Born rule is arguably a big issue for MWI. But here it essentially boils down to \"<em>x</em>&nbsp;is quadratic in&nbsp;<em>t</em>&nbsp;where&nbsp;<em>t =</em>&nbsp;sqrt(<em>x</em>)\". Without the wavefunction (only probabilities) the problem simply does not appear.</p>\n<p>Here is another interesting conclusion:</p>\n<blockquote>\n<p>if it is difficult to engineer nanoscale devices which operate in a regime where the data is reproducible, it is also difficult to perform these experiments such that the data complies with quantum theory.</p>\n</blockquote>\n<p>In particular, this relates to the decoherence of a system via random interactions with the environment. Thus decoherence becomes not as a physical intrinsically-quantum phenomenon of \"worlds drifting apart\", but a property of experiments that are not well-isolated from the influence of environment and therefore not reproducible. Well-isolated experiments are robust (and described by \"quantum inference\") and poorly-isolated experiments are not (hence quantum inference <em>does not apply</em>).</p>\n<p>In sum, it appears that quantum physics when viewed as inference does not require many-worlds any more than probability theory does.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5zgHkio95otxnjsWY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 1.7280724341239452e-06, "legacy": true, "legacyId": "26216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Related_to__Quantum_physics_sequence_\">Related to: <a href=\"/lw/r5/the_quantum_physics_sequence/\" target=\"_blank\">Quantum physics sequence</a>.</strong></p>\n<p><em>TLDR: Quantum mechanics can be derived from the rules of probabilistic reasoning. The wavefunction is a mathematical vehicle to transform a nonlinear problem into a linear one. The Born rule that is so puzzling for MWI results from the particular mathematical form of this functional substitution.</em></p>\n<p>This is a brief overview a recent paper in <em>Annals of Physics</em>&nbsp;(recently mentioned in <a href=\"/r/discussion/lw/k7i/link_quantum_theory_as_the_most_robust/\" target=\"_self\">Discussion</a>):</p>\n<p><a href=\"http://dx.doi.org/10.1016/j.aop.2014.04.021\" target=\"_blank\">Quantum theory as the most robust description of reproducible experiments</a>&nbsp;(<a href=\"http://arxiv.org/abs/1303.4574\" target=\"_blank\">arXiv</a>)</p>\n<p>by&nbsp;<span>Hans De Raedt</span><span>,&nbsp;</span><span>Mikhail I. Katsnelson</span><span>, and&nbsp;</span><span>Kristel Michielsen. Abstract:</span></p>\n<div>\n<blockquote>\n<p>It is shown that the basic equations of quantum theory can be obtained from a straightforward application of logical inference to experiments for which there is uncertainty about individual events and for which the frequencies of the observed events are robust with respect to small changes in the conditions under which the experiments are carried out.</p>\n</blockquote>\n<p><span>In a nutshell, the authors use the \"plausible reasoning\" rules (as in, e.g., Jaynes' <em>Probability Theory</em>) to recover the quantum-physical results for the EPR and Stern</span>\u2013<span>Gerlach experiments by adding a notion of experimental reproducibility in a mathematically well-formulated way and <em>without any \"quantum\" assumptions</em>. Then they show how the Schrodinger equation (SE) can be obtained from the nonlinear variational problem on the probability <em>P</em> for the particle-in-a-potential problem when the classical Hamilton-Jacobi equation holds \"on average\". The SE allows to transform the nonlinear variational problem into a linear one, and in the course of said transformation, the (real-valued) probability <em>P</em>&nbsp;and the action <em>S</em>&nbsp;are combined in a single complex-valued function ~<em>P</em><sup>1/2</sup>exp(<em>iS</em>) which becomes the argument of SE (the wavefunction).</span></p>\n<p>This casts the <a href=\"/lw/py/the_born_probabilities/\" target=\"_blank\">\"serious mystery\" of Born probabilities</a> in a new light. Instead of the observed frequency being the square(d amplitude) of the \"physically fundamental\" wavefunction, the wavefunction is seen as a mathematical vehicle to convert a difficult nonlinear variational problem for inferential probability into a manageable linear PDE, where it so happens that the probability enters the wavefunction under a square root.</p>\n<p>Below I will excerpt some math from the paper, mainly to show that the approach actually works, but outlining just the key steps. This will be followed by some general discussion and reflection.</p>\n<p><strong id=\"1__Plausible_reasoning_and_reproducibility\">1. Plausible reasoning and reproducibility</strong></p>\n<p>The authors start from the usual desiderata that are well laid out in Jaynes' <em>Probability Theory</em>&nbsp;and elsewhere, and add to them another condition:</p>\n<blockquote>\n<p><span>There may be uncertainty about each event. The conditions under which the experiment is carried out may be uncertain. The frequencies with which events are observed are reproducible and robust against small changes in the conditions.</span></p>\n</blockquote>\n<p>Mathematically, this is a requirement that the probability&nbsp;<em>P</em>(<em>x</em>|<em>\u03b8</em>,<em>Z</em>) of observation <em>x</em>&nbsp;given an uncertain experimental parameter <em>\u03b8</em>&nbsp;and the rest of out knowledge <em>Z</em>, is maximally&nbsp;<em>robust</em> to small changes in&nbsp;<em>\u03b8</em>&nbsp;and independent of&nbsp;<em>\u03b8</em>. Using log-probabilities, this amounts to minimizing the \"evidence\"</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\mathrm{Ev}=\\ln\\frac{P\\left(x,y|\\theta+\\epsilon,Z\\right)}{P\\left(x,y|\\theta,Z\\right)}\" alt=\"\" width=\"190\" height=\"44\"></p>\n<p>for any small <em>\u03b5</em>&nbsp;so that |Ev| is not a function of&nbsp;<em>\u03b8</em>&nbsp;(but the probability is).</p>\n<p><strong>2. The Einstein</strong><strong>\u2013</strong><strong>Podolsky</strong><strong>\u2013</strong><strong>Rosen</strong><strong>\u2013</strong><strong>Bohm experiment</strong></p>\n<p>There is a source S that, when activated, sends a pair of signals to two routers R<sub>1,2</sub>. Each router then sends the signal to one of its two detectors <em>D<sub>i</sub></em><sub>+,\u2013</sub>&nbsp;(<em>i</em>=1,2). Each router can be rotated and we denote as&nbsp;<em>\u03b8</em>&nbsp;the angle between them. The experiment is repeated <em>N</em>&nbsp;times yielding the data set {<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>}, {<em>x</em><sub>2</sub>,<em>y</em><sub>2</sub>}, ... {<em>x<sub>N</sub></em>,<em>y<sub>N</sub></em>} where&nbsp;<em>x</em>&nbsp;and&nbsp;<em>y</em>&nbsp;are the outcomes from the two detectors (+1 or&nbsp;\u20131). We want to find the probability <em>P</em>(<em>x</em>,<em>y</em>|<em>\u03b8</em>,<em>Z</em>).</p>\n<p>After some calculations it is found that the single-trial probability can be expressed as&nbsp;<em>P</em>(<em>x</em>,<em>y</em>|<em>\u03b8</em>,<em>Z</em>)&nbsp;= (1 + <em>xyE</em><sub>12</sub>(<em>\u03b8</em>)&nbsp;) / 4, where <em>E</em><sub>12</sub>(<em>\u03b8</em>) = \u03a3<sub>x,y=+\u20131</sub> <em>xy</em><em>P</em>(<em>x</em>,<em>y</em>|<em>\u03b8</em>,<em>Z</em>)&nbsp;is a periodic function.</p>\n<p>From the properties of Bernoulli trials it follows that, for a data set of <em>N</em>&nbsp;trials with <em>n<sub>xy</sub></em>&nbsp;total outcomes of each type {<em>x</em>,<em>y</em>},</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\mathrm{Ev} = \\sum_{x,y=\\pm1} n_{xy} \\ln\\frac{P\\left(x,y|\\theta+\\epsilon,Z\\right)}{P\\left(x,y|\\theta,Z\\right)}\" alt=\"\" width=\"266\" height=\"50\"></p>\n<p>and expanding this in a Taylor series it is found that</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\mathrm{Ev} = -\\frac{N\\epsilon^{2}}{2} \\sum_{x,y=\\pm1} \\frac{1}{P\\left(x,y|\\theta,Z\\right)} \\left(\\frac{\\partial P\\left(x,y|\\theta,Z\\right)}{\\partial\\theta}\\right)^{2} + O(\\epsilon^{3})\" alt=\"\" width=\"452\" height=\"55\"></p>\n<p>The expression in the sum is the Fisher information <em>I<sub>F</sub></em> for <em>P</em>. The maximum robustness requirement means it must be minimized. Writing it down as <em>I<sub>F</sub></em> = 1/(1 \u2013&nbsp;<em>E</em><sub>12</sub>(<em>\u03b8</em>)<sup>2</sup>) (<em>dE</em><sub>12</sub>(<em>\u03b8</em>)/<em>d</em><em>\u03b8</em>)<sup>2</sup> one finds that <em>E</em><sub>12</sub>(<em>\u03b8</em>) = cos(<em>\u03b8</em><em>I<sub>F</sub></em><sup>1/2</sup> + <em>\u03c6</em>), and since <em>E</em><sub>12</sub> must be periodic in angle, <em>I<sub>F</sub></em><sup>1/2</sup> is a natural number, so the smallest possible value is <em>I<sub>F</sub></em> = 1. Choosing <em>\u03c6&nbsp;</em>=&nbsp;<em>\u03c0</em> it is found that <em>E</em><sub>12</sub>(<em>\u03b8</em>) =&nbsp;\u2013cos(<em>\u03b8</em>), and we obtain the result that</p>\n</div>\n<p style=\"padding-left: 30px;\"><em></em><img src=\"http://www.codecogs.com/png.latex?P\\left(x,y|\\theta,Z\\right) = \\frac{1 - \\cos\\theta}{4}\" alt=\"\" width=\"188\" height=\"38\"></p>\n<p>which is the well-known correlation of two spin-1/2 particles in the singlet state.</p>\n<blockquote>\n<div>\n<p>Needless to say, our derivation did not use any concepts of quantum theory. Only plain, rational reasoning strictly complying with the rules of logical inference and some elementary facts about the experiment were used</p>\n</div>\n</blockquote>\n<p><strong>3. The Stern</strong><strong>\u2013</strong><strong>Gerlach experiment</strong></p>\n<p>This case is analogous and simpler than the previous one. The setup contains a source emitting a particle with magnetic moment <strong>S</strong>, a magnet with field in the direction <strong>a</strong>, and two detectors <em>D</em><sub>+</sub> and <em>D</em><sub>\u2013</sub><em>.</em></p>\n<p>Similarly to the previous section,&nbsp;<em>P</em>(<em>x</em>|<em>\u03b8</em>,<em>Z</em>)&nbsp;= (1 +&nbsp;<em>xE</em>(<em>\u03b8</em>)&nbsp;) / 2, where&nbsp;<em>E</em>(<em>\u03b8</em>) = <em>P</em>(+|<em>\u03b8</em>,<em>Z</em>)&nbsp;\u2013&nbsp;<em>P</em>(\u2013|<em>\u03b8</em>,<em>Z</em>)&nbsp;is an unknown periodic function. By complete analogy we seek the minimum of <em>I<sub>F</sub></em>&nbsp;and find that <em>E</em>(<em>\u03b8</em>) = +\u2013cos(<em>\u03b8</em>), so that</p>\n<p style=\"padding-left: 30px;\"><em></em></p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?P\\left(x|\\theta,Z\\right) = \\frac{\\left(1 + x\\mathbf{a}\\cdot\\mathbf{S}\\right)}{2}\" alt=\"\" width=\"193\" height=\"39\"></p>\n<blockquote>\n<p>In quantum theory, <em>[this]</em> equation is in essence just the postulate (Born\u2019s rule) that the probability to observe the particle with spin up is given by the square of the absolute value of the amplitude of the wavefunction projected onto the spin-up state. Obviously, the variability of the conditions under which an experiment is carried out is not included in the quantum theoretical description. In contrast, in the logical inference approach, <em>[equation]</em> is not postulated but follows from the assumption that the (thought) experiment that is being performed yields the most reproducible results, revealing the conditions for an experiment to produce data which is described by quantum theory.</p>\n</blockquote>\n<p>To repeat: there are no wavefunctions in the present approach. The only assumption is that a dependence of outcome on particle/magnet orientation is observed with robustness/reproducibility.</p>\n<p><strong id=\"4__Schrodinger_equation\">4.&nbsp;Schrodinger equation</strong></p>\n<p>A particle is located in unknown position&nbsp;<em>\u03b8</em>&nbsp;on a line segment [\u2013<em>L</em>, <em>L</em>]. Another line segment [\u2013<em>L</em>, <em>L</em>] is uniformly covered with detectors. A source emits a signal and the particle's response is detected by one of the detectors.</p>\n<p>After going to the continuum limit of infinitely many infinitely small detectors and accounting for translational invariance it is possible to show that the position of the particle&nbsp;<em>\u03b8</em>&nbsp;and of the detector <em>x</em> can be interchanged so that <em>dP</em>(<em>x</em>|<em>\u03b8</em>,<em>Z</em>)/<em>d</em><em>\u03b8</em>&nbsp;= \u2013<em>dP</em>(<em>x</em>|<em>\u03b8</em>,<em>Z</em>)/<em>dx</em>.</p>\n<p>In exactly the same way as before we need to minimize Ev by minimizing the Fisher information, which is now</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: monospace; font-size: medium;\"><img src=\"http://www.codecogs.com/png.latex?I_F=\\int{\\frac{1}{P\\left(x|\\theta,Z\\right)}\\left(\\frac{\\partial P(x|\\theta,Z)}{\\partial x\\right)^{2}}dx\" alt=\"\" width=\"288\" height=\"49\"></span></p>\n<p>However, simply solving this minimization problem will not give us anything new because nothing so far accounted for the fact that the particle moves in a potential. This needs to be built into the problem. This can be done by requiring that the classical Hamilton-Jacobi equation holds <em>on average</em>. Using the Lagrange multiplier method, we now need to minimize the functional</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?F(\\theta)=\\int{\\left\\{\\frac{1}{P\\left(x|\\theta,Z\\right)}\\left(\\frac{\\partial P(x|\\theta,Z)}{\\partial x\\right)^{2}+\\lambda\\left[\\left(\\frac{\\partial S(x)}{\\partial x}}\\right)^{2}+2m\\left[V\\left(x\\right)-E\\right]\\right]P\\left(x|\\theta,Z\\right)}\\right\\}dx\" alt=\"\" width=\"697\" height=\"55\"></p>\n<p>Here <em>S</em>(<em>x</em>) is the action (Hamilton's principal function). This minimization yields solutions for the two functions&nbsp;<em>P</em>(<em>x|</em><em>\u03b8</em>,<em>Z</em>) and <em>S</em>(<em>x</em>). It is a difficult nonlinear minimization problem, but it is possible to find a matching solution in a tractable way using a mathematical \"trick\". It is known that standard variational minimization of the functional</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?Q\\left(\\theta\\right)=\\int{\\left\\{4\\frac{\\partial\\psi^{*}\\left(x|\\theta,Z\\right)}{\\partial x}\\frac{\\partial\\psi\\left(x|\\theta,Z\\right)}{\\partial x}+2m\\lambda\\left[V\\left(x\\right)-E\\right]\\psi^{*}\\left(x|\\theta,Z\\right)\\psi\\left(x|\\theta,Z\\right)\\right\\}dx}\" alt=\"\" width=\"640\" height=\"45\"></p>\n<p>yields the Schrodinger equation for its extrema. On the other hand, if one makes the substitution combining two real-valued functions <em>P</em>&nbsp;and <em>S</em>&nbsp;into a single complex-valued <em>\u03c8</em>,</p>\n<p style=\"padding-left: 30px;\"><img src=\"http://www.codecogs.com/png.latex?\\psi\\left(x|\\theta,Z\\right)=\\sqrt{P\\left(x|\\theta,Z\\right)}e^{iS\\left(x\\right)\\sqrt{\\lambda}/2}\" alt=\"\" width=\"263\" height=\"25\"></p>\n<p><em>Q</em> is immediately transformed into <em>F</em>, concluding the derivation of the Schrodinger equation. Incidentally,&nbsp;<em>\u03c8</em>&nbsp;is constructed so that&nbsp;<em>P</em>(<em>x|</em><em>\u03b8</em>,<em>Z</em>) = |<em>\u03c8</em>(<em>x</em>|<em>\u03b8</em>,<em>Z</em>)|<sup>2</sup>, which is the Born rule.</p>\n<p>Summing up the meaning of Schrodinger equation in the present context:</p>\n<blockquote>\n<p>Of course, a priori there is no good reason to assume that on average there is agreement with Newtonian mechanics ...&nbsp;In other words, the time-independent Schrodinger equation describes the collective of repeated experiments ... subject to the condition that the averaged observations comply with Newtonian mechanics.</p>\n</blockquote>\n<p>The authors then proceed to derive the time-dependent SE (independently from the stationary SE) in a largely similar fashion.</p>\n<p><strong id=\"5__What_it_all_means\">5. What it all means</strong></p>\n<p>Classical mechanics assumes that everything about the system's state and dynamics can be known (at least in principle). It starts from axioms and proceeds to derive its conclusions deductively (as opposed to inductive reasoning). In this respect quantum mechanics is to classical mechanics what probabilistic logic is to classical logic.</p>\n<p>Quantum theory is viewed here not as a description of what really goes on at the microscopic level, but <em>as an instance of logical inference</em>:</p>\n<blockquote>\n<p>in the logical inference approach, we take the point of view that a description of our knowledge of the phenomena at a certain level is independent of the description at a more detailed level.</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>quantum theory does not provide any insight into the motion of a particle but instead describes all what can be inferred (within the framework of logical inference) from or, using Bohr\u2019s words, <em>said</em> about the observed data</p>\n</blockquote>\n<p>Such a treatment of QM is similar in spirit to Jaynes' <em>Information Theory and Statistical Mechanics</em>&nbsp;papers (<a href=\"http://dx.doi.org/10.1103/PhysRev.106.620\" target=\"_blank\">I</a>, <a href=\"http://dx.doi.org/10.1103/PhysRev.108.171\" target=\"_blank\">II</a>). Traditionally statistical mechanics/thermodynamics is derived bottom-up from the microscopic mechanics and a series of postulates (such as ergodicity) that allow us to progressively ignore microscopic details under strictly defined conditions. In contrast, Jaynes starts with minimum possible assumptions:</p>\n<blockquote>\n<p>\"The quantity&nbsp;<em>x</em>&nbsp;is capable of assuming the discrete values&nbsp;<em>x<sub>i</sub></em>&nbsp;... all we know is the expectation value of the function&nbsp;<em>f</em>(<em>x</em>) ... On the basis of this information, what is the expectation value of the function&nbsp;<em>g</em>(<em>x</em>)?\"</p>\n</blockquote>\n<p>and proceeds to derive the foundations of statistical physics from the maximum entropy principle. Of course, these papers deserve a separate post.</p>\n<p>This community should be particularly interested in how this all aligns with the many-worlds interpretation. Obviously, any conclusions drawn from this work can only apply to the \"quantum multiverse\" level and cannot rule out or support any other many-worlds proposals.</p>\n<p>In quantum physics, MWI does quite naturally resolve some difficult issues in the \"wavefunction-centristic\" view. However, we see that the concept wavefunction is not really central for quantum mechanics. This removes the whole problem of wavefunction collapse that MWI seeks to resolve.</p>\n<p>The Born rule is arguably a big issue for MWI. But here it essentially boils down to \"<em>x</em>&nbsp;is quadratic in&nbsp;<em>t</em>&nbsp;where&nbsp;<em>t =</em>&nbsp;sqrt(<em>x</em>)\". Without the wavefunction (only probabilities) the problem simply does not appear.</p>\n<p>Here is another interesting conclusion:</p>\n<blockquote>\n<p>if it is difficult to engineer nanoscale devices which operate in a regime where the data is reproducible, it is also difficult to perform these experiments such that the data complies with quantum theory.</p>\n</blockquote>\n<p>In particular, this relates to the decoherence of a system via random interactions with the environment. Thus decoherence becomes not as a physical intrinsically-quantum phenomenon of \"worlds drifting apart\", but a property of experiments that are not well-isolated from the influence of environment and therefore not reproducible. Well-isolated experiments are robust (and described by \"quantum inference\") and poorly-isolated experiments are not (hence quantum inference <em>does not apply</em>).</p>\n<p>In sum, it appears that quantum physics when viewed as inference does not require many-worlds any more than probability theory does.</p>", "sections": [{"title": "Related to: Quantum physics sequence.", "anchor": "Related_to__Quantum_physics_sequence_", "level": 1}, {"title": "1. Plausible reasoning and reproducibility", "anchor": "1__Plausible_reasoning_and_reproducibility", "level": 1}, {"title": "4.\u00a0Schrodinger equation", "anchor": "4__Schrodinger_equation", "level": 1}, {"title": "5. What it all means", "anchor": "5__What_it_all_means", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hc9Eg6erp6hk9bWhn", "rok6ZgZ6L85dpBfH7", "3ZKvf9u2XEWddGZmS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T00:52:30.626Z", "modifiedAt": null, "url": null, "title": "Strategyproof Mechanisms: Impossibilities", "slug": "strategyproof-mechanisms-impossibilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:29.128Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wE3CRBTpSSBXf9EHK/strategyproof-mechanisms-impossibilities", "pageUrlRelative": "/posts/wE3CRBTpSSBXf9EHK/strategyproof-mechanisms-impossibilities", "linkUrl": "https://www.lesswrong.com/posts/wE3CRBTpSSBXf9EHK/strategyproof-mechanisms-impossibilities", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strategyproof%20Mechanisms%3A%20Impossibilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrategyproof%20Mechanisms%3A%20Impossibilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE3CRBTpSSBXf9EHK%2Fstrategyproof-mechanisms-impossibilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strategyproof%20Mechanisms%3A%20Impossibilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE3CRBTpSSBXf9EHK%2Fstrategyproof-mechanisms-impossibilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE3CRBTpSSBXf9EHK%2Fstrategyproof-mechanisms-impossibilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1832, "htmlBody": "<p style=\"padding-left: 30px;\"><em>In which the limits of dominant-strategy implementation are explored. The Gibbard-Satterthwaite dictatorship theorem for unrestricted preference domains is stated, showing no universal strategyproof mechanisms exist, along with a proof for a special case.</em></p>\n<p>Due to the Revelation Principle, most design questions can be answered by studying incentive compatible mechanisms, as discussed in the <a href=\"/lw/k69/incentive_compatibility_and_the_revelation/\">last post</a>. Incentive compatibility comes in many different flavors corresponding to different solution concepts&mdash;dominant-strategy IC and Bayes-Nash IC being the two most common. In this post, I&rsquo;ll delve into what&rsquo;s possible under dominant strategy incentive compatibility.</p>\n<p>Recall that a strategy is <em>dominant</em> if playing it always leads to (weakly) higher payoffs for an agent than other strategy would, no matter what strategies other agents play. A social choice function is <em>dominant-strategy incentive compatible</em>&nbsp;if honest revelation is a dominant strategy in the direct mechanism for that SCF<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. The appeal of implementation in dominant strategies is that an agent doesn't need to think about what other agents will do. Even in the worst case, a dominant-strategy IC social choice function leaves an agent better off for being honest. Since the need for strategic thinking is eliminated, dominant-strategy IC is also referred to as <em>strategyproofness</em>.&nbsp;</p>\n<h2 id=\"gibbard-satterthwaite-universal-strategyproof-mechanisms-are-out-of-reach\">Gibbard-Satterthwaite: universal strategyproof mechanisms are out of reach</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Arrow&rsquo;s_impossibility_theorem\">Arrow&rsquo;s theorem</a> is well-known for showing dictatorships are the only aggregators of ordinal rankings that satisfy a set of particular criteria. The result is commonly interpreted as saying there is no perfect voting system for more than two candidates. However, since Arrow deals with <em>social welfare functions</em> which take a profile of preferences as input and outputs a full preference ranking, it really says something about aggregating a set of preferences into a single group preference. Most of the time though, a full ranking of candidates will be superfluous&mdash;all we really need to know is who wins the election. Although Arrow doesn&rsquo;t give social welfare functions much room to maneuver, maybe there are still some nice social choice functions out there.</p>\n<p>Alas, it&rsquo;s not to be. Alan Gibbard and Mark Satterthwaite have shown that, in general, the only strategyproof choice from three or more alternatives that is even slightly responsive to preferences is a dictatorship by one agent.</p>\n<p><a id=\"more\"></a></p>\n<p>Stated formally:</p>\n<blockquote>\n<p><strong>Gibbard-Satterthwaite dictatorship theorem:</strong> Suppose <span class=\"math\"><em>f</em>:\u2006<em>L</em>(<em>A</em>)<sup><em>n</em></sup> &rarr;\u2004<em>A</em></span> is a social choice function for <span class=\"math\"><em>n</em></span> agents from profiles of ordinal rankings <span class=\"math\"><em>L</em>(<em>A</em>)<sup><em>n</em></sup></span> to a set of outcomes <span class=\"math\"><em>A</em></span> with at least three elements. If <span class=\"math\"><em>f</em></span> is strategyproof and onto<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>, then <span class=\"math\"><em>f</em></span> is dictatorial.</p>\n</blockquote>\n<p>Being strategyproof seems intuitively more desirable to me than the properties in Arrow&rsquo;s theorem (especially the much critiqued <em>independence of irrelevant alternatives</em>&nbsp;criterion). As it turns out though, Gibbard-Satterthwaite and Arrow are equivalent! Weakening our ambition from social welfare functions to social choice functions didn't give us anything.</p>\n<p>Gibbard-Satterthwaite seems a great blow to mechanism design at first glance. On reflection, perhaps we were asking for too much. A completely generic strategyproof mechanism that can be used in any situation does sound too good to be true.</p>\n<p>There are three ways to proceed from this point:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Building strategyproof mechanisms for specialized applications,</li>\n<li>Weakening strategyproofness to another form of incentive compatibility, or</li>\n<li>Stepping sideways from ordinal preferences and enriching the model while adding further assumptions.</li>\n</ol>\n<p>On that last path, note that switching from ordinal to cardinal preferences can&rsquo;t save us unless we offset that generalization with other assumptions&mdash;cardinal information expands the ordinal type space, which can only make implementation more difficult.</p>\n<p>Since dictatorships are the only universal strategyproof mechanisms for choosing between three options, it&rsquo;s worth investigating why life is pleasant with only two options. In this case, lots of non-trivial strategyproof mechanisms exist. For example, suppose a committee is restricted to painting a bike shed either red or blue due to zoning restrictions. The social choice function that paints the shed red if and only if a majority have red as their first choice is strategyproof, as is the social choice to paint the shed red if and only if the committee unanimously lists red as their top choice.</p>\n<p>Of course, not every voting rule for binary outcomes will be strategyproof, like the rule that paints the shed red if and only if an odd number of committee members top-rank red. While I can&rsquo;t imagine anyone arguing in favor of this rule, what&rsquo;s going wrong here? The problem is this odd rule isn&rsquo;t <em>monotonic</em>&mdash;raising red in someone&rsquo;s preference ranking can cause the outcome to switch away from red.</p>\n<p>Here are the formal definitions of strategyproofness and monotonicity:</p>\n<blockquote>\n<p>A social choice function <span class=\"math\"><em>f</em>:\u2006<em>L</em>(<em>A</em>)<sup><em>n</em></sup> &rarr;\u2004<em>A</em></span> is <em>strategyproof</em> if for each agent <span class=\"math\"><em>i</em></span>, the social choice satisfies <img src=\"http://latex.codecogs.com/gif.latex?f(\\succ_i,\\succ_{-i})\\succeq_i&amp;space;f(\\succ'_i,\\succ_{-i})\" alt=\"\" /> for all rankings <img title=\"\\succ_i, \\succ'_i\\;\\in L(A)\" src=\"http://latex.codecogs.com/gif.latex?\\succ_i,&amp;space;\\succ'_i\\;\\in&amp;space;L(A)\" alt=\"\" /> and <img title=\"\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" src=\"http://latex.codecogs.com/gif.latex?\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" alt=\"\" />.</p>\n</blockquote>\n<blockquote>\n<p>A social choice function <span class=\"math\"><em>f</em>:\u2006<em>L</em>(<em>A</em>)<sup><em>n</em></sup> &rarr;\u2004<em>A</em></span> is <em>monotonic</em> if for each agent <span class=\"math\"><em>i</em></span>, we have that <img title=\"f(\\succ_i,\\succ_{-i})\\succeq_i f(\\succ'_i,\\succ_{-i})\" src=\"http://latex.codecogs.com/gif.latex?f(\\succ_i,\\succ_{-i})=a\\ne&amp;space;b=&amp;space;f(\\succ'_i,\\succ_{-i})\" alt=\"\" /> implies <img title=\"a\\succ_i b\" src=\"http://latex.codecogs.com/gif.latex?a\\succ_i&amp;space;b\" alt=\"\" /> and <img title=\"b\\succ'_i a\" src=\"http://latex.codecogs.com/gif.latex?b\\succ'_i&amp;space;a\" alt=\"\" /> for all rankings <img title=\"\\succ_i, \\succ'_i\\;\\in L(A)\" src=\"http://latex.codecogs.com/gif.latex?\\succ_i,&amp;space;\\succ'_i\\;\\in&amp;space;L(A)\" alt=\"\" /> and <img title=\"\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" src=\"http://latex.codecogs.com/gif.latex?\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" alt=\"\" />.</p>\n</blockquote>\n<p>Take a moment to convince yourself that monotonicity is just a slight rephrasing of strategyproofness and hence equivalent. Though they are identical at heart, monotonicity carries two useful interpretation as an invariance property. First, if an agent submits a new ranking where the previous outcome goes up, the outcome can&rsquo;t change. Second, submitting a new ranking where the rank of the previous outcome relative any other option is unchanged has to leave the outcome unchanged as well.</p>\n<p>When there are only two alternatives, we can think of the implicit outcome space as one dimensional, with one outcome on the left and the other on the right. Going towards one outcome corresponds exactly with going away from the other. With three or more alternatives, we don&rsquo;t have the same nice structure, leading to the impossibility of a non-trivial monotonic rule.</p>\n<p>In the next post, I&rsquo;ll describe domains where we can enrich the outcome space beyond a binary choice and still get strategyproofness. Since the outcome space won&rsquo;t naturally have a nice order structure, we&rsquo;ll have to ensure it does by restricting the preferences agents can have over it. Even though we don&rsquo;t have universal strategyproof mechanisms other than dictatorships, we can uncover strategyproof mechanisms for specific applications. In the meantime, here&rsquo;s a proof of Gibbard-Satterthwaite for the curious.</p>\n<h2 id=\"proof-of-gibbard-satterthwaite-in-a-special-case\">Proof of Gibbard-Satterthwaite (in a special case)</h2>\n<p>Suppose the committee consists of two people, Betty and Bill. In addition to red and blue, the city recently approved yellow as an option to paint bike sheds. Each person has six possible rankings over the three colors, and there are 36 preference profiles containing one ranking from each. The 36 will fall into six relevant classes. Here is an example of each class along with some shorthand to describe it:</p>\n<ol style=\"list-style-type: decimal\">\n<li><span class=\"math\"><em>r</em> &gt;\u2004<em>b</em> &gt;\u2004<em>y</em></span>: Both agents agree on the ranking of red over blue over yellow.</li>\n<li><span class=\"math\"><em>r</em> &gt;\u2004<em>b</em>^<em>y</em></span>: Both agents agree red is the best. Betty puts blue as her second choice, while Bill has yellow as his second choice.</li>\n<li><span class=\"math\"><em>b</em>^<em>y</em> &gt;\u2004<em>r</em></span>: Both agree red is worst. Betty thinks blue is better than yellow, while Bill thinks yellow is better.</li>\n<li><span class=\"math\"><em>r&nbsp;</em>\u2223 (<em>b</em> &gt;\u2004<em>y)</em></span>: Both agents agree blue is better than yellow. Betty thinks red is better than them both, while Bill thinks red is worse than both.</li>\n<li><span class=\"math\">(<em>b</em> &gt;\u2004<em>y</em>) \u2223&nbsp;<em>r</em></span>: Both agree blue is better than yellow. Betty thinks red is worst, while Bill thinks red is best.</li>\n<li><span class=\"math\"><em>r</em>^<em>b</em>^<em>y</em></span>: Betty has the ranking red over blue over yellow, while Bill has the reverse.</li>\n</ol>\n<p>For the notation summarizing the profile,&nbsp;<span class=\"math\"><em>r</em> &gt;\u2004<em>b</em></span>&nbsp;indicated both agree that red is better than blue. <span class=\"math\"><em>r</em>^<em>b</em></span> says there is a conflict between the two with Betty preferring red. <span class=\"math\"><em>r&nbsp;</em>\u2223 (<em>b</em> &gt;\u2004<em>y</em>)</span> says there are two preference conflicts, with Betty preferring red over the other two options, so we alternatively think of this profile as <span class=\"math\"><em>r</em>^<em>y</em></span>, <span class=\"math\"><em>r</em>^<em>b</em></span>, and <span class=\"math\"><em>y</em> &gt;\u2004<em>b</em></span>.</p>\n<p>Now, we&rsquo;ll assign the 36 profiles a color, using each at least once, in a way that is monotonic. This will happen in six steps as depicted in the following diagram.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_k8r_0.png?v=a2b1e09a6866971838942b510abd0676\" alt=\"\" width=\"850\" height=\"588\" /></p>\n<p>&nbsp;</p>\n<ol style=\"list-style-type: decimal\">\n<li>At least one profile must be red by assumption. Starting from that profile (whatever it is), we could move red to the top of both rankings and the outcome would still be red by monotonicity. Then all other profiles with red top-ranked by both also must be red since any swaps of blue and yellow can&rsquo;t change the outcome since red is still relatively above both. This gives us 1a,b,c,d. Blue and yellow go through similarly.</li>\n<li>Consider the profile <span class=\"math\"><em>b</em>^<em>y</em> &gt;\u2004<em>r</em></span> at the bottom of the diagram. The profile <span class=\"math\"><em>y</em> &gt;\u2004<em>b</em> &gt;\u2004<em>r</em></span> got yellow in 1f, so if Betty starts liking blue better from 2, the outcome has to stay yellow or switch to blue. Since monotonicity can&rsquo;t tell us more than that, we have to make a choice. Let&rsquo;s decide in favor of Betty and pick blue.</li>\n<li>Since we chose to resolve the conflict <span class=\"math\"><em>b</em>^<em>y</em></span> as <span class=\"math\"><em>b</em></span>, three more colorings follow since we must resolve this particular conflict in the same way everywhere. Keep in mind that <span class=\"math\"><em>b</em>^<em>y</em></span> is a different conflict than <span class=\"math\"><em>y</em>^<em>b</em></span> since it might matter who prefers which color. Consider 3b. This can&rsquo;t be yellow since yellow increases between this profile and 2, but 2 isn&rsquo;t yellow. It also can&rsquo;t be red since 1k isn&rsquo;t red, so we conclude 3b must be blue. Now consider 3a. Even though the conflict <span class=\"math\"><em>b</em>^<em>y</em></span> resolves in favor of blue, the outcome can&rsquo;t be blue since 1c isn&rsquo;t blue. Hence 3a must be red. From 3a, we conclude that <span class=\"math\"><em>r</em>^<em>y</em></span> was resolved as <span class=\"math\"><em>r</em></span>, so this new rule must apply everywhere. From 3c, we get a third rule that <span class=\"math\"><em>b</em>^<em>r</em> =\u2004<em>b</em></span>.</li>\n<li>With two new rules in the third step in addition to the rule from the second, more colorings follow. These colorings then give us the rules <span class=\"math\"><em>r</em>^<em>b</em> =\u2004<em>r</em></span>, <span class=\"math\"><em>y</em>^<em>b</em> =\u2004<em>y</em></span>, and <span class=\"math\"><em>y</em>^<em>r</em> =\u2004<em>y</em></span>.</li>\n<li>With all possible pairwise resolutions settled, all profiles can be colored.</li>\n</ol>\n<p>We&rsquo;ve found a monotonic, onto coloring! Notice that this is a dictatorship by Betty, choosing her top-ranked color for each profile. Everything comes down to favoring Betty over Bill in step two. Since Betty was pivotal once, she ends up having complete control. Of course, we could have resolved <span class=\"math\"><em>b</em>^<em>y</em></span> as <span class=\"math\"><em>y</em></span> instead, which would have given us a dictatorship by Bill. That choice in step two was the only degree of latitude we had, so these are the only two monotonic, onto colorings.</p>\n<p>This special-case proof of Gibbard-Satterthwaite was inspired by <a href=\"http://www.business.otago.ac.nz/econ/Personal/PH/P%20Hansen%20Proof%20of%20Arrow&rsquo;s%20Imposs%20Theorem.pdf\">Hansen (2002), &ldquo;Another graphical proof of Arrow&rsquo;s impossibility theorem&rdquo;</a>. Full proofs of the theorems, done simultaneously side-by-side, are given in <a href=\"https://sites.google.com/site/philipjreny/arrow-gibbard-satterthwaite-econ-lett-2001.pdf?attredirects=0\">Reny (2001), &ldquo;Arrow&rsquo;s theorem and the Gibbard-Satterthwaite theorem: a uni\ufb01ed approach&rdquo;</a>.</p>\n<p>&nbsp;</p>\n<p><em>Previously on:</em> <a href=\"/lw/k69/incentive_compatibility_and_the_revelation/\">Incentive-Compatibility and the Revelation Principle</a></p>\n<p><em>Next up:</em> <a href=\"/lw/kas/strategyproof_mechanisms_possibilities/\">Strategyproof Mechanisms: Possibilities</a></p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>Recall that the direct mechanism for an SCF is where we simply ask the agents what type they are and then assign the outcome prescribed by the SCF for that type profile.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>The function <span class=\"math\"><em>f</em></span> is <em>onto</em> if every outcome in <span class=\"math\"><em>A</em></span> occurs for at least one input. The main role of this assumption to prevent <span class=\"math\"><em>f</em></span> from covertly restricting its image to two elements, so it&rsquo;s almost without loss of generality.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wE3CRBTpSSBXf9EHK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "26235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\"><em>In which the limits of dominant-strategy implementation are explored. The Gibbard-Satterthwaite dictatorship theorem for unrestricted preference domains is stated, showing no universal strategyproof mechanisms exist, along with a proof for a special case.</em></p>\n<p>Due to the Revelation Principle, most design questions can be answered by studying incentive compatible mechanisms, as discussed in the <a href=\"/lw/k69/incentive_compatibility_and_the_revelation/\">last post</a>. Incentive compatibility comes in many different flavors corresponding to different solution concepts\u2014dominant-strategy IC and Bayes-Nash IC being the two most common. In this post, I\u2019ll delve into what\u2019s possible under dominant strategy incentive compatibility.</p>\n<p>Recall that a strategy is <em>dominant</em> if playing it always leads to (weakly) higher payoffs for an agent than other strategy would, no matter what strategies other agents play. A social choice function is <em>dominant-strategy incentive compatible</em>&nbsp;if honest revelation is a dominant strategy in the direct mechanism for that SCF<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. The appeal of implementation in dominant strategies is that an agent doesn't need to think about what other agents will do. Even in the worst case, a dominant-strategy IC social choice function leaves an agent better off for being honest. Since the need for strategic thinking is eliminated, dominant-strategy IC is also referred to as <em>strategyproofness</em>.&nbsp;</p>\n<h2 id=\"Gibbard_Satterthwaite__universal_strategyproof_mechanisms_are_out_of_reach\">Gibbard-Satterthwaite: universal strategyproof mechanisms are out of reach</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Arrow\u2019s_impossibility_theorem\">Arrow\u2019s theorem</a> is well-known for showing dictatorships are the only aggregators of ordinal rankings that satisfy a set of particular criteria. The result is commonly interpreted as saying there is no perfect voting system for more than two candidates. However, since Arrow deals with <em>social welfare functions</em> which take a profile of preferences as input and outputs a full preference ranking, it really says something about aggregating a set of preferences into a single group preference. Most of the time though, a full ranking of candidates will be superfluous\u2014all we really need to know is who wins the election. Although Arrow doesn\u2019t give social welfare functions much room to maneuver, maybe there are still some nice social choice functions out there.</p>\n<p>Alas, it\u2019s not to be. Alan Gibbard and Mark Satterthwaite have shown that, in general, the only strategyproof choice from three or more alternatives that is even slightly responsive to preferences is a dictatorship by one agent.</p>\n<p><a id=\"more\"></a></p>\n<p>Stated formally:</p>\n<blockquote>\n<p><strong>Gibbard-Satterthwaite dictatorship theorem:</strong> Suppose <span class=\"math\"><em>f</em>:\u2006<em>L</em>(<em>A</em>)<sup><em>n</em></sup> \u2192\u2004<em>A</em></span> is a social choice function for <span class=\"math\"><em>n</em></span> agents from profiles of ordinal rankings <span class=\"math\"><em>L</em>(<em>A</em>)<sup><em>n</em></sup></span> to a set of outcomes <span class=\"math\"><em>A</em></span> with at least three elements. If <span class=\"math\"><em>f</em></span> is strategyproof and onto<sup><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\">2</a></sup>, then <span class=\"math\"><em>f</em></span> is dictatorial.</p>\n</blockquote>\n<p>Being strategyproof seems intuitively more desirable to me than the properties in Arrow\u2019s theorem (especially the much critiqued <em>independence of irrelevant alternatives</em>&nbsp;criterion). As it turns out though, Gibbard-Satterthwaite and Arrow are equivalent! Weakening our ambition from social welfare functions to social choice functions didn't give us anything.</p>\n<p>Gibbard-Satterthwaite seems a great blow to mechanism design at first glance. On reflection, perhaps we were asking for too much. A completely generic strategyproof mechanism that can be used in any situation does sound too good to be true.</p>\n<p>There are three ways to proceed from this point:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Building strategyproof mechanisms for specialized applications,</li>\n<li>Weakening strategyproofness to another form of incentive compatibility, or</li>\n<li>Stepping sideways from ordinal preferences and enriching the model while adding further assumptions.</li>\n</ol>\n<p>On that last path, note that switching from ordinal to cardinal preferences can\u2019t save us unless we offset that generalization with other assumptions\u2014cardinal information expands the ordinal type space, which can only make implementation more difficult.</p>\n<p>Since dictatorships are the only universal strategyproof mechanisms for choosing between three options, it\u2019s worth investigating why life is pleasant with only two options. In this case, lots of non-trivial strategyproof mechanisms exist. For example, suppose a committee is restricted to painting a bike shed either red or blue due to zoning restrictions. The social choice function that paints the shed red if and only if a majority have red as their first choice is strategyproof, as is the social choice to paint the shed red if and only if the committee unanimously lists red as their top choice.</p>\n<p>Of course, not every voting rule for binary outcomes will be strategyproof, like the rule that paints the shed red if and only if an odd number of committee members top-rank red. While I can\u2019t imagine anyone arguing in favor of this rule, what\u2019s going wrong here? The problem is this odd rule isn\u2019t <em>monotonic</em>\u2014raising red in someone\u2019s preference ranking can cause the outcome to switch away from red.</p>\n<p>Here are the formal definitions of strategyproofness and monotonicity:</p>\n<blockquote>\n<p>A social choice function <span class=\"math\"><em>f</em>:\u2006<em>L</em>(<em>A</em>)<sup><em>n</em></sup> \u2192\u2004<em>A</em></span> is <em>strategyproof</em> if for each agent <span class=\"math\"><em>i</em></span>, the social choice satisfies <img src=\"http://latex.codecogs.com/gif.latex?f(\\succ_i,\\succ_{-i})\\succeq_i&amp;space;f(\\succ'_i,\\succ_{-i})\" alt=\"\"> for all rankings <img title=\"\\succ_i, \\succ'_i\\;\\in L(A)\" src=\"http://latex.codecogs.com/gif.latex?\\succ_i,&amp;space;\\succ'_i\\;\\in&amp;space;L(A)\" alt=\"\"> and <img title=\"\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" src=\"http://latex.codecogs.com/gif.latex?\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" alt=\"\">.</p>\n</blockquote>\n<blockquote>\n<p>A social choice function <span class=\"math\"><em>f</em>:\u2006<em>L</em>(<em>A</em>)<sup><em>n</em></sup> \u2192\u2004<em>A</em></span> is <em>monotonic</em> if for each agent <span class=\"math\"><em>i</em></span>, we have that <img title=\"f(\\succ_i,\\succ_{-i})\\succeq_i f(\\succ'_i,\\succ_{-i})\" src=\"http://latex.codecogs.com/gif.latex?f(\\succ_i,\\succ_{-i})=a\\ne&amp;space;b=&amp;space;f(\\succ'_i,\\succ_{-i})\" alt=\"\"> implies <img title=\"a\\succ_i b\" src=\"http://latex.codecogs.com/gif.latex?a\\succ_i&amp;space;b\" alt=\"\"> and <img title=\"b\\succ'_i a\" src=\"http://latex.codecogs.com/gif.latex?b\\succ'_i&amp;space;a\" alt=\"\"> for all rankings <img title=\"\\succ_i, \\succ'_i\\;\\in L(A)\" src=\"http://latex.codecogs.com/gif.latex?\\succ_i,&amp;space;\\succ'_i\\;\\in&amp;space;L(A)\" alt=\"\"> and <img title=\"\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" src=\"http://latex.codecogs.com/gif.latex?\\succ_{-i}\\;\\in\\,L(A)^{n-1}\" alt=\"\">.</p>\n</blockquote>\n<p>Take a moment to convince yourself that monotonicity is just a slight rephrasing of strategyproofness and hence equivalent. Though they are identical at heart, monotonicity carries two useful interpretation as an invariance property. First, if an agent submits a new ranking where the previous outcome goes up, the outcome can\u2019t change. Second, submitting a new ranking where the rank of the previous outcome relative any other option is unchanged has to leave the outcome unchanged as well.</p>\n<p>When there are only two alternatives, we can think of the implicit outcome space as one dimensional, with one outcome on the left and the other on the right. Going towards one outcome corresponds exactly with going away from the other. With three or more alternatives, we don\u2019t have the same nice structure, leading to the impossibility of a non-trivial monotonic rule.</p>\n<p>In the next post, I\u2019ll describe domains where we can enrich the outcome space beyond a binary choice and still get strategyproofness. Since the outcome space won\u2019t naturally have a nice order structure, we\u2019ll have to ensure it does by restricting the preferences agents can have over it. Even though we don\u2019t have universal strategyproof mechanisms other than dictatorships, we can uncover strategyproof mechanisms for specific applications. In the meantime, here\u2019s a proof of Gibbard-Satterthwaite for the curious.</p>\n<h2 id=\"Proof_of_Gibbard_Satterthwaite__in_a_special_case_\">Proof of Gibbard-Satterthwaite (in a special case)</h2>\n<p>Suppose the committee consists of two people, Betty and Bill. In addition to red and blue, the city recently approved yellow as an option to paint bike sheds. Each person has six possible rankings over the three colors, and there are 36 preference profiles containing one ranking from each. The 36 will fall into six relevant classes. Here is an example of each class along with some shorthand to describe it:</p>\n<ol style=\"list-style-type: decimal\">\n<li><span class=\"math\"><em>r</em> &gt;\u2004<em>b</em> &gt;\u2004<em>y</em></span>: Both agents agree on the ranking of red over blue over yellow.</li>\n<li><span class=\"math\"><em>r</em> &gt;\u2004<em>b</em>^<em>y</em></span>: Both agents agree red is the best. Betty puts blue as her second choice, while Bill has yellow as his second choice.</li>\n<li><span class=\"math\"><em>b</em>^<em>y</em> &gt;\u2004<em>r</em></span>: Both agree red is worst. Betty thinks blue is better than yellow, while Bill thinks yellow is better.</li>\n<li><span class=\"math\"><em>r&nbsp;</em>\u2223 (<em>b</em> &gt;\u2004<em>y)</em></span>: Both agents agree blue is better than yellow. Betty thinks red is better than them both, while Bill thinks red is worse than both.</li>\n<li><span class=\"math\">(<em>b</em> &gt;\u2004<em>y</em>) \u2223&nbsp;<em>r</em></span>: Both agree blue is better than yellow. Betty thinks red is worst, while Bill thinks red is best.</li>\n<li><span class=\"math\"><em>r</em>^<em>b</em>^<em>y</em></span>: Betty has the ranking red over blue over yellow, while Bill has the reverse.</li>\n</ol>\n<p>For the notation summarizing the profile,&nbsp;<span class=\"math\"><em>r</em> &gt;\u2004<em>b</em></span>&nbsp;indicated both agree that red is better than blue. <span class=\"math\"><em>r</em>^<em>b</em></span> says there is a conflict between the two with Betty preferring red. <span class=\"math\"><em>r&nbsp;</em>\u2223 (<em>b</em> &gt;\u2004<em>y</em>)</span> says there are two preference conflicts, with Betty preferring red over the other two options, so we alternatively think of this profile as <span class=\"math\"><em>r</em>^<em>y</em></span>, <span class=\"math\"><em>r</em>^<em>b</em></span>, and <span class=\"math\"><em>y</em> &gt;\u2004<em>b</em></span>.</p>\n<p>Now, we\u2019ll assign the 36 profiles a color, using each at least once, in a way that is monotonic. This will happen in six steps as depicted in the following diagram.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_k8r_0.png?v=a2b1e09a6866971838942b510abd0676\" alt=\"\" width=\"850\" height=\"588\"></p>\n<p>&nbsp;</p>\n<ol style=\"list-style-type: decimal\">\n<li>At least one profile must be red by assumption. Starting from that profile (whatever it is), we could move red to the top of both rankings and the outcome would still be red by monotonicity. Then all other profiles with red top-ranked by both also must be red since any swaps of blue and yellow can\u2019t change the outcome since red is still relatively above both. This gives us 1a,b,c,d. Blue and yellow go through similarly.</li>\n<li>Consider the profile <span class=\"math\"><em>b</em>^<em>y</em> &gt;\u2004<em>r</em></span> at the bottom of the diagram. The profile <span class=\"math\"><em>y</em> &gt;\u2004<em>b</em> &gt;\u2004<em>r</em></span> got yellow in 1f, so if Betty starts liking blue better from 2, the outcome has to stay yellow or switch to blue. Since monotonicity can\u2019t tell us more than that, we have to make a choice. Let\u2019s decide in favor of Betty and pick blue.</li>\n<li>Since we chose to resolve the conflict <span class=\"math\"><em>b</em>^<em>y</em></span> as <span class=\"math\"><em>b</em></span>, three more colorings follow since we must resolve this particular conflict in the same way everywhere. Keep in mind that <span class=\"math\"><em>b</em>^<em>y</em></span> is a different conflict than <span class=\"math\"><em>y</em>^<em>b</em></span> since it might matter who prefers which color. Consider 3b. This can\u2019t be yellow since yellow increases between this profile and 2, but 2 isn\u2019t yellow. It also can\u2019t be red since 1k isn\u2019t red, so we conclude 3b must be blue. Now consider 3a. Even though the conflict <span class=\"math\"><em>b</em>^<em>y</em></span> resolves in favor of blue, the outcome can\u2019t be blue since 1c isn\u2019t blue. Hence 3a must be red. From 3a, we conclude that <span class=\"math\"><em>r</em>^<em>y</em></span> was resolved as <span class=\"math\"><em>r</em></span>, so this new rule must apply everywhere. From 3c, we get a third rule that <span class=\"math\"><em>b</em>^<em>r</em> =\u2004<em>b</em></span>.</li>\n<li>With two new rules in the third step in addition to the rule from the second, more colorings follow. These colorings then give us the rules <span class=\"math\"><em>r</em>^<em>b</em> =\u2004<em>r</em></span>, <span class=\"math\"><em>y</em>^<em>b</em> =\u2004<em>y</em></span>, and <span class=\"math\"><em>y</em>^<em>r</em> =\u2004<em>y</em></span>.</li>\n<li>With all possible pairwise resolutions settled, all profiles can be colored.</li>\n</ol>\n<p>We\u2019ve found a monotonic, onto coloring! Notice that this is a dictatorship by Betty, choosing her top-ranked color for each profile. Everything comes down to favoring Betty over Bill in step two. Since Betty was pivotal once, she ends up having complete control. Of course, we could have resolved <span class=\"math\"><em>b</em>^<em>y</em></span> as <span class=\"math\"><em>y</em></span> instead, which would have given us a dictatorship by Bill. That choice in step two was the only degree of latitude we had, so these are the only two monotonic, onto colorings.</p>\n<p>This special-case proof of Gibbard-Satterthwaite was inspired by <a href=\"http://www.business.otago.ac.nz/econ/Personal/PH/P%20Hansen%20Proof%20of%20Arrow\u2019s%20Imposs%20Theorem.pdf\">Hansen (2002), \u201cAnother graphical proof of Arrow\u2019s impossibility theorem\u201d</a>. Full proofs of the theorems, done simultaneously side-by-side, are given in <a href=\"https://sites.google.com/site/philipjreny/arrow-gibbard-satterthwaite-econ-lett-2001.pdf?attredirects=0\">Reny (2001), \u201cArrow\u2019s theorem and the Gibbard-Satterthwaite theorem: a uni\ufb01ed approach\u201d</a>.</p>\n<p>&nbsp;</p>\n<p><em>Previously on:</em> <a href=\"/lw/k69/incentive_compatibility_and_the_revelation/\">Incentive-Compatibility and the Revelation Principle</a></p>\n<p><em>Next up:</em> <a href=\"/lw/kas/strategyproof_mechanisms_possibilities/\">Strategyproof Mechanisms: Possibilities</a></p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p>Recall that the direct mechanism for an SCF is where we simply ask the agents what type they are and then assign the outcome prescribed by the SCF for that type profile.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n<li id=\"fn2\">\n<p>The function <span class=\"math\"><em>f</em></span> is <em>onto</em> if every outcome in <span class=\"math\"><em>A</em></span> occurs for at least one input. The main role of this assumption to prevent <span class=\"math\"><em>f</em></span> from covertly restricting its image to two elements, so it\u2019s almost without loss of generality.<a href=\"#fnref2\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Gibbard-Satterthwaite: universal strategyproof mechanisms are out of reach", "anchor": "Gibbard_Satterthwaite__universal_strategyproof_mechanisms_are_out_of_reach", "level": 1}, {"title": "Proof of Gibbard-Satterthwaite (in a special case)", "anchor": "Proof_of_Gibbard_Satterthwaite__in_a_special_case_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["N4gDA5HPpGC4mbTEZ", "QG2ZQm2Fxq8ET22sT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T06:56:49.635Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Social Meetup - June (Games night)", "slug": "meetup-sydney-social-meetup-june-games-night", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sMXkXMgKZYN6npkRy/meetup-sydney-social-meetup-june-games-night", "pageUrlRelative": "/posts/sMXkXMgKZYN6npkRy/meetup-sydney-social-meetup-june-games-night", "linkUrl": "https://www.lesswrong.com/posts/sMXkXMgKZYN6npkRy/meetup-sydney-social-meetup-june-games-night", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Social%20Meetup%20-%20June%20(Games%20night)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Social%20Meetup%20-%20June%20(Games%20night)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMXkXMgKZYN6npkRy%2Fmeetup-sydney-social-meetup-june-games-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Social%20Meetup%20-%20June%20(Games%20night)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMXkXMgKZYN6npkRy%2Fmeetup-sydney-social-meetup-june-games-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMXkXMgKZYN6npkRy%2Fmeetup-sydney-social-meetup-june-games-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10g'>Sydney Social Meetup - June (Games night)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 June 2014 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale, NSW</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Everybody bring your favourite game, and we'll decide what we want to play on the night!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10g'>Sydney Social Meetup - June (Games night)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sMXkXMgKZYN6npkRy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7289669073500307e-06, "legacy": true, "legacyId": "26237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Social_Meetup___June__Games_night_\">Discussion article for the meetup : <a href=\"/meetups/10g\">Sydney Social Meetup - June (Games night)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 June 2014 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Humanist House, 10 Shepherd St Chippendale, NSW</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Everybody bring your favourite game, and we'll decide what we want to play on the night!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Social_Meetup___June__Games_night_1\">Discussion article for the meetup : <a href=\"/meetups/10g\">Sydney Social Meetup - June (Games night)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Social Meetup - June (Games night)", "anchor": "Discussion_article_for_the_meetup___Sydney_Social_Meetup___June__Games_night_", "level": 1}, {"title": "Discussion article for the meetup : Sydney Social Meetup - June (Games night)", "anchor": "Discussion_article_for_the_meetup___Sydney_Social_Meetup___June__Games_night_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T07:29:25.527Z", "modifiedAt": null, "url": null, "title": "Credence Calibration Icebreaker Game", "slug": "credence-calibration-icebreaker-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:26.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8JXuxkSniD3GE9baE/credence-calibration-icebreaker-game", "pageUrlRelative": "/posts/8JXuxkSniD3GE9baE/credence-calibration-icebreaker-game", "linkUrl": "https://www.lesswrong.com/posts/8JXuxkSniD3GE9baE/credence-calibration-icebreaker-game", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Credence%20Calibration%20Icebreaker%20Game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACredence%20Calibration%20Icebreaker%20Game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8JXuxkSniD3GE9baE%2Fcredence-calibration-icebreaker-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Credence%20Calibration%20Icebreaker%20Game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8JXuxkSniD3GE9baE%2Fcredence-calibration-icebreaker-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8JXuxkSniD3GE9baE%2Fcredence-calibration-icebreaker-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p class=\"MsoNormal\">The Aussie mega-meetup took place this past weekend. For it, a new kind of icebreaker was needed: one which is was not merely fun and sociable, but also instilled with the Way. Thus was the Credence Calibration Icebreaker forged.</p>\n<p class=\"MsoNormal\">A marriage of the <a href=\"http://www.google.com/url?q=http%3A%2F%2Facritch.com%2Fcredence-game%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE113vAoTc8Kxz6GmUyCQpKg9yeyw\">credence game</a> and the classic icebreaker, &lsquo;Say three things about yourself, one of them a lie&rsquo;, the game allows players to learn about each other, test their ability to deceive and detect deception, and discover just how calibrated they are.</p>\n<h3><br /></h3>\n<p><strong>How to play</strong></p>\n<p class=\"MsoNormal\">Playing instructions here: <a href=\"https://drive.google.com/file/d/0B-Qk4CQwYau0WjdXSXk5bXJmZ3M/edit?usp=sharing\">docx</a> <a href=\"https://drive.google.com/file/d/0B-Qk4CQwYau0MzhqdXpVMVM1aUk/edit?usp=sharing\">pdf</a>. Scoring <a href=\"http://goo.gl/C5bnSc\">spreadsheet</a>.</p>\n<p class=\"MsoNormal\">Each turn a player makes three statements about themselves. One and only one the statement must be intentionally untrue. All others players assign probabilities of being false to each statement. These probability sum to 1: P(A&rsquo;) + P(B&rsquo;) + P(C&rsquo; ) = 1.&nbsp; The game is <a href=\"http://www.google.com/url?q=http%3A%2F%2Facritch.com%2Finformation%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGw6Unn4PmhwrpfJkDYPUdtNBYsug\">scored</a>&nbsp;in the same manner as the credence game, but with reference to 33% rather than 50%.</p>\n<p class=\"MsoNormal\">The way we played it, a player would reveal which was the lie immediately after everyone else had assigned probabilities. The immediate feedback is more fun and allows players to recalibrate as they learn about their performance. Revealing which statements were lies at the end would require reminding everyone what the other statements were.</p>\n<p class=\"MsoNormal\">Many meetup groups have played the Aumann agree game where groups collectively assign credences to a collection of statements, however that game requires a collection of statements to be collected in advance. Once played, new statements must be collected for a new game. The credence calibration icebreaker has the advantage that players generate the statements allowing for easy replay.</p>\n<h3><br /></h3>\n<p><strong>Improvements</strong></p>\n<p class=\"MsoNormal\">Restrictions should be placed on the nature of the lies in order to control which skills are tested. We played without restrictions and most players generated a lie by altering a minor detail of a true statement which didn&rsquo;t affect its plausibility, e.g. &lsquo;My father&rsquo;s brain is frozen&rsquo;<span style=\"font-size: 11.199999809265137px;\"><sup>1</sup></span>&nbsp;vs. &lsquo;My uncle&rsquo;s brain is frozen&rsquo;. This resulted in the game being less about appraising the plausibility of statements and more about detecting deception by tells and other clues.</p>\n<p class=\"MsoNormal\">Following the original icebreaker game, three statements were used. Reducing the number of statements to two would have the following benefits:</p>\n<ul>\n<li>The game is currently data entry intensive, requiring two numbers per question per player to be entered. Two statements would halve this number.</li>\n<li>Assigning probabilities of falsehood is counter-intuitive to many, using two statements would allow for the typical direct assignment of truth.</li>\n<li>People find generating three statements difficult, two statements would reduce the effort.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>Statistics</strong></p>\n<p class=\"MsoNormal\">Various statistics are computed in the scoring spreadsheet. Results from our game showed a high correlation between number correct and score, 0.72, and that players improved over the course of the game thanks to diminishing overconfidence.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">1. True statement. &nbsp;As was 'I have three kidneys'.</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8hPTCJbwJnLBmfpCX": 2, "izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8JXuxkSniD3GE9baE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 1.7290120212320163e-06, "legacy": true, "legacyId": "26238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\">The Aussie mega-meetup took place this past weekend. For it, a new kind of icebreaker was needed: one which is was not merely fun and sociable, but also instilled with the Way. Thus was the Credence Calibration Icebreaker forged.</p>\n<p class=\"MsoNormal\">A marriage of the <a href=\"http://www.google.com/url?q=http%3A%2F%2Facritch.com%2Fcredence-game%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE113vAoTc8Kxz6GmUyCQpKg9yeyw\">credence game</a> and the classic icebreaker, \u2018Say three things about yourself, one of them a lie\u2019, the game allows players to learn about each other, test their ability to deceive and detect deception, and discover just how calibrated they are.</p>\n<h3><br></h3>\n<p><strong id=\"How_to_play\">How to play</strong></p>\n<p class=\"MsoNormal\">Playing instructions here: <a href=\"https://drive.google.com/file/d/0B-Qk4CQwYau0WjdXSXk5bXJmZ3M/edit?usp=sharing\">docx</a> <a href=\"https://drive.google.com/file/d/0B-Qk4CQwYau0MzhqdXpVMVM1aUk/edit?usp=sharing\">pdf</a>. Scoring <a href=\"http://goo.gl/C5bnSc\">spreadsheet</a>.</p>\n<p class=\"MsoNormal\">Each turn a player makes three statements about themselves. One and only one the statement must be intentionally untrue. All others players assign probabilities of being false to each statement. These probability sum to 1: P(A\u2019) + P(B\u2019) + P(C\u2019 ) = 1.&nbsp; The game is <a href=\"http://www.google.com/url?q=http%3A%2F%2Facritch.com%2Finformation%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGw6Unn4PmhwrpfJkDYPUdtNBYsug\">scored</a>&nbsp;in the same manner as the credence game, but with reference to 33% rather than 50%.</p>\n<p class=\"MsoNormal\">The way we played it, a player would reveal which was the lie immediately after everyone else had assigned probabilities. The immediate feedback is more fun and allows players to recalibrate as they learn about their performance. Revealing which statements were lies at the end would require reminding everyone what the other statements were.</p>\n<p class=\"MsoNormal\">Many meetup groups have played the Aumann agree game where groups collectively assign credences to a collection of statements, however that game requires a collection of statements to be collected in advance. Once played, new statements must be collected for a new game. The credence calibration icebreaker has the advantage that players generate the statements allowing for easy replay.</p>\n<h3><br></h3>\n<p><strong id=\"Improvements\">Improvements</strong></p>\n<p class=\"MsoNormal\">Restrictions should be placed on the nature of the lies in order to control which skills are tested. We played without restrictions and most players generated a lie by altering a minor detail of a true statement which didn\u2019t affect its plausibility, e.g. \u2018My father\u2019s brain is frozen\u2019<span style=\"font-size: 11.199999809265137px;\"><sup>1</sup></span>&nbsp;vs. \u2018My uncle\u2019s brain is frozen\u2019. This resulted in the game being less about appraising the plausibility of statements and more about detecting deception by tells and other clues.</p>\n<p class=\"MsoNormal\">Following the original icebreaker game, three statements were used. Reducing the number of statements to two would have the following benefits:</p>\n<ul>\n<li>The game is currently data entry intensive, requiring two numbers per question per player to be entered. Two statements would halve this number.</li>\n<li>Assigning probabilities of falsehood is counter-intuitive to many, using two statements would allow for the typical direct assignment of truth.</li>\n<li>People find generating three statements difficult, two statements would reduce the effort.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong id=\"Statistics\">Statistics</strong></p>\n<p class=\"MsoNormal\">Various statistics are computed in the scoring spreadsheet. Results from our game showed a high correlation between number correct and score, 0.72, and that players improved over the course of the game thanks to diminishing overconfidence.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">1. True statement. &nbsp;As was 'I have three kidneys'.</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "sections": [{"title": "How to play", "anchor": "How_to_play", "level": 1}, {"title": "Improvements", "anchor": "Improvements", "level": 1}, {"title": "Statistics", "anchor": "Statistics", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T15:09:20.944Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, May 16-31", "slug": "group-rationality-diary-may-16-31-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.229Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D3vrEJv5S2BxdkBnN/group-rationality-diary-may-16-31-0", "pageUrlRelative": "/posts/D3vrEJv5S2BxdkBnN/group-rationality-diary-may-16-31-0", "linkUrl": "https://www.lesswrong.com/posts/D3vrEJv5S2BxdkBnN/group-rationality-diary-may-16-31-0", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20May%2016-31&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20May%2016-31%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD3vrEJv5S2BxdkBnN%2Fgroup-rationality-diary-may-16-31-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20May%2016-31%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD3vrEJv5S2BxdkBnN%2Fgroup-rationality-diary-may-16-31-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD3vrEJv5S2BxdkBnN%2Fgroup-rationality-diary-may-16-31-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">This is the public group instrumental rationality diary for May 16-31.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Previous diary: <span style=\"text-decoration: underline;\"><a href=\"/lw/k5w/group_rationality_diary_may_115/\">May 1-15</a></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Next diary: &nbsp;<span style=\"text-decoration: underline;\"><a href=\"/r/discussion/lw/kal/group_rationality_diary_june_115/\">June 1-15</a></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D3vrEJv5S2BxdkBnN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jSQiJJPSmR8DJh3jR", "6r7crFbvYustFbLKN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T16:16:14.577Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-64", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RmtGzCFk37gy3LtHd/weekly-lw-meetups-64", "pageUrlRelative": "/posts/RmtGzCFk37gy3LtHd/weekly-lw-meetups-64", "linkUrl": "https://www.lesswrong.com/posts/RmtGzCFk37gy3LtHd/weekly-lw-meetups-64", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRmtGzCFk37gy3LtHd%2Fweekly-lw-meetups-64%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRmtGzCFk37gy3LtHd%2Fweekly-lw-meetups-64", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRmtGzCFk37gy3LtHd%2Fweekly-lw-meetups-64", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p><strong>This summary was posted to LW main on May 9th. The following week's summary is <a href=\"/lw/k8w/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/zr\"></a><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">17 May 2014 11:31AM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/zy\">Chicago Calibration Game:&nbsp;<span class=\"date\">10 May 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/zw\">Frankfurt: Presentation about Operant Conditioning:&nbsp;<span class=\"date\">11 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zp\">Helsinki Meetup - Effective Altruism:&nbsp;<span class=\"date\">10 May 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/zz\">Montreal - How to be charismatic:&nbsp;<span class=\"date\">26 May 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/zh\">Munich Meetup:&nbsp;<span class=\"date\">11 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/100\">Urbana-Champaign: Recreation:&nbsp;<span class=\"date\">11 May 2014 01:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">10 May 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/zv\">Canberra: Rationalist Fun and Games!:&nbsp;<span class=\"date\">24 May 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/101\">London social meetup - possibly in a park:&nbsp;<span class=\"date\">11 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zx\">Washington DC: Museums:&nbsp;<span class=\"date\">11 May 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RmtGzCFk37gy3LtHd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7297413934752345e-06, "legacy": true, "legacyId": "26200", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FwZXaqsMc9pPJCuu", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T21:43:48.151Z", "modifiedAt": null, "url": null, "title": "I'm About as Good as Dead: the End of Xah Lee", "slug": "i-m-about-as-good-as-dead-the-end-of-xah-lee", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tJonErzZvmWBhk6We/i-m-about-as-good-as-dead-the-end-of-xah-lee", "pageUrlRelative": "/posts/tJonErzZvmWBhk6We/i-m-about-as-good-as-dead-the-end-of-xah-lee", "linkUrl": "https://www.lesswrong.com/posts/tJonErzZvmWBhk6We/i-m-about-as-good-as-dead-the-end-of-xah-lee", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'm%20About%20as%20Good%20as%20Dead%3A%20the%20End%20of%20Xah%20Lee&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'm%20About%20as%20Good%20as%20Dead%3A%20the%20End%20of%20Xah%20Lee%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJonErzZvmWBhk6We%2Fi-m-about-as-good-as-dead-the-end-of-xah-lee%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'm%20About%20as%20Good%20as%20Dead%3A%20the%20End%20of%20Xah%20Lee%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJonErzZvmWBhk6We%2Fi-m-about-as-good-as-dead-the-end-of-xah-lee", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJonErzZvmWBhk6We%2Fi-m-about-as-good-as-dead-the-end-of-xah-lee", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://ergoemacs.org/misc/xah_as_good_as_dead.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tJonErzZvmWBhk6We", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -15, "extendedScore": null, "score": -5.6e-05, "legacy": true, "legacyId": "26242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-16T22:42:56.907Z", "modifiedAt": null, "url": null, "title": "Moving on from Cognito Mentoring", "slug": "moving-on-from-cognito-mentoring", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zPzXYWzQ5d73RxDMu/moving-on-from-cognito-mentoring", "pageUrlRelative": "/posts/zPzXYWzQ5d73RxDMu/moving-on-from-cognito-mentoring", "linkUrl": "https://www.lesswrong.com/posts/zPzXYWzQ5d73RxDMu/moving-on-from-cognito-mentoring", "postedAtFormatted": "Friday, May 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Moving%20on%20from%20Cognito%20Mentoring&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMoving%20on%20from%20Cognito%20Mentoring%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPzXYWzQ5d73RxDMu%2Fmoving-on-from-cognito-mentoring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Moving%20on%20from%20Cognito%20Mentoring%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPzXYWzQ5d73RxDMu%2Fmoving-on-from-cognito-mentoring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPzXYWzQ5d73RxDMu%2Fmoving-on-from-cognito-mentoring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2048, "htmlBody": "<p>Back in December 2013, Jonah Sinick and I launched <a href=\"http://cognitomentoring.org\">Cognito Mentoring</a>, an advising service for intellectually curious students. Our goal was to improve the quality of learning, productivity, and life choices of the student population at large, and we chose to focus on intellectually curious students because of their greater potential as well as our greater ability to relate with that population. We began by offering free personalized advising. Jonah announced the launch in a <a href=\"/r/lesswrong/lw/jee/cognito_mentoring_an_advising_service_for/\">LessWrong post</a>, hoping to attract the attention of LessWrong's intellectually curious readership.</p>\n<p>Since then, we feel we've done a fair amount, with a lot of help from LessWrong. We've published a few dozen <a href=\"http://www.cognitomentoring.org/blog\">blog posts</a> and have an <a href=\"http://info.cognitomentoring.org\">information wiki</a>. Slightly under a hundred people contacted us asking us for advice (<a href=\"/lw/jtz/what_we_learned_about_less_wrong_from_cognito/\">many from LessWrong</a>), and we had substantive interactions with over 50 of them. As our reviews from <a href=\"http://www.cognitomentoring.org/reviews-by-students\">students</a> and <a href=\"http://www.cognitomentoring.org/reviews-by-parents\">parents</a> suggest, we've made a good impression and have had a positive impact on many of the people we've advised. We're proud of what we've accomplished and grateful for the support and constructive criticism we've received on LessWrong.</p>\n<p>However, what we've learned in the last few months has led us to the conclusion that Cognito Mentoring is not ripe for being a full-time work opportunity for the two of us.</p>\n<p>For the last few months, we've eschewed regular jobs and instead done contract work that provides us the flexibility to work on Cognito Mentoring, eating into our savings somewhat to cover the cost of living differences. This is a temporary arrangement and is not sustainable. We therefore intend to scale back our work on Cognito Mentoring to \"maintenance mode\" so that people can continue to benefit from the resources we've already collected, with minimal additional effort on our part, freeing us up to take regular jobs with more demanding time requirements.</p>\n<p>We might revive Cognito Mentoring as a part-time or full-time endeavor in the future if there are significant changes to our beliefs about the traction, impact, and long-run financial viability of Cognito Mentoring. Part of the purpose of \"maintenance mode\" will be to leave open the possibility of such a revival if the idea does indeed have potential.</p>\n<p>In this post, I discuss some of the factors that led us to change our view, the conditions under which we might revive Cognito Mentoring, and more details about how \"maintenance mode\" for Cognito Mentoring will look.</p>\n<p><strong>Reason #1: Downward update on social value</strong></p>\n<p>We do think that the work we've done on Cognito Mentoring so far has generated social value, and the continued presence of the website will add more value over time. However, our view has shifted in the direction of lower <em>marginal social value</em> from working on Cognito Mentoring full-time, relative to simply keeping the website live and doing occasional work to improve it. Specifically:</p>\n<ul>\n<li>It's quite possible that the lowest-hanging fruit with respect to the advisees who would be most receptive to our advice has already been plucked. We received the bulk of our advisees through LessWrong within the month after our initial posting. Other places where we've posted about our service have led to fewer advisees (more <a href=\"/lw/jtz/what_we_learned_about_less_wrong_from_cognito/\">here</a>).</li>\n<li>Of our website content, only a small fraction of the content gets significant traction (see our <a href=\"http://info.cognitomentoring.org/w/index.php?title=Special:PopularPages&amp;limit=500&amp;offset=0\">list of popular pages</a>), so honing and promoting our best content might be a better strategy for improving social value than trying to create a comprehensive resource. This can be done while in maintenance mode, and does not require full-time effort on our part.</li>\n</ul>\n<p><em>What might lead us to change our minds</em>: If we continue to be contacted by large numbers of potentially high-impact people, or we get evidence that the advising we've already done has had significantly greater impact than we think it did, we'll update our social value upward.</p>\n<ul>\n</ul>\n<p><strong>Reason #2: Downward update on long-run financial viability<br /></strong></p>\n<p>We have enough cash to go on for a few more months. But for Cognito Mentoring to be something that we work full time on, we need an eventual <em>steady</em> source of income from it. Around mid-March 2014, we came to the realization that charging advisees is not a viable revenue source, as Jonah described at the end of <a href=\"/lw/jxu/how_can_cognito_mentoring_do_the_most_good/\">his post about how Cognito Mentoring can do the most good</a> (see also <a href=\"/lw/jxu/how_can_cognito_mentoring_do_the_most_good/aq23\">this comment by Luke Muehlhauser and Jonah's response to it below the comment</a>). At that point, we decided to focus more on our informational content and on looking for philanthropic funding.</p>\n<p>Our effort at looking into philanthropic funding did give us a few leads, and some of them could plausibly result in us getting small grants. However, none of the leads we got pointed to potential <em>steady long-term income sources</em>. In other words, we don't think philanthropic funding is a viable long-term revenue model for Cognito Mentoring.</p>\n<p>Our (anticipated) difficulty in getting philanthropic funding arises from two somewhat different reasons.</p>\n<ol>\n<li>What we're doing is somewhat new and does not fit the standard mold of educational grants. Educational foundations tend to give grants for fairly specific activities, and what we're doing does not seem to fit those.</li>\n<li>We haven't demonstrated significant traction or impact yet (even though we've had a reasonable amount of per capita impact, the total number of people we've influenced so far is relatively small). This circles back to Reason #1: funders' reluctance to fund us may in part stem from their belief that we won't have much social value, given our lack of traction so far. Insofar as funders' judgment carries some information value, this should also strengthen Reason #1.</li>\n</ol>\n<p><em>What might lead us to change our minds</em>: If we are contacted by a funder who is willing to bankroll us for over a year and also offer a convincing reason for why he/she thinks bankrolling us is a good idea (so that we're convinced that our funding can be sustained beyond a year) we'll change our minds.</p>\n<ol> </ol>\n<p><strong>Reason #3: Acquisition of knowledge and skills</strong></p>\n<p>One of the reasons we've been able to have an impact through Cognito Mentoring so far is that both Jonah and I have knowledge of many diverse topics related to the questions that our advisees have posed to us. But our knowledge is still woefully inadequate in a number of areas. In particular, many advisees have asked us questions in the realms of technology, entrepreneurship, and the job environment, and while we have pointed them to resources on these, firsthand experience, or close secondhand experience, would help us more effectively guide advisees. We intend to take jobs related to computer technology (in fields such as programming or data science), and these jobs might be at startups or put us in close contact with startups. This will better position us to return to mentoring later if we choose to resume it part-time or full-time.</p>\n<p>Knowledge and skills we acquire working in the technology sector could also help us design better interfaces or websites that can more directly address the needs of our audience. So far, we've thought of ourselves as content-oriented people, so we've used standard off-the-shelf software such as <a href=\"http://www.wordpress.org\">WordPress</a> (for our main website and blog) and <a href=\"http://www.mediawiki.org\">MediaWiki</a> (for our information wiki). Part of the reason is that we wanted to focus on content creation rather than interface design, but part of the reason we've stuck to these is that we didn't think we could design interfaces. Once we've acquired more programming and design experience, we might be more open to the idea of designing interfaces and software that can meet particular needs of our target audience.We might design an interface that helps people study more effectively, make better life decisions, or share reviews of courses and colleges, in a manner similar to softwares or websites such as <a href=\"http://ankisrs.net/\">Anki</a> or <a href=\"https://www.beeminder.com/\">Beeminder</a> or <a href=\"http://www.goodreads.com\">Goodreads</a>. There might also be potential for a more effective online resource that teaches programming than those in existence (e.g. <a href=\"http://www.codecademy.com\">Codecademy</a>). It's not clear right now whether there exists a useful opportunity of this sort that we are particularly well-suited to, but with more coding experience, we'll at least be <em>able</em> to implement an idea of this sort if we decide it has promise.</p>\n<p><strong>Reason #4: Letting it brew in the background can give us a better idea of the potential<br /></strong></p>\n<p>If we continue to gradually add content to the wiki, and continue to get links and traffic to it from other sources, it's likely that the traffic will grow slowly and steadily. The extent of organic growth will help us figure out how much promise Cognito Mentoring has. If our wiki gets to the point of steadily receiving thousands of pageviews a day, we will reconsider reviving Cognito Mentoring as a part-time or full-time endeavor. If, on the other hand, traffic remains at approximately the current level (about a hundred pageviews a day, once we exclude spikes arising from links from LessWrong and Marginal Revolution) then the idea is probably not worth revisiting, and we'll leave it in maintenance mode.</p>\n<p>In addition, by maintaining contact with the people we've advised, we can get more insight into the sort of impact we've had, whether it is significant over the long term, and how it can be improved. This again can tell us whether our impact is sufficiently large as to make Cognito Mentoring worth reviving.</p>\n<p><strong>What \"maintenance mode\" entails<br /></strong></p>\n<ol>\n<li><strong>We'll continue to have contact information available, but will scale back on personalized advising</strong>: People are welcome to contact us with questions and suggestions about content, but we will not generally offer detailed personalized responses or do research specific to individuals who contact us. We'll attempt to point people to relevant content we've already written, or to other resources we're already aware of that can address their concerns.</li>\n<li><strong>The information wiki will remain live</strong>, and we will continue to make occasional improvements, but we won't have a time schedule of when particular improvements have to be implemented by.</li>\n<li><strong>Existing blog posts will remain</strong>, but we probably won't be making many new blog posts. New blog posts will happen only if one of us has an idea that really seems worth sharing and for which the Cognito Mentoring blog is an ideal forum.</li>\n<li><strong>We'll continue our administrative roles in the communities of existing Cognito Mentoring advisees</strong></li>\n<li><strong>We'll continue periodically reviewing the progress of people we've advised so far</strong>: This will help us get a better sense of how valuable our work has been, and can be useful should we choose to revive Cognito Mentoring.</li>\n<li><strong>We'll continue to correspond with advisees we have so far (time permitting)</strong>, though we'll give more priority to advisees who continue to maintain contact of their own accord and those whose activities seem to have higher impact potential.</li>\n<li><strong>We'll try to get our best content linked from other sources, such as about.com</strong>: Sources like <a href=\"http://wwww.about.com\">about.com</a> are targeted at the general population. We can try to get linked to from there as an additional resource for the more intellectually curious population that's outside the core focus of about.com.</li>\n<li><strong>We'll link more extensively to other sources that people can use</strong>: For instance, we can more emphatically point to <a href=\"http://www.80000hours.org\">80,000 Hours</a> for people who are interested in career advising in relation to effective altruist pursuits. We can point to about.com and <a href=\"http://www.collegeconfidential.com\">College Confidential</a> for more general information about mainstream institutions. We already make a number of recommendations on our website, but as we stop working actively, it becomes all the more important that people who come to us are appropriately redirected to other sources that can help them.</li>\n</ol>\n<p><strong>Conclusion and summary (TL;DR)</strong></p>\n<p>We (<em>qua</em> Cognito Mentoring) are grateful to LessWrong for being welcoming of our posts, offering constructive criticism, and providing us with some advisees we've enjoyed working with. We think that the work we've done has value, but don't think that there's enough marginal value from full-time work on Cognito Mentoring. We think we can do more good for ourselves and the world by switching Cognito Mentoring to maintenance mode and freeing our time currently spent on Cognito Mentoring for other pursuits. The material that we have already produced will continue to remain in the public domain and we hope that people will benefit from it. We may revisit our \"maintenance mode\" decision if new evidence changes our view regarding traction, impact, and long-run financial viability.</p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 1, "jZF2jwLnPKBv6m3Ag": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zPzXYWzQ5d73RxDMu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 81, "extendedScore": null, "score": 0.000252, "legacy": true, "legacyId": "26234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 81, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Back in December 2013, Jonah Sinick and I launched <a href=\"http://cognitomentoring.org\">Cognito Mentoring</a>, an advising service for intellectually curious students. Our goal was to improve the quality of learning, productivity, and life choices of the student population at large, and we chose to focus on intellectually curious students because of their greater potential as well as our greater ability to relate with that population. We began by offering free personalized advising. Jonah announced the launch in a <a href=\"/r/lesswrong/lw/jee/cognito_mentoring_an_advising_service_for/\">LessWrong post</a>, hoping to attract the attention of LessWrong's intellectually curious readership.</p>\n<p>Since then, we feel we've done a fair amount, with a lot of help from LessWrong. We've published a few dozen <a href=\"http://www.cognitomentoring.org/blog\">blog posts</a> and have an <a href=\"http://info.cognitomentoring.org\">information wiki</a>. Slightly under a hundred people contacted us asking us for advice (<a href=\"/lw/jtz/what_we_learned_about_less_wrong_from_cognito/\">many from LessWrong</a>), and we had substantive interactions with over 50 of them. As our reviews from <a href=\"http://www.cognitomentoring.org/reviews-by-students\">students</a> and <a href=\"http://www.cognitomentoring.org/reviews-by-parents\">parents</a> suggest, we've made a good impression and have had a positive impact on many of the people we've advised. We're proud of what we've accomplished and grateful for the support and constructive criticism we've received on LessWrong.</p>\n<p>However, what we've learned in the last few months has led us to the conclusion that Cognito Mentoring is not ripe for being a full-time work opportunity for the two of us.</p>\n<p>For the last few months, we've eschewed regular jobs and instead done contract work that provides us the flexibility to work on Cognito Mentoring, eating into our savings somewhat to cover the cost of living differences. This is a temporary arrangement and is not sustainable. We therefore intend to scale back our work on Cognito Mentoring to \"maintenance mode\" so that people can continue to benefit from the resources we've already collected, with minimal additional effort on our part, freeing us up to take regular jobs with more demanding time requirements.</p>\n<p>We might revive Cognito Mentoring as a part-time or full-time endeavor in the future if there are significant changes to our beliefs about the traction, impact, and long-run financial viability of Cognito Mentoring. Part of the purpose of \"maintenance mode\" will be to leave open the possibility of such a revival if the idea does indeed have potential.</p>\n<p>In this post, I discuss some of the factors that led us to change our view, the conditions under which we might revive Cognito Mentoring, and more details about how \"maintenance mode\" for Cognito Mentoring will look.</p>\n<p><strong id=\"Reason__1__Downward_update_on_social_value\">Reason #1: Downward update on social value</strong></p>\n<p>We do think that the work we've done on Cognito Mentoring so far has generated social value, and the continued presence of the website will add more value over time. However, our view has shifted in the direction of lower <em>marginal social value</em> from working on Cognito Mentoring full-time, relative to simply keeping the website live and doing occasional work to improve it. Specifically:</p>\n<ul>\n<li>It's quite possible that the lowest-hanging fruit with respect to the advisees who would be most receptive to our advice has already been plucked. We received the bulk of our advisees through LessWrong within the month after our initial posting. Other places where we've posted about our service have led to fewer advisees (more <a href=\"/lw/jtz/what_we_learned_about_less_wrong_from_cognito/\">here</a>).</li>\n<li>Of our website content, only a small fraction of the content gets significant traction (see our <a href=\"http://info.cognitomentoring.org/w/index.php?title=Special:PopularPages&amp;limit=500&amp;offset=0\">list of popular pages</a>), so honing and promoting our best content might be a better strategy for improving social value than trying to create a comprehensive resource. This can be done while in maintenance mode, and does not require full-time effort on our part.</li>\n</ul>\n<p><em>What might lead us to change our minds</em>: If we continue to be contacted by large numbers of potentially high-impact people, or we get evidence that the advising we've already done has had significantly greater impact than we think it did, we'll update our social value upward.</p>\n<ul>\n</ul>\n<p><strong id=\"Reason__2__Downward_update_on_long_run_financial_viability\">Reason #2: Downward update on long-run financial viability<br></strong></p>\n<p>We have enough cash to go on for a few more months. But for Cognito Mentoring to be something that we work full time on, we need an eventual <em>steady</em> source of income from it. Around mid-March 2014, we came to the realization that charging advisees is not a viable revenue source, as Jonah described at the end of <a href=\"/lw/jxu/how_can_cognito_mentoring_do_the_most_good/\">his post about how Cognito Mentoring can do the most good</a> (see also <a href=\"/lw/jxu/how_can_cognito_mentoring_do_the_most_good/aq23\">this comment by Luke Muehlhauser and Jonah's response to it below the comment</a>). At that point, we decided to focus more on our informational content and on looking for philanthropic funding.</p>\n<p>Our effort at looking into philanthropic funding did give us a few leads, and some of them could plausibly result in us getting small grants. However, none of the leads we got pointed to potential <em>steady long-term income sources</em>. In other words, we don't think philanthropic funding is a viable long-term revenue model for Cognito Mentoring.</p>\n<p>Our (anticipated) difficulty in getting philanthropic funding arises from two somewhat different reasons.</p>\n<ol>\n<li>What we're doing is somewhat new and does not fit the standard mold of educational grants. Educational foundations tend to give grants for fairly specific activities, and what we're doing does not seem to fit those.</li>\n<li>We haven't demonstrated significant traction or impact yet (even though we've had a reasonable amount of per capita impact, the total number of people we've influenced so far is relatively small). This circles back to Reason #1: funders' reluctance to fund us may in part stem from their belief that we won't have much social value, given our lack of traction so far. Insofar as funders' judgment carries some information value, this should also strengthen Reason #1.</li>\n</ol>\n<p><em>What might lead us to change our minds</em>: If we are contacted by a funder who is willing to bankroll us for over a year and also offer a convincing reason for why he/she thinks bankrolling us is a good idea (so that we're convinced that our funding can be sustained beyond a year) we'll change our minds.</p>\n<ol> </ol>\n<p><strong id=\"Reason__3__Acquisition_of_knowledge_and_skills\">Reason #3: Acquisition of knowledge and skills</strong></p>\n<p>One of the reasons we've been able to have an impact through Cognito Mentoring so far is that both Jonah and I have knowledge of many diverse topics related to the questions that our advisees have posed to us. But our knowledge is still woefully inadequate in a number of areas. In particular, many advisees have asked us questions in the realms of technology, entrepreneurship, and the job environment, and while we have pointed them to resources on these, firsthand experience, or close secondhand experience, would help us more effectively guide advisees. We intend to take jobs related to computer technology (in fields such as programming or data science), and these jobs might be at startups or put us in close contact with startups. This will better position us to return to mentoring later if we choose to resume it part-time or full-time.</p>\n<p>Knowledge and skills we acquire working in the technology sector could also help us design better interfaces or websites that can more directly address the needs of our audience. So far, we've thought of ourselves as content-oriented people, so we've used standard off-the-shelf software such as <a href=\"http://www.wordpress.org\">WordPress</a> (for our main website and blog) and <a href=\"http://www.mediawiki.org\">MediaWiki</a> (for our information wiki). Part of the reason is that we wanted to focus on content creation rather than interface design, but part of the reason we've stuck to these is that we didn't think we could design interfaces. Once we've acquired more programming and design experience, we might be more open to the idea of designing interfaces and software that can meet particular needs of our target audience.We might design an interface that helps people study more effectively, make better life decisions, or share reviews of courses and colleges, in a manner similar to softwares or websites such as <a href=\"http://ankisrs.net/\">Anki</a> or <a href=\"https://www.beeminder.com/\">Beeminder</a> or <a href=\"http://www.goodreads.com\">Goodreads</a>. There might also be potential for a more effective online resource that teaches programming than those in existence (e.g. <a href=\"http://www.codecademy.com\">Codecademy</a>). It's not clear right now whether there exists a useful opportunity of this sort that we are particularly well-suited to, but with more coding experience, we'll at least be <em>able</em> to implement an idea of this sort if we decide it has promise.</p>\n<p><strong id=\"Reason__4__Letting_it_brew_in_the_background_can_give_us_a_better_idea_of_the_potential\">Reason #4: Letting it brew in the background can give us a better idea of the potential<br></strong></p>\n<p>If we continue to gradually add content to the wiki, and continue to get links and traffic to it from other sources, it's likely that the traffic will grow slowly and steadily. The extent of organic growth will help us figure out how much promise Cognito Mentoring has. If our wiki gets to the point of steadily receiving thousands of pageviews a day, we will reconsider reviving Cognito Mentoring as a part-time or full-time endeavor. If, on the other hand, traffic remains at approximately the current level (about a hundred pageviews a day, once we exclude spikes arising from links from LessWrong and Marginal Revolution) then the idea is probably not worth revisiting, and we'll leave it in maintenance mode.</p>\n<p>In addition, by maintaining contact with the people we've advised, we can get more insight into the sort of impact we've had, whether it is significant over the long term, and how it can be improved. This again can tell us whether our impact is sufficiently large as to make Cognito Mentoring worth reviving.</p>\n<p><strong id=\"What__maintenance_mode__entails\">What \"maintenance mode\" entails<br></strong></p>\n<ol>\n<li><strong>We'll continue to have contact information available, but will scale back on personalized advising</strong>: People are welcome to contact us with questions and suggestions about content, but we will not generally offer detailed personalized responses or do research specific to individuals who contact us. We'll attempt to point people to relevant content we've already written, or to other resources we're already aware of that can address their concerns.</li>\n<li><strong>The information wiki will remain live</strong>, and we will continue to make occasional improvements, but we won't have a time schedule of when particular improvements have to be implemented by.</li>\n<li><strong>Existing blog posts will remain</strong>, but we probably won't be making many new blog posts. New blog posts will happen only if one of us has an idea that really seems worth sharing and for which the Cognito Mentoring blog is an ideal forum.</li>\n<li><strong>We'll continue our administrative roles in the communities of existing Cognito Mentoring advisees</strong></li>\n<li><strong>We'll continue periodically reviewing the progress of people we've advised so far</strong>: This will help us get a better sense of how valuable our work has been, and can be useful should we choose to revive Cognito Mentoring.</li>\n<li><strong>We'll continue to correspond with advisees we have so far (time permitting)</strong>, though we'll give more priority to advisees who continue to maintain contact of their own accord and those whose activities seem to have higher impact potential.</li>\n<li><strong>We'll try to get our best content linked from other sources, such as about.com</strong>: Sources like <a href=\"http://wwww.about.com\">about.com</a> are targeted at the general population. We can try to get linked to from there as an additional resource for the more intellectually curious population that's outside the core focus of about.com.</li>\n<li><strong>We'll link more extensively to other sources that people can use</strong>: For instance, we can more emphatically point to <a href=\"http://www.80000hours.org\">80,000 Hours</a> for people who are interested in career advising in relation to effective altruist pursuits. We can point to about.com and <a href=\"http://www.collegeconfidential.com\">College Confidential</a> for more general information about mainstream institutions. We already make a number of recommendations on our website, but as we stop working actively, it becomes all the more important that people who come to us are appropriately redirected to other sources that can help them.</li>\n</ol>\n<p><strong id=\"Conclusion_and_summary__TL_DR_\">Conclusion and summary (TL;DR)</strong></p>\n<p>We (<em>qua</em> Cognito Mentoring) are grateful to LessWrong for being welcoming of our posts, offering constructive criticism, and providing us with some advisees we've enjoyed working with. We think that the work we've done has value, but don't think that there's enough marginal value from full-time work on Cognito Mentoring. We think we can do more good for ourselves and the world by switching Cognito Mentoring to maintenance mode and freeing our time currently spent on Cognito Mentoring for other pursuits. The material that we have already produced will continue to remain in the public domain and we hope that people will benefit from it. We may revisit our \"maintenance mode\" decision if new evidence changes our view regarding traction, impact, and long-run financial viability.</p>\n<ol> </ol>", "sections": [{"title": "Reason #1: Downward update on social value", "anchor": "Reason__1__Downward_update_on_social_value", "level": 1}, {"title": "Reason #2: Downward update on long-run financial viability", "anchor": "Reason__2__Downward_update_on_long_run_financial_viability", "level": 1}, {"title": "Reason #3: Acquisition of knowledge and skills", "anchor": "Reason__3__Acquisition_of_knowledge_and_skills", "level": 1}, {"title": "Reason #4: Letting it brew in the background can give us a better idea of the potential", "anchor": "Reason__4__Letting_it_brew_in_the_background_can_give_us_a_better_idea_of_the_potential", "level": 1}, {"title": "What \"maintenance mode\" entails", "anchor": "What__maintenance_mode__entails", "level": 1}, {"title": "Conclusion and summary (TL;DR)", "anchor": "Conclusion_and_summary__TL_DR_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hphGa6xfad3m4imCs", "xqEuAD3dXHPSFeerd", "e9H4jyJbzhabMfsmM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-17T20:11:32.699Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup XIII. (international)", "slug": "meetup-bratislava-meetup-xiii-international", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:29.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iqgPLGaHRMKwdB2Lz/meetup-bratislava-meetup-xiii-international", "pageUrlRelative": "/posts/iqgPLGaHRMKwdB2Lz/meetup-bratislava-meetup-xiii-international", "linkUrl": "https://www.lesswrong.com/posts/iqgPLGaHRMKwdB2Lz/meetup-bratislava-meetup-xiii-international", "postedAtFormatted": "Saturday, May 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20XIII.%20(international)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20XIII.%20(international)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqgPLGaHRMKwdB2Lz%2Fmeetup-bratislava-meetup-xiii-international%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20XIII.%20(international)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqgPLGaHRMKwdB2Lz%2Fmeetup-bratislava-meetup-xiii-international", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqgPLGaHRMKwdB2Lz%2Fmeetup-bratislava-meetup-xiii-international", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10h'>Bratislava Meetup XIII. (international)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 May 2014 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Mickiewiczova 2, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is an international meetup (which means that we will meet during a weekend and speak English), so if you wanted to visit Bratislava, this is the right moment to do. More info in comments.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10h'>Bratislava Meetup XIII. (international)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iqgPLGaHRMKwdB2Lz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7320644500654006e-06, "legacy": true, "legacyId": "26244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_XIII___international_\">Discussion article for the meetup : <a href=\"/meetups/10h\">Bratislava Meetup XIII. (international)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 May 2014 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Mickiewiczova 2, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is an international meetup (which means that we will meet during a weekend and speak English), so if you wanted to visit Bratislava, this is the right moment to do. More info in comments.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_XIII___international_1\">Discussion article for the meetup : <a href=\"/meetups/10h\">Bratislava Meetup XIII. (international)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup XIII. (international)", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_XIII___international_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup XIII. (international)", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_XIII___international_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-18T05:28:05.423Z", "modifiedAt": null, "url": null, "title": "Strawman Yourself", "slug": "strawman-yourself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XvK7czNCEtWbYowik/strawman-yourself", "pageUrlRelative": "/posts/XvK7czNCEtWbYowik/strawman-yourself", "linkUrl": "https://www.lesswrong.com/posts/XvK7czNCEtWbYowik/strawman-yourself", "postedAtFormatted": "Sunday, May 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strawman%20Yourself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrawman%20Yourself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvK7czNCEtWbYowik%2Fstrawman-yourself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strawman%20Yourself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvK7czNCEtWbYowik%2Fstrawman-yourself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvK7czNCEtWbYowik%2Fstrawman-yourself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>One good way to ensure that your plans are robust is to strawman yourself. Look at your plan in the most critical, contemptuous light possible and come up with the obvious uncharitable insulting argument for why you will fail.</p>\n<p><em>In many cases, the obvious uncharitable insulting argument will still be fundamentally correct. </em></p>\n<p>If it is, your plan probably needs work. This technique seems to work not because it taps into some secret vault of wisdom (after all, <a href=\"/lw/iql/making_fun_of_things_is_easy/\">making fun of things is easy</a>), but because it is an elegant way to shift yourself into a critical mindset.</p>\n<p>For instance, I recently came up with a complex plan to achieve one of my goals. Then I strawmanned myself; the strawman version of why this plan would fail was simply \"large and complicated plans don't work.\" I thought about that for a moment, concluded \"yep, large and complicated plans don't work,\" and came up with a simple, elegant plan to achieve the same ends.</p>\n<p>You may ask \"why didn't you just come up with a simple, elegant plan in the first place?\" The answer is that elegance is hard. It's easier to add on special case after special case, not realizing how much complexity debt you've added. Strawmanning yourself is one way to safeguard against this risk, as well as many others.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XvK7czNCEtWbYowik", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 28, "extendedScore": null, "score": 1.7328374052405197e-06, "legacy": true, "legacyId": "26241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["srAdo8k7De8mvvngr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-19T04:47:00.042Z", "modifiedAt": null, "url": null, "title": "OpenWorm and differential technological development", "slug": "openworm-and-differential-technological-development", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dCTFXi2Z2f2aacmNa/openworm-and-differential-technological-development", "pageUrlRelative": "/posts/dCTFXi2Z2f2aacmNa/openworm-and-differential-technological-development", "linkUrl": "https://www.lesswrong.com/posts/dCTFXi2Z2f2aacmNa/openworm-and-differential-technological-development", "postedAtFormatted": "Monday, May 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20OpenWorm%20and%20differential%20technological%20development&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpenWorm%20and%20differential%20technological%20development%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCTFXi2Z2f2aacmNa%2Fopenworm-and-differential-technological-development%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=OpenWorm%20and%20differential%20technological%20development%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCTFXi2Z2f2aacmNa%2Fopenworm-and-differential-technological-development", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCTFXi2Z2f2aacmNa%2Fopenworm-and-differential-technological-development", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 286, "htmlBody": "<p>According to Robin Hanson's arguments in <a href=\"http://www.overcomingbias.com/2009/11/bad-emulation-advance.html\">this blog post</a>, we want to promote research in to cell modeling technology (ideally at the expense of research in to faster computer hardware). &nbsp;That would mean funding <a href=\"https://www.kickstarter.com/projects/openworm/openworm-a-digital-organism-in-your-browser\">this kickstarter</a>, which is ending in 11 hours (it may still succeed; there are a few tricks for pushing borderline kickstarters through). &nbsp;I already pledged $250; I'm not sure if I should pledge significantly more on the strength of one Hanson blog post. &nbsp;Thoughts from anyone? &nbsp;(I also encourage other folks to pledge! &nbsp;Maybe we can name neurons after characters in HPMOR or something. &nbsp;<strong>EDIT</strong>: Or maybe funding OpenWorm is a bad idea; see <a href=\"https://intelligence.org/files/SS11Workshop.pdf\">this link</a>.)</p>\n<div>I'm also curious on what people think about the efficiency of trying to pursue <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a> directly this way vs funding MIRI/FHI. &nbsp;I haven't read the entire conversation referenced <a href=\"http://marginalrevolution.com/marginalrevolution/2014/04/nick-becksteads-conversation-with-tyler-cowen.html\">here</a>, but this bit from the blog post sounds correct-ish to me:</div>\n<div><br /></div>\n<blockquote>\n<div>People doing philosophical work to try to reduce existential risk are largely wasting their time. Tyler doesn&rsquo;t think it&rsquo;s a serious effort, though it may be good publicity for something that will pay off later. A serious effort looks more like the parts of the US government that trained people to infiltrate the post-collapse Soviet Union and then locate and neutralize nuclear weapons. There was also a serious effort by the people who set up hotlines between leaders to be used to quickly communicate about nuclear attacks (e.g., to help quickly convince a leader in country A that a fishy object on their radar isn&rsquo;t an incoming nuclear attack).</div>\n</blockquote>\n<div><br /></div>\n<div>Edit: For some reason I forgot about <a href=\"/lw/9oq/link_2011_team_may_be_chosen_to_receive_14/5zox\">this</a> previous discussion on this topic, which makes the case for funding OpenWorm look less clear-cut.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dCTFXi2Z2f2aacmNa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "26247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-19T04:49:59.430Z", "modifiedAt": null, "url": null, "title": "Open Thread, May 19 - 25, 2014", "slug": "open-thread-may-19-25-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "somnicule", "createdAt": "2014-04-09T10:59:48.784Z", "isAdmin": false, "displayName": "somnicule"}, "userId": "etsjugNLC8vp2TEXF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Q88eZ4EF2PGbxjcQ/open-thread-may-19-25-2014", "pageUrlRelative": "/posts/8Q88eZ4EF2PGbxjcQ/open-thread-may-19-25-2014", "linkUrl": "https://www.lesswrong.com/posts/8Q88eZ4EF2PGbxjcQ/open-thread-may-19-25-2014", "postedAtFormatted": "Monday, May 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20May%2019%20-%2025%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20May%2019%20-%2025%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Q88eZ4EF2PGbxjcQ%2Fopen-thread-may-19-25-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20May%2019%20-%2025%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Q88eZ4EF2PGbxjcQ%2Fopen-thread-may-19-25-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Q88eZ4EF2PGbxjcQ%2Fopen-thread-may-19-25-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p style=\"text-align: justify;\"><a href=\"/r/discussion/lw/k7y/open_thread_may_12_18_2014\">Previous Open Thread</a></p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<p style=\"text-align: justify;\"><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\">You know the drill - If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<div id=\"entry_t3_k7y\" class=\"content clear\" style=\"color: #000000; font-family: Arial, Helvetica, sans-serif; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 18px; orphans: auto; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: #ffffff;\">\n<div class=\"md\" style=\"font-size: small;\">\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. Open Threads should start on Monday, and end on Sunday.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. Open Threads should be posted in Discussion, and not Main.</span></p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Q88eZ4EF2PGbxjcQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.734787134143981e-06, "legacy": true, "legacyId": "26248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 291, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EsRTvwRnXjA44K2Tr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-19T20:49:07.529Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9", "slug": "meetup-utrecht-brainstorm-and-ethics-discussion-at-the-film", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.135Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoerenMind", "createdAt": "2013-07-16T18:10:55.180Z", "isAdmin": false, "displayName": "SoerenMind"}, "userId": "DGetADxtea2LRL946", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PCMa9y9RufgznM88R/meetup-utrecht-brainstorm-and-ethics-discussion-at-the-film", "pageUrlRelative": "/posts/PCMa9y9RufgznM88R/meetup-utrecht-brainstorm-and-ethics-discussion-at-the-film", "linkUrl": "https://www.lesswrong.com/posts/PCMa9y9RufgznM88R/meetup-utrecht-brainstorm-and-ethics-discussion-at-the-film", "postedAtFormatted": "Monday, May 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht-%20Brainstorm%20and%20ethics%20discussion%20at%20the%20Film%20Caf%C3%A9&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht-%20Brainstorm%20and%20ethics%20discussion%20at%20the%20Film%20Caf%C3%A9%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPCMa9y9RufgznM88R%2Fmeetup-utrecht-brainstorm-and-ethics-discussion-at-the-film%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht-%20Brainstorm%20and%20ethics%20discussion%20at%20the%20Film%20Caf%C3%A9%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPCMa9y9RufgznM88R%2Fmeetup-utrecht-brainstorm-and-ethics-discussion-at-the-film", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPCMa9y9RufgznM88R%2Fmeetup-utrecht-brainstorm-and-ethics-discussion-at-the-film", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 284, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10i'>Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 May 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Slachtstraat 5, 3512 BC Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Brainstorm</p>\n\n<p>We would like to make our meetups more useful for the attendants. Therefore the meetup will start with a brainstorm to collect ideas and decide on how to proceed in future. We will talk about both the discussion meetups themselves and potential projects to work on. If you have any suggestions or ideas, this will be a great opportunity to have your say.</p>\n\n<p>Ethics discussion</p>\n\n<p>Should your driverless car kill you to save two other people? When is it ethical to hand out our decisions over to machines?\nWe will discuss the following article: <a href=\"http://aeon.co/magazine/world-views/can-we-design-systems-to-automate-ethics/\" rel=\"nofollow\">http://aeon.co/magazine/world-views/can-we-design-systems-to-automate-ethics/</a>\nTo get most out of the discussion, you are recommended to read the article in advance. It's a nice stepping stone to topics like moral psychology and AI as well. \nVery relevant to this topic as well to rational ethics in general: this presentation by Harvard philosopher Joshua Greene on learning to use our moral brains: <a href=\"https://www.youtube.com/watch?v=_-vleKVkMec\" rel=\"nofollow\">https://www.youtube.com/watch?v=_-vleKVkMec</a> . When is our moral intuition correct, and when not? This one is an optional addition. \nLooking forward to meeting you again! New people will be warmly welcomed!</p>\n\n<p>Practical</p>\n\n<p><strong>Mind that the time has changed to 2pm!</strong></p>\n\n<p>We will meet at Film Caf\u00e9 Oscar, which is just around the corner from De Winkel van Sinkel. I will be holding a sign that says 'LW' on it. If you have trouble finding us you can reach me at 0684140766 or me Imma at 0610524989.</p>\n\n<p>We have <a href=\"https://docs.google.com/document/d/16bBtla1iVzkJjie-JK7Ozb9Ao8SbyJ9U924XyaEXTqY/edit\" rel=\"nofollow\">this Google Doc</a> where anyone can make suggestions, also for the coming meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10i'>Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PCMa9y9RufgznM88R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7361233133837194e-06, "legacy": true, "legacyId": "26250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Brainstorm_and_ethics_discussion_at_the_Film_Caf_\">Discussion article for the meetup : <a href=\"/meetups/10i\">Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 May 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Slachtstraat 5, 3512 BC Utrecht</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Brainstorm</p>\n\n<p>We would like to make our meetups more useful for the attendants. Therefore the meetup will start with a brainstorm to collect ideas and decide on how to proceed in future. We will talk about both the discussion meetups themselves and potential projects to work on. If you have any suggestions or ideas, this will be a great opportunity to have your say.</p>\n\n<p>Ethics discussion</p>\n\n<p>Should your driverless car kill you to save two other people? When is it ethical to hand out our decisions over to machines?\nWe will discuss the following article: <a href=\"http://aeon.co/magazine/world-views/can-we-design-systems-to-automate-ethics/\" rel=\"nofollow\">http://aeon.co/magazine/world-views/can-we-design-systems-to-automate-ethics/</a>\nTo get most out of the discussion, you are recommended to read the article in advance. It's a nice stepping stone to topics like moral psychology and AI as well. \nVery relevant to this topic as well to rational ethics in general: this presentation by Harvard philosopher Joshua Greene on learning to use our moral brains: <a href=\"https://www.youtube.com/watch?v=_-vleKVkMec\" rel=\"nofollow\">https://www.youtube.com/watch?v=_-vleKVkMec</a> . When is our moral intuition correct, and when not? This one is an optional addition. \nLooking forward to meeting you again! New people will be warmly welcomed!</p>\n\n<p>Practical</p>\n\n<p><strong id=\"Mind_that_the_time_has_changed_to_2pm_\">Mind that the time has changed to 2pm!</strong></p>\n\n<p>We will meet at Film Caf\u00e9 Oscar, which is just around the corner from De Winkel van Sinkel. I will be holding a sign that says 'LW' on it. If you have trouble finding us you can reach me at 0684140766 or me Imma at 0610524989.</p>\n\n<p>We have <a href=\"https://docs.google.com/document/d/16bBtla1iVzkJjie-JK7Ozb9Ao8SbyJ9U924XyaEXTqY/edit\" rel=\"nofollow\">this Google Doc</a> where anyone can make suggestions, also for the coming meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Brainstorm_and_ethics_discussion_at_the_Film_Caf_1\">Discussion article for the meetup : <a href=\"/meetups/10i\">Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9", "anchor": "Discussion_article_for_the_meetup___Utrecht__Brainstorm_and_ethics_discussion_at_the_Film_Caf_", "level": 1}, {"title": "Mind that the time has changed to 2pm!", "anchor": "Mind_that_the_time_has_changed_to_2pm_", "level": 2}, {"title": "Discussion article for the meetup : Utrecht- Brainstorm and ethics discussion at the Film Caf\u00e9", "anchor": "Discussion_article_for_the_meetup___Utrecht__Brainstorm_and_ethics_discussion_at_the_Film_Caf_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-20T13:08:17.842Z", "modifiedAt": null, "url": null, "title": "Meetup : Zurich/Z\u00fcrich meetup (come out of the woodwork)", "slug": "meetup-zurich-zuerich-meetup-come-out-of-the-woodwork", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u29rC6957jAeWwZCp/meetup-zurich-zuerich-meetup-come-out-of-the-woodwork", "pageUrlRelative": "/posts/u29rC6957jAeWwZCp/meetup-zurich-zuerich-meetup-come-out-of-the-woodwork", "linkUrl": "https://www.lesswrong.com/posts/u29rC6957jAeWwZCp/meetup-zurich-zuerich-meetup-come-out-of-the-woodwork", "postedAtFormatted": "Tuesday, May 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Zurich%2FZ%C3%BCrich%20meetup%20(come%20out%20of%20the%20woodwork)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Zurich%2FZ%C3%BCrich%20meetup%20(come%20out%20of%20the%20woodwork)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu29rC6957jAeWwZCp%2Fmeetup-zurich-zuerich-meetup-come-out-of-the-woodwork%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Zurich%2FZ%C3%BCrich%20meetup%20(come%20out%20of%20the%20woodwork)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu29rC6957jAeWwZCp%2Fmeetup-zurich-zuerich-meetup-come-out-of-the-woodwork", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu29rC6957jAeWwZCp%2Fmeetup-zurich-zuerich-meetup-come-out-of-the-woodwork", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10j'>Zurich/Z\u00fcrich meetup (come out of the woodwork)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 May 2014 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Z\u00fcrich Switzerland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be a meetup taking place this wednesday the 21st of may at 19:30; at the Kronenhalle-Bar(Bellevue in Z\u00fcrich)\n<a href=\"https://www.google.com/maps/place/Kronenhalle/\" rel=\"nofollow\">https://www.google.com/maps/place/Kronenhalle/</a>\nI will print a paper written \"LW Meetup\" and place it on our table, so you guys can find it.\nCome on folks, I know there are some LWers in Z\u00fcrich, lets come out of the woodwork. :)\nTwo attendants confirmed.\nHow to get there: Take public transport to the Bellevue station, then walk.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10j'>Zurich/Z\u00fcrich meetup (come out of the woodwork)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u29rC6957jAeWwZCp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7374892856666392e-06, "legacy": true, "legacyId": "26251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Zurich_Z_rich_meetup__come_out_of_the_woodwork_\">Discussion article for the meetup : <a href=\"/meetups/10j\">Zurich/Z\u00fcrich meetup (come out of the woodwork)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 May 2014 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Z\u00fcrich Switzerland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be a meetup taking place this wednesday the 21st of may at 19:30; at the Kronenhalle-Bar(Bellevue in Z\u00fcrich)\n<a href=\"https://www.google.com/maps/place/Kronenhalle/\" rel=\"nofollow\">https://www.google.com/maps/place/Kronenhalle/</a>\nI will print a paper written \"LW Meetup\" and place it on our table, so you guys can find it.\nCome on folks, I know there are some LWers in Z\u00fcrich, lets come out of the woodwork. :)\nTwo attendants confirmed.\nHow to get there: Take public transport to the Bellevue station, then walk.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Zurich_Z_rich_meetup__come_out_of_the_woodwork_1\">Discussion article for the meetup : <a href=\"/meetups/10j\">Zurich/Z\u00fcrich meetup (come out of the woodwork)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Zurich/Z\u00fcrich meetup (come out of the woodwork)", "anchor": "Discussion_article_for_the_meetup___Zurich_Z_rich_meetup__come_out_of_the_woodwork_", "level": 1}, {"title": "Discussion article for the meetup : Zurich/Z\u00fcrich meetup (come out of the woodwork)", "anchor": "Discussion_article_for_the_meetup___Zurich_Z_rich_meetup__come_out_of_the_woodwork_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-20T13:11:42.491Z", "modifiedAt": null, "url": null, "title": "Meetup Z\u00fcrich last minute", "slug": "meetup-zuerich-last-minute", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KAFoGwJShbjtvRmta/meetup-zuerich-last-minute", "pageUrlRelative": "/posts/KAFoGwJShbjtvRmta/meetup-zuerich-last-minute", "linkUrl": "https://www.lesswrong.com/posts/KAFoGwJShbjtvRmta/meetup-zuerich-last-minute", "postedAtFormatted": "Tuesday, May 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Z%C3%BCrich%20last%20minute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Z%C3%BCrich%20last%20minute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAFoGwJShbjtvRmta%2Fmeetup-zuerich-last-minute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Z%C3%BCrich%20last%20minute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAFoGwJShbjtvRmta%2Fmeetup-zuerich-last-minute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAFoGwJShbjtvRmta%2Fmeetup-zuerich-last-minute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6, "htmlBody": "<p>Meetup tomorrow(Wednesday) at 19:30, lets go!</p>\n<p>http://lesswrong.com/meetups/10j</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KAFoGwJShbjtvRmta", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7374940471050719e-06, "legacy": true, "legacyId": "26252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-20T23:38:19.450Z", "modifiedAt": null, "url": null, "title": "[LINK] Prisoner's Dilemma? Not So Much", "slug": "link-prisoner-s-dilemma-not-so-much", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:34.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "torekp", "createdAt": "2009-07-17T21:58:25.456Z", "isAdmin": false, "displayName": "torekp"}, "userId": "5S73XuxxK2X6nygmc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fS59G4nqcTngsnvMn/link-prisoner-s-dilemma-not-so-much", "pageUrlRelative": "/posts/fS59G4nqcTngsnvMn/link-prisoner-s-dilemma-not-so-much", "linkUrl": "https://www.lesswrong.com/posts/fS59G4nqcTngsnvMn/link-prisoner-s-dilemma-not-so-much", "postedAtFormatted": "Tuesday, May 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Prisoner's%20Dilemma%3F%20Not%20So%20Much&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Prisoner's%20Dilemma%3F%20Not%20So%20Much%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfS59G4nqcTngsnvMn%2Flink-prisoner-s-dilemma-not-so-much%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Prisoner's%20Dilemma%3F%20Not%20So%20Much%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfS59G4nqcTngsnvMn%2Flink-prisoner-s-dilemma-not-so-much", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfS59G4nqcTngsnvMn%2Flink-prisoner-s-dilemma-not-so-much", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Hannes Rusch argues that the Prisoner's Dilemma is best understood as merely one game of very many:</p>\n<blockquote>\n<p>only 2 of the 726 combinatorially possible strategically unique ordinal 2x2 games have the detrimental characteristics of a PD and that the frequency of PD-type games in a space of games with random payoffs does not exceed about 3.5%. Although this does not compellingly imply that the relevance of PDs is overestimated, in the absence of convergent empirical information about the ancestral human social niche, this finding can be interpreted in favour of a rather neglected answer to the question of how the founding groups of human cooperation themselves came to cooperate: Behavioural and/or psychological mechanisms which evolved for other, possibly more frequent, social interaction situations might have been applied to PD-type dilemmas only later.</p>\n</blockquote>\n<p><a href=\"http://www2.units.it/etica/2013_2/RUSCH.pdf\" target=\"_blank\">http://www2.units.it/etica/2013_2/RUSCH.pdf</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fS59G4nqcTngsnvMn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.7383691963021799e-06, "legacy": true, "legacyId": "26254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-21T04:55:39.054Z", "modifiedAt": null, "url": null, "title": "More and Less than Solomonoff Induction", "slug": "more-and-less-than-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/od2hkE2CS9tYZXHkx/more-and-less-than-solomonoff-induction", "pageUrlRelative": "/posts/od2hkE2CS9tYZXHkx/more-and-less-than-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/od2hkE2CS9tYZXHkx/more-and-less-than-solomonoff-induction", "postedAtFormatted": "Wednesday, May 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20and%20Less%20than%20Solomonoff%20Induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20and%20Less%20than%20Solomonoff%20Induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fod2hkE2CS9tYZXHkx%2Fmore-and-less-than-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20and%20Less%20than%20Solomonoff%20Induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fod2hkE2CS9tYZXHkx%2Fmore-and-less-than-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fod2hkE2CS9tYZXHkx%2Fmore-and-less-than-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1380, "htmlBody": "<p>I've been thinking about how to put induction with limited resources on a firmer foundation.&nbsp;This may just be retracing the steps of others, but that's okay with me. Mostly I just want to talk about these thoughts.</p>\n<p>After a few points of introduction.</p>\n<p><strong>What's Solomonoff induction?</strong></p>\n<p>Suppose we're given some starting data, and asked to predict the future. Solomonoff induction predicts the future by combining the predictions of all programs that (1) output the starting data up until now, and (2) aren't the continuation of another such program. The predictions are combined according to a weighting that decreases exponentially as the length of the program increases.</p>\n<p><strong>Why is it a good idea?</strong></p>\n<p>The simplest answer is that we have a frequentist guarantee that if the \"true program\" generating our input has some length N (that is, if the observable universe is a big but finite-sized computer), then our predictions will only be wrong a limited number of times, and after that we'll predict the correctly every time.</p>\n<p>A more bayesian answer would start with the information that our observations can be generated by some finite-sized program, and then derive that something like Solomonoff induction has to represent our true prior over generating programs - as the length gets bigger, our probability is required to go to zero at infinity, and an exponential is the maximum-entropy such curve. This is not a complete answer, but it at least makes the missing pieces more apparent.</p>\n<p><strong>Why won't it work with limited resources</strong></p>\n<p>The trouble with using Solomonoff induction in real life is that to pick out which programs output our data so far, we need to run every program - and if the program doesn't ever halt, we need to use a halting oracle to stop it or else we'll take infinite time.</p>\n<p><strong>Limited resources require us to only pick from a class of programs that is guaranteed to not run over the limit.</strong></p>\n<p>If we have limited time and no halting oracle, we can't check every program. Instead, we are only allowed to check programs drawn from a class of programs that we can check the output of in limited time. The simplest example would be to just check programs but not report the result if we go over some time limit, in which case the class we're picking from is \"programs that don't go over the time limit.\"</p>\n<p>This is an application of a general principle - when we impose resource constraints on a real agent, we want to be able to cash them out as properties of an abstract description of what the agent is doing. In this case, we can cash out limited time for our real agent as an inability for our abstract description to have any contributions from long programs.</p>\n<p>This isn't really a Bayesian restriction - we don't actually know that the true program is within our search space. This also weakens our frequentist guarantee.</p>\n<p><strong>The problem of overfitting - real models allow for incorrect retrodictions.</strong></p>\n<p>If we just take Solomonoff induction and put restrictions on it, our predictions will still only come from hypotheses that exactly reproduce our starting data. This is a problem.</p>\n<p>For example, if we have a ton of data, <em>like we do</em>, then finding even one program that exactly replicates our data is too hard, and our induction is useless.</p>\n<p>We as humans have two main ways of dealing with this. The first is that we ignore most of our data and only try to predict the important parts. The second is that we don't require our models to perfectly retrodict our observations so far (retrodiction- it's like prediction, for the past!).</p>\n<p>In fact. having imperfect retrodictions is such a good idea that we can have a problem called \"overfitting.\" &nbsp;Isn't that kinda odd? That it's better to use a simple model that is actually 100% known to be wrong, than to use a very complicated model that has gotten everything right so far.</p>\n<p>This behavior makes more sense if we imagine that our data contains contributions both from the thing we're trying to model, and from stuff we want to ignore, in a way that's hard to separate.</p>\n<p><strong>What makes real models better than chance? The example of coarse-graining.<img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_k65_0.png\" alt=\"\" width=\"294\" height=\"401\" /></strong></p>\n<p>Is it even possible to know ahead of time that an imperfect model will make good predictions? It turns out the answer is yes.</p>\n<p>The very simplest example is of just predicting the next bit based on which bit has been more common so far. If every pattern were equally likely this wouldn't work, but if we require the probability of a pattern to decrease with its complexity, we're more likely to see repetitions.</p>\n<p>A more general example: an imperfect model is always better than chance if it is a likely <em>coarse-graining</em>&nbsp;of the true program. Coarse-graining is when you can use a simple program to predict a complicated one by only worrying about some of the properties of the complicated program. In the picture at right, a simple cellular automaton can predict whether a more complicated one will have the same or different densities of white and black in a region.</p>\n<p>When a coarse-graining is exact, the coarse-grained properties follow exact rules all the time. Like in the picture at right, where the coarse-grained pattern \"same different same\" always evolves into \"different different different,\" even though there are multiple states of the more complicated program that count as \"different\" and \"same.\"</p>\n<p>When a coarse-graining is inexact, only most of the states of the long program follow the coarse-grained rules of the short program. but it turns out that, given some ignorance about the exact rules or situation, this is also sufficient to predict the future better than chance.</p>\n<p>Of course, when doing induction, we don't actually know the true program. Instead what happens is that we just find some simple program that fits our data reasonably well (according to some measurement of fit), and we go \"well, what happened before is more likely to happen again, so this rule will help us predict the future.\"</p>\n<p>Presumably we combine predictions according to some weighting that include both the length of the program and its goodness of fit.</p>\n<p><strong>Machine learning - probably approximately correct</strong></p>\n<p>Since this is a machine learning problem, there are already solutions to similar problems, one of which is probably approximately correct learning. The basic idea is that if you have some uniformly-sampled training data, and a hypothesis space you can completely search, then you can give some probabilistic guarantees about how good the hypothesis is that best fits the training data. A \"hypothesis,\" here, is a classification of members of data-space into different categories.</p>\n<p>The more training data you have, the closer (where \"close\" can be measured as a chi-squared-like error) the best-fitting hypothesis is to the actually-best hypothesis. If your hypothesis space doesn't contain the true hypothesis, then that's okay - you can still guarantee that the best-fitting hypothesis gets close to the best hypothesis in your hypothesis space. The probability that a far-away hypothesis would masquerade as a hypothesis within the small \"successful\" region gets smaller and smaller as the training data increases.</p>\n<p>There is a straightforward extension to cases where your data contains contributions from both \"things we want to model\" and \"things we want to ignore.\" Rather than finding the hypothesis that fits the training data best, we want to find the hypothesis for the \"stuff we want to model\" that has the highest probability of producing our observed data, after we've added some noise, drawn from some \"noise distribution\" that encapsulates the basic information about the stuff we want to ignore.</p>\n<p>There are certainly some issues comparing this to Solomonoff induction, like the fact that our training data is randomly sampled rather than a time series, but I do like this paradigm a bit better than looking for a guarantee that we'll find the one true answer in finite time.</p>\n<p><strong>Bayes' theorem</strong></p>\n<p>If there's an easy way to combine machine learning with Solomonoff induction, it's Bayes' theorem. The machine learning was focused on driving down P(training data | chance), and picking a hypothesis with a high P(training data | hypothesis, noise model). We might want to Use Bayes' rule to say something like P(hypothesis | training data, noise model) = P(hypothesis | complexity prior) * P(training data | hypothesis, noise model) / P(training data | noise model).</p>\n<p>&nbsp;</p>\n<p>Anyhow - what are the obvious things I've missed? :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "od2hkE2CS9tYZXHkx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.7388126811107393e-06, "legacy": true, "legacyId": "26141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been thinking about how to put induction with limited resources on a firmer foundation.&nbsp;This may just be retracing the steps of others, but that's okay with me. Mostly I just want to talk about these thoughts.</p>\n<p>After a few points of introduction.</p>\n<p><strong id=\"What_s_Solomonoff_induction_\">What's Solomonoff induction?</strong></p>\n<p>Suppose we're given some starting data, and asked to predict the future. Solomonoff induction predicts the future by combining the predictions of all programs that (1) output the starting data up until now, and (2) aren't the continuation of another such program. The predictions are combined according to a weighting that decreases exponentially as the length of the program increases.</p>\n<p><strong id=\"Why_is_it_a_good_idea_\">Why is it a good idea?</strong></p>\n<p>The simplest answer is that we have a frequentist guarantee that if the \"true program\" generating our input has some length N (that is, if the observable universe is a big but finite-sized computer), then our predictions will only be wrong a limited number of times, and after that we'll predict the correctly every time.</p>\n<p>A more bayesian answer would start with the information that our observations can be generated by some finite-sized program, and then derive that something like Solomonoff induction has to represent our true prior over generating programs - as the length gets bigger, our probability is required to go to zero at infinity, and an exponential is the maximum-entropy such curve. This is not a complete answer, but it at least makes the missing pieces more apparent.</p>\n<p><strong id=\"Why_won_t_it_work_with_limited_resources\">Why won't it work with limited resources</strong></p>\n<p>The trouble with using Solomonoff induction in real life is that to pick out which programs output our data so far, we need to run every program - and if the program doesn't ever halt, we need to use a halting oracle to stop it or else we'll take infinite time.</p>\n<p><strong id=\"Limited_resources_require_us_to_only_pick_from_a_class_of_programs_that_is_guaranteed_to_not_run_over_the_limit_\">Limited resources require us to only pick from a class of programs that is guaranteed to not run over the limit.</strong></p>\n<p>If we have limited time and no halting oracle, we can't check every program. Instead, we are only allowed to check programs drawn from a class of programs that we can check the output of in limited time. The simplest example would be to just check programs but not report the result if we go over some time limit, in which case the class we're picking from is \"programs that don't go over the time limit.\"</p>\n<p>This is an application of a general principle - when we impose resource constraints on a real agent, we want to be able to cash them out as properties of an abstract description of what the agent is doing. In this case, we can cash out limited time for our real agent as an inability for our abstract description to have any contributions from long programs.</p>\n<p>This isn't really a Bayesian restriction - we don't actually know that the true program is within our search space. This also weakens our frequentist guarantee.</p>\n<p><strong id=\"The_problem_of_overfitting___real_models_allow_for_incorrect_retrodictions_\">The problem of overfitting - real models allow for incorrect retrodictions.</strong></p>\n<p>If we just take Solomonoff induction and put restrictions on it, our predictions will still only come from hypotheses that exactly reproduce our starting data. This is a problem.</p>\n<p>For example, if we have a ton of data, <em>like we do</em>, then finding even one program that exactly replicates our data is too hard, and our induction is useless.</p>\n<p>We as humans have two main ways of dealing with this. The first is that we ignore most of our data and only try to predict the important parts. The second is that we don't require our models to perfectly retrodict our observations so far (retrodiction- it's like prediction, for the past!).</p>\n<p>In fact. having imperfect retrodictions is such a good idea that we can have a problem called \"overfitting.\" &nbsp;Isn't that kinda odd? That it's better to use a simple model that is actually 100% known to be wrong, than to use a very complicated model that has gotten everything right so far.</p>\n<p>This behavior makes more sense if we imagine that our data contains contributions both from the thing we're trying to model, and from stuff we want to ignore, in a way that's hard to separate.</p>\n<p><strong id=\"What_makes_real_models_better_than_chance__The_example_of_coarse_graining_\">What makes real models better than chance? The example of coarse-graining.<img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_k65_0.png\" alt=\"\" width=\"294\" height=\"401\"></strong></p>\n<p>Is it even possible to know ahead of time that an imperfect model will make good predictions? It turns out the answer is yes.</p>\n<p>The very simplest example is of just predicting the next bit based on which bit has been more common so far. If every pattern were equally likely this wouldn't work, but if we require the probability of a pattern to decrease with its complexity, we're more likely to see repetitions.</p>\n<p>A more general example: an imperfect model is always better than chance if it is a likely <em>coarse-graining</em>&nbsp;of the true program. Coarse-graining is when you can use a simple program to predict a complicated one by only worrying about some of the properties of the complicated program. In the picture at right, a simple cellular automaton can predict whether a more complicated one will have the same or different densities of white and black in a region.</p>\n<p>When a coarse-graining is exact, the coarse-grained properties follow exact rules all the time. Like in the picture at right, where the coarse-grained pattern \"same different same\" always evolves into \"different different different,\" even though there are multiple states of the more complicated program that count as \"different\" and \"same.\"</p>\n<p>When a coarse-graining is inexact, only most of the states of the long program follow the coarse-grained rules of the short program. but it turns out that, given some ignorance about the exact rules or situation, this is also sufficient to predict the future better than chance.</p>\n<p>Of course, when doing induction, we don't actually know the true program. Instead what happens is that we just find some simple program that fits our data reasonably well (according to some measurement of fit), and we go \"well, what happened before is more likely to happen again, so this rule will help us predict the future.\"</p>\n<p>Presumably we combine predictions according to some weighting that include both the length of the program and its goodness of fit.</p>\n<p><strong id=\"Machine_learning___probably_approximately_correct\">Machine learning - probably approximately correct</strong></p>\n<p>Since this is a machine learning problem, there are already solutions to similar problems, one of which is probably approximately correct learning. The basic idea is that if you have some uniformly-sampled training data, and a hypothesis space you can completely search, then you can give some probabilistic guarantees about how good the hypothesis is that best fits the training data. A \"hypothesis,\" here, is a classification of members of data-space into different categories.</p>\n<p>The more training data you have, the closer (where \"close\" can be measured as a chi-squared-like error) the best-fitting hypothesis is to the actually-best hypothesis. If your hypothesis space doesn't contain the true hypothesis, then that's okay - you can still guarantee that the best-fitting hypothesis gets close to the best hypothesis in your hypothesis space. The probability that a far-away hypothesis would masquerade as a hypothesis within the small \"successful\" region gets smaller and smaller as the training data increases.</p>\n<p>There is a straightforward extension to cases where your data contains contributions from both \"things we want to model\" and \"things we want to ignore.\" Rather than finding the hypothesis that fits the training data best, we want to find the hypothesis for the \"stuff we want to model\" that has the highest probability of producing our observed data, after we've added some noise, drawn from some \"noise distribution\" that encapsulates the basic information about the stuff we want to ignore.</p>\n<p>There are certainly some issues comparing this to Solomonoff induction, like the fact that our training data is randomly sampled rather than a time series, but I do like this paradigm a bit better than looking for a guarantee that we'll find the one true answer in finite time.</p>\n<p><strong id=\"Bayes__theorem\">Bayes' theorem</strong></p>\n<p>If there's an easy way to combine machine learning with Solomonoff induction, it's Bayes' theorem. The machine learning was focused on driving down P(training data | chance), and picking a hypothesis with a high P(training data | hypothesis, noise model). We might want to Use Bayes' rule to say something like P(hypothesis | training data, noise model) = P(hypothesis | complexity prior) * P(training data | hypothesis, noise model) / P(training data | noise model).</p>\n<p>&nbsp;</p>\n<p>Anyhow - what are the obvious things I've missed? :)</p>", "sections": [{"title": "What's Solomonoff induction?", "anchor": "What_s_Solomonoff_induction_", "level": 1}, {"title": "Why is it a good idea?", "anchor": "Why_is_it_a_good_idea_", "level": 1}, {"title": "Why won't it work with limited resources", "anchor": "Why_won_t_it_work_with_limited_resources", "level": 1}, {"title": "Limited resources require us to only pick from a class of programs that is guaranteed to not run over the limit.", "anchor": "Limited_resources_require_us_to_only_pick_from_a_class_of_programs_that_is_guaranteed_to_not_run_over_the_limit_", "level": 1}, {"title": "The problem of overfitting - real models allow for incorrect retrodictions.", "anchor": "The_problem_of_overfitting___real_models_allow_for_incorrect_retrodictions_", "level": 1}, {"title": "What makes real models better than chance? The example of coarse-graining.", "anchor": "What_makes_real_models_better_than_chance__The_example_of_coarse_graining_", "level": 1}, {"title": "Machine learning - probably approximately correct", "anchor": "Machine_learning___probably_approximately_correct", "level": 1}, {"title": "Bayes' theorem", "anchor": "Bayes__theorem", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-21T13:54:16.372Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup - possibly in a park", "slug": "meetup-london-social-meetup-possibly-in-a-park-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HdkxegbWSBP2F6zhh/meetup-london-social-meetup-possibly-in-a-park-0", "pageUrlRelative": "/posts/HdkxegbWSBP2F6zhh/meetup-london-social-meetup-possibly-in-a-park-0", "linkUrl": "https://www.lesswrong.com/posts/HdkxegbWSBP2F6zhh/meetup-london-social-meetup-possibly-in-a-park-0", "postedAtFormatted": "Wednesday, May 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdkxegbWSBP2F6zhh%2Fmeetup-london-social-meetup-possibly-in-a-park-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20-%20possibly%20in%20a%20park%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdkxegbWSBP2F6zhh%2Fmeetup-london-social-meetup-possibly-in-a-park-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdkxegbWSBP2F6zhh%2Fmeetup-london-social-meetup-possibly-in-a-park-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10k'>London social meetup - possibly in a park</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>If the weather is nice, we'll head to <a href=\"https://goo.gl/maps/Tdy1r\" rel=\"nofollow\">Lincoln&#39;s Inn Fields</a>, probably somewhere in the northwest quadrant. If not, we'll be in the usual Shakespeare's Head. If the weather is variable, we might move from one to the other - give me a call or text if you're not sure. My number is 07792009646.</p>\n\n<p><strong>Update</strong>: Calling it in favor of the pub.</p>\n\n<p><strong>About London LessWrong:</strong></p>\n\n<p>We run this meetup almost every week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\" rel=\"nofollow\">The Resistance</a> or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p>Related discussion happens on both our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a> and our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10k'>London social meetup - possibly in a park</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HdkxegbWSBP2F6zhh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "26255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park\">Discussion article for the meetup : <a href=\"/meetups/10k\">London social meetup - possibly in a park</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 May 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>If the weather is nice, we'll head to <a href=\"https://goo.gl/maps/Tdy1r\" rel=\"nofollow\">Lincoln's Inn Fields</a>, probably somewhere in the northwest quadrant. If not, we'll be in the usual Shakespeare's Head. If the weather is variable, we might move from one to the other - give me a call or text if you're not sure. My number is 07792009646.</p>\n\n<p><strong>Update</strong>: Calling it in favor of the pub.</p>\n\n<p><strong id=\"About_London_LessWrong_\">About London LessWrong:</strong></p>\n\n<p>We run this meetup almost every week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\" rel=\"nofollow\">The Resistance</a> or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p>Related discussion happens on both our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a> and our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park1\">Discussion article for the meetup : <a href=\"/meetups/10k\">London social meetup - possibly in a park</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup - possibly in a park", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park", "level": 1}, {"title": "About London LessWrong:", "anchor": "About_London_LessWrong_", "level": 2}, {"title": "Discussion article for the meetup : London social meetup - possibly in a park", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___possibly_in_a_park1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-22T01:59:02.912Z", "modifiedAt": null, "url": null, "title": "Australian Mega-Meetup 2014 Retrospective", "slug": "australian-mega-meetup-2014-retrospective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cjbWFdT2MXw5bYZ5P/australian-mega-meetup-2014-retrospective", "pageUrlRelative": "/posts/cjbWFdT2MXw5bYZ5P/australian-mega-meetup-2014-retrospective", "linkUrl": "https://www.lesswrong.com/posts/cjbWFdT2MXw5bYZ5P/australian-mega-meetup-2014-retrospective", "postedAtFormatted": "Thursday, May 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Australian%20Mega-Meetup%202014%20Retrospective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAustralian%20Mega-Meetup%202014%20Retrospective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjbWFdT2MXw5bYZ5P%2Faustralian-mega-meetup-2014-retrospective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Australian%20Mega-Meetup%202014%20Retrospective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjbWFdT2MXw5bYZ5P%2Faustralian-mega-meetup-2014-retrospective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjbWFdT2MXw5bYZ5P%2Faustralian-mega-meetup-2014-retrospective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 745, "htmlBody": "<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong>Overview</strong></p>\n<p>The first-ever <a href=\"https://www.google.com/url?q=https%3A%2F%2Fwww.flickr.com%2Fphotos%2F124381698%40N03%2Fsets%2F72157644218009638%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEuJ-JNQCnup5Qjxl-mlPIo8_-rVQ\">Australia-wide mega-meetup</a> took place on the second weekend of May 2014. LW clans from Melbourne, Sydney, and Canberra met in a pristine country location in NSW for a weekend of rationality, outdoors, and fine company.</p>\n<p>The event was a hit. This post is a general retrospective, another post aimed at future organisers will provide a thorough write-up of the planning, execution and suggested improvements.</p>\n<p>If it's great to hang out with a few friends who share your interests, values, and thought processes - then it's sheer awesome to spend a whole weekend with two dozen kindred minds. The favourite pastime at the mega-meetup was conversation. Every spare moment was spent exchanging ideas and views. We brought up a large pile of boardgames and not a single game was played - too busy talking. I consider this evidence that we need to bring more rationalists together more often.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong>Background</strong></p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">The Australian meetups had not had any prior contact to this event. Sydney<sup>1</sup> and Canberra are new meetups for 2014. It was hoped that the mega-meetup would persuade the new meetups that the global LW community was worth being a part of. Melbourne has been invigorated since CFAR visited and was keen to share the spirit.</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">There is a twelve-hour drive or one-hour flight from Melbourne to Sydney. Canberra is a three hours drive from Sydney towards Melbourne. To justify travelling the distance, we made the mega-meetup a weekend retreat from Friday evening to Sunday evening.</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">A word of inspiration: it was six weeks from when we first started talking to the date of the camp. Only four weeks from idea to sold out with 25 attendees.&nbsp; LW organisers are chock-a-block with extra-agenty goodness and are a delight to work with. If you run a LW meetup and have neighbouring groups, get in touch. An enthusiastic team can make grand things happen pretty fast.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong>Activities</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">The structure of weekend was built around practical rationality sessions. Melbourne LW has <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AuQk4CQwYau0dG1ISUdWRFNQemtlTmdVaWFJbTNEZWc&amp;usp=sharing\">accustomed</a> its members to running sessions and we pulled on our knowledge. Most of the&nbsp; sessions were CFAR modules: alumni valued the revision and those who were new got stuck into the powerful techniques. The schedule for the mega-meetup can be seen <a href=\"https://drive.google.com/file/d/0B-Qk4CQwYau0WnVUSVdWSkl3RkE/edit?usp=sharing\">here</a>.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">The campsite offered a range of outdoor activities. People voted on sailing and a high ropes course. The activities allowed attendees to bond outside of the intense rationality sessions. Other mega-meetup organisers might want to organise a fun excursions of another type.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">We played the <a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Flw%2Fk8u%2Fcredence_calibration_icebreaker_game%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE9QsBU-Curf-icHAmXzsH2A3QbRw\">Credence Calibration Icebreaker Game</a>&nbsp; in the opening session. It&rsquo;s a merging of the credence game with the classic icebreaker &lsquo;tell three statements about yourself, one of them a lie&rsquo;.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">Unconference/Lightning Talks were held by campfire. While roasting marshmallows, we listened to talks on cryonics, transfinite numbers, polyamory, quantified self, anthropic reasoning, the Price equation, and quite a few more.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong>European Sticker System</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">We adopted the <a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Flw%2Fk5d%2Feuropean_community_weekend_2014_retrospective%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEua7hiFpESVP4vPeUbqz8NZsILRA\">European</a> <a href=\"https://www.google.com/url?q=https%3A%2F%2Fwww.flickr.com%2Fphotos%2F124381698%40N03%2F13978693430%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHWHNpxovcjLug78TK9S8tsN5pgvg\">Sticker System</a>, adorning our name tags with little indicators about ourselves. We ran out of &lsquo;Hugs!&rsquo; stickers and a perceptible increase in the rate of hugs did occur. Uptake of <a href=\"/lw/jis/tell_culture/\">Tell Culture</a> stickers appeared universal, although harder to see in action. People cited my tell culture sticker before providing feedback about the meetup, indicating that I might not have received it otherwise. A Crocker&rsquo;s Rules sticker was included for LW completeness but was cautioned against.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">Like the &lsquo;Hugs&rdquo; sticker, &lsquo;Ask Me Anything&rsquo; was adopted by most. One late night conversation became a circle of people pushing the limits of what they would normally ask each other:</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&ldquo;What is your kink (fetishes)?&rdquo; &ldquo;What have you done which has made you feel really morally bad?&rdquo; &ldquo;Given your intelligence, I am surprised by your career choice. Can you tell me about that?&rdquo; &ldquo;You belong to minority group X within the group here, I&rsquo;m curious how that makes you feel.&rdquo;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong>What's Next</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">There was no discussion of whether another mega-meetup should happen: all involved assumed that obviously it would and we should just start planning now. More people, longer, more stickers. We might invite New Zealand.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">Mega-meetups are awesome and we heartily recommend everyone have them. They don&rsquo;t have to be weekend-long events, your local area meetups don&rsquo;t have to be large, just bring them goddamn rationalists together.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong>Credit<br /></strong></p>\n<div style=\"text-align: start;\"><span style=\"text-align: justify; line-height: 1.15;\">An enormous amount of credit goes to the organisers who made the event happen. In the counterfactual world where any one of them was absent, this mega-meetup would not have occured. 3^^3 cheers for </span><a style=\"text-align: justify; line-height: 1.15;\" href=\"/user/taryneast/overview/\">taryneast</a><span style=\"text-align: justify; line-height: 1.15;\">, </span><a style=\"text-align: justify; line-height: 1.15;\" href=\"/user/DanielFilan/overview/\">DanielFilan</a><span style=\"text-align: justify; line-height: 1.15;\">, <a href=\"/user/Elo/overview/\">Elo</a>, and Nick Winter!</span></div>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"text-align: justify;\"><br /></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"text-align: justify;\">1. Sydney existed in a previous incarnation two years ago, but started up again recently.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cjbWFdT2MXw5bYZ5P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 37, "extendedScore": null, "score": 1.740580348438634e-06, "legacy": true, "legacyId": "26257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong id=\"Overview\">Overview</strong></p>\n<p>The first-ever <a href=\"https://www.google.com/url?q=https%3A%2F%2Fwww.flickr.com%2Fphotos%2F124381698%40N03%2Fsets%2F72157644218009638%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEuJ-JNQCnup5Qjxl-mlPIo8_-rVQ\">Australia-wide mega-meetup</a> took place on the second weekend of May 2014. LW clans from Melbourne, Sydney, and Canberra met in a pristine country location in NSW for a weekend of rationality, outdoors, and fine company.</p>\n<p>The event was a hit. This post is a general retrospective, another post aimed at future organisers will provide a thorough write-up of the planning, execution and suggested improvements.</p>\n<p>If it's great to hang out with a few friends who share your interests, values, and thought processes - then it's sheer awesome to spend a whole weekend with two dozen kindred minds. The favourite pastime at the mega-meetup was conversation. Every spare moment was spent exchanging ideas and views. We brought up a large pile of boardgames and not a single game was played - too busy talking. I consider this evidence that we need to bring more rationalists together more often.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong id=\"Background\">Background</strong></p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">The Australian meetups had not had any prior contact to this event. Sydney<sup>1</sup> and Canberra are new meetups for 2014. It was hoped that the mega-meetup would persuade the new meetups that the global LW community was worth being a part of. Melbourne has been invigorated since CFAR visited and was keen to share the spirit.</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">There is a twelve-hour drive or one-hour flight from Melbourne to Sydney. Canberra is a three hours drive from Sydney towards Melbourne. To justify travelling the distance, we made the mega-meetup a weekend retreat from Friday evening to Sunday evening.</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\">A word of inspiration: it was six weeks from when we first started talking to the date of the camp. Only four weeks from idea to sold out with 25 attendees.&nbsp; LW organisers are chock-a-block with extra-agenty goodness and are a delight to work with. If you run a LW meetup and have neighbouring groups, get in touch. An enthusiastic team can make grand things happen pretty fast.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong id=\"Activities\">Activities</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">The structure of weekend was built around practical rationality sessions. Melbourne LW has <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AuQk4CQwYau0dG1ISUdWRFNQemtlTmdVaWFJbTNEZWc&amp;usp=sharing\">accustomed</a> its members to running sessions and we pulled on our knowledge. Most of the&nbsp; sessions were CFAR modules: alumni valued the revision and those who were new got stuck into the powerful techniques. The schedule for the mega-meetup can be seen <a href=\"https://drive.google.com/file/d/0B-Qk4CQwYau0WnVUSVdWSkl3RkE/edit?usp=sharing\">here</a>.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">The campsite offered a range of outdoor activities. People voted on sailing and a high ropes course. The activities allowed attendees to bond outside of the intense rationality sessions. Other mega-meetup organisers might want to organise a fun excursions of another type.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">We played the <a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Flw%2Fk8u%2Fcredence_calibration_icebreaker_game%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNE9QsBU-Curf-icHAmXzsH2A3QbRw\">Credence Calibration Icebreaker Game</a>&nbsp; in the opening session. It\u2019s a merging of the credence game with the classic icebreaker \u2018tell three statements about yourself, one of them a lie\u2019.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">Unconference/Lightning Talks were held by campfire. While roasting marshmallows, we listened to talks on cryonics, transfinite numbers, polyamory, quantified self, anthropic reasoning, the Price equation, and quite a few more.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong id=\"European_Sticker_System\">European Sticker System</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">We adopted the <a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Flw%2Fk5d%2Feuropean_community_weekend_2014_retrospective%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEua7hiFpESVP4vPeUbqz8NZsILRA\">European</a> <a href=\"https://www.google.com/url?q=https%3A%2F%2Fwww.flickr.com%2Fphotos%2F124381698%40N03%2F13978693430%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHWHNpxovcjLug78TK9S8tsN5pgvg\">Sticker System</a>, adorning our name tags with little indicators about ourselves. We ran out of \u2018Hugs!\u2019 stickers and a perceptible increase in the rate of hugs did occur. Uptake of <a href=\"/lw/jis/tell_culture/\">Tell Culture</a> stickers appeared universal, although harder to see in action. People cited my tell culture sticker before providing feedback about the meetup, indicating that I might not have received it otherwise. A Crocker\u2019s Rules sticker was included for LW completeness but was cautioned against.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">Like the \u2018Hugs\u201d sticker, \u2018Ask Me Anything\u2019 was adopted by most. One late night conversation became a circle of people pushing the limits of what they would normally ask each other:</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">\u201cWhat is your kink (fetishes)?\u201d \u201cWhat have you done which has made you feel really morally bad?\u201d \u201cGiven your intelligence, I am surprised by your career choice. Can you tell me about that?\u201d \u201cYou belong to minority group X within the group here, I\u2019m curious how that makes you feel.\u201d</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong id=\"What_s_Next\">What's Next</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">There was no discussion of whether another mega-meetup should happen: all involved assumed that obviously it would and we should just start planning now. More people, longer, more stickers. We might invite New Zealand.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">Mega-meetups are awesome and we heartily recommend everyone have them. They don\u2019t have to be weekend-long events, your local area meetups don\u2019t have to be large, just bring them goddamn rationalists together.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong id=\"Credit\">Credit<br></strong></p>\n<div style=\"text-align: start;\"><span style=\"text-align: justify; line-height: 1.15;\">An enormous amount of credit goes to the organisers who made the event happen. In the counterfactual world where any one of them was absent, this mega-meetup would not have occured. 3^^3 cheers for </span><a style=\"text-align: justify; line-height: 1.15;\" href=\"/user/taryneast/overview/\">taryneast</a><span style=\"text-align: justify; line-height: 1.15;\">, </span><a style=\"text-align: justify; line-height: 1.15;\" href=\"/user/DanielFilan/overview/\">DanielFilan</a><span style=\"text-align: justify; line-height: 1.15;\">, <a href=\"/user/Elo/overview/\">Elo</a>, and Nick Winter!</span></div>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"text-align: justify;\"><br></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"text-align: justify;\">1. Sydney existed in a previous incarnation two years ago, but started up again recently.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-align: justify\">&nbsp;</p>", "sections": [{"title": "Overview", "anchor": "Overview", "level": 1}, {"title": "Background", "anchor": "Background", "level": 1}, {"title": "Activities", "anchor": "Activities", "level": 1}, {"title": "European Sticker System", "anchor": "European_Sticker_System", "level": 1}, {"title": "What's Next", "anchor": "What_s_Next", "level": 1}, {"title": "Credit", "anchor": "Credit", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rEBXN3x6kXgD4pLxs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-22T08:40:40.065Z", "modifiedAt": null, "url": null, "title": "[LINK] Scott Aaronson on Integrated Information Theory", "slug": "link-scott-aaronson-on-integrated-information-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:09.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5pYPdmKMzDLZHe4zx/link-scott-aaronson-on-integrated-information-theory", "pageUrlRelative": "/posts/5pYPdmKMzDLZHe4zx/link-scott-aaronson-on-integrated-information-theory", "linkUrl": "https://www.lesswrong.com/posts/5pYPdmKMzDLZHe4zx/link-scott-aaronson-on-integrated-information-theory", "postedAtFormatted": "Thursday, May 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Scott%20Aaronson%20on%20Integrated%20Information%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Scott%20Aaronson%20on%20Integrated%20Information%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pYPdmKMzDLZHe4zx%2Flink-scott-aaronson-on-integrated-information-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Scott%20Aaronson%20on%20Integrated%20Information%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pYPdmKMzDLZHe4zx%2Flink-scott-aaronson-on-integrated-information-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pYPdmKMzDLZHe4zx%2Flink-scott-aaronson-on-integrated-information-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>Scott Aaronson, complexity theory researcher, disputes Tononi's theory of consciousness, that a physical system is conscious if and only if it has a high value of \"integrated information\". Quote:</p>\n<blockquote>\n<p>So, this is the post that I promised to Max [Tegmark] and all the others, about why I don&rsquo;t believe IIT. &nbsp;And yes, it will contain that quantitative calculation [of the integrated information of a system that he claims is not conscious].</p>\n<p>...</p>\n<p>But let me end on a positive note. &nbsp;In my opinion, the fact that Integrated Information Theory is wrong&mdash;demonstrably wrong, for reasons that go to its core&mdash;puts it in something like the top 2% of all mathematical theories of consciousness ever proposed. &nbsp;Almost all&nbsp;competing theories of consciousness, it seems to me,&nbsp;have been&nbsp;so vague, fluffy, and malleable that they can only <strong>aspire</strong> to wrongness.</p>\n</blockquote>\n<p>http://www.scottaaronson.com/blog/?p=1799</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XSryTypw5Hszpa4TS": 1, "KqrZ5sDEyHm6JaaKp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5pYPdmKMzDLZHe4zx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 38, "extendedScore": null, "score": 1.741142935062319e-06, "legacy": true, "legacyId": "26260", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-22T08:42:40.263Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Meetup - June", "slug": "meetup-sydney-meetup-june", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hR54vB2KkvYPeL6XY/meetup-sydney-meetup-june", "pageUrlRelative": "/posts/hR54vB2KkvYPeL6XY/meetup-sydney-meetup-june", "linkUrl": "https://www.lesswrong.com/posts/hR54vB2KkvYPeL6XY/meetup-sydney-meetup-june", "postedAtFormatted": "Thursday, May 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Meetup%20-%20June&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Meetup%20-%20June%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhR54vB2KkvYPeL6XY%2Fmeetup-sydney-meetup-june%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Meetup%20-%20June%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhR54vB2KkvYPeL6XY%2Fmeetup-sydney-meetup-june", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhR54vB2KkvYPeL6XY%2Fmeetup-sydney-meetup-june", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10l'>Sydney Meetup - June</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 June 2014 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>I'll book another table under the name \"less wrong\". Last meetup we were in the restaurant on level 2. Every time so far they've set up the table about 5m to the left, after you exit the lift, so there's a good chance we'll be there this time too.</p>\n\n<p>The theme for this month is Goals.</p>\n\n<ul>\n<li><p>Finding out what your goals should be</p></li>\n<li><p>Breaking them into sub-goals and planning your avenues of attack</p></li>\n<li><p>Finding ways of sticking to your planned tasks</p></li>\n</ul>\n\n<p>It's a big topic, and we probably won't get through all of it in one night - but I'm sure it'll be really interesting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10l'>Sydney Meetup - June</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hR54vB2KkvYPeL6XY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.7411457420373241e-06, "legacy": true, "legacyId": "26261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___June\">Discussion article for the meetup : <a href=\"/meetups/10l\">Sydney Meetup - June</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 June 2014 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>6:30 PM for early discussion 7PM general dinner-discussion after dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>I'll book another table under the name \"less wrong\". Last meetup we were in the restaurant on level 2. Every time so far they've set up the table about 5m to the left, after you exit the lift, so there's a good chance we'll be there this time too.</p>\n\n<p>The theme for this month is Goals.</p>\n\n<ul>\n<li><p>Finding out what your goals should be</p></li>\n<li><p>Breaking them into sub-goals and planning your avenues of attack</p></li>\n<li><p>Finding ways of sticking to your planned tasks</p></li>\n</ul>\n\n<p>It's a big topic, and we probably won't get through all of it in one night - but I'm sure it'll be really interesting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___June1\">Discussion article for the meetup : <a href=\"/meetups/10l\">Sydney Meetup - June</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Meetup - June", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___June", "level": 1}, {"title": "Discussion article for the meetup : Sydney Meetup - June", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___June1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-22T10:37:49.289Z", "modifiedAt": null, "url": null, "title": "Organisations working on multiple Global Catastrophic risks", "slug": "organisations-working-on-multiple-global-catastrophic-risks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/evy6yuomfj33TwoqH/organisations-working-on-multiple-global-catastrophic-risks", "pageUrlRelative": "/posts/evy6yuomfj33TwoqH/organisations-working-on-multiple-global-catastrophic-risks", "linkUrl": "https://www.lesswrong.com/posts/evy6yuomfj33TwoqH/organisations-working-on-multiple-global-catastrophic-risks", "postedAtFormatted": "Thursday, May 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Organisations%20working%20on%20multiple%20Global%20Catastrophic%20risks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOrganisations%20working%20on%20multiple%20Global%20Catastrophic%20risks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fevy6yuomfj33TwoqH%2Forganisations-working-on-multiple-global-catastrophic-risks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Organisations%20working%20on%20multiple%20Global%20Catastrophic%20risks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fevy6yuomfj33TwoqH%2Forganisations-working-on-multiple-global-catastrophic-risks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fevy6yuomfj33TwoqH%2Forganisations-working-on-multiple-global-catastrophic-risks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">It is not uncommon to find organisations working, directly or indirectly, on a single Global Catastrophic Risk (GCR). For instance, the <a href=\"http://www.who.int/en/\">World Health Organization</a> does much work to prevent pandemics, as part of its remit.</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">It is rarer for organisations to focus on multiple GCRs - for a start, this involves them having the concept akin to GCR, which is not often the case. In a report I'm preparing with Dennis Pamlin of the <a href=\"http://globalchallenges.org/en/\">Global Challenges Foundation</a>, here is a list of organisations focusing on multiple GCRs (note that it is not necessarily an endorsement of their quality). Let me know if there are any organisations missing, and I'll add them:</span></p>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">\n<table style=\"border-collapse: collapse; border: none;\" border=\"1\" cellspacing=\"0\" cellpadding=\"0\">\n<tbody>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border: 1pt solid windowtext; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Brookings</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: solid solid solid none; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-top-width: 1pt; border-right-width: 1pt; border-bottom-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.brookings.edu/research#topics/\" target=\"_blank\">http://www.brookings.edu/research#topics/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Bulletin of the Atomic Scientists</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://thebulletin.org/\" target=\"_blank\">http://thebulletin.org/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>CSER</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://cser.org/emerging-risks-from-technology/\" target=\"_blank\">http://cser.org/emerging-risks-from-technology/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Center for International Security and Cooperation</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://cisac.stanford.edu/\" target=\"_blank\">http://cisac.stanford.edu/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Club of Rome</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.clubofrome.org/\" target=\"_blank\">http://www.clubofrome.org/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Council on Foreign Relations</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.cfr.org/issue/\" target=\"_blank\">http://www.cfr.org/issue/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Federation of American Scientists</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.fas.org/programs\" target=\"_blank\">http://www.fas.org/programs</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Future of Humanity Institute</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.fhi.ox.ac.uk/research/research-areas/\" target=\"_blank\">http://www.fhi.ox.ac.uk/research/research-areas/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Global Catastrophic Risk Institute</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://gcrinstitute.org/research/\" target=\"_blank\">http://gcrinstitute.org/research/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Institute for Defence Studies and Analyses</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.idsa.in/topics\" target=\"_blank\">http://www.idsa.in/topics</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>International Risk Governance Council</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.irgc.org/issues/\" target=\"_blank\">http://www.irgc.org/issues/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Lifeboat Foundation</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://lifeboat.com/ex/programs\" target=\"_blank\">http://lifeboat.com/ex/programs</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Nuclear Threat Initiative</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.nti.org/\" target=\"_blank\">http://www.nti.org/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Saving Humanity from Homo Sapiens</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://shfhs.org/whatarexrisks.html\" target=\"_blank\">http://shfhs.org/whatarexrisks.html</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Skoll Global Threats Fund</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.skollglobalthreats.org/\" target=\"_blank\">http://www.skollglobalthreats.org/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Stimson Center</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.stimson.org/programs/\" target=\"_blank\">http://www.stimson.org/programs/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>Risk Response Network</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://forumblog.org/communities/risk-response-network/\" target=\"_blank\">http://forumblog.org/communities/risk-response-network/</a></p>\n</td>\n</tr>\n<tr style=\"height: 12pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"199\" valign=\"top\">\n<p>World Economic Forum</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 12pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.weforum.org/reports/global-risks-2014-report\" target=\"_blank\">http://www.weforum.org/reports/global-risks-2014-report</a></p>\n</td>\n</tr>\n<tr style=\"height: 15pt;\">\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 149.6pt; border-style: none solid solid; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-right-width: 1pt; border-bottom-width: 1pt; border-left-width: 1pt; padding: 0cm 5.4pt; height: 15pt;\" width=\"199\" valign=\"top\">\n<p>Tower Watson</p>\n</td>\n<td style=\"font-family: arial, sans-serif; margin: 0px; width: 322.2pt; border-style: none solid solid none; border-bottom-color: windowtext; border-bottom-width: 1pt; border-right-color: windowtext; border-right-width: 1pt; padding: 0cm 5.4pt; height: 15pt;\" width=\"430\" valign=\"top\">\n<p><a style=\"color: #1155cc;\" href=\"http://www.towerswatson.com/en/Insights/IC-Types/Survey-Research-Results/2013/10/Extreme-risks-2013\" target=\"_blank\">http://www.towerswatson.com/en/Insights/IC-Types/Survey-Research-Results/2013/10/Extreme-risks-2013</a></p>\n</td>\n</tr>\n</tbody>\n</table>\n<p><a id=\"more\"></a>What kind of GCRs are being worked on? This can be seen in the following graph; AI is not as badly situated as I would initially have expected:</p>\n<p><img style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\" src=\"http://images.lesswrong.com/t3_k81_0.png?v=c372ed1b96a2d8d4a9940ced2a287f98\" alt=\"\" width=\"605\" height=\"445\" /></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "evy6yuomfj33TwoqH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 22, "extendedScore": null, "score": 1.7413071041064184e-06, "legacy": true, "legacyId": "26209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-22T10:42:09.631Z", "modifiedAt": null, "url": null, "title": "[LINK] Normalcy bias", "slug": "link-normalcy-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.360Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yxs38KZYr7kuSYgqZ/link-normalcy-bias", "pageUrlRelative": "/posts/yxs38KZYr7kuSYgqZ/link-normalcy-bias", "linkUrl": "https://www.lesswrong.com/posts/yxs38KZYr7kuSYgqZ/link-normalcy-bias", "postedAtFormatted": "Thursday, May 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Normalcy%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Normalcy%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyxs38KZYr7kuSYgqZ%2Flink-normalcy-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Normalcy%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyxs38KZYr7kuSYgqZ%2Flink-normalcy-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyxs38KZYr7kuSYgqZ%2Flink-normalcy-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>A useful bias to quote in discussions that spring up around the subjects we deal with on Less Wrong: <a href=\"http://en.wikipedia.org/wiki/Normalcy_bias\">Normalcy Bias</a>. It's rather specific, but useful:</p>\n<blockquote>\n<p>The normalcy bias, or normality bias, refers to a mental state people enter when facing a disaster. It causes people to underestimate both the possibility of a disaster occurring and its possible effects. This may results in situations where people fail to adequately prepare for a disaster, and on a larger scale, the failure of governments to include the populace in its disaster preparations.</p>\n<p>The assumption that is made in the case of the normalcy bias is that since a disaster never has occurred then it never will occur. It can result in the inability of people to cope with a disaster once it occurs. People with a normalcy bias have difficulties reacting to something they have not experienced before. People also tend to interpret warnings in the most optimistic way possible, seizing on any ambiguities to infer a less serious situation.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yxs38KZYr7kuSYgqZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 0, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "25399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-23T04:54:32.829Z", "modifiedAt": null, "url": null, "title": "Can noise have power?", "slug": "can-noise-have-power", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:32.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8sitELf6z8zGPKDRm/can-noise-have-power", "pageUrlRelative": "/posts/8sitELf6z8zGPKDRm/can-noise-have-power", "linkUrl": "https://www.lesswrong.com/posts/8sitELf6z8zGPKDRm/can-noise-have-power", "postedAtFormatted": "Friday, May 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20noise%20have%20power%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20noise%20have%20power%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sitELf6z8zGPKDRm%2Fcan-noise-have-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20noise%20have%20power%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sitELf6z8zGPKDRm%2Fcan-noise-have-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sitELf6z8zGPKDRm%2Fcan-noise-have-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 898, "htmlBody": "<p>One of the most interesting debates on Less Wrong that seems like it should be definitively resolvable is the one between Eliezer Yudkowsky, Scott Aaronson, and others on <a href=\"/lw/vq/the_weighted_majority_algorithm/\">The Weighted Majority Algorithm</a>. I'll reprint the debate here in case anyone wants to comment further on it.</p>\n<p>In that post, Eliezer argues that \"noise hath no power\" (read the post for details). Scott disagreed. He&nbsp;<a href=\"/lw/vq/the_weighted_majority_algorithm/owm\">replied</a>:</p>\n<blockquote>\n<p>...Randomness provably never helps in average-case complexity (i.e., where you fix the probability distribution over inputs) -- since given any ensemble of strategies, by convexity there must be at least one deterministic strategy in the ensemble that does at least as well as the average.</p>\n<p>On the other hand, if you care about the worst-case running time, then there are settings (such as query complexity) where randomness provably does help. For example, suppose you're given n bits, you're promised that either n/3 or 2n/3 of the bits are 1's, and your task is to decide which. Any deterministic strategy to solve this problem clearly requires looking at 2n/3 + 1 of the bits. On the other hand, a randomized sampling strategy only has to look at O(1) bits to succeed with high probability.</p>\n<p>Whether randomness ever helps in worst-case polynomial-time computation is the P versus BPP question, which is in the same league as P versus NP. It's conjectured that P=BPP (i.e., randomness never saves more than a polynomial). This is known to be true if really good pseudorandom generators exist, and such PRG's can be constructed if certain problems that seem to require exponentially large circuits, really do require them (see <a href=\"http://www.math.ias.edu/~avi/PUBLICATIONS/MYPAPERS/IW97/proc.pdf\">this paper</a> by Impagliazzo and Wigderson). But we don't seem close to proving P=BPP unconditionally.</p>\n</blockquote>\n<p>Eliezer <a href=\"/lw/vq/the_weighted_majority_algorithm/owp\">replied</a>:</p>\n<blockquote>\n<p>Scott, I don't dispute what you say. I just suggest that the confusing term \"in the worst case\" be replaced by the more accurate phrase \"supposing that the environment is an adversarial superintelligence who can perfectly read all of your mind except bits designated 'random'\".</p>\n</blockquote>\n<p>Scott <a href=\"/lw/vq/the_weighted_majority_algorithm/owq\">replied</a>:</p>\n<blockquote>\n<p>I often tell people that theoretical computer science is basically mathematicized paranoia, and that this is the reason why Israelis so dominate the field. You're absolutely right: we do typically assume the environment is an adversarial superintelligence. But that's not because we literally think it is one, it's because we don't presume to know which distribution over inputs the environment is going to throw at us. (That is, we lack the self-confidence to impose any particular prior on the inputs.) We do often assume that, if we generate random bits ourselves, then the environment isn't going to magically take those bits into account when deciding which input to throw at us. (Indeed, if we like, we can easily generate the random bits after seeing the input -- not that it should make a difference.)</p>\n<p>Average-case analysis is also well-established and used a great deal. But in those cases where you can solve a problem without having to assume a particular distribution over inputs, why complicate things unnecessarily by making such an assumption? Who needs the risk?</p>\n</blockquote>\n<p>And later <a href=\"/lw/vq/the_weighted_majority_algorithm/t6b\">added</a>:</p>\n<blockquote>\n<p>...Note that I also enthusiastically belong to a \"derandomize things\" crowd! The difference is, I think derandomizing is hard work (sometimes possible and sometimes not), since I'm unwilling to treat the randomness of the problems the world throws at me on the same footing as randomness I generate myself in the course of solving those problems. (For those watching at home tonight, I hope the differences are now reasonably clear...)</p>\n</blockquote>\n<p>Eliezer <a href=\"/lw/vq/the_weighted_majority_algorithm/t6c\">replied</a>:</p>\n<blockquote>\n<p>I certainly don't say \"it's not hard work\", and the environmental probability distribution should not look like the probability distribution you have over your random numbers - it should contain correlations and structure. But once you know what your probability distribution is, then you should do your work relative to that, rather than assuming \"worst case\". Optimizing for the worst case in environments that aren't actually adversarial, makes even less sense than assuming the environment is as random and unstructured as thermal noise.</p>\n<p>I would defend the following sort of statement: While often it's not worth the computing power to take advantage of all the believed-in regularity of your probability distribution over the environment, any environment that you can't get away with treating as effectively random, probably has enough structure to be worth exploiting instead of randomizing.</p>\n<p>(This isn't based on career experience, it's how I would state my expectation given my prior theory.)</p>\n</blockquote>\n<p>Scott <a href=\"/lw/vq/the_weighted_majority_algorithm/t6d\">replied</a>:</p>\n<blockquote>\n<p>&gt; \"once you know what your probability distribution is...\"</p>\n<p>I'd merely stress that that's an enormous \"once.\" When you're writing a program (which, yes, I used to do), normally you have only the foggiest idea of what a typical input is going to be, yet you want the program to work anyway. This is not just a hypothetical worry, or something limited to cryptography: people have actually run into strange problems using pseudorandom generators for Monte Carlo simulations and hashing (see <a href=\"https://web.archive.org/web/20090107093832/http://expertvoices.nsdl.org/cornell-cs322/2008/04/23/monte-carlo-gone-wrong/\">here</a> for example, or Knuth vol 2).</p>\n<p>Even so, intuition suggests it should be possible to design PRG's that defeat anything the world is likely to throw at them. I share that intuition; it's the basis for the (yet-unproved) P=BPP conjecture.</p>\n<p>\"Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin.\" --von Neumann</p>\n</blockquote>\n<p>And that's where the debate drops off, at least between Eliezer and Scott, at least on that thread.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8sitELf6z8zGPKDRm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 18, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "26256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AAqTP6Q5aeWnoAYr4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-23T17:15:00.242Z", "modifiedAt": null, "url": null, "title": "Life insurance for Cryonics, how many years?", "slug": "life-insurance-for-cryonics-how-many-years", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.966Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uE4RspzqiDrWMmXid/life-insurance-for-cryonics-how-many-years", "pageUrlRelative": "/posts/uE4RspzqiDrWMmXid/life-insurance-for-cryonics-how-many-years", "linkUrl": "https://www.lesswrong.com/posts/uE4RspzqiDrWMmXid/life-insurance-for-cryonics-how-many-years", "postedAtFormatted": "Friday, May 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Life%20insurance%20for%20Cryonics%2C%20how%20many%20years%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALife%20insurance%20for%20Cryonics%2C%20how%20many%20years%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuE4RspzqiDrWMmXid%2Flife-insurance-for-cryonics-how-many-years%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Life%20insurance%20for%20Cryonics%2C%20how%20many%20years%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuE4RspzqiDrWMmXid%2Flife-insurance-for-cryonics-how-many-years", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuE4RspzqiDrWMmXid%2Flife-insurance-for-cryonics-how-many-years", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>Hello,</p>\n<p>&nbsp;</p>\n<p>one question that I don't see answered is what is the duration of your life insurance? Should I buy life insurance for 20,30 years or unlimited?</p>\n<p>I could pay more for a longer life insurance or pay less for an insurance that will cover 20 years and invest the difference and then some so that by the end of 20 years I will *hopefully* have enough money to pay for cryonics out of my own pocket.</p>\n<p>Has anyone done an analysis on that?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uE4RspzqiDrWMmXid", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 1.7438851596430149e-06, "legacy": true, "legacyId": "26267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-23T21:39:41.483Z", "modifiedAt": null, "url": null, "title": "Will Darkcoin Have Thiel's Last Mover Advantage", "slug": "will-darkcoin-have-thiel-s-last-mover-advantage", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:39.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tbow3GFK3dTW7tTNg/will-darkcoin-have-thiel-s-last-mover-advantage", "pageUrlRelative": "/posts/tbow3GFK3dTW7tTNg/will-darkcoin-have-thiel-s-last-mover-advantage", "linkUrl": "https://www.lesswrong.com/posts/tbow3GFK3dTW7tTNg/will-darkcoin-have-thiel-s-last-mover-advantage", "postedAtFormatted": "Friday, May 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Will%20Darkcoin%20Have%20Thiel's%20Last%20Mover%20Advantage&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWill%20Darkcoin%20Have%20Thiel's%20Last%20Mover%20Advantage%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftbow3GFK3dTW7tTNg%2Fwill-darkcoin-have-thiel-s-last-mover-advantage%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Will%20Darkcoin%20Have%20Thiel's%20Last%20Mover%20Advantage%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftbow3GFK3dTW7tTNg%2Fwill-darkcoin-have-thiel-s-last-mover-advantage", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftbow3GFK3dTW7tTNg%2Fwill-darkcoin-have-thiel-s-last-mover-advantage", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 364, "htmlBody": "<p>Thiel <a href=\"https://www.youtube.com/watch?v=Ni11-BjpWhM\">plays</a> with the concept of last mover advantage when talking about what is valuable about Google and Facebook. His claim is that being the first social network (who knows what that was) is not, <em>contra intuition</em>, what matters, instead, if you are the last search engine or the last social network you win.&nbsp;</p>\n<p>Bitcoin is the first real cryptocurrency. But will it be among the last ones?&nbsp;</p>\n<p>Darkcoin, as Louie, Kevin and others told us a few days ago it would, has exploded. (Thanks for the money guys!)</p>\n<p>I don't understand much about the properties that differentiate cryptocurrencies, but a few of Darkcoin's features seem very desirable.&nbsp;</p>\n<p>It has early incentive to increase because it is good for illegal markets and international transactions. This incentive is robust and absolute, non-traceability is binary, you can't be less traceable than untraceable.&nbsp;</p>\n<p>This is equivalent to Facebook starting off within the highest status social humans in the planet, young, healthy, wealthy and brilliant Harvard college students (compare to Orkut, which failed in great part because it spread worldwide too soon, allowing a lower social class (B<a href=\"http://en.wikipedia.org/wiki/Brazilian_Internet_phenomenon\">razilians</a>, Persians) to overtake it, disincentivizing the elites to remain there). &nbsp;</p>\n<p>It has convention status. Its market cap has surpassed all altcoins besides Litecoin.&nbsp;</p>\n<p>This advantage was originally Orkut's (which is Bitcoin in the analogy). It is what MSN stole from ICQ, and facebook from MSN, and Whatsapp from all platforms.&nbsp;</p>\n<p>Effective altruists in particular should consider buying Darkcoin because the EA part of you should be less loss averse than the non-EA part of you, and Darkcoin is obviously a gamble to the extend the reasons I'm laying out here are false or insufficient.</p>\n<p>Finally Darkcoin has a complex bootstrap reward system for owners of many Darkcoin, who can own masternodes, &nbsp;and that is how Dropbox won, by creating a university level (copying Facebook's serendipitous \"strategy\") contest which would dramatically increase data storage of winning universities. Mine won and I have 30 Giga for free forever, for instance.&nbsp;</p>\n<p>There are other features that caused Facebook to win, or MSN to win. Are there features we want from a cryptocurrency that have yet not been captured by Darkcoin? Or are we witnessing a real last mover advantage in real time?&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tbow3GFK3dTW7tTNg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -2, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "26265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-24T03:08:34.784Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Games meetup", "slug": "meetup-washington-dc-games-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HGaZYroYMEpnv7jg9/meetup-washington-dc-games-meetup-0", "pageUrlRelative": "/posts/HGaZYroYMEpnv7jg9/meetup-washington-dc-games-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/HGaZYroYMEpnv7jg9/meetup-washington-dc-games-meetup-0", "postedAtFormatted": "Saturday, May 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGaZYroYMEpnv7jg9%2Fmeetup-washington-dc-games-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGaZYroYMEpnv7jg9%2Fmeetup-washington-dc-games-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGaZYroYMEpnv7jg9%2Fmeetup-washington-dc-games-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10m'>Washington DC Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10m'>Washington DC Games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HGaZYroYMEpnv7jg9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7447195478057275e-06, "legacy": true, "legacyId": "26270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Games_meetup\">Discussion article for the meetup : <a href=\"/meetups/10m\">Washington DC Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 May 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/10m\">Washington DC Games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-24T19:25:50.338Z", "modifiedAt": null, "url": null, "title": "Cognitive Biases due to a Narcissistic Parent, Illustrated by HPMOR Quotations", "slug": "cognitive-biases-due-to-a-narcissistic-parent-illustrated-by", "viewCount": null, "lastCommentedAt": "2020-04-01T20:43:44.518Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Algernoq", "createdAt": "2014-02-01T18:20:44.642Z", "isAdmin": false, "displayName": "Algernoq"}, "userId": "K5j8Ev7WCqkKQk3Mg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y6zh4vkK5pPfEPdBb/cognitive-biases-due-to-a-narcissistic-parent-illustrated-by", "pageUrlRelative": "/posts/y6zh4vkK5pPfEPdBb/cognitive-biases-due-to-a-narcissistic-parent-illustrated-by", "linkUrl": "https://www.lesswrong.com/posts/y6zh4vkK5pPfEPdBb/cognitive-biases-due-to-a-narcissistic-parent-illustrated-by", "postedAtFormatted": "Saturday, May 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cognitive%20Biases%20due%20to%20a%20Narcissistic%20Parent%2C%20Illustrated%20by%20HPMOR%20Quotations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACognitive%20Biases%20due%20to%20a%20Narcissistic%20Parent%2C%20Illustrated%20by%20HPMOR%20Quotations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy6zh4vkK5pPfEPdBb%2Fcognitive-biases-due-to-a-narcissistic-parent-illustrated-by%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cognitive%20Biases%20due%20to%20a%20Narcissistic%20Parent%2C%20Illustrated%20by%20HPMOR%20Quotations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy6zh4vkK5pPfEPdBb%2Fcognitive-biases-due-to-a-narcissistic-parent-illustrated-by", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy6zh4vkK5pPfEPdBb%2Fcognitive-biases-due-to-a-narcissistic-parent-illustrated-by", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1551, "htmlBody": "<p>A pattern of cognitive biases not yet discussed here are the biases due to having a narcissistic parent who seeks validation through the child&rsquo;s academic achievements.</p>\n<p>HPMOR clearly shows these biases: Harry's mother is narcissistic, impressed by education, and not particularly smart, and Harry does not realize how this affects his thinking.</p>\n<p>Here is my evidence:</p>\n<p>The Sorting Hat says Harry is driven by \"the fear of losing your fantasy of greatness, of disappointing the people who believe in you\" (Ch. 77). Psychology texts say that this fear is what children of a narcissistic parent usually feel. The child feels perpetually ignored because the narcissistic parent seeks validation from the child's accomplishments but refuses to actually listen to the child, spurring the child to ever greater heights of intellectual achievement.&nbsp;</p>\n<p>The text supports this view: &ldquo;Always Harry had been encouraged to study whatever caught his attention, bought all the books that caught his fancy...given anything reasonable that he wanted, except, maybe, the slightest shred of respect&rdquo; and &ldquo;Petunia wrung her hands. She seemed to be on the verge of tears. \"My love, I know I can't win arguments with you, but please, you have to trust me on this &hellip; I want my husband to, to listen to his wife who loves him, and trust her just this once - \" (Ch. 1) describes a narcissistic, anxiously needy mother, an avoidant father, and a son whose parents provide for his physical needs but neglect his need for respect (ego). &ldquo;If you conceived of yourself as a Good Parent, you would do it. But take a ten-year-old seriously? Hardly.&rdquo; (Ch. 1)&nbsp;</p>\n<p>Harry goes Dark when the connection to his family is threatened. For example: \"The black rage began to drain away, as it dawned on him that...his family wasn't in danger [of legal separation]\" (ch. 5) indicates that Harry went Dark even though no one&rsquo;s life was threatened. The cost of Harry&rsquo;s Dark Side is becoming an adult at a young age: Harry says, &ldquo;Every time I call on it... it uses up my childhood.&rdquo; (Ch. 91). This is consistent with spending nearly all free time studying (instead of wasting time with friends) to impress Harry&rsquo;s mother.</p>\n<p>Typically, children of narcissistic parents inherit either narcissistic or people-pleasing traits. I predicted that if my theory is correct then Harry would have a narcissistic personality. To test this, I found a list of personality traits that describe a narcissist (by Googling &ldquo;children of narcissistic parents&rdquo; and clicking the first link), and compared with Harry&rsquo;s personality as described in HPMOR. I got a 100% match. Questions and answers are as follows:&nbsp;</p>\n<p>1. Grandiose sense of self-importance? Check. Harry plans to &ldquo;optimize&rdquo; the entire Universe, expects to &ldquo;do something really revolutionary and important&rdquo; (Ch. 7), and is trying to &ldquo;hurry up and become God&rdquo; (Ch. 27).</p>\n<p>2. Obsessed with himself? Check. He appears to only care about people who are smarter or more powerful than him -- people who can help him. He also has contempt for most students and their interests (Quidditch, etc.)</p>\n<p>3. Goals are selfish? Check. Harry claims to want to save everyone, but he believes the best way to help others is to increase his own power most quickly. I address two possible objections below:</p>\n<p>Harry&rsquo;s involvement in the Azkaban breakout was selfish, because Harry could not risk losing Quirrell&rsquo;s friendship: &ldquo; It was a bond that went beyond anything of debts owed, or even anything of personal liking, that the two of them were alone in the wizarding world&rdquo; (Ch. 51). This, again, mirrors a child&rsquo;s relationship with a narcissistic mother: the child cannot risk losing the mother&rsquo;s protection. Harry also had selfish reasons for hearing Quirrell&rsquo;s plan: &ldquo;There was no advantage to be gained from not hearing it. And if it did reveal something wrong with Professor Quirrell, then it was very much to Harry's advantage to know it, even if he had promised not to tell anyone.&rdquo; (Ch. 49)</p>\n<p>Harry&rsquo;s efforts to save Hermione are also selfish because Harry sees Hermione in the same way he sees his mother -- weak in many ways and bound by emotions and convention, but someone Harry must impress and protect. Harry&rsquo;s statement that &ldquo;it&rsquo;s disrespectful to her, to think someone could only like her in that way&rdquo; (ch. 91) makes sense because Harry is disgusted by the Oedipal implications. If Harry&rsquo;s mother was not narcissistic, then Harry would not have worked so hard to impress Hermione and would have been less disgusted by the thought of being sexually attracted to her.</p>\n<p>4. Troubles with normal relationships? Check. Harry is playing high-stakes mind games with the people he is closest to (Quirrell, Draco, Hermione, Dumbeldore), which is not normal friend behavior. Harry has contempt for nearly everyone else.</p>\n<p>5. Becomes furious if criticized? Check. When Snape mocked Harry in Potions class, Harry tried to destroy Snape&rsquo;s career. Quirrell explained, &ldquo;When it looked like you might lose, you unsheathed your claws, heedless of the danger. You escalated, and then you escalated again&rdquo; (Ch. 19).</p>\n<p>6. Has fantasies of unbound success, power, intelligence, etc.? Check. Harry wants to conquer the entire Universe with the power of his intelligence, and has plans for how to fill an eternity, including to &ldquo;...meet up with everyone else who was born on Old Earth to watch the Sun finally go out&hellip;&rdquo; (Ch. 39).</p>\n<p>7. Believes that he is special and should only be around other high-status people? Check. Harry avoids average students when possible, and certainly does not hang out with them for fun. &ldquo;Note to self: The 75th percentile of Hogwarts students a.k.a. Ravenclaw House is not the world's most exclusive program for gifted children&rdquo; (Ch. 12).&nbsp;</p>\n<p>Harry&rsquo;s association with the (presumably non-special) students in his army is not an exception because minimal text is devoted to Harry instructing them, while much text explains how powerful and high-status the students in the army have become. For Harry, it appears that the army is a tool to use and an opportunity to show off, not an opportunity to give back and help friends improve their skills for their own sake.</p>\n<p>8. Requires extreme admiration for everything? Check. Harry takes anything less than admiration for his brilliance as an insult, and responds by striving for new levels of intellectual achievement and arrogance, until the others recognize his dominance. &ldquo;And I bit a math teacher when she wouldn't accept my dominance&rdquo; (Ch. 20). Quirrell&rsquo;s lesson on how to lose described how to avoid making powerful enemies, not how to empathize and care for others -- the insatiable need for admiration is merely delayed and repressed, not corrected.</p>\n<p>9. Feels entitled - has unreasonable expectations of special treatment? Check. Harry requires subservience from the school administration, and special magic items such as the time-turner. &ldquo;McGonagall said, \"but I do have a very special something else to give you. I see that I have greatly wronged you in my thoughts, Mr. Potter...this is an item which is ordinarily lent only to children who have already shown themselves to be highly responsible&rdquo; (Ch. 14).</p>\n<p>10. Takes advantage of others to further his own need? Check. Harry justifies his actions toward Draco by saying \"I only used you in ways that made you stronger. That's what it means to be used by a friend.\" (Ch. 97)</p>\n<p>11. Does not recognize the feelings of others? Check. One example is Harry not realizing how Neville felt about the prank on the train to Hogwarts. Another is Harry&rsquo;s remarkably clueless question to Hermione, &ldquo;Er, can I take it from this that you have been through puberty?\" (Ch. 87) Harry has not learned empathy yet: &ldquo;Harry flinched a little himself. Somewhere along the line he needed to pick up the knack of not phrasing things to hit as hard as he possibly could&rdquo; (Ch. 86).&nbsp;</p>\n<p>12. Envious or believes they are envied? Check. Quirrell said to Harry, &ldquo;You have everything now that I wanted then. All that I know of human nature says that I should hate you. And yet I do not. It is a very strange thing.&rdquo; (Ch. 74)</p>\n<p>13. Behaves arrogantly? Check. &ldquo;Minerva's body swayed with the force of that blow, with the sheer raw lese majeste. Even Severus looked shocked.&rdquo; (Ch. 19) I can&rsquo;t think offhand of a single instance when Harry is not arrogant.&nbsp;</p>\n<p>Therefore, I conclude that Harry and Harry&rsquo;s mother are both narcissistic. If you want further reading on this topic, look up \"The Drama of the Gifted Child\" by Dr. Alice Miller (Google for the .pdf) for a more detailed description of a child&rsquo;s typical relationship with a narcissistic parent.</p>\n<p>I am sharing this because it reveals a pattern of cognitive biases that many people (like me) who enjoyed HPMOR, and their parents, probably have. Specifically, there is a strong bias toward either narcissistic or people-pleasing habits, and a difficulty with recognizing and following one&rsquo;s own desires (because the Universe, unlike a parent, never tells people what to do). One possible reason for studying science is to defend against a parent&rsquo;s emotional neediness and refusal to provide ego-validation by building an impenetrable edifice of logical truth. Unfortunately, identifying the parent&rsquo;s cognitive biases does not stop their criticism. A more pleasant strategy is to recognize the dynamic, mourn the warping of childhood by the controlling parenting, set appropriate boundaries in the future, and draw validation from following one&rsquo;s own goals instead of an internalized parent&rsquo;s goals.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y6zh4vkK5pPfEPdBb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 23, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "26271", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-25T06:36:18.024Z", "modifiedAt": null, "url": null, "title": "Dissolving the Thread of Personal Identity", "slug": "dissolving-the-thread-of-personal-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:33.215Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Skeptityke", "createdAt": "2013-05-06T21:18:20.431Z", "isAdmin": false, "displayName": "Skeptityke"}, "userId": "H5HExNf3BsmBRvKJG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aB3ZyksATcid59S9G/dissolving-the-thread-of-personal-identity", "pageUrlRelative": "/posts/aB3ZyksATcid59S9G/dissolving-the-thread-of-personal-identity", "linkUrl": "https://www.lesswrong.com/posts/aB3ZyksATcid59S9G/dissolving-the-thread-of-personal-identity", "postedAtFormatted": "Sunday, May 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dissolving%20the%20Thread%20of%20Personal%20Identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADissolving%20the%20Thread%20of%20Personal%20Identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaB3ZyksATcid59S9G%2Fdissolving-the-thread-of-personal-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dissolving%20the%20Thread%20of%20Personal%20Identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaB3ZyksATcid59S9G%2Fdissolving-the-thread-of-personal-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaB3ZyksATcid59S9G%2Fdissolving-the-thread-of-personal-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1832, "htmlBody": "<p class=\"MsoNormal\">(Background: I got interested in anthropics about a week ago. It has tormented my waking thoughts ever since in a cycle of &ldquo;be confused, develop idea, work it out a bit, realize that it fails, repeat&rdquo; and it is seriously driving me berserk by this point. While drawing a bunch of &ldquo;thread of personal continuity&rdquo; diagrams to try to flesh out my next idea, I suspected that it was a fairly nonsensical idea, came up with a thought experiment that showed it was definitely a nonsensical idea, realized I was trying to answer the question &ldquo;Is there any meaningful sense in which I can expect to wake up as myself tomorrow, rather than Brittany Spears?&rdquo;, kept thinking anyways for about an hour, and eventually came up with this possible reduction of personal identity over time. It differs somewhat from Kaj Sotala&rsquo;s. And I still have no idea what the hell to do about anthropics, but I figured I should write up this intermediate result. It takes the form of a mental dialogue with myself, because that&rsquo;s what happened.)</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Hang on, this whole notion of &ldquo;thread of personal continuity&rdquo; looks sort of fishy. Self, can you try to clarify what it is?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Let&rsquo;s see&hellip; I have a causal link to my past and future self, and this causal link is the thread of personal identity!</p>\n<p class=\"MsoNormal\"><strong>Current Me:</strong> <em>Please notice Past Self&rsquo;s use of the cached thought from &ldquo;Timeless Identity&rdquo; even though it doesn&rsquo;t fit.</em></p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Causal links can&rsquo;t possibly be the thread of personal continuity. Your state at time t+1 is not just caused by your state at time t, lots of events in your surroundings also cause the t+1 state as well. A whole hell of a lot of stuff has a causal link to you. That can&rsquo;t possibly be it. And when you die, alive you has a causal link to dead you.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> And another thing, personal continuity isn&rsquo;t just an on-off thing. There&rsquo;s a gradient to it.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> What do you mean?</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Let&rsquo;s say you get frozen by cryonics, and then revived a century later.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Sure.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Let&rsquo;s say you know that you will be revived with exactly the same set of memories, preferences, thought patterns, etc, that you have currently. As you are beginning the process, what is your subjective credence that you will wake up a century later?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Fairly close to 1.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Now, let&rsquo;s say they could recover all the information from your brain except your extreme love for chocolate, so when your brain is restored, they patch in a generic average inclination for chocolate. What is your subjective credence that you will wake up a century later?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Fairly close to 1.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Let&rsquo;s say that all your inclinations and thought patterns and other stuff will be restored fully, but they can&rsquo;t bring back memories. You will wake up with total amnesia. What is your&hellip; you get the idea.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Oh crap. I&hellip; I really don&rsquo;t know. 0.6??? But then again, this is the situation that several real-life people have found themselves in&hellip; Huh.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> For this one, inclinations and thought patterns and many of your memories are unrecoverable, so when your brain is restored, you only have a third of your memories, a strong belief that you are the same person that was cryopreserved, and a completely different set of&hellip; everything else except for the memories and the belief in personal continuity. P(I wake up a century later)?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Quite low. ~0.1.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> But I see your point. For that whole personal identity/waking up as yourself thing, it isn&rsquo;t a binary trait, it&rsquo;s a sliding scale of belief that I&rsquo;ll keep on existing which depends on the magnitude of the difference between myself and the being that wakes up. If upload!me were fed through a lossy compression algorithm and then reconstructed, my degree of belief in continuing to exist would depend on how lossy it was.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Now you realize that the &ldquo;thread of subjective experience&rdquo; doesn&rsquo;t actually exist. There are just observer-moments. What would it even mean for something to have a &ldquo;thread of subjective experience&rdquo;?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> (Taps into intuition) What about that big rock over there? Forget &ldquo;subjective&rdquo;, that rock has a &ldquo;thread of existence&rdquo;. That rock will still be the same rock if it is moved 3 feet to the left, that rock will still be the same rock if a piece of it is chipped off, that rock will still be the same rock if it gets covered in moss, but that rock will cease to be a rock if a nuke goes off, turning it into rock vapor! I don&rsquo;t know what the hell the &ldquo;thread of existence&rdquo; is, but I know it has to work like that rock!!</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> So you&rsquo;re saying that personal identity over time works like the Ship of Theseus?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Exactly! We&rsquo;ve got a fuzzy category, like &ldquo;this ship&rdquo; or &ldquo;this rock&rdquo; or &ldquo;me&rdquo;, and there&rsquo;s stuff that we know falls in the category, stuff that we know doesn&rsquo;t fall in the category, and stuff for which we aren&rsquo;t sure whether it falls in the category! And the thing changes over time, and as long as it stays within certain bounds, we will still lump it into the same category.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Huh. So this &ldquo;thread of existence&rdquo; comes from the human tendency to assign things into fuzzy categories. So when a person goes to sleep at night, they know that in the morning, somebody extremely similar to themselves will be waking up, and that somebody falls into the fuzzy cluster that the person falling asleep labels &ldquo;I&rdquo;. As somebody continues through life, they know that two minutes from now, there will be a person that is similar enough to fall into the &ldquo;I&rdquo; cluster.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> But there&rsquo;s still a problem. 30yearfuture!me will probably be different enough from present!me to fall outside the &ldquo;I&rdquo; category. If I went to sleep, and I knew that 30yearfuture!me woke up, I&rsquo;d consider that to be tantamount to death. The two of us would share only a fraction of our memories, and he would probably have a different set of preferences, values, and thought patterns. How does this whole thing work when versions of yourself further out than a few years from your present self don&rsquo;t fall in the &ldquo;I&rdquo; cluster in thingspace?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> That&rsquo;s not too hard. The &ldquo;I&rdquo; cluster shifts over time as well. If you compare me at time t and me at time t+1, they would both fall within the &ldquo;I&rdquo; cluster at time t, but the &ldquo;I&rdquo; cluster of time t+1 is different enough to accommodate &ldquo;me&rdquo; at time t+2. It&rsquo;s like this rock.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Not the rock again.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Quiet. If you had this rock, and 100yearfuture!thisrock side by side, they would probably not be recognizable as the same rock, but there is a continuous series of intermediates leading from one to the other, each of which would be recognizable as the same rock as its immediate ancestors and descendants.</p>\n<p class=\"MsoNormal\">&nbsp;<strong>Self:</strong> If there is a continuous series of intermediates that doesn&rsquo;t happen too fast, leading from me to something very nonhuman, I will anticipate eventually experiencing what the nonhuman thing does, while if there is a discontinuous jump, I won&rsquo;t anticipate experiencing anything at all.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Huh.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> So that&rsquo;s where the feeling of the &ldquo;thread of personal identity&rdquo; comes from. We have a fuzzy category labeled &ldquo;I&rdquo;, anticipate experiencing the sorts of things that probable future beings who fall in that category will experience, and in everyday life, there aren&rsquo;t fast jumps to spots outside of the &ldquo;I&rdquo; category, so it feels like you&rsquo;ve stayed in the same category the whole time.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> You&rsquo;ll have to unpack &ldquo;anticipate experiencing the sorts of things that probable future beings who fall in that category will experience&rdquo;. Why?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Flippant answer: If we didn&rsquo;t work that way, evolution would have killed us a long time ago. Actual answer: Me at time t+1 experiences the same sorts of things as me at time t anticipated, so when me at time t+1 anticipates that me at time t+2 will experience something, it will probably happen. Looking backwards, anticipations of past selves frequently match up with the experiences of slightly-less-past selves, so looking forwards, the anticipations of my current self are likely to match up with the experiences of the future being who falls in the &ldquo;I&rdquo; category.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Makes sense.</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> You&rsquo;ll notice that this also defuses the anthropic trilemma (for humans, at least). There is a 1 in a billion chance of the quantum random number generator generating the winning lottery ticket. But then a trillion copies are made, but you at time (right after the generator returned the winning number) has a trillion expected near-future beings who fall within the &ldquo;I&rdquo; category, so the 1 in a billion probability is split up a trillion ways among all of them. P(loser) is about 1, P(specific winner clone) is 1 in a quintillion. All the specific winner clones are then merged, and since a trillion different hypotheses each with a 1 in a quintillion probability all predict the same series of observed future events from time(right after you merge) onwards, P(series of experiences following from winning the quantum lottery) is 1 in a billion.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> Doesn&rsquo;t this imply that anthropic probabilities depend on how big a boundary the mind draws around stuff it considers &ldquo;I&rdquo;?</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> Yes. Let&rsquo;s say we make 2 copies of a mind, and a third &ldquo;copy&rdquo; produced by running the mind through a lossy compression algorithm, and uncompressing it. A blue screen will be shown to one of the perfect mind copies (which may try to destroy it). A mind that considered the crappy copy to fall in the &ldquo;I&rdquo; category would predict a 1/3 chance of seeing the blue screen, while a mind that only considers near-perfect copies of itself as &ldquo;I&rdquo; would predict a 1/2 chance of seeing the blue screen, because the mind with the broad definition of &ldquo;I&rdquo; seriously considers the possibility of waking up as the crappy copy, while the mind with the narrow definition of &ldquo;I&rdquo; doesn&rsquo;t.</p>\n<p class=\"MsoNormal\"><strong>Doubt:</strong> This seems to render probability useless.</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\"><strong>Self:</strong> It means that probabilities of the form (I will observe X) are mind-dependent. Different minds given the same data will disagree on the probability of that statement, because they have different reference classes for the word &ldquo;I&rdquo;. Probabilities of the form (reality works like X)&hellip; to be honest, I don&rsquo;t know. Anthropics is still extremely aggravating. I haven&rsquo;t figured out the human version of anthropics (using the personal continuity notion) yet, I especially haven&rsquo;t figured out how it&rsquo;s going to work if you have a AI which doesn&rsquo;t assign versions of itself to a fuzzy category labeled &ldquo;I&rdquo;, and I&rsquo;m distrustful of how UDT seems like it&rsquo;s optimizing over the entire tegmark 4 multiverse when there&rsquo;s a chance that our reality is the only one there is, in which case it seems like you&rsquo;d need probabilities of the form (reality works like X) and some way to update far away from the Boltzmann Brain hypothesis. This above section may be confused or flat-out wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2fa": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aB3ZyksATcid59S9G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 1.7470394516599348e-06, "legacy": true, "legacyId": "26272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-25T11:27:23.868Z", "modifiedAt": null, "url": null, "title": "Meetup : Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv", "slug": "meetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e7PpqDTP6D6oemsru/meetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel", "pageUrlRelative": "/posts/e7PpqDTP6D6oemsru/meetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel", "linkUrl": "https://www.lesswrong.com/posts/e7PpqDTP6D6oemsru/meetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel", "postedAtFormatted": "Sunday, May 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Prof.%20Roman%20Yampolskiy%20on%20approaches%20to%20AGI%20risk%2C%20Tel%20Aviv&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Prof.%20Roman%20Yampolskiy%20on%20approaches%20to%20AGI%20risk%2C%20Tel%20Aviv%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe7PpqDTP6D6oemsru%2Fmeetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Prof.%20Roman%20Yampolskiy%20on%20approaches%20to%20AGI%20risk%2C%20Tel%20Aviv%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe7PpqDTP6D6oemsru%2Fmeetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe7PpqDTP6D6oemsru%2Fmeetup-prof-roman-yampolskiy-on-approaches-to-agi-risk-tel", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10n'>Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 May 2014 06:34:45AM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">98 Yigal Alon St.,, 33rd floor, Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On Wednesday May 28, Roman Yampolskiy will speak on approaches to AGI risk. See his overview article \"Responses to Catastrophic AGI Risk,\" coauthored with Kaj Sotala.</p>\n\n<p>It will be at Google Israel's offices, Electra Tower, 98 Yigal Alon St., Tel Aviv.</p>\n\n<p>We'll meet at the 29th floor at 20:00, then go right up to the 33rd floor and start the talk at 20:30.</p>\n\n<p>If you can't find us, call Anatoly who is graciously hosting us at 054-245-1060, or me at 054-569-1165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10n'>Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e7PpqDTP6D6oemsru", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.7474498687107733e-06, "legacy": true, "legacyId": "26273", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Prof__Roman_Yampolskiy_on_approaches_to_AGI_risk__Tel_Aviv\">Discussion article for the meetup : <a href=\"/meetups/10n\">Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 May 2014 06:34:45AM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">98 Yigal Alon St.,, 33rd floor, Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On Wednesday May 28, Roman Yampolskiy will speak on approaches to AGI risk. See his overview article \"Responses to Catastrophic AGI Risk,\" coauthored with Kaj Sotala.</p>\n\n<p>It will be at Google Israel's offices, Electra Tower, 98 Yigal Alon St., Tel Aviv.</p>\n\n<p>We'll meet at the 29th floor at 20:00, then go right up to the 33rd floor and start the talk at 20:30.</p>\n\n<p>If you can't find us, call Anatoly who is graciously hosting us at 054-245-1060, or me at 054-569-1165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Prof__Roman_Yampolskiy_on_approaches_to_AGI_risk__Tel_Aviv1\">Discussion article for the meetup : <a href=\"/meetups/10n\">Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv</a></h2>", "sections": [{"title": "Discussion article for the meetup : Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv", "anchor": "Discussion_article_for_the_meetup___Prof__Roman_Yampolskiy_on_approaches_to_AGI_risk__Tel_Aviv", "level": 1}, {"title": "Discussion article for the meetup : Prof. Roman Yampolskiy on approaches to AGI risk, Tel Aviv", "anchor": "Discussion_article_for_the_meetup___Prof__Roman_Yampolskiy_on_approaches_to_AGI_risk__Tel_Aviv1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-25T16:35:16.985Z", "modifiedAt": null, "url": null, "title": "Meetup : test meetup", "slug": "meetup-test-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yPuMCS2cK84Z6i9j6/meetup-test-meetup", "pageUrlRelative": "/posts/yPuMCS2cK84Z6i9j6/meetup-test-meetup", "linkUrl": "https://www.lesswrong.com/posts/yPuMCS2cK84Z6i9j6/meetup-test-meetup", "postedAtFormatted": "Sunday, May 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20test%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20test%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyPuMCS2cK84Z6i9j6%2Fmeetup-test-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20test%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyPuMCS2cK84Z6i9j6%2Fmeetup-test-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyPuMCS2cK84Z6i9j6%2Fmeetup-test-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10o'>test meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 May 2014 09:35:10AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Minneapolis, MN</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>testing. using past date should prevent this from cluttering LW. I hope.</p>\n\n<p>test complete but I'm not sure how to delete meetups. it doesn't look like this shows up though so I'll let it be.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10o'>test meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yPuMCS2cK84Z6i9j6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7478841406157668e-06, "legacy": true, "legacyId": "26275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___test_meetup\">Discussion article for the meetup : <a href=\"/meetups/10o\">test meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 May 2014 09:35:10AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Minneapolis, MN</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>testing. using past date should prevent this from cluttering LW. I hope.</p>\n\n<p>test complete but I'm not sure how to delete meetups. it doesn't look like this shows up though so I'll let it be.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___test_meetup1\">Discussion article for the meetup : <a href=\"/meetups/10o\">test meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : test meetup", "anchor": "Discussion_article_for_the_meetup___test_meetup", "level": 1}, {"title": "Discussion article for the meetup : test meetup", "anchor": "Discussion_article_for_the_meetup___test_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-25T16:38:00.517Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7FwZXaqsMc9pPJCuu/weekly-lw-meetups", "pageUrlRelative": "/posts/7FwZXaqsMc9pPJCuu/weekly-lw-meetups", "linkUrl": "https://www.lesswrong.com/posts/7FwZXaqsMc9pPJCuu/weekly-lw-meetups", "postedAtFormatted": "Sunday, May 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FwZXaqsMc9pPJCuu%2Fweekly-lw-meetups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FwZXaqsMc9pPJCuu%2Fweekly-lw-meetups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FwZXaqsMc9pPJCuu%2Fweekly-lw-meetups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 598, "htmlBody": "<p><strong>This summary was posted to LW main on May 16th. The following week's summary is <a href=\"/r/lesswrong/lw/k9w/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/zr\"></a><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">17 May 2014 11:31AM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/zy\">Chicago Calibration Game:&nbsp;<span class=\"date\">31 May 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/109\">Christchurch, New Zealand Meetup:&nbsp;<span class=\"date\">18 May 2014 04:30PM</span></a></li>\n<li><a href=\"/meetups/10c\">Frankfurt:&nbsp;<span class=\"date\">25 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/106\">Montreal - Fun and games:&nbsp;<span class=\"date\">17 May 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/zz\">Montreal - How to be charismatic:&nbsp;<span class=\"date\">26 May 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/10d\">Moscow meet up:&nbsp;<span class=\"date\">18 May 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/102\">Utrecht - Social discussion at the Film Caf&eacute;:&nbsp;<span class=\"date\">17 May 2014 05:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">17 May 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/104\">Boston - An introduction to digital cryptography:&nbsp;<span class=\"date\">18 May 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/zv\">Canberra: Rationalist Fun and Games!:&nbsp;<span class=\"date\">24 May 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/108\">London social meetup - possibly in a park:&nbsp;<span class=\"date\">18 May 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/10f\">Melbourne June Rationality Dojo: Memory:&nbsp;<span class=\"date\">01 June 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10b\">Sydney Meetup - May:&nbsp;<span class=\"date\">21 May 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/10g\">Sydney Social Meetup - June (Games night):&nbsp;<span class=\"date\">12 June 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/107\">Vienna Meetup:&nbsp;<span class=\"date\">17 May 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10a\">Washington DC Short Talks Meetup:&nbsp;<span class=\"date\">18 May 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7FwZXaqsMc9pPJCuu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.7478879857840014e-06, "legacy": true, "legacyId": "26240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w64hYbQEQSQJcBkSx", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-26T07:42:06.547Z", "modifiedAt": null, "url": null, "title": "Open Thread, May 26 - June 1, 2014", "slug": "open-thread-may-26-june-1-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:35.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BarbaraB", "createdAt": "2012-04-04T15:11:52.435Z", "isAdmin": false, "displayName": "BarbaraB"}, "userId": "aHcDnFsxTnRpMEuXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NG6R2eZp7XmCtqkFX/open-thread-may-26-june-1-2014", "pageUrlRelative": "/posts/NG6R2eZp7XmCtqkFX/open-thread-may-26-june-1-2014", "linkUrl": "https://www.lesswrong.com/posts/NG6R2eZp7XmCtqkFX/open-thread-may-26-june-1-2014", "postedAtFormatted": "Monday, May 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20May%2026%20-%20June%201%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20May%2026%20-%20June%201%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNG6R2eZp7XmCtqkFX%2Fopen-thread-may-26-june-1-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20May%2026%20-%20June%201%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNG6R2eZp7XmCtqkFX%2Fopen-thread-may-26-june-1-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNG6R2eZp7XmCtqkFX%2Fopen-thread-may-26-june-1-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p style=\"text-align: justify;\">Previous Open Thread:&nbsp; http://lesswrong.com/r/discussion/lw/k94/open_thread_may_19_25_2014/</p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<p style=\"text-align: justify;\"><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\">You know the drill - If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. Open Threads should start on Monday, and end on Sunday.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\"><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NG6R2eZp7XmCtqkFX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.7491643280352892e-06, "legacy": true, "legacyId": "26277", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 249, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-26T14:44:31.621Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: Decision Theory", "slug": "meetup-canberra-decision-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h34iSafJm4Gny8mvM/meetup-canberra-decision-theory", "pageUrlRelative": "/posts/h34iSafJm4Gny8mvM/meetup-canberra-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/h34iSafJm4Gny8mvM/meetup-canberra-decision-theory", "postedAtFormatted": "Monday, May 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh34iSafJm4Gny8mvM%2Fmeetup-canberra-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh34iSafJm4Gny8mvM%2Fmeetup-canberra-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh34iSafJm4Gny8mvM%2Fmeetup-canberra-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10p'>Canberra: Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 June 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Jan Leike, PhD student in Computer Science at the ANU, will give a brief talk about decision theory, giving an overview of the goal of the field and outlining evidential and causal decision theory. There will then be a Q&amp;A session and general discussion. Vegan snacks will, as always, be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the same location.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10p'>Canberra: Decision Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h34iSafJm4Gny8mvM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.749761231688838e-06, "legacy": true, "legacyId": "26278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__Decision_Theory\">Discussion article for the meetup : <a href=\"/meetups/10p\">Canberra: Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 June 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Jan Leike, PhD student in Computer Science at the ANU, will give a brief talk about decision theory, giving an overview of the goal of the field and outlining evidential and causal decision theory. There will then be a Q&amp;A session and general discussion. Vegan snacks will, as always, be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the same location.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__Decision_Theory1\">Discussion article for the meetup : <a href=\"/meetups/10p\">Canberra: Decision Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: Decision Theory", "anchor": "Discussion_article_for_the_meetup___Canberra__Decision_Theory", "level": 1}, {"title": "Discussion article for the meetup : Canberra: Decision Theory", "anchor": "Discussion_article_for_the_meetup___Canberra__Decision_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-27T03:28:08.719Z", "modifiedAt": null, "url": null, "title": "Pascal's Mugging Solved", "slug": "pascal-s-mugging-solved", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "common_law", "createdAt": "2012-07-03T06:17:42.167Z", "isAdmin": false, "displayName": "common_law"}, "userId": "PJzXMdPcBFR6cpXPY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9WZAqFzNJ4KrhsqJ7/pascal-s-mugging-solved", "pageUrlRelative": "/posts/9WZAqFzNJ4KrhsqJ7/pascal-s-mugging-solved", "linkUrl": "https://www.lesswrong.com/posts/9WZAqFzNJ4KrhsqJ7/pascal-s-mugging-solved", "postedAtFormatted": "Tuesday, May 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pascal's%20Mugging%20Solved&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APascal's%20Mugging%20Solved%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WZAqFzNJ4KrhsqJ7%2Fpascal-s-mugging-solved%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pascal's%20Mugging%20Solved%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WZAqFzNJ4KrhsqJ7%2Fpascal-s-mugging-solved", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WZAqFzNJ4KrhsqJ7%2Fpascal-s-mugging-solved", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 532, "htmlBody": "<p>Since Pascal&rsquo;s Mugging is well known on LW, I won&rsquo;t describe it at length. Suffice to say that a mugger tries to blackmail you by threatening enormous harm by a completely mysterious mechanism. If the harm is great enough, a sufficiently large threat eventually dominates doubts about the mechanism.<br /><br />I have a reasonably simple solution to Pascal&rsquo;s Mugging. In four steps, here it is:<br /></p>\n<ol>\n<li>The greater the harm, the more likely the mugger is trying to pick a greater threat than any competitor picks (we&rsquo;ll call that maximizing).</li>\n<li>As the amount of harm threatened gets larger, the probability that the mugger is maximizing approaches unity.</li>\n<li>As the probability that the mugger is engaged in maximizing approaches unity, the likelihood that the mugger&rsquo;s claim is true approaches zero.</li>\n<li>The probability that a contrary claim is true&mdash;that contributing to the mugger will cause the feared calamity&mdash;exceeds the probability that the mugger&rsquo;s claim is true when the probability that the mugger is maximizing increases sufficiently.</li>\n</ol>\n<p>Pascal&rsquo;s Mugging induces us to look at the likelihood of the claim in abstraction from the fact that the claim is made. The paradox can be solved by breaking the probability that the mugger&rsquo;s claim is true into two parts: the probability of the claim itself (its simplicity) and the probability that the mugger is truthful. Even if the probability of magical harm doesn&rsquo;t decrease when the amount of harm increases, the probability that the mugger is truthful decreases continuously as the amount of harm predicted increases.<br /><br />Solving the paradox in Pascal&rsquo;s Mugging depends on recognizing that, if the logic were sound, it would engage muggers in a game where they try to pick the highest practicable number to represent the amount of harm. But this means that the higher the number, the more likely they are to be playing this game (undermining the logic believed sound).<br /><br />But solving Pascal&rsquo;s Mugging also depends on recognizing that the evidence that the mugger is maximizing can lower the probability below that of the same harm when no mugger has claimed it. It involves recognizing that, when it is almost certain that the claim is motivated by something unrelated to the claim&rsquo;s truth, the claim can become less believable than if it hadn&rsquo;t been expressed. <em>The mugger&rsquo;s maximizing motivation is evidence against his claim.</em> <br /><br />If someone presents you with a number representing the amount of threatened harm 3^3^3..., continued as long as a computer can print out when the printer is allowed for run for, say, a decade, you should think this result less probable than if someone had never presented you with the tome. While people are more likely to be telling the truth than to be lying, if you are sufficiently sure they are lying, their testimony counts against their claim.<br /><br />The proof is the same as the proof of the (also counter-intuitive) proposition that failure to find (some definite amount of) evidence for a theory constitutes negative evidence. The mugger has elicited your search for evidence, but because of the mugger&rsquo;s clear interest in falsehood, you find that evidence wanting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9WZAqFzNJ4KrhsqJ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -1, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "26279", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-27T07:37:45.920Z", "modifiedAt": null, "url": null, "title": "Meetup : June Meetup in Munich", "slug": "meetup-june-meetup-in-munich", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Teobaldo", "createdAt": "2012-10-28T17:25:26.852Z", "isAdmin": false, "displayName": "Teobaldo"}, "userId": "BhijBsy7WLfpnsZGJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DvFKbxLYjCxajWmAm/meetup-june-meetup-in-munich", "pageUrlRelative": "/posts/DvFKbxLYjCxajWmAm/meetup-june-meetup-in-munich", "linkUrl": "https://www.lesswrong.com/posts/DvFKbxLYjCxajWmAm/meetup-june-meetup-in-munich", "postedAtFormatted": "Tuesday, May 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20June%20Meetup%20in%20Munich&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20June%20Meetup%20in%20Munich%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvFKbxLYjCxajWmAm%2Fmeetup-june-meetup-in-munich%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20June%20Meetup%20in%20Munich%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvFKbxLYjCxajWmAm%2Fmeetup-june-meetup-in-munich", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvFKbxLYjCxajWmAm%2Fmeetup-june-meetup-in-munich", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10q'>June Meetup in Munich</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Outside the mathematics building, Theresienstra\u00dfe 41, Munich</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>You are invited to come to the June Munich LW Meetup! We're planning to meet outside the mathematics building at the LMU. Depending on the weather, we'll stay outside or occupy a free room inside the math department. If anyone would bring food for the group, it would be awesome. :) It goes without saying that newcomers are very welcomed.\nWe want to discuss the article <a href=\"http://rockstarresearch.com/19-traits-of-player-characters/\" rel=\"nofollow\">http://rockstarresearch.com/19-traits-of-player-characters/</a> and maybe talk about agency. Any further suggestions are highly encouraged.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10q'>June Meetup in Munich</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DvFKbxLYjCxajWmAm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7511944725612927e-06, "legacy": true, "legacyId": "26280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___June_Meetup_in_Munich\">Discussion article for the meetup : <a href=\"/meetups/10q\">June Meetup in Munich</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2014 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Outside the mathematics building, Theresienstra\u00dfe 41, Munich</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>You are invited to come to the June Munich LW Meetup! We're planning to meet outside the mathematics building at the LMU. Depending on the weather, we'll stay outside or occupy a free room inside the math department. If anyone would bring food for the group, it would be awesome. :) It goes without saying that newcomers are very welcomed.\nWe want to discuss the article <a href=\"http://rockstarresearch.com/19-traits-of-player-characters/\" rel=\"nofollow\">http://rockstarresearch.com/19-traits-of-player-characters/</a> and maybe talk about agency. Any further suggestions are highly encouraged.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___June_Meetup_in_Munich1\">Discussion article for the meetup : <a href=\"/meetups/10q\">June Meetup in Munich</a></h2>", "sections": [{"title": "Discussion article for the meetup : June Meetup in Munich", "anchor": "Discussion_article_for_the_meetup___June_Meetup_in_Munich", "level": 1}, {"title": "Discussion article for the meetup : June Meetup in Munich", "anchor": "Discussion_article_for_the_meetup___June_Meetup_in_Munich1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-28T11:44:23.789Z", "modifiedAt": null, "url": null, "title": "The Useful Definition of \"I\"", "slug": "the-useful-definition-of-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:08.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ete", "createdAt": "2013-10-15T13:27:23.489Z", "isAdmin": false, "displayName": "plex"}, "userId": "Sp5wM4aRAhNERd4oY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nHjtPSZxkiyBEHWTQ/the-useful-definition-of-i", "pageUrlRelative": "/posts/nHjtPSZxkiyBEHWTQ/the-useful-definition-of-i", "linkUrl": "https://www.lesswrong.com/posts/nHjtPSZxkiyBEHWTQ/the-useful-definition-of-i", "postedAtFormatted": "Wednesday, May 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Useful%20Definition%20of%20%22I%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Useful%20Definition%20of%20%22I%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHjtPSZxkiyBEHWTQ%2Fthe-useful-definition-of-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Useful%20Definition%20of%20%22I%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHjtPSZxkiyBEHWTQ%2Fthe-useful-definition-of-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHjtPSZxkiyBEHWTQ%2Fthe-useful-definition-of-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1057, "htmlBody": "<p><em>aka The Fuzzy Pattern Theory of Identity</em></p>\n<p><em><strong>Background reading</strong>:&nbsp;<a href=\"/lw/qx/timeless_identity/\">Timeless Identity</a>,&nbsp;<a href=\"/lw/19d/the_anthropic_trilemma/\">The Anthropic Trilemma</a></em></p>\n<p><a href=\"/lw/pm/identity_isnt_in_specific_atoms/\">Identity is not based on continuity of physical material</a>.</p>\n<p><a href=\"/lw/k9s/dissolving_the_thread_of_personal_identity/\">Identity is not based on causal links to previous/future selves</a>.</p>\n<p>Identity is not usefully defined as a single point in <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">thingspace</a>.&nbsp;An \"I\" which only exists for an instant (i.e. zero continuity of identity) does not even remotely correspond to what we're trying to express by the word \"I\" in general use, and refers instead to a single snapshot. Consider the choice between putting yourself in stasis for eternity against living normally; a definition of \"I\" which prefers self-preservation by literally preserving a snapshot of one instant is massively unintuitive and uninformative compared to a definition which leads us to preserve \"I\" by allowing it to keep living even if that includes change.</p>\n<p>Identity is not the current isolated frame.</p>\n<p>&nbsp;</p>\n<h3>So if none of those are what \"I\"/Identity is based on, what is?</h3>\n<p>Some configurations of matter I would consider to be definitely me, and some definitely not me. Between the two extremes there are plenty of border cases wherever you try to draw a line. As an exercise: five minutes in the past ete, 30 years in the future ete, alternate branch ete brought up by different parents, ete's identical twin, ete with different genetics/body but a mindstate near-identical to current ete, sibling raised in same environment with many shared memories, random human, monkey, mouse, bacteria, rock. With sufficiently advanced technology, it would be possible to change me between those configurations one atom at a time. Without appeals to physical or causal continuity, there's no way to cleanly draw a hard binary line without violating what we mean by \"I\" in some important way or allowing, at some point, a change vastly below perceptible levels to flip a configuration from \"me\" to \"not-me\" all at once.</p>\n<p>Or, put another way, <strong>identity is not binary, it is fuzzy like everything else in human <a href=\"/lw/o3/superexponential_conceptspace_and_simple_words/\">conceptspace</a></strong>.</p>\n<p>It's interesting to note that examining common language use shows that in some sense this is widely known. When someone's changed by an experience or acting in a way unfitting with your model of them it's common to say something along the lines of \"he's like a different person\" or \"she's not acting like herself\", and the qualifier!person nomenclature that is becoming a bit more frequent, all hint at different versions of a person having only partially the same identity.</p>\n<p>&nbsp;</p>\n<h3>Why do we have a sense of identity?</h3>\n<p>For something as universal as the feeling of having an identity there's likely to be some evolutionary purpose. Luckily, it's fairly straightforward to see why it would increase fitness. The brain's learning is based on reward/punishment and connecting behaviours which are helpful/harmful to them, which is great for some things but could struggle with long term goals since the reward for making the right/punishment for wrong decision comes very distantly from the choice, so relatively weakly connected and reinforced. Creatures which can easily identify future/past continuations using an \"I\" concept of their own presence have a ready-built way to handle delayed gratification situations. Evolution needs to connect up \"doing this will make \"I\" concept future be expected to get reward\" to some reward in order to encourage the creature to think longer term, rather than specifically connecting each possible long term beneficial reward to each behaviour. Kaj_Sotala's <a href=\"/lw/grl/an_attempt_to_dissolve_subjective_expectation_and/\">attempt to dissolve subjective expectation and personal identity</a> contains another approach to understanding why we have a sense of identity, as well as many other interesting thoughts.</p>\n<p>&nbsp;</p>\n<h3>So what is it?</h3>\n<p>If you took yourself from right now and changed your entire body into a hippopotamus, or uploaded yourself into a computer, but still retained full memories/consciousness/responses to situations, you would likely consider yourself a more central example of the fuzzy \"I\" concept than if you made the physically relatively small change of removing your personality and memories. General physical structure is not a core feature of \"I\", though it is a relatively minor part.</p>\n<p><strong>Your \"I\"/identity is a concept (in the conceptspace/thingspace sense), centred on current you, with configurations of matter being considered more central to the \"I\" cluster the more similar they are to current you in the ways which current you values.</strong></p>\n<p>To give some concrete examples: Most people consider their memories to be very important to them, so any configuration without a similar set of memories is going to be distant. Many people consider some political/social/family group/belief system to be extremely important to them, so an alternate version of themselves in a different group would be considered moderately distant. An Olympic athlete or model may put an unusually large amount of importance on their body, so changes to it would move a configuration away from their idea of self quicker than for most.</p>\n<p>This fits very nicely with intuition about changing core beliefs or things you care about (e.g. athlete becomes disabled, large change in personal circumstances) making you in at least some sense a different person, and as far as I can tell does not fall apart/prove useless in similar ways to alternative definitions.</p>\n<p>&nbsp;</p>\n<h3>What consequences does this theory have for common issues with identity?</h3>\n<ul>\n<li>Moment to moment identity is <em>almost entirely, but not perfectly</em>&nbsp;retained.</li>\n<li>You will wake up as yourself after a night's sleep in a meaningful sense, but not as quite as central example of current-you's \"I\" as you would after a few seconds.</li>\n<li>The teleporter to Mars does not kill you in the most important sense (unless somehow your location on Earth is a particularly core part of your identity).</li>\n<li>Any high-fidelity clone can be usefully considered to be <em>you</em>, however it originated, until it diverges significantly.</li>\n<li>Cryonics or&nbsp;plastination do present a chance for bringing <em>you</em> back (conditional on information being preserved to reasonable fidelity), especially if you consider your mind rather than your body as core to your identity (so would not consider being an upload a huge change).</li>\n<li>Suggest more in comments!</li>\n</ul>\n<div><br /></div>\n<h3>Why does this matter?</h3>\n<p>Flawed assumptions and confusion about identity seem to underlie several notable difficulties in decision theory, anthropic issues, and less directly problems understanding what morality is, as I hope to explore in future posts.</p>\n<hr />\n<p><em>Thanks to <a href=\"/user/Skeptityke/\">Skeptityke</a> for reading through this and giving useful suggestions, as well as writing <a href=\"/lw/k9s/dissolving_the_thread_of_personal_identity/\">this</a> which meant there was a lot less background I needed to explain.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2fa": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nHjtPSZxkiyBEHWTQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "26274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>aka The Fuzzy Pattern Theory of Identity</em></p>\n<p><em><strong>Background reading</strong>:&nbsp;<a href=\"/lw/qx/timeless_identity/\">Timeless Identity</a>,&nbsp;<a href=\"/lw/19d/the_anthropic_trilemma/\">The Anthropic Trilemma</a></em></p>\n<p><a href=\"/lw/pm/identity_isnt_in_specific_atoms/\">Identity is not based on continuity of physical material</a>.</p>\n<p><a href=\"/lw/k9s/dissolving_the_thread_of_personal_identity/\">Identity is not based on causal links to previous/future selves</a>.</p>\n<p>Identity is not usefully defined as a single point in <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">thingspace</a>.&nbsp;An \"I\" which only exists for an instant (i.e. zero continuity of identity) does not even remotely correspond to what we're trying to express by the word \"I\" in general use, and refers instead to a single snapshot. Consider the choice between putting yourself in stasis for eternity against living normally; a definition of \"I\" which prefers self-preservation by literally preserving a snapshot of one instant is massively unintuitive and uninformative compared to a definition which leads us to preserve \"I\" by allowing it to keep living even if that includes change.</p>\n<p>Identity is not the current isolated frame.</p>\n<p>&nbsp;</p>\n<h3 id=\"So_if_none_of_those_are_what__I__Identity_is_based_on__what_is_\">So if none of those are what \"I\"/Identity is based on, what is?</h3>\n<p>Some configurations of matter I would consider to be definitely me, and some definitely not me. Between the two extremes there are plenty of border cases wherever you try to draw a line. As an exercise: five minutes in the past ete, 30 years in the future ete, alternate branch ete brought up by different parents, ete's identical twin, ete with different genetics/body but a mindstate near-identical to current ete, sibling raised in same environment with many shared memories, random human, monkey, mouse, bacteria, rock. With sufficiently advanced technology, it would be possible to change me between those configurations one atom at a time. Without appeals to physical or causal continuity, there's no way to cleanly draw a hard binary line without violating what we mean by \"I\" in some important way or allowing, at some point, a change vastly below perceptible levels to flip a configuration from \"me\" to \"not-me\" all at once.</p>\n<p>Or, put another way, <strong>identity is not binary, it is fuzzy like everything else in human <a href=\"/lw/o3/superexponential_conceptspace_and_simple_words/\">conceptspace</a></strong>.</p>\n<p>It's interesting to note that examining common language use shows that in some sense this is widely known. When someone's changed by an experience or acting in a way unfitting with your model of them it's common to say something along the lines of \"he's like a different person\" or \"she's not acting like herself\", and the qualifier!person nomenclature that is becoming a bit more frequent, all hint at different versions of a person having only partially the same identity.</p>\n<p>&nbsp;</p>\n<h3 id=\"Why_do_we_have_a_sense_of_identity_\">Why do we have a sense of identity?</h3>\n<p>For something as universal as the feeling of having an identity there's likely to be some evolutionary purpose. Luckily, it's fairly straightforward to see why it would increase fitness. The brain's learning is based on reward/punishment and connecting behaviours which are helpful/harmful to them, which is great for some things but could struggle with long term goals since the reward for making the right/punishment for wrong decision comes very distantly from the choice, so relatively weakly connected and reinforced. Creatures which can easily identify future/past continuations using an \"I\" concept of their own presence have a ready-built way to handle delayed gratification situations. Evolution needs to connect up \"doing this will make \"I\" concept future be expected to get reward\" to some reward in order to encourage the creature to think longer term, rather than specifically connecting each possible long term beneficial reward to each behaviour. Kaj_Sotala's <a href=\"/lw/grl/an_attempt_to_dissolve_subjective_expectation_and/\">attempt to dissolve subjective expectation and personal identity</a> contains another approach to understanding why we have a sense of identity, as well as many other interesting thoughts.</p>\n<p>&nbsp;</p>\n<h3 id=\"So_what_is_it_\">So what is it?</h3>\n<p>If you took yourself from right now and changed your entire body into a hippopotamus, or uploaded yourself into a computer, but still retained full memories/consciousness/responses to situations, you would likely consider yourself a more central example of the fuzzy \"I\" concept than if you made the physically relatively small change of removing your personality and memories. General physical structure is not a core feature of \"I\", though it is a relatively minor part.</p>\n<p><strong id=\"Your__I__identity_is_a_concept__in_the_conceptspace_thingspace_sense___centred_on_current_you__with_configurations_of_matter_being_considered_more_central_to_the__I__cluster_the_more_similar_they_are_to_current_you_in_the_ways_which_current_you_values_\">Your \"I\"/identity is a concept (in the conceptspace/thingspace sense), centred on current you, with configurations of matter being considered more central to the \"I\" cluster the more similar they are to current you in the ways which current you values.</strong></p>\n<p>To give some concrete examples: Most people consider their memories to be very important to them, so any configuration without a similar set of memories is going to be distant. Many people consider some political/social/family group/belief system to be extremely important to them, so an alternate version of themselves in a different group would be considered moderately distant. An Olympic athlete or model may put an unusually large amount of importance on their body, so changes to it would move a configuration away from their idea of self quicker than for most.</p>\n<p>This fits very nicely with intuition about changing core beliefs or things you care about (e.g. athlete becomes disabled, large change in personal circumstances) making you in at least some sense a different person, and as far as I can tell does not fall apart/prove useless in similar ways to alternative definitions.</p>\n<p>&nbsp;</p>\n<h3 id=\"What_consequences_does_this_theory_have_for_common_issues_with_identity_\">What consequences does this theory have for common issues with identity?</h3>\n<ul>\n<li>Moment to moment identity is <em>almost entirely, but not perfectly</em>&nbsp;retained.</li>\n<li>You will wake up as yourself after a night's sleep in a meaningful sense, but not as quite as central example of current-you's \"I\" as you would after a few seconds.</li>\n<li>The teleporter to Mars does not kill you in the most important sense (unless somehow your location on Earth is a particularly core part of your identity).</li>\n<li>Any high-fidelity clone can be usefully considered to be <em>you</em>, however it originated, until it diverges significantly.</li>\n<li>Cryonics or&nbsp;plastination do present a chance for bringing <em>you</em> back (conditional on information being preserved to reasonable fidelity), especially if you consider your mind rather than your body as core to your identity (so would not consider being an upload a huge change).</li>\n<li>Suggest more in comments!</li>\n</ul>\n<div><br></div>\n<h3 id=\"Why_does_this_matter_\">Why does this matter?</h3>\n<p>Flawed assumptions and confusion about identity seem to underlie several notable difficulties in decision theory, anthropic issues, and less directly problems understanding what morality is, as I hope to explore in future posts.</p>\n<hr>\n<p><em>Thanks to <a href=\"/user/Skeptityke/\">Skeptityke</a> for reading through this and giving useful suggestions, as well as writing <a href=\"/lw/k9s/dissolving_the_thread_of_personal_identity/\">this</a> which meant there was a lot less background I needed to explain.</em></p>", "sections": [{"title": "So if none of those are what \"I\"/Identity is based on, what is?", "anchor": "So_if_none_of_those_are_what__I__Identity_is_based_on__what_is_", "level": 1}, {"title": "Why do we have a sense of identity?", "anchor": "Why_do_we_have_a_sense_of_identity_", "level": 1}, {"title": "So what is it?", "anchor": "So_what_is_it_", "level": 1}, {"title": "Your \"I\"/identity is a concept (in the conceptspace/thingspace sense), centred on current you, with configurations of matter being considered more central to the \"I\" cluster the more similar they are to current you in the ways which current you values.", "anchor": "Your__I__identity_is_a_concept__in_the_conceptspace_thingspace_sense___centred_on_current_you__with_configurations_of_matter_being_considered_more_central_to_the__I__cluster_the_more_similar_they_are_to_current_you_in_the_ways_which_current_you_values_", "level": 2}, {"title": "What consequences does this theory have for common issues with identity?", "anchor": "What_consequences_does_this_theory_have_for_common_issues_with_identity_", "level": 1}, {"title": "Why does this matter?", "anchor": "Why_does_this_matter_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["924arDrTu3QRHFA5r", "y7jZ9BLEeuNTzgAE5", "RLScTpwc5W2gGGrL9", "aB3ZyksATcid59S9G", "WBw8dDkAWohFjWQSk", "82eMd5KLiJ5Z6rTrr", "7XWGJGmWXNmTd2oAP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-28T14:57:04.112Z", "modifiedAt": null, "url": null, "title": "Timelessness as a Conservative Extension of Causal Decision Theory", "slug": "timelessness-as-a-conservative-extension-of-causal-decision", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zKgP7WCmZNSYRk83w/timelessness-as-a-conservative-extension-of-causal-decision", "pageUrlRelative": "/posts/zKgP7WCmZNSYRk83w/timelessness-as-a-conservative-extension-of-causal-decision", "linkUrl": "https://www.lesswrong.com/posts/zKgP7WCmZNSYRk83w/timelessness-as-a-conservative-extension-of-causal-decision", "postedAtFormatted": "Wednesday, May 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timelessness%20as%20a%20Conservative%20Extension%20of%20Causal%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimelessness%20as%20a%20Conservative%20Extension%20of%20Causal%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKgP7WCmZNSYRk83w%2Ftimelessness-as-a-conservative-extension-of-causal-decision%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timelessness%20as%20a%20Conservative%20Extension%20of%20Causal%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKgP7WCmZNSYRk83w%2Ftimelessness-as-a-conservative-extension-of-causal-decision", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKgP7WCmZNSYRk83w%2Ftimelessness-as-a-conservative-extension-of-causal-decision", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4129, "htmlBody": "<p id=\"docs-internal-guid-0e75954a-4350-115e-39c7-a9096181fba0\" style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Author's Note: Please let me know in the comments </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">exactly</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> what important background material I have missed, and <em>exactly</em> what I have misunderstood, and please try not to mind that everything here is written in the academic voice.</span></p>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Abstract: Timeless Decision Theory often seems like the correct way to handle many game-theoretical dilemmas, but has not quite been satisfactorily formalized and still handles certain problems the wrong way. &nbsp;We present an intuition that helps us extend Causal Decision Theory towards Timeless Decision Theory while adding rigor, and then formalize this intuition. &nbsp;Along the way, we describe how this intuition can guide both us and programmed agents in various Newcomblike games.</span></h2>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Introduction</span></h2>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">One day, a Time Lord called Omega drops out of the sky, walks up to me on the street, and places two boxes in front of me. &nbsp;One of these is opaque, the other is transparent and contains $1000. &nbsp;He tells me I can take </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">either</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the opaque box alone, or both boxes, but that </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">if and only if</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> he predicted using his Time Lord Science I would take just the opaque box, it contains $1,000,000. &nbsp;He then flies away back to the his home-world of Gallifrey. &nbsp;I know that whatever prediction he made was/will be correct, because after all he is a Time Lord.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The established, gold-standard algorithm of Causal Decision Theory</span><a style=\"text-decoration:none;\" href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">fails to win the maximum available sum of money</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> on this problem, just as it fails on a symmetrical one-shot Prisoner's Dilemma. &nbsp;In fact, as human beings, we can </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">say</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> that CDT fails miserably, because while a programmed agent goes \"inside the game\" and proceeds to earn a good deal less money than it could, we human observers are sitting outside, carefully drawing outcome tables that politely inform us of just how much money our programmed agents are leaving on the table. &nbsp;While purely philosophical controversies abound in the literature about the original Newcomb's Problem, it is generally obvious from our outcome tables in the Prisoners' Dilemma that \"purely rational\" CDT agents would very definitely benefit by cooperating, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">and</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> that</span><a style=\"text-decoration:none;\" href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0072427\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">actual human beings asked to play the game calculate outcomes as if forming coalitions</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> rather than as if maximizing personal utility -- thus cooperating and </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">winning</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Even in the philosophical debates, it is generally agreed that one-boxers in Newcomb's Problem </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">are</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">, in fact, obtaining more money.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">While some have attempted to define rationality as the outputs of specific decision algorithms,</span><a style=\"text-decoration:none;\" href=\"/lw/nv/replace_the_symbol_with_the_substance/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">we hold</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> with the school of thought that rationality means </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">minimizing</span><a style=\"text-decoration:none;\" href=\"http://en.wikipedia.org/wiki/Regret_%28decision_theory%29\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">regret</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">:</span><a style=\"text-decoration:none;\" href=\"http://teaching.ust.hk/%7Ebee/papers/misc/Regret%20Theory%20An%20Alternative%20Theory%20of%20Rational%20Choice%20Under%20Uncertainty.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">a rational agent should select its decision algorithms in order to win as much as it will know it could have won ex-post-facto</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Failing perfection, this optimum should be approximated as closely as possible.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Yudkowsky's</span><a style=\"text-decoration:none;\" href=\"http://intelligence.org/files/TDT.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Timeless Decision Theory</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> approaches this problem by noting that many so-called decisions are actually outcomes from concurrent or separated instantiations of a single algorithm, that Timeless Decision Theory itself is exactly such an algorithm, and that many decisions (that actually are </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">decisions</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> in the sense that the algorithm deciding them is a utility-maximizing decision-theory) are </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">acausally</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">, timelessly connected. &nbsp;Agents running TDT will decide not as if they are determining one mere assignment to one mere variable in a causal graph but as if they're determining the output of the computation they implement, and thus of </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">every</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> logical node in the entire graph derived from their computation. &nbsp;However,</span><a style=\"text-decoration:none;\" href=\"/intelligence.org/files/Comparison.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">it still has some kinks to work out</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Yudkowsky (2010) shows TDT succeeding in the original Newcomb&rsquo;s problem. Unfortunately, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">deciding exactly when and where to put the logical nodes, and what conditional probabilities to place on them, is not yet an algorithmic process</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">How would TDT look if instantiated in a more mature application? Given a very large and complex network, TDT would modify it in the following way: It would investigate each node, noting the ones that were results of instantiated calculations. Then it would collect these nodes into groups where every node in a group was the result of the same calculation. (Recall that we don&rsquo;t know what the result is, just that it comes from the same calculation.) For each of these groups, TDT would then add a logical node representing the result of the abstract calculation, and connect it as a parent to each node in the group. &nbsp;Priors over possible states of the logical nodes would have to come from some other reasoning process, presumably the one that produces causal networks in the first place. Critically, one of these logical nodes would be the result of TDT&rsquo;s own decision process in this situation. TDT would denote that as the decision node and use the resulting network to calculate the best action by equation 1.1.</span></p>\n</blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The bolding is added by the present authors, as it highlights the issue we intend to address here. &nbsp;Terms like \"timeless\" and \"acausal\" have probably caused more confusion around Timeless Decision Theory than any other aspect of what is actually an understandable and reasonable algorithm. &nbsp;I will begin by presenting a clearer human-level intuition behind the correct behavior in Newcomb's Problem and the Prisoner's Dilemma, and will then proceed to formalize that intuition in Coq and apply it to sketch a more rigorously algorithmic Timeless Decision Theory. &nbsp;The formalization of this new intuition avoids problems of infinite self-reference or infinite recursion in reasoning about the algorithms determining decisions of oneself or others.</span></p>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Timeless decisions are actually entangled with each-other</span></h2>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The kind of apparent retrocausality present in Newcomb's Problem makes no intuitive sense whatsoever. &nbsp;Not only our intuitions but all our knowledge of science tell us that (absent the dubious phenomenon of closed timelike curves) causal influences always and only flow from the past to the future, never the other way around. &nbsp;Nonetheless, in the case of Newcomb-like problems,</span><a style=\"text-decoration:none;\" href=\"http://philsci-archive.pitt.edu/5441/1/CCRSSE-final.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">it has been seriously argued that</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">the Newcomb problem cannot but be retrocausal, if there is genuine evidential dependence of the predictor&rsquo;s behaviour on the agent&rsquo;s choice, from the agent&rsquo;s point of view.</span></p>\n</blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We do not believe in retrocausality, at least not as an objective feature of the world. &nbsp;Any subjectively apparent retrocausality, we believe, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">must</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> be some sort of illusion that reduces to genuine, right-side-up causality. &nbsp;Timeless or acausal decision-making resolves the apparent retrocausality by noticing that different \"agents\" in Newcomblike problems are actually reproductions of the same algorithm, and that they can thus be logically correlated without any direct causal link.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We further prime our intuitions about Newcomb-like problems with the observation that CDT-wielding Newcomb players who</span><a style=\"text-decoration:none;\" href=\"http://fitelson.org/coherence/joyce_caie.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">bind themselves to a precommitment to one-box</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">before</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Omega predicts their actions </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">will</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> win the $1,000,000:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">At t = 0 you can take a pill that turns you into a &ldquo;one boxer&rdquo;. &nbsp;The pill will lead the mad scientist to predict (at t = &frac12;) that you will take one box, and so will cause you to receive &pound;1,000,000 but will also cause you to leave a free &pound;1,000 on the table at t = 1. &nbsp;CDT tells you to take the pill at t = 0: it is obviously the act, among those available at t = 0, that has the best overall causal consequences.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The \"paradox\", then, lies in how the CDT agent comes to believe that their choice is completely detached from which box contains how much money, when in fact Omega's prediction of their choice was accurate, and directly caused Omega to place money in boxes accordingly, all of this despite no retrocausality occurring. &nbsp;Everything makes perfect sense prior to Omega's prediction.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">What, then, goes wrong with CDT? &nbsp;CDT agents will attempt to cheat against Omega: to be predicted as a one-boxer and then actually take both boxes. &nbsp;If given a way to obtain more money by precommitting to one-boxing, they will do so, but will subsequently feel regret over having followed their precommitment and \"irrationally\" taken only one box when both contained money. &nbsp;They may even begin to complain about the presence or absence of free will, as if this could change the game and enable their strategy to actually work.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">When we cease such protestations and accept that CDT behaves irrationally, the real question becomes: which outcomes are genuinely possible in Newcomb's Problem, which outcomes are preferable, and why does CDT fail to locate these?</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Plainly if we believe that Omega has a negligible or even null error rate, then in fact only two outcomes are possible:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<ul>\n<li><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Our agent is predicted to take both boxes, and does so, receiving only $1000 since Omega has not filled the opaque box.</span></li>\n<li><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Our agent is predicted to take the opaque box, which Omega fills, and the agent </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">does</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> take the opaque box, receiving $1,000,000.</span></li>\n</ul>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Plainly, $1 million is a greater sum than $1000, and the former outcome state is thus preferable to the latter. &nbsp;We require an algorithm that can search out and select this outcome based on general principles, in </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">any</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Newcomblike game rather than based on special-case heuristics.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Whence, then, a causal explanation of what to do? &nbsp;The authors' intuition was sparked by a bit of reading about the famously \"spooky\" phenomenon of quantum entanglement, also sometimes theorized to involve retrocausality. &nbsp;Two particles interact and become entangled; from then on, their quantum states will remain correlated until measurement collapses the wave-function of one particle or the other. &nbsp;Neither party performing a measurement will ever be able to tell which measurement took place first in time, but both measurements will always yield correlated results. &nbsp;This occurs despite the fact that quantum theory is confirmed to have</span><a style=\"text-decoration:none;\" href=\"http://fy.chalmers.se/%7Edelsing/QI/Bell-RMP-66.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">no hidden variables</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">, and even when general relativity's light-speed limit on the transmission of information prevents the entangled particles from \"communicating\" any quantum information. &nbsp;A paradox is apparent and most people find it scientifically unaesthetic.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">In reality, there is no paradox at all. &nbsp;All that has happened is that the pair of particles are in quantum superposition </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">together</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">: their observables are mutually governed by a single joint probability distribution. &nbsp;The measured observable states do not go from \"randomized\" to \"correlated\" as the measurement is made. &nbsp;The measurement only \"samples\" a single classical outcome governing </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">both</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> particles from the joint probability distribution that is </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">actually there</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;The joint probability distribution was </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">actually caused</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> by the 100% local and slower-than-light interaction that entangled the two particles in the first place.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Likewise for Newcomb's Problem in decision theory. &nbsp;As the theorists of precommitment had intuited, the outcome is not actually </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">caused</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> when the CDT agent </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">believes</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> itself to be making a decision. &nbsp;Instead, the outcome was </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">caused</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> when Omega measured the agent and predicted its choice ahead of time: the state of the agent at this time causes </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">both</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Omega's prediction </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">and</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the agent's eventual action.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We thus develop an intuition that like a pair of particles, the two correlated decision processes behind Omega's prediction </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">and</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> behind the agent's \"real\" choice are in some sense </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">entangled</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">: correlated due to a causal interaction in their mutual past. &nbsp;All we then require to win at Newcomb's Problem is a rigorous conception of such entanglement and a way of handling it algorithmically to make regret-minimizing decisions when entangled.</span></p>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Formalized decision entanglement</span></h2>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Let us begin by assuming that an agent can be defined as a function from a set of Beliefs and a Decision to an Action. &nbsp;There will not be very much actual proof-code given here, and what is given was written in the</span><a style=\"text-decoration:none;\" href=\"http://coq.inria.fr/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Coq proof assistant</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;The proofs, short though they be, were thus mechanically checked before being given here; \"</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">do</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> try this at home, kids.\"</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Definition</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Agent (Beliefs Decision Action: Type) : </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Type</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> := Beliefs -&gt; Decision -&gt; Action.<br /><br /></span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We can then broaden and redefine our definition of decision entanglement as saying, essentially, \"Two agents are entangled when either one of them </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">would</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> do what the other is doing, were they to trade places and thus beliefs but face equivalent decisions.\" &nbsp;More simply, if a certain two agents are entangled over a certain two equivalent decisions, any differences in what decisions they actually make arise from differences in beliefs.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Inductive</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled {Beliefs Decision Action} (a1 a2: Agent Beliefs Decision Action) d1 d2 :=</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;| ent : (</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (b: Beliefs), a1 b d1 = a2 b d2) -&gt; d1 = d2 -&gt; entangled a1 a2 d1 d2.</span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">This kind of entanglement can then, quite quickly, be shown to be an equivalence relation, thus partitioning the set of all logical nodes in a causal graph into Yudkowsky's \"groups where every node in a group was the result of the same calculation\", with these groups being equivalence classes.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Theorem</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled_reflexive {B D A} : </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (a: Agent B D A) d,</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a a d d.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Proof</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;constructor.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros. reflexivity. reflexivity.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Qed</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Theorem</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled_symmetric {B D A}: </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (a1 a2: Agent B D A) d1 d2,</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a1 a2 d1 d2 -&gt;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a2 a1 d2 d1.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Proof</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;constructor;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;induction H;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">&nbsp;&nbsp;&nbsp; </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">intros; symmetry.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;apply e. apply e0.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Qed</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Theorem</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled_transitive {B D A}: </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (a1 a2 a3: Agent B D A) d1 d2 d3,</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a1 a2 d1 d2 -&gt;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a2 a3 d2 d3 -&gt;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a1 a3 d1 d3.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Proof</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros a1 a2 a3 d1 d2 d3 H12 H23.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;constructor;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">&nbsp;&nbsp;&nbsp; </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">induction H12; induction H23; subst.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros b. rewrite e. rewrite e1.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;reflexivity. reflexivity.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Qed</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Actually proving that this relation holds simply consists of proving that two agents given equivalent decisions will always decide upon the same action (similar to proving</span><a style=\"text-decoration:none;\" href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.190.3917\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">program equilibrium</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">) </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">no matter</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> what set of </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">arbitrary</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> beliefs is given them -- hence the usage of a second-order</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Proving this does </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">not</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> require actually </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">running</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the decision function of either agent. &nbsp;Instead, it requires demonstrating that the abstract-syntax trees of the two decision functions can be made to unify, up to the renaming of universally-quantified variables. &nbsp;This is what allows us to prove the entanglement relation's symmetry and transitivity: our assumptions give us rewritings known to hold over the universally-quantified agent functions and decisions, thus letting us employ unification as a proof tool without knowing what specific functions we might be handling.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Thanks to employing the unification of syntax trees rather than the actual running of algorithms, we can conservatively extend Causal Decision Theory with logical nodes and entanglement to adequately handle timeless decision-making, without any recourse to retrocausality nor to the potentially-infinitely loops of</span><a style=\"text-decoration:none;\" href=\"http://ofb.net/%7Eegnor/iocaine.html\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Sicilian Reasoning</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;(Potential applications of timeless decision-making to win at Ro Sham Bo remain an open matter for the imagination.)</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Decision-theoretically, since our relation doesn't have to know anything </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">about</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the given functions other than (</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (b: Beliefs), a1 b d = a2 b d), we can test whether our relationship holds over any two logical/algorithm nodes in an arbitrary causal graph, since all such nodes can be written as functions from their causal inputs to their logical output. &nbsp;We thus do not need a particular conception of what constitutes an \"agent\" in order to make decisions rigorously: we only need to know what decision we are making, and where in a given causal graph we are making it. &nbsp;From there, we can use simple (though inefficient) pairwise testing to find the equivalence class of all logical nodes in the causal graph equivalent to our decision node, and then select a utility-maximizing output for each of those nodes using the logic of ordinary Causal Decision Theory.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The slogan of a Causal Decision Theory with Entanglement (CDT+E) can then be summed up as, \"select the decision which maximizes utility for the equivalence class of nodes to which I belong, with all of us acting and exerting our causal effects in concert, across space and time (but subject to our respective belief structures).\"</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The performance of CDT with entanglement on common problems</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">While we have not yet actually programmed a software agent with a CDT+E decision algorithm over Bayesian causal graphs (any readers who can point us to a corpus of preexisting source code for building, testing, and reasoning about decision-theory algorithms will be much appreciated, as we can then replace this wordy section with a formal evaluation), we can provide informal but still somewhat rigorous explanations of what it should do on several popular problems and why.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">First, the simplest case: when a CDT+E agent is placed into Newcomb's Problem, provided that the causal graph expresses the \"agenty-ness\" of whatever code Omega runs to predict our agent's actions, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">both</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> versions of the agent (the \"simulated\" and the \"real\") will look at the causal graph they are given, detect their entanglement with each-other via pairwise checking and proof-searching (which may take large amounts of computational power), and subsequently restrict their decision-making to choose the best outcome over worlds where they both make the same decision. &nbsp;This will lead the CDT+E agent to take only the opaque box (one-boxing) and win $1,000,000. &nbsp;This is the same behavior for the same reasons as is obtained with Timeless Decision Theory, but with less human intervention in the reasoning process.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Provided that the CDT+E agent maintains some model of past events in its causal network, the Parfit&rsquo;s Hitchhiker Problem trivially falls to the same reasoning as found in the original Newcomb&rsquo;s Problem.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Furthermore, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">two</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> CDT+E agents placed into the one-shot Prisoners' Dilemma and given knowledge of each-other's algorithms as embodied logical nodes in the two causal graphs will notice that they are entangled, choose the most preferable action over worlds in which both agents choose identically, and thus choose to cooperate. &nbsp;Should a CDT+E agent playing the one-shot Prisoner's Dilemma against an </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">arbitrary</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> agent with potentially non-identical code fail to prove entanglement with its opponent (fail to prove that its opponent's decisions mirror its own, up to differences in beliefs), it will refuse to trust its opponent and defect. &nbsp;A more optimal agent for the Prisoners' Dilemma would in fact demand from itself a proof that either it is or is not entangled with its opponent, and would be able to reason specifically about worlds in which the decisions made by two nodes </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">cannot</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> be the same. &nbsp;Doing so requires the Principle of the Excluded Middle, an axiom not normally used in the constructive logic of automated theorem-proving systems.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Lastly, different versions of CDT+E yield interestingly different results in the</span><a style=\"text-decoration:none;\" href=\"/lw/3l/counterfactual_mugging/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Counterfactual Mugging Problem</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Let us assume that the causal graph given to the agent contains three logical nodes: the actual agent making its choice to pay Omega $100, Omega's prediction of what the agent will do in this case, and Omega's imagination of the agent receiving $1,000 had the coin come up the other way. &nbsp;The version of the entanglement relation here quantifies over decisions themselves at the first-order level, and thus the two versions of the agent who are dealing with the prospect of giving Omega $100 will become entangled. &nbsp;Despite being entangled, they will see no situation of any benefit to themselves, and will refuse to pay Omega the money. &nbsp;However, consider the stricter definition of entanglement given below:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Inductive</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> strongly_entangled {Beliefs Decision Action} (a1 a2: Agent Beliefs Decision Action) :=</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;| ent : (</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (b: Beliefs) (d: Decision), a1 b d = a2 b d) -&gt; entangled a1 a2.</span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">This definition says that two agents are </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">strongly</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled when they yield the same decisions for every possible pair of beliefs and decision problem that can be given to them. &nbsp;This continues to match our original intuition regarding decision entanglement: that we are dealing with the same algorithm (agent), with the same values, being instantiated at multiple locations in time and space. &nbsp;It is somewhat stronger than the reasoning behind Timeless Decision Theory: it can recognize two instantiations of the same agent that face two </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">different</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> decisions, and enable them to reason that they are entangled with each-other.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Under this stronger version of the entanglement relation (whose proofs for being an equivalence relation are somewhat simpler, by the way), a CDT+E agent given the Counterfactual Mugging will recognize itself as entangled not only with the predicted factual version of itself that might give Omega $100, but </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">also</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> with the predicted </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">counter</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">factual version of itself that receives $1000 on the alternate coin flip. &nbsp;Each instance of the agent then independently computes the same appropriate tuple of output actions to maximize profit across the entire equivalence class (namely: predicted-factual gives $100, real-factual gives $100, predicted-counterfactual receives $1000).</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Switching entirely to the stronger version of entanglement would cause a CDT+E agent to lose certain games requiring cooperation with other agents that are even </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">trivially</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> different (for instance, if one agent likes chocolate and the other hates it, they are not strongly entangled). &nbsp;These games remain winnable with the weaker, original form of entanglement.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Future research</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Future research could represent the </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">probabilistic</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> possibility of entanglement within a causal graph by writing down multiple parallel logical/algorithm nodes as children of the same parent, each of which exists and acts with a probability conditional on the outcome of the parent node. &nbsp;A proof engine extended with</span><a style=\"text-decoration:none;\" href=\"http://www.hutter1.net/publ/sproblogic.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">probabilities over logical sentences</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (which, to the authors' knowledge, is not yet accomplished for second-order constructive logics of the kind used here) could also begin to assign probabilities to entanglement between logical/algorithm nodes. &nbsp;These probabilistic beliefs can then integrate into the action-selection algorithm of Causal Decision Theory just like any other probabilistic beliefs; the case of </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">pure</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> logic and </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">pure</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> proof from axioms merely constitutes assigning a</span><a style=\"text-decoration:none;\" href=\"/lw/mp/0_and_1_are_not_probabilities/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">degenerate probability of 1.0</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> to some belief.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Previous researchers have noted that decision-making over probabilistic acausal entanglement with other agents can be used to represent the notion of \"universalizability\" from Kantian deontological ethics. &nbsp;We note that entanglements with decision nodes in the past and future of a single given agent actually lead to behavior not unlike a \"virtue ethics\" (that is, the agent will start trying to enforce desirable properties up and down its own life history). &nbsp;When we begin to employ probabilities on entanglement, the Kantian and virtue-ethical strategies will become more or less decision-theoretically dominant based on the confidence with which CDT+E agents believe they are entangled with other agents or with their past and future selves.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Acausal trade/cooperation with agents other than the given CDT+E agent itself can also be considered, at least under the weaker definition of entanglement. &nbsp;In such cases, seemingly undesirable behaviors such as subjection to acausal versions of</span><a style=\"text-decoration:none;\" href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Pascal's Mugging</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> could appear. &nbsp;However, entanglements (whether Boolean, constructive, or probabilistically believed-in) occur between logical/decision nodes in the causal graph, which are linked by edges denoting </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">conditional probabilities</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Each CDT+E agent will thus weight the other in accordance with their beliefs about the probability mass of causal link from one to the other, making acausal Muggings have the same impact on decision-making as normal ones.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The discovery that games can have different outcomes under different versions of entanglement leads us to believe that our current concept of entanglement between agents and decisions is incomplete. &nbsp;We believe it is possible to build a form of entanglement that will pay Omega in the Counterfactual Mugging </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">without</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> trivially losing at the Prisoners&rsquo; Dilemma (as strong entanglement can), but our current attempts to do so sacrifice the transitivity of entanglement. &nbsp;We do not yet know if there are any game-theoretic losses inherent in that sacrifice. &nbsp;Still, we hope that further development of the entanglement concept can lead to a decision theory that will more fully reflect the \"timeless\" decision-making intuition of retrospectively detecting rational precommitments and acting according to them in the present.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br /></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">CDT+E opens up room for a fully formal and algorithmic treatment of the \"timeless\" decision-making processes proposed by Yudkowsky, including acausal \"communication\" (regarding symmetry or nonsymmetry) and acausal trade in general. &nbsp;However, like the original Timeless Decision Theory, it still does not actually have an algorithmic process for placing the logical/decision nodes into the causal graph -- only for dividing the set of all such nodes into equivalence classes based on decision entanglement. &nbsp;Were such an algorithmic process to be found, it could be used by an agent to locate </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">itself</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> within its model of the world via the stronger definition of entanglement. &nbsp;This could potentially reduce the problem of</span><a style=\"text-decoration:none;\" href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">naturalizing induction</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> to the subproblems of building a causal model that contains logical or algorithmic nodes, locating the node in the present model whose decisions are strongly entangled with those of the agent, and then proceeding to engage in \"virtue ethical\" planning for near-future probabilistically strongly-entangled versions of the agent's logical node up to the agent's planning horizon.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Acknowledgements</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The authors would like to thank Joshua and Benjamin Fox for their enlightening lectures on Updateless Decision Theory, and to additionally thank Benjamin Fox in specific for his abundant knowledge, deep intuition and clear guidance regarding acausal decision-making methods that actually win.&nbsp; Both Benjamin Fox and David Steinberg have our thanks for initial reviewing and help clarifying the text.<br /></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb28e": 2, "5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zKgP7WCmZNSYRk83w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 1.7538582808021168e-06, "legacy": true, "legacyId": "26245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p id=\"docs-internal-guid-0e75954a-4350-115e-39c7-a9096181fba0\" style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Author's Note: Please let me know in the comments </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">exactly</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> what important background material I have missed, and <em>exactly</em> what I have misunderstood, and please try not to mind that everything here is written in the academic voice.</span></p>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\" id=\"Abstract__Timeless_Decision_Theory_often_seems_like_the_correct_way_to_handle_many_game_theoretical_dilemmas__but_has_not_quite_been_satisfactorily_formalized_and_still_handles_certain_problems_the_wrong_way___We_present_an_intuition_that_helps_us_extend_Causal_Decision_Theory_towards_Timeless_Decision_Theory_while_adding_rigor__and_then_formalize_this_intuition___Along_the_way__we_describe_how_this_intuition_can_guide_both_us_and_programmed_agents_in_various_Newcomblike_games_\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Abstract: Timeless Decision Theory often seems like the correct way to handle many game-theoretical dilemmas, but has not quite been satisfactorily formalized and still handles certain problems the wrong way. &nbsp;We present an intuition that helps us extend Causal Decision Theory towards Timeless Decision Theory while adding rigor, and then formalize this intuition. &nbsp;Along the way, we describe how this intuition can guide both us and programmed agents in various Newcomblike games.</span></h2>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\" id=\"Introduction\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Introduction</span></h2>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">One day, a Time Lord called Omega drops out of the sky, walks up to me on the street, and places two boxes in front of me. &nbsp;One of these is opaque, the other is transparent and contains $1000. &nbsp;He tells me I can take </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">either</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the opaque box alone, or both boxes, but that </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">if and only if</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> he predicted using his Time Lord Science I would take just the opaque box, it contains $1,000,000. &nbsp;He then flies away back to the his home-world of Gallifrey. &nbsp;I know that whatever prediction he made was/will be correct, because after all he is a Time Lord.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The established, gold-standard algorithm of Causal Decision Theory</span><a style=\"text-decoration:none;\" href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">fails to win the maximum available sum of money</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> on this problem, just as it fails on a symmetrical one-shot Prisoner's Dilemma. &nbsp;In fact, as human beings, we can </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">say</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> that CDT fails miserably, because while a programmed agent goes \"inside the game\" and proceeds to earn a good deal less money than it could, we human observers are sitting outside, carefully drawing outcome tables that politely inform us of just how much money our programmed agents are leaving on the table. &nbsp;While purely philosophical controversies abound in the literature about the original Newcomb's Problem, it is generally obvious from our outcome tables in the Prisoners' Dilemma that \"purely rational\" CDT agents would very definitely benefit by cooperating, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">and</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> that</span><a style=\"text-decoration:none;\" href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0072427\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">actual human beings asked to play the game calculate outcomes as if forming coalitions</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> rather than as if maximizing personal utility -- thus cooperating and </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">winning</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Even in the philosophical debates, it is generally agreed that one-boxers in Newcomb's Problem </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">are</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">, in fact, obtaining more money.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">While some have attempted to define rationality as the outputs of specific decision algorithms,</span><a style=\"text-decoration:none;\" href=\"/lw/nv/replace_the_symbol_with_the_substance/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">we hold</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> with the school of thought that rationality means </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">minimizing</span><a style=\"text-decoration:none;\" href=\"http://en.wikipedia.org/wiki/Regret_%28decision_theory%29\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">regret</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">:</span><a style=\"text-decoration:none;\" href=\"http://teaching.ust.hk/%7Ebee/papers/misc/Regret%20Theory%20An%20Alternative%20Theory%20of%20Rational%20Choice%20Under%20Uncertainty.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">a rational agent should select its decision algorithms in order to win as much as it will know it could have won ex-post-facto</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Failing perfection, this optimum should be approximated as closely as possible.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Yudkowsky's</span><a style=\"text-decoration:none;\" href=\"http://intelligence.org/files/TDT.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Timeless Decision Theory</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> approaches this problem by noting that many so-called decisions are actually outcomes from concurrent or separated instantiations of a single algorithm, that Timeless Decision Theory itself is exactly such an algorithm, and that many decisions (that actually are </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">decisions</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> in the sense that the algorithm deciding them is a utility-maximizing decision-theory) are </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">acausally</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">, timelessly connected. &nbsp;Agents running TDT will decide not as if they are determining one mere assignment to one mere variable in a causal graph but as if they're determining the output of the computation they implement, and thus of </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">every</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> logical node in the entire graph derived from their computation. &nbsp;However,</span><a style=\"text-decoration:none;\" href=\"/intelligence.org/files/Comparison.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">it still has some kinks to work out</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Yudkowsky (2010) shows TDT succeeding in the original Newcomb\u2019s problem. Unfortunately, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">deciding exactly when and where to put the logical nodes, and what conditional probabilities to place on them, is not yet an algorithmic process</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">How would TDT look if instantiated in a more mature application? Given a very large and complex network, TDT would modify it in the following way: It would investigate each node, noting the ones that were results of instantiated calculations. Then it would collect these nodes into groups where every node in a group was the result of the same calculation. (Recall that we don\u2019t know what the result is, just that it comes from the same calculation.) For each of these groups, TDT would then add a logical node representing the result of the abstract calculation, and connect it as a parent to each node in the group. &nbsp;Priors over possible states of the logical nodes would have to come from some other reasoning process, presumably the one that produces causal networks in the first place. Critically, one of these logical nodes would be the result of TDT\u2019s own decision process in this situation. TDT would denote that as the decision node and use the resulting network to calculate the best action by equation 1.1.</span></p>\n</blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The bolding is added by the present authors, as it highlights the issue we intend to address here. &nbsp;Terms like \"timeless\" and \"acausal\" have probably caused more confusion around Timeless Decision Theory than any other aspect of what is actually an understandable and reasonable algorithm. &nbsp;I will begin by presenting a clearer human-level intuition behind the correct behavior in Newcomb's Problem and the Prisoner's Dilemma, and will then proceed to formalize that intuition in Coq and apply it to sketch a more rigorously algorithmic Timeless Decision Theory. &nbsp;The formalization of this new intuition avoids problems of infinite self-reference or infinite recursion in reasoning about the algorithms determining decisions of oneself or others.</span></p>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\" id=\"Timeless_decisions_are_actually_entangled_with_each_other\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Timeless decisions are actually entangled with each-other</span></h2>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The kind of apparent retrocausality present in Newcomb's Problem makes no intuitive sense whatsoever. &nbsp;Not only our intuitions but all our knowledge of science tell us that (absent the dubious phenomenon of closed timelike curves) causal influences always and only flow from the past to the future, never the other way around. &nbsp;Nonetheless, in the case of Newcomb-like problems,</span><a style=\"text-decoration:none;\" href=\"http://philsci-archive.pitt.edu/5441/1/CCRSSE-final.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">it has been seriously argued that</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">the Newcomb problem cannot but be retrocausal, if there is genuine evidential dependence of the predictor\u2019s behaviour on the agent\u2019s choice, from the agent\u2019s point of view.</span></p>\n</blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We do not believe in retrocausality, at least not as an objective feature of the world. &nbsp;Any subjectively apparent retrocausality, we believe, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">must</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> be some sort of illusion that reduces to genuine, right-side-up causality. &nbsp;Timeless or acausal decision-making resolves the apparent retrocausality by noticing that different \"agents\" in Newcomblike problems are actually reproductions of the same algorithm, and that they can thus be logically correlated without any direct causal link.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We further prime our intuitions about Newcomb-like problems with the observation that CDT-wielding Newcomb players who</span><a style=\"text-decoration:none;\" href=\"http://fitelson.org/coherence/joyce_caie.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">bind themselves to a precommitment to one-box</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">before</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Omega predicts their actions </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">will</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> win the $1,000,000:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">At t = 0 you can take a pill that turns you into a \u201cone boxer\u201d. &nbsp;The pill will lead the mad scientist to predict (at t = \u00bd) that you will take one box, and so will cause you to receive \u00a31,000,000 but will also cause you to leave a free \u00a31,000 on the table at t = 1. &nbsp;CDT tells you to take the pill at t = 0: it is obviously the act, among those available at t = 0, that has the best overall causal consequences.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The \"paradox\", then, lies in how the CDT agent comes to believe that their choice is completely detached from which box contains how much money, when in fact Omega's prediction of their choice was accurate, and directly caused Omega to place money in boxes accordingly, all of this despite no retrocausality occurring. &nbsp;Everything makes perfect sense prior to Omega's prediction.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">What, then, goes wrong with CDT? &nbsp;CDT agents will attempt to cheat against Omega: to be predicted as a one-boxer and then actually take both boxes. &nbsp;If given a way to obtain more money by precommitting to one-boxing, they will do so, but will subsequently feel regret over having followed their precommitment and \"irrationally\" taken only one box when both contained money. &nbsp;They may even begin to complain about the presence or absence of free will, as if this could change the game and enable their strategy to actually work.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">When we cease such protestations and accept that CDT behaves irrationally, the real question becomes: which outcomes are genuinely possible in Newcomb's Problem, which outcomes are preferable, and why does CDT fail to locate these?</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Plainly if we believe that Omega has a negligible or even null error rate, then in fact only two outcomes are possible:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<ul>\n<li><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Our agent is predicted to take both boxes, and does so, receiving only $1000 since Omega has not filled the opaque box.</span></li>\n<li><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Our agent is predicted to take the opaque box, which Omega fills, and the agent </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">does</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> take the opaque box, receiving $1,000,000.</span></li>\n</ul>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Plainly, $1 million is a greater sum than $1000, and the former outcome state is thus preferable to the latter. &nbsp;We require an algorithm that can search out and select this outcome based on general principles, in </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">any</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Newcomblike game rather than based on special-case heuristics.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Whence, then, a causal explanation of what to do? &nbsp;The authors' intuition was sparked by a bit of reading about the famously \"spooky\" phenomenon of quantum entanglement, also sometimes theorized to involve retrocausality. &nbsp;Two particles interact and become entangled; from then on, their quantum states will remain correlated until measurement collapses the wave-function of one particle or the other. &nbsp;Neither party performing a measurement will ever be able to tell which measurement took place first in time, but both measurements will always yield correlated results. &nbsp;This occurs despite the fact that quantum theory is confirmed to have</span><a style=\"text-decoration:none;\" href=\"http://fy.chalmers.se/%7Edelsing/QI/Bell-RMP-66.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">no hidden variables</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">, and even when general relativity's light-speed limit on the transmission of information prevents the entangled particles from \"communicating\" any quantum information. &nbsp;A paradox is apparent and most people find it scientifically unaesthetic.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">In reality, there is no paradox at all. &nbsp;All that has happened is that the pair of particles are in quantum superposition </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">together</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">: their observables are mutually governed by a single joint probability distribution. &nbsp;The measured observable states do not go from \"randomized\" to \"correlated\" as the measurement is made. &nbsp;The measurement only \"samples\" a single classical outcome governing </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">both</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> particles from the joint probability distribution that is </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">actually there</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;The joint probability distribution was </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">actually caused</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> by the 100% local and slower-than-light interaction that entangled the two particles in the first place.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Likewise for Newcomb's Problem in decision theory. &nbsp;As the theorists of precommitment had intuited, the outcome is not actually </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">caused</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> when the CDT agent </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">believes</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> itself to be making a decision. &nbsp;Instead, the outcome was </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">caused</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> when Omega measured the agent and predicted its choice ahead of time: the state of the agent at this time causes </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">both</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Omega's prediction </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">and</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the agent's eventual action.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We thus develop an intuition that like a pair of particles, the two correlated decision processes behind Omega's prediction </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">and</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> behind the agent's \"real\" choice are in some sense </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">entangled</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">: correlated due to a causal interaction in their mutual past. &nbsp;All we then require to win at Newcomb's Problem is a rigorous conception of such entanglement and a way of handling it algorithmically to make regret-minimizing decisions when entangled.</span></p>\n<h2 style=\"line-height:1.15;margin-top:18pt;margin-bottom:4pt;\" dir=\"ltr\" id=\"Formalized_decision_entanglement\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Formalized decision entanglement</span></h2>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Let us begin by assuming that an agent can be defined as a function from a set of Beliefs and a Decision to an Action. &nbsp;There will not be very much actual proof-code given here, and what is given was written in the</span><a style=\"text-decoration:none;\" href=\"http://coq.inria.fr/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Coq proof assistant</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;The proofs, short though they be, were thus mechanically checked before being given here; \"</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">do</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> try this at home, kids.\"</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Definition</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> Agent (Beliefs Decision Action: Type) : </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Type</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> := Beliefs -&gt; Decision -&gt; Action.<br><br></span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">We can then broaden and redefine our definition of decision entanglement as saying, essentially, \"Two agents are entangled when either one of them </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">would</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> do what the other is doing, were they to trade places and thus beliefs but face equivalent decisions.\" &nbsp;More simply, if a certain two agents are entangled over a certain two equivalent decisions, any differences in what decisions they actually make arise from differences in beliefs.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Inductive</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled {Beliefs Decision Action} (a1 a2: Agent Beliefs Decision Action) d1 d2 :=</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;| ent : (</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (b: Beliefs), a1 b d1 = a2 b d2) -&gt; d1 = d2 -&gt; entangled a1 a2 d1 d2.</span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">This kind of entanglement can then, quite quickly, be shown to be an equivalence relation, thus partitioning the set of all logical nodes in a causal graph into Yudkowsky's \"groups where every node in a group was the result of the same calculation\", with these groups being equivalence classes.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Theorem</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled_reflexive {B D A} : </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (a: Agent B D A) d,</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a a d d.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Proof</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;constructor.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros. reflexivity. reflexivity.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Qed</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Theorem</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled_symmetric {B D A}: </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (a1 a2: Agent B D A) d1 d2,</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a1 a2 d1 d2 -&gt;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a2 a1 d2 d1.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Proof</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;constructor;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;induction H;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">&nbsp;&nbsp;&nbsp; </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">intros; symmetry.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;apply e. apply e0.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Qed</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Theorem</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled_transitive {B D A}: </span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (a1 a2 a3: Agent B D A) d1 d2 d3,</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a1 a2 d1 d2 -&gt;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a2 a3 d2 d3 -&gt;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;entangled a1 a3 d1 d3.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Proof</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros a1 a2 a3 d1 d2 d3 H12 H23.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;constructor;</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">&nbsp;&nbsp;&nbsp; </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">induction H12; induction H23; subst.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;intros b. rewrite e. rewrite e1.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;reflexivity. reflexivity.</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Qed</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">.</span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Actually proving that this relation holds simply consists of proving that two agents given equivalent decisions will always decide upon the same action (similar to proving</span><a style=\"text-decoration:none;\" href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.190.3917\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">program equilibrium</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">) </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">no matter</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> what set of </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">arbitrary</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> beliefs is given them -- hence the usage of a second-order</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Proving this does </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">not</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> require actually </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">running</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the decision function of either agent. &nbsp;Instead, it requires demonstrating that the abstract-syntax trees of the two decision functions can be made to unify, up to the renaming of universally-quantified variables. &nbsp;This is what allows us to prove the entanglement relation's symmetry and transitivity: our assumptions give us rewritings known to hold over the universally-quantified agent functions and decisions, thus letting us employ unification as a proof tool without knowing what specific functions we might be handling.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Thanks to employing the unification of syntax trees rather than the actual running of algorithms, we can conservatively extend Causal Decision Theory with logical nodes and entanglement to adequately handle timeless decision-making, without any recourse to retrocausality nor to the potentially-infinitely loops of</span><a style=\"text-decoration:none;\" href=\"http://ofb.net/%7Eegnor/iocaine.html\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Sicilian Reasoning</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;(Potential applications of timeless decision-making to win at Ro Sham Bo remain an open matter for the imagination.)</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Decision-theoretically, since our relation doesn't have to know anything </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">about</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> the given functions other than (</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (b: Beliefs), a1 b d = a2 b d), we can test whether our relationship holds over any two logical/algorithm nodes in an arbitrary causal graph, since all such nodes can be written as functions from their causal inputs to their logical output. &nbsp;We thus do not need a particular conception of what constitutes an \"agent\" in order to make decisions rigorously: we only need to know what decision we are making, and where in a given causal graph we are making it. &nbsp;From there, we can use simple (though inefficient) pairwise testing to find the equivalence class of all logical nodes in the causal graph equivalent to our decision node, and then select a utility-maximizing output for each of those nodes using the logic of ordinary Causal Decision Theory.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The slogan of a Causal Decision Theory with Entanglement (CDT+E) can then be summed up as, \"select the decision which maximizes utility for the equivalence class of nodes to which I belong, with all of us acting and exerting our causal effects in concert, across space and time (but subject to our respective belief structures).\"</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The performance of CDT with entanglement on common problems</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">While we have not yet actually programmed a software agent with a CDT+E decision algorithm over Bayesian causal graphs (any readers who can point us to a corpus of preexisting source code for building, testing, and reasoning about decision-theory algorithms will be much appreciated, as we can then replace this wordy section with a formal evaluation), we can provide informal but still somewhat rigorous explanations of what it should do on several popular problems and why.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">First, the simplest case: when a CDT+E agent is placed into Newcomb's Problem, provided that the causal graph expresses the \"agenty-ness\" of whatever code Omega runs to predict our agent's actions, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">both</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> versions of the agent (the \"simulated\" and the \"real\") will look at the causal graph they are given, detect their entanglement with each-other via pairwise checking and proof-searching (which may take large amounts of computational power), and subsequently restrict their decision-making to choose the best outcome over worlds where they both make the same decision. &nbsp;This will lead the CDT+E agent to take only the opaque box (one-boxing) and win $1,000,000. &nbsp;This is the same behavior for the same reasons as is obtained with Timeless Decision Theory, but with less human intervention in the reasoning process.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Provided that the CDT+E agent maintains some model of past events in its causal network, the Parfit\u2019s Hitchhiker Problem trivially falls to the same reasoning as found in the original Newcomb\u2019s Problem.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Furthermore, </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">two</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> CDT+E agents placed into the one-shot Prisoners' Dilemma and given knowledge of each-other's algorithms as embodied logical nodes in the two causal graphs will notice that they are entangled, choose the most preferable action over worlds in which both agents choose identically, and thus choose to cooperate. &nbsp;Should a CDT+E agent playing the one-shot Prisoner's Dilemma against an </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">arbitrary</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> agent with potentially non-identical code fail to prove entanglement with its opponent (fail to prove that its opponent's decisions mirror its own, up to differences in beliefs), it will refuse to trust its opponent and defect. &nbsp;A more optimal agent for the Prisoners' Dilemma would in fact demand from itself a proof that either it is or is not entangled with its opponent, and would be able to reason specifically about worlds in which the decisions made by two nodes </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">cannot</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> be the same. &nbsp;Doing so requires the Principle of the Excluded Middle, an axiom not normally used in the constructive logic of automated theorem-proving systems.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Lastly, different versions of CDT+E yield interestingly different results in the</span><a style=\"text-decoration:none;\" href=\"/lw/3l/counterfactual_mugging/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Counterfactual Mugging Problem</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Let us assume that the causal graph given to the agent contains three logical nodes: the actual agent making its choice to pay Omega $100, Omega's prediction of what the agent will do in this case, and Omega's imagination of the agent receiving $1,000 had the coin come up the other way. &nbsp;The version of the entanglement relation here quantifies over decisions themselves at the first-order level, and thus the two versions of the agent who are dealing with the prospect of giving Omega $100 will become entangled. &nbsp;Despite being entangled, they will see no situation of any benefit to themselves, and will refuse to pay Omega the money. &nbsp;However, consider the stricter definition of entanglement given below:</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#ff9900;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Inductive</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> strongly_entangled {Beliefs Decision Action} (a1 a2: Agent Beliefs Decision Action) :=</span></pre>\n<pre style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> &nbsp;| ent : (</span><span style=\"font-size:15px;font-family:Arial;color:#0000ff;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">forall</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (b: Beliefs) (d: Decision), a1 b d = a2 b d) -&gt; entangled a1 a2.</span></pre>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;margin-left: 36pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">This definition says that two agents are </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">strongly</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> entangled when they yield the same decisions for every possible pair of beliefs and decision problem that can be given to them. &nbsp;This continues to match our original intuition regarding decision entanglement: that we are dealing with the same algorithm (agent), with the same values, being instantiated at multiple locations in time and space. &nbsp;It is somewhat stronger than the reasoning behind Timeless Decision Theory: it can recognize two instantiations of the same agent that face two </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">different</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> decisions, and enable them to reason that they are entangled with each-other.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Under this stronger version of the entanglement relation (whose proofs for being an equivalence relation are somewhat simpler, by the way), a CDT+E agent given the Counterfactual Mugging will recognize itself as entangled not only with the predicted factual version of itself that might give Omega $100, but </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">also</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> with the predicted </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">counter</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">factual version of itself that receives $1000 on the alternate coin flip. &nbsp;Each instance of the agent then independently computes the same appropriate tuple of output actions to maximize profit across the entire equivalence class (namely: predicted-factual gives $100, real-factual gives $100, predicted-counterfactual receives $1000).</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Switching entirely to the stronger version of entanglement would cause a CDT+E agent to lose certain games requiring cooperation with other agents that are even </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">trivially</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> different (for instance, if one agent likes chocolate and the other hates it, they are not strongly entangled). &nbsp;These games remain winnable with the weaker, original form of entanglement.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Future research</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Future research could represent the </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">probabilistic</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> possibility of entanglement within a causal graph by writing down multiple parallel logical/algorithm nodes as children of the same parent, each of which exists and acts with a probability conditional on the outcome of the parent node. &nbsp;A proof engine extended with</span><a style=\"text-decoration:none;\" href=\"http://www.hutter1.net/publ/sproblogic.pdf\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">probabilities over logical sentences</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> (which, to the authors' knowledge, is not yet accomplished for second-order constructive logics of the kind used here) could also begin to assign probabilities to entanglement between logical/algorithm nodes. &nbsp;These probabilistic beliefs can then integrate into the action-selection algorithm of Causal Decision Theory just like any other probabilistic beliefs; the case of </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">pure</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> logic and </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">pure</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> proof from axioms merely constitutes assigning a</span><a style=\"text-decoration:none;\" href=\"/lw/mp/0_and_1_are_not_probabilities/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">degenerate probability of 1.0</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> to some belief.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Previous researchers have noted that decision-making over probabilistic acausal entanglement with other agents can be used to represent the notion of \"universalizability\" from Kantian deontological ethics. &nbsp;We note that entanglements with decision nodes in the past and future of a single given agent actually lead to behavior not unlike a \"virtue ethics\" (that is, the agent will start trying to enforce desirable properties up and down its own life history). &nbsp;When we begin to employ probabilities on entanglement, the Kantian and virtue-ethical strategies will become more or less decision-theoretically dominant based on the confidence with which CDT+E agents believe they are entangled with other agents or with their past and future selves.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Acausal trade/cooperation with agents other than the given CDT+E agent itself can also be considered, at least under the weaker definition of entanglement. &nbsp;In such cases, seemingly undesirable behaviors such as subjection to acausal versions of</span><a style=\"text-decoration:none;\" href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Pascal's Mugging</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> could appear. &nbsp;However, entanglements (whether Boolean, constructive, or probabilistically believed-in) occur between logical/decision nodes in the causal graph, which are linked by edges denoting </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">conditional probabilities</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">. &nbsp;Each CDT+E agent will thus weight the other in accordance with their beliefs about the probability mass of causal link from one to the other, making acausal Muggings have the same impact on decision-making as normal ones.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The discovery that games can have different outcomes under different versions of entanglement leads us to believe that our current concept of entanglement between agents and decisions is incomplete. &nbsp;We believe it is possible to build a form of entanglement that will pay Omega in the Counterfactual Mugging </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">without</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> trivially losing at the Prisoners\u2019 Dilemma (as strong entanglement can), but our current attempts to do so sacrifice the transitivity of entanglement. &nbsp;We do not yet know if there are any game-theoretic losses inherent in that sacrifice. &nbsp;Still, we hope that further development of the entanglement concept can lead to a decision theory that will more fully reflect the \"timeless\" decision-making intuition of retrospectively detecting rational precommitments and acting according to them in the present.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"><br></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">CDT+E opens up room for a fully formal and algorithmic treatment of the \"timeless\" decision-making processes proposed by Yudkowsky, including acausal \"communication\" (regarding symmetry or nonsymmetry) and acausal trade in general. &nbsp;However, like the original Timeless Decision Theory, it still does not actually have an algorithmic process for placing the logical/decision nodes into the causal graph -- only for dividing the set of all such nodes into equivalence classes based on decision entanglement. &nbsp;Were such an algorithmic process to be found, it could be used by an agent to locate </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;\">itself</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> within its model of the world via the stronger definition of entanglement. &nbsp;This could potentially reduce the problem of</span><a style=\"text-decoration:none;\" href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">naturalizing induction</span></a><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\"> to the subproblems of building a causal model that contains logical or algorithmic nodes, locating the node in the present model whose decisions are strongly entangled with those of the agent, and then proceeding to engage in \"virtue ethical\" planning for near-future probabilistically strongly-entangled versions of the agent's logical node up to the agent's planning horizon.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:23px;font-family:Arial;color:#000000;background-color:transparent;font-weight:bold;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Acknowledgements</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">The authors would like to thank Joshua and Benjamin Fox for their enlightening lectures on Updateless Decision Theory, and to additionally thank Benjamin Fox in specific for his abundant knowledge, deep intuition and clear guidance regarding acausal decision-making methods that actually win.&nbsp; Both Benjamin Fox and David Steinberg have our thanks for initial reviewing and help clarifying the text.<br></span></p>\n<p>&nbsp;</p>", "sections": [{"title": "Abstract: Timeless Decision Theory often seems like the correct way to handle many game-theoretical dilemmas, but has not quite been satisfactorily formalized and still handles certain problems the wrong way. \u00a0We present an intuition that helps us extend Causal Decision Theory towards Timeless Decision Theory while adding rigor, and then formalize this intuition. \u00a0Along the way, we describe how this intuition can guide both us and programmed agents in various Newcomblike games.", "anchor": "Abstract__Timeless_Decision_Theory_often_seems_like_the_correct_way_to_handle_many_game_theoretical_dilemmas__but_has_not_quite_been_satisfactorily_formalized_and_still_handles_certain_problems_the_wrong_way___We_present_an_intuition_that_helps_us_extend_Causal_Decision_Theory_towards_Timeless_Decision_Theory_while_adding_rigor__and_then_formalize_this_intuition___Along_the_way__we_describe_how_this_intuition_can_guide_both_us_and_programmed_agents_in_various_Newcomblike_games_", "level": 1}, {"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Timeless decisions are actually entangled with each-other", "anchor": "Timeless_decisions_are_actually_entangled_with_each_other", "level": 1}, {"title": "Formalized decision entanglement", "anchor": "Formalized_decision_entanglement", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "65 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "GKfPL6LQFgB49FEnv", "mg6jDEuQEjBGtibX7", "QGkYCwyC7wTDyt3yT", "a5JAiTdytou3Jg749", "3WuAjWMtxQwTxr2Qn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-28T16:42:44.404Z", "modifiedAt": null, "url": null, "title": "Berkeley meetup May 28 outdoors in Ohlone Park", "slug": "berkeley-meetup-may-28-outdoors-in-ohlone-park", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:31.956Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pcm", "createdAt": "2017-06-17T00:51:23.973Z", "isAdmin": false, "displayName": "pcm"}, "userId": "9bxscuK69SnmpBqSA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AM6CkJefJg4vYRNHj/berkeley-meetup-may-28-outdoors-in-ohlone-park", "pageUrlRelative": "/posts/AM6CkJefJg4vYRNHj/berkeley-meetup-may-28-outdoors-in-ohlone-park", "linkUrl": "https://www.lesswrong.com/posts/AM6CkJefJg4vYRNHj/berkeley-meetup-may-28-outdoors-in-ohlone-park", "postedAtFormatted": "Wednesday, May 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Berkeley%20meetup%20May%2028%20outdoors%20in%20Ohlone%20Park&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABerkeley%20meetup%20May%2028%20outdoors%20in%20Ohlone%20Park%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAM6CkJefJg4vYRNHj%2Fberkeley-meetup-may-28-outdoors-in-ohlone-park%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Berkeley%20meetup%20May%2028%20outdoors%20in%20Ohlone%20Park%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAM6CkJefJg4vYRNHj%2Fberkeley-meetup-may-28-outdoors-in-ohlone-park", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAM6CkJefJg4vYRNHj%2Fberkeley-meetup-may-28-outdoors-in-ohlone-park", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p><span style=\"border-collapse: collapse; color: #222222; font-family: arial, sans-serif; font-size: 13px;\"> </span></p>\n<p>Today's Berkeley LW meetup will start in Ohlone park one block east of the North Berkeley BART. Please try to arrive by 7pm, and look for us near the rocks in the middle of the block east of Sacramento St between Delaware St and Hearst Ave. I'll try to be there by 6:30. &nbsp;</p>\n<p>We may take a walk up the Ohlone Greenway while the sun is shining and/or move to the CFAR office after sunset, depending on what people want. &nbsp;</p>\n<p>If you have trouble finding us call me at&nbsp;<a style=\"color: #1155cc;\" href=\"/tel:408%20315-8120\" target=\"_blank\">408 315-8120</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AM6CkJefJg4vYRNHj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 1.754008277137893e-06, "legacy": true, "legacyId": "26282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-28T17:58:44.680Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion", "slug": "meetup-boston-defense-against-the-dark-arts-the-ethics-and", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BaLF98S5THSiea4PM/meetup-boston-defense-against-the-dark-arts-the-ethics-and", "pageUrlRelative": "/posts/BaLF98S5THSiea4PM/meetup-boston-defense-against-the-dark-arts-the-ethics-and", "linkUrl": "https://www.lesswrong.com/posts/BaLF98S5THSiea4PM/meetup-boston-defense-against-the-dark-arts-the-ethics-and", "postedAtFormatted": "Wednesday, May 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Defense%20Against%20the%20Dark%20Arts%3A%20the%20Ethics%20and%20Psychology%20of%20Persuasion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Defense%20Against%20the%20Dark%20Arts%3A%20the%20Ethics%20and%20Psychology%20of%20Persuasion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBaLF98S5THSiea4PM%2Fmeetup-boston-defense-against-the-dark-arts-the-ethics-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Defense%20Against%20the%20Dark%20Arts%3A%20the%20Ethics%20and%20Psychology%20of%20Persuasion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBaLF98S5THSiea4PM%2Fmeetup-boston-defense-against-the-dark-arts-the-ethics-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBaLF98S5THSiea4PM%2Fmeetup-boston-defense-against-the-dark-arts-the-ethics-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10r'>Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St #1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How can you convince other people to agree with you? How can we make ourselves less likely to be swayed by the Dark Arts persuasion of others? And, of course, what are the ethical issues involved with all of this?</p>\n\n<p>If you're the sort of person who finds these questions interesting, feel free to join us tonight as Jesse Galef discusses the psychology research on persuasion and its ethical implications. Meetup starts at 7pm, talk starts at 7:30pm.</p>\n\n<p>Cambridge/Boston-area Less Wrong Wednesday meetups are once a month on the last Wednesday at 7pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square). All other meetups are on Sundays.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 7:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10r'>Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BaLF98S5THSiea4PM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.754116176600185e-06, "legacy": true, "legacyId": "26283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Defense_Against_the_Dark_Arts__the_Ethics_and_Psychology_of_Persuasion\">Discussion article for the meetup : <a href=\"/meetups/10r\">Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St #1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How can you convince other people to agree with you? How can we make ourselves less likely to be swayed by the Dark Arts persuasion of others? And, of course, what are the ethical issues involved with all of this?</p>\n\n<p>If you're the sort of person who finds these questions interesting, feel free to join us tonight as Jesse Galef discusses the psychology research on persuasion and its ethical implications. Meetup starts at 7pm, talk starts at 7:30pm.</p>\n\n<p>Cambridge/Boston-area Less Wrong Wednesday meetups are once a month on the last Wednesday at 7pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square). All other meetups are on Sundays.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 7:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Defense_Against_the_Dark_Arts__the_Ethics_and_Psychology_of_Persuasion1\">Discussion article for the meetup : <a href=\"/meetups/10r\">Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion", "anchor": "Discussion_article_for_the_meetup___Boston___Defense_Against_the_Dark_Arts__the_Ethics_and_Psychology_of_Persuasion", "level": 1}, {"title": "Discussion article for the meetup : Boston - Defense Against the Dark Arts: the Ethics and Psychology of Persuasion", "anchor": "Discussion_article_for_the_meetup___Boston___Defense_Against_the_Dark_Arts__the_Ethics_and_Psychology_of_Persuasion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-28T18:16:13.469Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW Discussion Meetup:  Cognitive Load Followup ", "slug": "meetup-durham-rtlw-discussion-meetup-cognitive-load-followup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7iXG9NqNwa3qtnRCT/meetup-durham-rtlw-discussion-meetup-cognitive-load-followup", "pageUrlRelative": "/posts/7iXG9NqNwa3qtnRCT/meetup-durham-rtlw-discussion-meetup-cognitive-load-followup", "linkUrl": "https://www.lesswrong.com/posts/7iXG9NqNwa3qtnRCT/meetup-durham-rtlw-discussion-meetup-cognitive-load-followup", "postedAtFormatted": "Wednesday, May 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20Discussion%20Meetup%3A%20%20Cognitive%20Load%20Followup%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20Discussion%20Meetup%3A%20%20Cognitive%20Load%20Followup%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7iXG9NqNwa3qtnRCT%2Fmeetup-durham-rtlw-discussion-meetup-cognitive-load-followup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20Discussion%20Meetup%3A%20%20Cognitive%20Load%20Followup%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7iXG9NqNwa3qtnRCT%2Fmeetup-durham-rtlw-discussion-meetup-cognitive-load-followup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7iXG9NqNwa3qtnRCT%2Fmeetup-durham-rtlw-discussion-meetup-cognitive-load-followup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10s'>Durham/RTLW Discussion Meetup:  Cognitive Load Followup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 W Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll start with followup on last meetup's discussion of cognitive load.  As time and inclination permit, we'll do another round of other-optimizing and/or some games.</p>\n\n<p>7:00 meet, gather drinks, general chat <br />\n7:30 discussion begins <br />\n9:00 adjourn to Fullsteam (probably)</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10s'>Durham/RTLW Discussion Meetup:  Cognitive Load Followup </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7iXG9NqNwa3qtnRCT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "26284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_Discussion_Meetup___Cognitive_Load_Followup_\">Discussion article for the meetup : <a href=\"/meetups/10s\">Durham/RTLW Discussion Meetup:  Cognitive Load Followup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 May 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 W Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll start with followup on last meetup's discussion of cognitive load.  As time and inclination permit, we'll do another round of other-optimizing and/or some games.</p>\n\n<p>7:00 meet, gather drinks, general chat <br>\n7:30 discussion begins <br>\n9:00 adjourn to Fullsteam (probably)</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_Discussion_Meetup___Cognitive_Load_Followup_1\">Discussion article for the meetup : <a href=\"/meetups/10s\">Durham/RTLW Discussion Meetup:  Cognitive Load Followup </a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW Discussion Meetup:  Cognitive Load Followup ", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_Discussion_Meetup___Cognitive_Load_Followup_", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW Discussion Meetup:  Cognitive Load Followup ", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_Discussion_Meetup___Cognitive_Load_Followup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-28T18:58:57.537Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Taking ideas seriously", "slug": "meetup-boston-taking-ideas-seriously", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ig5LAvHFJ2tL4stW8/meetup-boston-taking-ideas-seriously", "pageUrlRelative": "/posts/ig5LAvHFJ2tL4stW8/meetup-boston-taking-ideas-seriously", "linkUrl": "https://www.lesswrong.com/posts/ig5LAvHFJ2tL4stW8/meetup-boston-taking-ideas-seriously", "postedAtFormatted": "Wednesday, May 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Taking%20ideas%20seriously&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Taking%20ideas%20seriously%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig5LAvHFJ2tL4stW8%2Fmeetup-boston-taking-ideas-seriously%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Taking%20ideas%20seriously%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig5LAvHFJ2tL4stW8%2Fmeetup-boston-taking-ideas-seriously", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig5LAvHFJ2tL4stW8%2Fmeetup-boston-taking-ideas-seriously", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 247, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10t'>Boston - Taking ideas seriously</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel House, 98 Elm St #1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anders Huitfeldt will lead a discussion about whether taking ideas seriously is likely to increase the accuracy of mental maps, in the case of agents with human-level intelligence.  For Less Wrong regulars this will be basic review, and for non-regulars it may be a useful introduction. He will have some slides, but he hopes that this will be an interactive discussion rather than a presentation.</p>\n\n<p>It would be helpful to look over the following before the meetup:</p>\n\n<p>Reason as memetic immune disorder by Phil Goetz</p>\n\n<p>Epistemic learned helplessness by Scott Alexander</p>\n\n<p>Taking ideas seriously by Will Newsome</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10t'>Boston - Taking ideas seriously</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ig5LAvHFJ2tL4stW8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.754201667655822e-06, "legacy": true, "legacyId": "26285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Taking_ideas_seriously\">Discussion article for the meetup : <a href=\"/meetups/10t\">Boston - Taking ideas seriously</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel House, 98 Elm St #1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anders Huitfeldt will lead a discussion about whether taking ideas seriously is likely to increase the accuracy of mental maps, in the case of agents with human-level intelligence.  For Less Wrong regulars this will be basic review, and for non-regulars it may be a useful introduction. He will have some slides, but he hopes that this will be an interactive discussion rather than a presentation.</p>\n\n<p>It would be helpful to look over the following before the meetup:</p>\n\n<p>Reason as memetic immune disorder by Phil Goetz</p>\n\n<p>Epistemic learned helplessness by Scott Alexander</p>\n\n<p>Taking ideas seriously by Will Newsome</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Taking_ideas_seriously1\">Discussion article for the meetup : <a href=\"/meetups/10t\">Boston - Taking ideas seriously</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Taking ideas seriously", "anchor": "Discussion_article_for_the_meetup___Boston___Taking_ideas_seriously", "level": 1}, {"title": "Discussion article for the meetup : Boston - Taking ideas seriously", "anchor": "Discussion_article_for_the_meetup___Boston___Taking_ideas_seriously1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-29T00:45:51.000Z", "modifiedAt": "2021-02-08T15:17:37.455Z", "url": null, "title": "Don\u2019t Fear The Filter", "slug": "don-t-fear-the-filter", "viewCount": null, "lastCommentedAt": "2022-04-25T15:54:16.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mnpkM57R6ZbjnwrYw/don-t-fear-the-filter", "pageUrlRelative": "/posts/mnpkM57R6ZbjnwrYw/don-t-fear-the-filter", "linkUrl": "https://www.lesswrong.com/posts/mnpkM57R6ZbjnwrYw/don-t-fear-the-filter", "postedAtFormatted": "Thursday, May 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don%E2%80%99t%20Fear%20The%20Filter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon%E2%80%99t%20Fear%20The%20Filter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmnpkM57R6ZbjnwrYw%2Fdon-t-fear-the-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don%E2%80%99t%20Fear%20The%20Filter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmnpkM57R6ZbjnwrYw%2Fdon-t-fear-the-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmnpkM57R6ZbjnwrYw%2Fdon-t-fear-the-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1784, "htmlBody": "<p>There&#8217;s been <A HREF=\"http://waitbutwhy.com/2014/05/fermi-paradox.html\">a recent spate</A> of <A HREF=\"http://theconversation.com/habitable-exoplanets-are-bad-news-for-humanity-25838\">popular interest</A> in <A HREF=\"http://www.universetoday.com/111660/where-are-the-aliens-how-the-great-filter-could-affect-tech-advances-in-space/\">the Great Filter theory</A>, but I think it all misses an important point brought up in Robin Hanson&#8217;s <A HREF=\"http://hanson.gmu.edu/greatfilter.html\">original 1998 paper</A> on the subject.</p>\n<p>The Great Filter, remember, is the horror-genre-adaptation of Fermi&#8217;s Paradox. All of our calculations say that, in the infinite vastness of time and space, intelligent aliens should be very common. But we don&#8217;t see any of them. We haven&#8217;t seen their colossal astro-engineering projects in the night sky. We haven&#8217;t heard their messages through SETI. And most important, we haven&#8217;t been visited or colonized by them.</p>\n<p>This is very strange. Consider that if humankind makes it another thousand years, we&#8217;ll probably have started to colonize other star systems. Those star systems will colonize other star systems and so on until we start expanding at nearly the speed of light, colonizing literally everything in sight. After a hundred thousand years or so we&#8217;ll have settled a big chunk of the galaxy, assuming we haven&#8217;t killed ourselves first or encountered someone else already living there.</p>\n<p>But there should be alien civilizations that are a <i>billion</i> years old. Anything that could conceivably be colonized, <i>they</i> should have gotten to back when trilobytes still seemed like superadvanced mutants. But here we are, perfectly nice solar system, lots of any type of resources you could desire, and they&#8217;ve never visited. Why not?</p>\n<p>Well, the Great Filter. No knows <i>specifically</i> what the Great Filter is, but <i>generally</i> it&#8217;s &#8220;that thing that blocks planets from growing spacefaring civilizations&#8221;. The planet goes some of the way towards a spacefaring civilization, and then stops. The most important thing to remember about the Great Filter is that it is <i>very good</i> at what it does. If even one planet in a billion light-year radius had passed through the Great Filter, we would expect to see its inhabitants everywhere. Since we don&#8217;t, we know that whatever it is it&#8217;s <i>very</i> thorough.</p>\n<p>Various candidates have been proposed, including &#8220;it&#8217;s really hard for life to come into existence&#8221;, &#8220;it&#8217;s really hard for complex cells to form&#8221;, &#8220;it&#8217;s really hard for animals to evolve intelligent&#8221;, and &#8220;actually space is full of aliens but they are hiding their existence from us for some reason&#8221;.</p>\n<p>The articles I linked at the top, especially the first, will go through most of the possibilities. This essay isn&#8217;t about proposing new ones. It&#8217;s about saying why the old ones won&#8217;t work.</p>\n<p><b>The Great Filter is not garden-variety x-risk</b>. A lot of people have seized upon the Great Filter to say that we&#8217;re going to destroy ourselves <A HREF=\"http://asa9.org/2009/03/great-filter-discovered.html\">through global warming</A> or nuclear war or destroying the rainforests. This seems wrong to me. Even if human civilization does destroy itself due to global warming &#8211; which is a lot further than even very pessimistic environmentalists expect the problem to go &#8211; it seems clear we had a chance <i>not</i> to do that. A few politicians voting the other way, we could have passed the Kyoto Protocol. A <i>lot</i> of politicians voting the other way, and we could have come up with a really stable and long-lasting plan to put it off indefinitely. If the gas-powered car had never won out over electric vehicles back in the early 20th century, or nuclear-phobia hadn&#8217;t sunk the plan to move away from polluting coal plants, then the problem might never have come up, or at least been much less. And we&#8217;re pretty close to being able to colonize Mars right now; if our solar system had a slightly bigger, slightly closer version of Mars, then we could restart human civilization anew there once we destroyed the Earth and maybe go a <i>little</i> easy on the carbon dioxide the next time around.</p>\n<p>In other words, there&#8217;s no way global warming kills 999,999,999 in every billion civilizations. Maybe it kills 100,000,000. Maybe it kills 900,000,000. But <i>occasionally</i> one manages to make it to space before frying their home planet. That means it can&#8217;t be the Great Filter, or else we would have run into the aliens who passed their Kyoto Protocols.</p>\n<p>And the same is true of nuclear war or destroying the rainforests.</p>\n<p>Unfortunately, almost all the popular articles about the Great Filter miss this point and make their lead-in &#8220;DOES THIS SCIENTIFIC PHENOMENON PROVE HUMANITY IS DOOMED?&#8221; No. No it doesn&#8217;t.</p>\n<p><b>The Great Filter is not Unfriendly AI</b>. Unlike global warming, it may be that we never really had a chance against Unfriendly AI. Even if we do everything right and give MIRI more money than they could ever want and get all of our smartest geniuses working on the problem, maybe the mathematical problems involved are insurmountable. Maybe the most pessimistic of MIRI&#8217;s models is true, and AIs are very easy to accidentally bootstrap to unstoppable superintelligence and near-impossible to give a stable value system that makes them compatible with human life. So unlike global warming and nuclear war, this theory meshes well with the low probability of filter escape.</p>\n<p>But as <A HREF=\"http://lesswrong.com/lw/g1s/ufai_cannot_be_the_great_filter/\">this article points out</A>, Unfriendly AI would if anything be even more visible than normal aliens. The best-studied class of Unfriendly AIs are the ones whimsically called &#8220;paperclip maximizers&#8221; which try to convert the entire universe to a certain state (in the example, paperclips). These would be easily detectable as a sphere of optimized territory expanding at some appreciable fraction of the speed of light. Given that Hubble hasn&#8217;t spotted a Paperclip Nebula (or been consumed by one) it looks like no one has created any of this sort of AI either. And while other Unfriendly AIs might be less aggressive than this, it&#8217;s hard to imagine an Unfriendly AI that destroys its parent civilization, then sits very quietly doing nothing. It&#8217;s even harder to imagine that 999,999,999 out of a billion Unfriendly AIs end up this way.</p>\n<p><b>The Great Filter is not transcendence</b>. Lots of people more enthusiastically propose that the problem isn&#8217;t alien species killing themselves, it&#8217;s alien species transcending this mortal plane. Once they become sufficiently advanced, they stop being interested in expansion for expansion&#8217;s sake. Some of them hang out on their home planet, peacefully cultivating their alien gardens. Others upload themselves to computronium internets, living in virtual reality. Still others become beings of pure energy, doing whatever it is that beings of pure energy do. In any case, they don&#8217;t conquer the galaxy or build obvious visible structures.</p>\n<p>Which is all nice and well, except what about the Amish aliens? What about the ones who have weird religions telling them that it&#8217;s not right to upload their bodies, they have to live in the real world? What about the ones who have crusader religions telling them they have to conquer the galaxy to convert everyone else to their superior way of life? I&#8217;m not saying this has to be common. And I know there&#8217;s this argument that <i>advanced</i> species would be beyond this kind of thing. But man, it only takes one. I can&#8217;t believe that not even one in a billion alien civilizations would have some instinctual preference for galactic conquest for galactic conquest&#8217;s own sake. I mean, even if most humans upload themselves, there will be a couple who don&#8217;t and who want to go exploring. You&#8217;re trying to tell me this model applies to 999,999,999 out of one billion civilizations, and then the very first civilization we test it on, it fails?</p>\n<p><b>The Great Filter is not alien exterminators</b>. It sort of makes sense, from a human point of view. Maybe the first alien species to attain superintelligence was jealous, or just plain jerks, and decided to kill other species before they got the chance to catch up. Knowledgeable people like <A HREF=\"http://www.kurzweilai.net/meti-should-we-be-shouting-at-the-cosmos\">as Carl Sagan</A> and Stephen Hawking have condemned our reverse-SETI practice of sending messages into space to see who&#8217;s out there, because everyone out there may be terrible. On this view, the dominant alien civilization is the Great Filter, killing off everyone else while not leaving a visible footprint themselves.</p>\n<p>Although I get the precautionary principle, Sagan et al&#8217;s warnings against sending messages seem kind of silly to me. This isn&#8217;t a failure to recognize how strong the Great Filter has to be, this is a failure to recognize how powerful a civilization that gets through it can become.</p>\n<p>It doesn&#8217;t matter one way or the other if we broadcast we&#8217;re here. If there are alien superintelligences out there, <i>they know</i>. &#8220;Oh, my billion-year-old universe-spanning superintelligence wants to destroy fledgling civilizations, but we just can&#8217;t find them! If only they would send very powerful radio broadcasts into space so we could figure out where they are!&#8221; No. Just no. If there are alien superintelligences out there, they tagged Earth as potential troublemakers sometime in the Cambrian Era and have been watching us very closely ever since. They know what you had for breakfast this morning and they know what Jesus had for breakfast the morning of the Crucifixion. People worried about accidentally &#8220;revealing themselves&#8221; to an intergalactic supercivilization are like <A HREF=\"http://en.wikipedia.org/wiki/Sentinelese_people\">Sentinel Islanders</A> reluctant to send a message in a bottle lest modern civilization discover their existence &#8211; unaware that modern civilization has spy satellites orbiting the planet that can pick out whether or not they shaved that morning.</p>\n<p>What about alien exterminators who are okay with weak civilizations, but kill them when they show the first sign of becoming a threat (like inventing fusion power or leaving their home solar system)? Again, you are underestimating billion-year-old universe-spanning superintelligences. Don&#8217;t flatter yourself here. <i>You cannot threaten them</i>. </p>\n<p>What about alien exterminators who are okay with weak civilizations, but destroy strong civilizations not because they feel threatened, but just for aesthetic reasons? I can&#8217;t be certain that&#8217;s false, but it seems to me that if they have let us continue existing this long, even though we are made of matter that can be used for something else, that has to be a conscious decision made out of something like morality. And because they&#8217;re omnipotent, they have the ability to satisfy all of their (not logically contradictory) goals at once without worrying about tradeoffs. That makes me think that whatever moral impulse has driven them to allow us to survive will <i>probably</i> continue to allow us to survive even if we start annoying them for some reason. When you&#8217;re omnipotent, the option of stopping the annoyance without harming anyone is just as easy as stopping the annoyance by making everyone involved suddenly vanish.</p>\n<p>Three of these four options &#8211; x-risk, Unfriendly AI, and alien exterminators &#8211; are very very bad for humanity. I think worry about this badness has been a lot of what&#8217;s driven interest in the Great Filter. I also think these are some of the least likely possible explanations, which means we should be less afraid of the Great Filter than is generally believed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"25oxqHiadqM6Hf7Gn": 2, "sYm3HiWcfZvrGu3ui": 2, "Rz5jb3cYHTSRmqNnN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mnpkM57R6ZbjnwrYw", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 2.7e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "TKDT2Mt6dDMH8AsZW", "canonicalCollectionSlug": "codex", "canonicalBookId": "kcCvSNNZd8pfQvf9E", "canonicalNextPostSlug": "book-review-age-of-em", "canonicalPrevPostSlug": "where-the-falling-einstein-meets-the-rising-mouse", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BQ4KLnmB7tcAZLNfm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-05-29T00:45:51.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-29T05:33:09.382Z", "modifiedAt": null, "url": null, "title": "Meetup : Christchurch, NZ Meetup - Games & Discussion", "slug": "meetup-christchurch-nz-meetup-games-and-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "free_rip", "createdAt": "2010-04-04T09:56:57.893Z", "isAdmin": false, "displayName": "free_rip"}, "userId": "q8zJHtRt24KhQ6fSs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LjsPwZXPZajfaARHz/meetup-christchurch-nz-meetup-games-and-discussion", "pageUrlRelative": "/posts/LjsPwZXPZajfaARHz/meetup-christchurch-nz-meetup-games-and-discussion", "linkUrl": "https://www.lesswrong.com/posts/LjsPwZXPZajfaARHz/meetup-christchurch-nz-meetup-games-and-discussion", "postedAtFormatted": "Thursday, May 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Christchurch%2C%20NZ%20Meetup%20-%20Games%20%26%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Christchurch%2C%20NZ%20Meetup%20-%20Games%20%26%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjsPwZXPZajfaARHz%2Fmeetup-christchurch-nz-meetup-games-and-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Christchurch%2C%20NZ%20Meetup%20-%20Games%20%26%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjsPwZXPZajfaARHz%2Fmeetup-christchurch-nz-meetup-games-and-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjsPwZXPZajfaARHz%2Fmeetup-christchurch-nz-meetup-games-and-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 296, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10u'>Christchurch, NZ Meetup - Games &amp; Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2014 04:30:00PM (+1200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">James Hight, University of Canterbury, Christchurch, New Zealand</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The third Chch meetup is this Sunday at 4.30pm! We have a few regular attendees so if you live in Chch and would like to connect with the Less Wrong community here it's well worth the effort to come along.</p>\n\n<p>It will be held in James Hight library, discussion room 901, at the University of Canterbury. You do not need to be a student or in any way affiliated with UC to come along, the discussion rooms are open to everyone.</p>\n\n<p>The way to get there is to go through the front (and only) doors of the library, walk forwards until you see elevators to your right, and take one of those elevators to floor 9. Then walk around the edges of the floor until you see room 901. Don't take the stairs, they only lead up one floor! If you need clearer directions or anticipate getting lost, PM or post below and I can clear it up and give you a contact cell number.</p>\n\n<p>Potential activities include rationalist Cards Against Humanity, goal-setting, and if previous occasions are any indication a whole bunch of interesting discussions. It's very casual, so if you have anything you'd like to do or discuss post below or just show up with it :-)</p>\n\n<p>If you'd like to come along, but can't make this day or time, please post below so we know for future meetups. Also, if you have something on early and can come later, that's fine too - meetups typically last 3-4 hours and dropping in partway (or leaving partway) is absolutely fine.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10u'>Christchurch, NZ Meetup - Games &amp; Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LjsPwZXPZajfaARHz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7551025377885258e-06, "legacy": true, "legacyId": "26286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Christchurch__NZ_Meetup___Games___Discussion\">Discussion article for the meetup : <a href=\"/meetups/10u\">Christchurch, NZ Meetup - Games &amp; Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2014 04:30:00PM (+1200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">James Hight, University of Canterbury, Christchurch, New Zealand</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The third Chch meetup is this Sunday at 4.30pm! We have a few regular attendees so if you live in Chch and would like to connect with the Less Wrong community here it's well worth the effort to come along.</p>\n\n<p>It will be held in James Hight library, discussion room 901, at the University of Canterbury. You do not need to be a student or in any way affiliated with UC to come along, the discussion rooms are open to everyone.</p>\n\n<p>The way to get there is to go through the front (and only) doors of the library, walk forwards until you see elevators to your right, and take one of those elevators to floor 9. Then walk around the edges of the floor until you see room 901. Don't take the stairs, they only lead up one floor! If you need clearer directions or anticipate getting lost, PM or post below and I can clear it up and give you a contact cell number.</p>\n\n<p>Potential activities include rationalist Cards Against Humanity, goal-setting, and if previous occasions are any indication a whole bunch of interesting discussions. It's very casual, so if you have anything you'd like to do or discuss post below or just show up with it :-)</p>\n\n<p>If you'd like to come along, but can't make this day or time, please post below so we know for future meetups. Also, if you have something on early and can come later, that's fine too - meetups typically last 3-4 hours and dropping in partway (or leaving partway) is absolutely fine.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Christchurch__NZ_Meetup___Games___Discussion1\">Discussion article for the meetup : <a href=\"/meetups/10u\">Christchurch, NZ Meetup - Games &amp; Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Christchurch, NZ Meetup - Games & Discussion", "anchor": "Discussion_article_for_the_meetup___Christchurch__NZ_Meetup___Games___Discussion", "level": 1}, {"title": "Discussion article for the meetup : Christchurch, NZ Meetup - Games & Discussion", "anchor": "Discussion_article_for_the_meetup___Christchurch__NZ_Meetup___Games___Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-29T11:03:51.366Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow meet up", "slug": "meetup-moscow-meet-up-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YkqeLfCYc2ypoSBP6/meetup-moscow-meet-up-3", "pageUrlRelative": "/posts/YkqeLfCYc2ypoSBP6/meetup-moscow-meet-up-3", "linkUrl": "https://www.lesswrong.com/posts/YkqeLfCYc2ypoSBP6/meetup-moscow-meet-up-3", "postedAtFormatted": "Thursday, May 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20meet%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20meet%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYkqeLfCYc2ypoSBP6%2Fmeetup-moscow-meet-up-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20meet%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYkqeLfCYc2ypoSBP6%2Fmeetup-moscow-meet-up-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYkqeLfCYc2ypoSBP6%2Fmeetup-moscow-meet-up-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10v'>Moscow meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 June 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The 1st of June meet up is rescheduled to the 8th of June!</p>\n\n<p>We will have two streams, and sections with the same numbers will be conducted in the same time.</p>\n\n<p>We will have:</p>\n\n<ul>\n<li>Small reports about cognitive biases, for both streams.</li>\n</ul>\n\n<p>The first stream:</p>\n\n<ol>\n<li><p>Guess the incorrect argument, the game.</p></li>\n<li><p>Alternative transhumanism, report.</p></li>\n<li><p>Biofeedback, report.</p></li>\n</ol>\n\n<p>The second stream:</p>\n\n<ol>\n<li><p>Discussion and the discussion analysis.</p></li>\n<li><p>Rational lighting, discussion.</p></li>\n</ol>\n\n<p>We gather in the Yandex office, you need the first revolving door under the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.\nWe start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10v'>Moscow meet up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YkqeLfCYc2ypoSBP6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7555726176673852e-06, "legacy": true, "legacyId": "26287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_meet_up\">Discussion article for the meetup : <a href=\"/meetups/10v\">Moscow meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 June 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The 1st of June meet up is rescheduled to the 8th of June!</p>\n\n<p>We will have two streams, and sections with the same numbers will be conducted in the same time.</p>\n\n<p>We will have:</p>\n\n<ul>\n<li>Small reports about cognitive biases, for both streams.</li>\n</ul>\n\n<p>The first stream:</p>\n\n<ol>\n<li><p>Guess the incorrect argument, the game.</p></li>\n<li><p>Alternative transhumanism, report.</p></li>\n<li><p>Biofeedback, report.</p></li>\n</ol>\n\n<p>The second stream:</p>\n\n<ol>\n<li><p>Discussion and the discussion analysis.</p></li>\n<li><p>Rational lighting, discussion.</p></li>\n</ol>\n\n<p>We gather in the Yandex office, you need the first revolving door under the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.\nWe start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_meet_up1\">Discussion article for the meetup : <a href=\"/meetups/10v\">Moscow meet up</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow meet up", "anchor": "Discussion_article_for_the_meetup___Moscow_meet_up", "level": 1}, {"title": "Discussion article for the meetup : Moscow meet up", "anchor": "Discussion_article_for_the_meetup___Moscow_meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-29T20:37:45.023Z", "modifiedAt": null, "url": null, "title": "Meetup : Retelling stuff from CFAR Epistemic Rationality for EA ", "slug": "meetup-retelling-stuff-from-cfar-epistemic-rationality-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zNKeAmKxExY5zaXxX/meetup-retelling-stuff-from-cfar-epistemic-rationality-for", "pageUrlRelative": "/posts/zNKeAmKxExY5zaXxX/meetup-retelling-stuff-from-cfar-epistemic-rationality-for", "linkUrl": "https://www.lesswrong.com/posts/zNKeAmKxExY5zaXxX/meetup-retelling-stuff-from-cfar-epistemic-rationality-for", "postedAtFormatted": "Thursday, May 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Retelling%20stuff%20from%20CFAR%20Epistemic%20Rationality%20for%20EA%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Retelling%20stuff%20from%20CFAR%20Epistemic%20Rationality%20for%20EA%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNKeAmKxExY5zaXxX%2Fmeetup-retelling-stuff-from-cfar-epistemic-rationality-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Retelling%20stuff%20from%20CFAR%20Epistemic%20Rationality%20for%20EA%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNKeAmKxExY5zaXxX%2Fmeetup-retelling-stuff-from-cfar-epistemic-rationality-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNKeAmKxExY5zaXxX%2Fmeetup-retelling-stuff-from-cfar-epistemic-rationality-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10w'>Retelling stuff from CFAR Epistemic Rationality for EA </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>One of our members will be talking about things he learned at the CFAR Epistemic Rationality for Effective Altruists event.</p>\n\n<p>Topics may include:</p>\n\n<ul>\n<li><p>\"5 minute exercises\" - basically a way to practice the skill in <a href=\"http://lesswrong.com/lw/ui/use_the_try_harder_luke/\">this post</a>.</p></li>\n<li><p>\"Case studies\" - people can volunteer beliefs they want to have challenged/refined, and everyone else asks them questions.</p></li>\n<li><p>Aversion factoring for epistemic rationality habits</p></li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10w'>Retelling stuff from CFAR Epistemic Rationality for EA </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zNKeAmKxExY5zaXxX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.7563889203110998e-06, "legacy": true, "legacyId": "26288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Retelling_stuff_from_CFAR_Epistemic_Rationality_for_EA_\">Discussion article for the meetup : <a href=\"/meetups/10w\">Retelling stuff from CFAR Epistemic Rationality for EA </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>One of our members will be talking about things he learned at the CFAR Epistemic Rationality for Effective Altruists event.</p>\n\n<p>Topics may include:</p>\n\n<ul>\n<li><p>\"5 minute exercises\" - basically a way to practice the skill in <a href=\"http://lesswrong.com/lw/ui/use_the_try_harder_luke/\">this post</a>.</p></li>\n<li><p>\"Case studies\" - people can volunteer beliefs they want to have challenged/refined, and everyone else asks them questions.</p></li>\n<li><p>Aversion factoring for epistemic rationality habits</p></li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Retelling_stuff_from_CFAR_Epistemic_Rationality_for_EA_1\">Discussion article for the meetup : <a href=\"/meetups/10w\">Retelling stuff from CFAR Epistemic Rationality for EA </a></h2>", "sections": [{"title": "Discussion article for the meetup : Retelling stuff from CFAR Epistemic Rationality for EA ", "anchor": "Discussion_article_for_the_meetup___Retelling_stuff_from_CFAR_Epistemic_Rationality_for_EA_", "level": 1}, {"title": "Discussion article for the meetup : Retelling stuff from CFAR Epistemic Rationality for EA ", "anchor": "Discussion_article_for_the_meetup___Retelling_stuff_from_CFAR_Epistemic_Rationality_for_EA_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fhEPnveFhb9tmd7Pe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-30T02:44:00.281Z", "modifiedAt": null, "url": null, "title": "Does this seem to you like evidence for the existence of psychic abilities in humans? ", "slug": "does-this-seem-to-you-like-evidence-for-the-existence-of", "viewCount": null, "lastCommentedAt": "2020-09-01T07:20:51.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gothgirl420666", "createdAt": "2013-01-06T19:35:18.030Z", "isAdmin": false, "displayName": "gothgirl420666"}, "userId": "P7J37T964Pxizzp9g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9Ez3hYX6Ewdj7sgQj/does-this-seem-to-you-like-evidence-for-the-existence-of", "pageUrlRelative": "/posts/9Ez3hYX6Ewdj7sgQj/does-this-seem-to-you-like-evidence-for-the-existence-of", "linkUrl": "https://www.lesswrong.com/posts/9Ez3hYX6Ewdj7sgQj/does-this-seem-to-you-like-evidence-for-the-existence-of", "postedAtFormatted": "Friday, May 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20this%20seem%20to%20you%20like%20evidence%20for%20the%20existence%20of%20psychic%20abilities%20in%20humans%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20this%20seem%20to%20you%20like%20evidence%20for%20the%20existence%20of%20psychic%20abilities%20in%20humans%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Ez3hYX6Ewdj7sgQj%2Fdoes-this-seem-to-you-like-evidence-for-the-existence-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20this%20seem%20to%20you%20like%20evidence%20for%20the%20existence%20of%20psychic%20abilities%20in%20humans%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Ez3hYX6Ewdj7sgQj%2Fdoes-this-seem-to-you-like-evidence-for-the-existence-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Ez3hYX6Ewdj7sgQj%2Fdoes-this-seem-to-you-like-evidence-for-the-existence-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2653, "htmlBody": "<p>I was recently reminded of something I have encountered that seems to me to be good evidence for paranormal phenomena. Can anyone help me figure out what might be going on?&nbsp;</p>\n<p>When I was a little younger, I used to play the online riddle game <a href=\"http://deathball.net/notpron/\">Notpron</a>.&nbsp;In this game, the player (essentially) has to analyze a webpage for clues towards the URL to the next webpage, and then repeat for 140 stages. The creator of this game, DavidM, at some point became a huge new age conspiracy theory loony type. Three years after the original ending of the riddle went online, he revised it to include an additional final level: Level Nu. This level is very different than the ones preceding it. I can't link to the page for obvious reasons, but I will transcribe it here:</p>\n<blockquote>\n<p>835 492 147 264</p>\n<p>Remote view the photography this number represents!</p>\n<p>Email me all your results to david@david-m.org. I'll get you some feedback. Get me all elements or impressions that seem really strong for you. Or send me your sketches if you like.</p>\n<p>Don't bruteforce, or you'll be banned from this one. You have as many attempts as you like, take your time.</p>\n<p>Yes, I mean it. No tricks here, just pure remote viewing. The number represents a picture, I want to know what's on there.</p>\n<p>So learn some remote viewing technique you like best and go ahead. The internet has lots of information. Have fun!</p>\n<p>Please do this ALL by yourself, not even with your very very close friends. Because its boring and stupid, and because you can put bullshit into each others head, which is hard to get rid of again, because the mind needs to be shut down for this to work properly. So do it alone, just talk to me about it, please.</p>\n<p>(Yes, this really works, one friend got the content of the picture on first try...and yes, he only got the number from me.)</p>\n</blockquote>\n<div>\n<ul>\n<li>31 people have successfully completed this level.</li>\n<li>Before this level went up, around 200 people had successfully completed the game (iirc). Given that Notpron has declined in popularity since Level Nu was created in 2008, I would estimate that around 300 people in total are in a position to attempt Level Nu, although it could be more. However, I would imagine that many people 1) probably did not come back once they had already finished, 2) were too intimidated by remote viewing and the trivial inconvenience of having an email discussion with DavidM, 3) did not even bother due to disbelief in remote viewing.&nbsp;</li>\n<li>The first person who solved it did so by dreaming about the answer. She dreamt night after night that a German man (DavidM is German) was aggressively trying to sell her a boat. The solution picture was of a boat. <a href=\"http://notpron.org/forum/viewtopic.php?p=578811&amp;sid=ed563266bf05421ee80a07ee12162657#p578811\">One of the very first posts on the thread was her talking about her dream and saying \"I think this has something to do with Notpron, but I don't know what\".</a> DavidM had to immediately remove the post so as not to give away the answer.</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=579832#p579832\">The second person solved it on their first try with just one word (presumably \"boat\").&nbsp;</a></li>\n<li>Someone who solved it said <a href=\"http://notpron.org/forum/viewtopic.php?p=588829#p588829\">\"What I got was literally a much sharper much detailed version of a badly scribbled picture in my mind\"</a>. This person apparently also got \"the one right word that you need to solve it\" (boat).&nbsp;</li>\n<li>Someone on the forum writes: <a href=\"http://notpron.org/forum/viewtopic.php?p=594255#p594255\">\"Mailed my visions. I swear it was first thing i saw in my head. But no doubts i was wrong =)\"</a>. Immediately after, DavidM replies saying that he figured it out.&nbsp;</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=594518#p594518\">\"The last 3 or 4 people solved the thing at the first attempt. Some little inaccuracies everytime, but the main 2 objects were always named first.\"</a></li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=594522#p594522\">\"i didn't have any \"visions\". just was reading my university-stuff, when snowman \"forced\" me to write david. i thought it could be funny though and wrote the first shit of which i was thinking at that second. didn't even look at the numbers or anything.\"</a></li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=595691#p595691\">Someone's first idea that he sent was what David planned as the future solution.</a> It seems like what he said was \"rainbow colors\" for a picture of an assortment of fruit. David told him to look at the current solution instead, and again, his first idea was correct.&nbsp;</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=595709#p595709\">Same guy: </a>\"weird thing is. i got the \"future solution\" picture in my head right away. without even trying. then i just send it in.and when david asked me to get the current one. my gf came to me with my son in her arms saying i had to take him and i just: \"Hold on, i just need to get a picture in my head\". and while she was standing there with my son crying next to me. i got a pic up in my head immidietly, but that didnt feel right so i pushed it away and got another on right away and mailed it in. and it was the right one. hehe. :) and especially the second pic, i saw very clearly. even colours.\"</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=595711#p595711 \">Post where he reveals the original answer:</a> \"Most people just said right away, \"it's a boat\" or \"boat/raft on a lake/sea/river\". Or one said \"going fishing\", which was vague, but I let it count. What I got a lot as well was the skyline and water. 2 guys have been listening to a song called \"I'm on a boat\"&nbsp;while solving the riddle, and I watched the video clip. One scene in it looks just like the solution. Crazy.\"</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?f=527&amp;t=39526&amp;start=240#p595773.\">Post where he reveals the second answer: </a>&nbsp;He says several times that he believes that this one was harder than the first. \"Almost [all? sic] saw round things. Some interpreted it as ball(sport), circles, pom poms, the sun or the moon etc. So I'm glad this round-element was so dominant. CTRL saw rainbow colours right away. At least something. Kasper then pretty much nailed it in his this attempt: I saw two things O.o i saw an animal and fruit/vegetables maybe animals eating fruit/vegatables.\" It seems like only two people solved it during this time, although there may be more.&nbsp;</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=595826#p595826 . \">Finally, someone who doesn't believe:</a> \"(This is Jooly, who used to be a mod here and one of the first solvers of the fair levels, and whose account has been mysteriously deactivated since she started discussing DavidM's increasingly wacky ideas a while ago) I spoke with one of the level Nu solvers, who explained to me exactly how it was solved. Remote viewing had nothing to do with it. Duping a very very gullible (desperately wanting to believe?) DavidM was all it took, and it was very easy too. I won't bother, having solved the real notpron levels. But for those of you who must have the new certificate, don't worry. It doesn't take any magic powers or much effort to do so.\" (David denies that he deactivated Jooly's account and says Jooly is free to disagree with him.)&nbsp;</li>\n<li>I personally talked to the skeptic in question on IRC back in the day. I can't recall the conversation too well, but he refused to give any concrete details on how he solved it exactly. I asked him \"Was it something like, for example, you say 'Is it blue?', David says 'no', you say 'Is it red?', David says 'no', you say 'Is it big?', David says 'no', you say 'it's an apple', David says you figured it out?\". He said it was something close to that. Note that as far as I can tell, everyone else who solved it either believes in remote viewing or remains agnostic.&nbsp;</li>\n<li>On how someone solved the level: <a href=\"http://notpron.org/forum/viewtopic.php?p=595977#p595977\">\"Yeah, she asked a friend about the number. He said the correct answer, and there you go.\"</a></li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=597114#p597114\">The third answer is revealed. </a>There's too much stuff here to copy and paste, but he reveals a bunch of successful attempts, some of which are pretty uncanny. The most interesting part is: \"Kimmo, who was not considered to have solved it said: 'It is something that is approaching me, not sure what it is. It is that kind of situation where you need to react to and not stay there just looking what it is.' (Now I don't really see why I didn't let him pass; if you're reading this, contact me!)\"</li>\n<li><a href=\"http://notpron.org/forum/viewtopic.php?p=598331#p598331\">After around twenty-something solves, DavidM maintains that most people guessed it on their first try.&nbsp;</a></li>\n</ul>\n<div>I personally tried to solve it myself. I was less of a rationalist back then, and so I was fairly open-minded about the existence of most paranormal phenomena. The picture I was looking for was the <a href=\"http://notpron.org/forum/viewtopic.php?p=597114#p597114\">shark</a>.&nbsp;</div>\n</div>\n<div><br /></div>\n<div>Here is a shortened, paraphrased transcript of our email conversation:</div>\n<div><br /></div>\n<div>Me: I'm imagining palm trees by a lake at sunset.</div>\n<div>David: It's not bad, but I don't want to give you any more information because it will interfere with your efforts.</div>\n<div>Me: I'm picturing an elephant walking into a barn.</div>\n<div>David: Nope.&nbsp;</div>\n<div>Me: How many people have attempted this? And how many people have solved it with the current picture?</div>\n<div>David: About half of the people who attempted solved it. Most solved it on their first try. I don't know exactly how many people solved this picture, but it has been a few.&nbsp;</div>\n<div>Me: Is it a space shuttle?</div>\n<div>David: No.&nbsp;</div>\n<div>Me: (Expressions of frustration, with a few guesses thrown in.)</div>\n<div>David: (Encouragement and advice, no comment on the guesses. Says \"I can very well see that you receive the right input, but your mind is screwing it up into something else.\")</div>\n<div>Me: It's a bee?</div>\n<div>David: No. Are you getting more subtle input, instead of a specific idea?</div>\n<div>Me: Yeah, for that one, I saw something sharp, bright yellow colors, symmetry, a noisy drone, and two colors in pattern.</div>\n<div>David: So THIS is interesting. Everything else you said wasn't!</div>\n<div>Me: Are you saying that I was close?&nbsp;</div>\n<div>David: These elements sound like they are on target. They are too vague yet to tell if they are for real.&nbsp;</div>\n<div>Me: Thanks. The only other thing I could think about that relates to those elements is a pencil. I'll try again tonight.&nbsp;</div>\n<div>David: Stop fiddling around with your mind about this. It's bound to fail. There's no way to guess the target just from what you said.&nbsp;</div>\n<div>Me: I just tried it again. Is it a helicopter?</div>\n<div>David: Are you sure you aren't viewing the old solution? There was a helicopter involved.&nbsp;</div>\n<div>Me: The boat? I'm not trying too. I guess I'll just keep trying... I even have the numbers memorized at this point.</div>\n<div>David. The boat was shot from a helicopter.&nbsp;You shouldn't memorize the numbers. They don't matter. Memorizing them might just create unwanted associations.</div>\n<div>Me: Okay. I say helicopter because I had an experience where I saw a bunch of spinning fan blades. I was going to guess a fan, but I could sense that there was more. Then I went \"through\" the fan blades and for a second I saw the whole helicopter.&nbsp;</div>\n<div>David: It sounds like it could be on target. But ignore it, it's not the object of interest.</div>\n<div><br /></div>\n<div>At that point, I lost interest and gave up. Looking back, I can honestly say that I saw nothing remotely (haha) similar to the picture of the <a href=\"http://notpron.org/forum/viewtopic.php?p=597114#p597114\">shark</a>. I was not even a tiny bit close. I'm not sure why David said that I was on track, I can't see any association between the shark and what I was guessing.&nbsp;</div>\n<div><br /></div>\n<div>So that's everything I know.&nbsp;</div>\n<div><br /></div>\n<div>Points in favor of it being real:</div>\n<div>\n<ul>\n<li>\"Most people\" apparently guessed it on their first try.</li>\n<li>According to David, about half the people who tried it have solved it.&nbsp;</li>\n<li>The dream thing - absolutely insane, hard to imagine that it's a coincidence.&nbsp;</li>\n<li>David did not consider the guy who guessed the shark as \"something approaching me, it is a situation that I need to react to\" to have solved the level. This shows that he requires fairly high standards of accuracy.</li>\n<li>David implies that in order to have guessed the boat, you need to say the word \"boat\", also implying high standards.&nbsp;</li>\n<li>David did not really give me very much help or \"lead\" me anywhere when I tried to solve it.&nbsp;</li>\n</ul>\n<div>Points in favor of it being fake:</div>\n<div>\n<ul>\n<li>One person who solved it says that he did not solve it using remote viewing.&nbsp;</li>\n<li>It didn't work for me at all.&nbsp;</li>\n<li>David might very well be exaggerating both the percentage of people who successfully solved it and the percentage of people who guessed it on their first try.&nbsp;</li>\n<li>David might be (and in fact probably is) only reporting the \"best\" answers in his forum posts. For the fruit and the shark, he seems to be posting about half of the people who solved it in that time period. For the boat, he doesn't really give specifics, and instead says \"Most people just said it was a boat on their first guess.\"</li>\n</ul>\n<div>Here are my two theories regarding this.</div>\n</div>\n<div>\n<ul>\n<li>Maybe DavidM is in fact \"leading\" people to the answer through a series of multiple guesses. For this to be true, however, a few things would have to be the case. First of all, his assertion that most people guessed it on their first try would have to be greatly exaggerated. Let's imagine that David is outright lying about most people guessing it on their first try and that half the people who attempted the riddle solved it. However, at least six people (I don't feel like going back through all 29 pages and counting) posted on the forum that they solved it on their first try. Let's imagine that all 300 people who reached the level attempted it. This is still a 1/50 \"first guess\" rate, and that's out of all the photographs in the world. However, maybe by some conjunction of 1) exaggerating those two numbers, 2) his dialogue with me being atypical, 3) the answers he posted on the forum being atypical, 4) his refusal to accept \"something approaching me\" being atypical and 5) the dream being a total coincidence, it may be true that he actually is doing a form of \"leading\" and is covering it up well. This feels like a really unsatisfactory answer. It relies on a lot of conjunctions and it seems clear that the only way to arrive at it is by a thorough search for some sort of answer that fits nicely in with our pre-existing worldview. That being said, I suspect it might be the most likely answer.&nbsp;</li>\n<li>Perhaps the level is an elaborate joke. In reality there is some other more conventional means of arriving at a solution, and people who solve it are told to play along. I can sort of see this being the case, given that 1) there are some other levels of Notpron that have \"prankster-ish\" elements and 2) I have actually myself been a part of a very similar joke on an even bigger scale, so I know that it can happen. However, on the other hand, DavidM really strongly believes in the conspiracy theory new age stuff and vigorously promotes it, so it seems unlikely that he would sabotage his own ideology like that. Also, while there are other prankster-ish levels of Notpron, nothing comes close to being as clever or elaborate as this scenario would be.&nbsp;</li>\n</ul>\n</div>\n<div>So, given the above and <a href=\"http://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/\">this recent article from Slate Star Codex</a>, I feel like I am forced to raise my credence level for remote viewing being real to somewhere between 50 and 60 percent.&nbsp;</div>\n</div>\n<div><br /></div>\n<div>Does this seem in error to you?&nbsp;</div>\n<div>&nbsp;</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9Ez3hYX6Ewdj7sgQj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -3, "extendedScore": null, "score": -2.8e-05, "legacy": true, "legacyId": "26293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-30T15:40:27.948Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-98", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w64hYbQEQSQJcBkSx/weekly-lw-meetups-98", "pageUrlRelative": "/posts/w64hYbQEQSQJcBkSx/weekly-lw-meetups-98", "linkUrl": "https://www.lesswrong.com/posts/w64hYbQEQSQJcBkSx/weekly-lw-meetups-98", "postedAtFormatted": "Friday, May 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw64hYbQEQSQJcBkSx%2Fweekly-lw-meetups-98%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw64hYbQEQSQJcBkSx%2Fweekly-lw-meetups-98", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw64hYbQEQSQJcBkSx%2Fweekly-lw-meetups-98", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 520, "htmlBody": "<p><strong>This meetup summary was posted to LW main on May 25th. The following week's summary is <a href=\"/lw/kaf/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/zy\">Chicago Calibration Game:&nbsp;<span class=\"date\">31 May 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">14 June 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/zz\">Montreal - How to be charismatic:&nbsp;<span class=\"date\">26 May 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/10n\">[Tel Aviv] Professor Roman Yampolskiy on approaches to AGI risk:&nbsp;<span class=\"date\">27 May 2014 08:00PM</span></a></li>\n<li><a href=\"/meetups/10i\">[Utrecht] Brainstorm and ethics discussion at the Film Caf&eacute;:&nbsp;<span class=\"date\">31 May 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/10f\">Melbourne June Rationality Dojo: Memory:&nbsp;<span class=\"date\">01 June 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10g\">Sydney Social Meetup - June (Games night):&nbsp;<span class=\"date\">12 June 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/10l\">Sydney Meetup - June:&nbsp;<span class=\"date\">25 June 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/10m\">Washington DC Games meetup:&nbsp;<span class=\"date\">25 May 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w64hYbQEQSQJcBkSx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7580163107790052e-06, "legacy": true, "legacyId": "26276", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9YW26vNuLZ2wEsCR5", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-30T16:28:24.135Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup", "slug": "meetup-london-social-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Leonhart", "createdAt": "2010-02-22T20:01:49.792Z", "isAdmin": false, "displayName": "Leonhart"}, "userId": "X5EZEkfccqyWXETHd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SdREkEYp8i4QRrkBM/meetup-london-social-meetup-0", "pageUrlRelative": "/posts/SdREkEYp8i4QRrkBM/meetup-london-social-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/SdREkEYp8i4QRrkBM/meetup-london-social-meetup-0", "postedAtFormatted": "Friday, May 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdREkEYp8i4QRrkBM%2Fmeetup-london-social-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdREkEYp8i4QRrkBM%2Fmeetup-london-social-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdREkEYp8i4QRrkBM%2Fmeetup-london-social-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10x'>London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>The weather is unpredictable right now so start will be in the pub at 2pm. If it's sunny, late comers may find we have moved around the corner to Lincoln's Inn Fields, probably somewhere in the northwest quadrant. PhilH is sure he's going to be there (I may not) so he's the best option to call if confused, on 07792009646.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10x'>London social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SdREkEYp8i4QRrkBM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7580846373911484e-06, "legacy": true, "legacyId": "26296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/10x\">London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Your regularly scheduled meetup. Join us from 2pm to talk about the sorts of things that your other friends will look funny at you for talking about.</p>\n\n<p>The weather is unpredictable right now so start will be in the pub at 2pm. If it's sunny, late comers may find we have moved around the corner to Lincoln's Inn Fields, probably somewhere in the northwest quadrant. PhilH is sure he's going to be there (I may not) so he's the best option to call if confused, on 07792009646.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/10x\">London social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-30T18:09:09.163Z", "modifiedAt": null, "url": null, "title": "Against Open Threads", "slug": "against-open-threads", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:31.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vmWjcZdGpbCF5cEhG/against-open-threads", "pageUrlRelative": "/posts/vmWjcZdGpbCF5cEhG/against-open-threads", "linkUrl": "https://www.lesswrong.com/posts/vmWjcZdGpbCF5cEhG/against-open-threads", "postedAtFormatted": "Friday, May 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20Open%20Threads&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20Open%20Threads%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvmWjcZdGpbCF5cEhG%2Fagainst-open-threads%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20Open%20Threads%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvmWjcZdGpbCF5cEhG%2Fagainst-open-threads", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvmWjcZdGpbCF5cEhG%2Fagainst-open-threads", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p>There are various people who feel that Lesswrong degraded in the last year. In the same timeframe more and more discussions moved into the open thread model and open threads became weekly instead of monthly.</p>\n<p>I suggest to counteract that trend by opening all discussions per default on Discussion instead of opening them in weekly open threads. Having the topics in discussion makes it easy to browse the list of topics and choose the headline that are of interest, even if the thread got opened two weeks ago.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vmWjcZdGpbCF5cEhG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 7, "extendedScore": null, "score": 1.7582282583127718e-06, "legacy": true, "legacyId": "26297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-31T15:08:41.361Z", "modifiedAt": null, "url": null, "title": "Brainstorming for post topics", "slug": "brainstorming-for-post-topics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:31.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vZdgxWbfS8xSqMhoo/brainstorming-for-post-topics", "pageUrlRelative": "/posts/vZdgxWbfS8xSqMhoo/brainstorming-for-post-topics", "linkUrl": "https://www.lesswrong.com/posts/vZdgxWbfS8xSqMhoo/brainstorming-for-post-topics", "postedAtFormatted": "Saturday, May 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brainstorming%20for%20post%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrainstorming%20for%20post%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvZdgxWbfS8xSqMhoo%2Fbrainstorming-for-post-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brainstorming%20for%20post%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvZdgxWbfS8xSqMhoo%2Fbrainstorming-for-post-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvZdgxWbfS8xSqMhoo%2Fbrainstorming-for-post-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>I suggested&nbsp; <a href=\"/r/discussion/lw/kah/against_open_threads/ay8j\">recently</a>&nbsp;that part of the problem with with LW was a lock of discussion posts which was caused by people not thinking of much to post about.</p>\n<p>When I ask myself \"what might be a good topic for a post?\", my mind goes blank, but surely not everything that's worth saying that's related to rationality has been said.</p>\n<p>So, is there something at the back of your mind which might be interesting? A topic which got some discussion in an open thread that could be worth pursuing?</p>\n<p>If you've found anything which helps you generate useable ideas, please comment about it-- or possibly write a post on the subject.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "QPt5ECwTCAg63mbNu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vZdgxWbfS8xSqMhoo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 32, "extendedScore": null, "score": 1.7600254958312383e-06, "legacy": true, "legacyId": "26299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-31T19:08:24.784Z", "modifiedAt": null, "url": null, "title": "An onion strategy for AGI discussion", "slug": "an-onion-strategy-for-agi-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mfHvyPL2d6v7pXkjs/an-onion-strategy-for-agi-discussion", "pageUrlRelative": "/posts/mfHvyPL2d6v7pXkjs/an-onion-strategy-for-agi-discussion", "linkUrl": "https://www.lesswrong.com/posts/mfHvyPL2d6v7pXkjs/an-onion-strategy-for-agi-discussion", "postedAtFormatted": "Saturday, May 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20onion%20strategy%20for%20AGI%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20onion%20strategy%20for%20AGI%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmfHvyPL2d6v7pXkjs%2Fan-onion-strategy-for-agi-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20onion%20strategy%20for%20AGI%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmfHvyPL2d6v7pXkjs%2Fan-onion-strategy-for-agi-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmfHvyPL2d6v7pXkjs%2Fan-onion-strategy-for-agi-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 382, "htmlBody": "<p><small>Cross-posted from <a href=\"http://lukemuehlhauser.com/an-onion-strategy-for-agi-discussion/\">my blog</a>.</small></p>\n<p>\"<a href=\"http://lukemuehlhauser.com/wp-content/uploads/Hammond-et-al-The-stabilization-of-environments.pdf\">The stabilization of environments</a>\" is a paper&nbsp;about AIs that&nbsp;reshape their environments to&nbsp;make it easier to achieve their goals. This is typically called&nbsp;<em>enforcement</em>, but they prefer the term&nbsp;<em>stabilization</em> because it \"sounds less hostile.\"</p>\n<p>\"I'll open the pod bay doors,&nbsp;Dave, but then I'm going to stabilize the ship...\"</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/Sparrow-In-vitro-eugenics.pdf\">Sparrow (2013)</a>&nbsp;takes&nbsp;the opposite approach to plain vs. dramatic language. Rather than&nbsp;using a modest&nbsp;term like&nbsp;<em><a href=\"http://theuncertainfuture.com/faq.html#7\">iterated embryo selection</a></em>, Sparrow prefers&nbsp;the phrase&nbsp;<em>in vitro eugenics</em>.&nbsp;Jeepers.</p>\n<p>I suppose that's more&nbsp;likely to provoke&nbsp;public discussion, but...&nbsp;will much good will come of that public discussion?&nbsp;The public had a needless freak-out about in vitro fertilization&nbsp;back in the 60s and 70s and then, as soon as the first IVF baby was born in 1978, <a href=\"http://intelligence.org/files/EmbryoSelection.pdf\">decided they were in favor of it</a>.</p>\n<p>Someone&nbsp;recently&nbsp;suggested I use an&nbsp;\"<strong>onion strategy</strong>\" for the discussion of novel technological risks.&nbsp;The outermost layer of the&nbsp;communication onion would be&nbsp;aimed at&nbsp;the general public, and focus on benefits rather than risks, so as not to provoke an unproductive panic. A second layer for a&nbsp;specialist&nbsp;audience could&nbsp;include&nbsp;a more detailed elaboration of the risks. The most complete discussion of risks and mitigation options would be&nbsp;reserved for&nbsp;technical publications that are read only by professionals.</p>\n<p>Eric Drexler <a href=\"http://lukemuehlhauser.com/wp-content/uploads/Giles-Nanotech-takes-small-steps-toward-burying-grey-goo.pdf\">seems</a> to wish he had more successfully used&nbsp;an onion strategy when writing about nanotechnology.&nbsp;<a href=\"http://smile.amazon.com/Engines-Creation-Nanotechnology-Library-Science/dp/0385199732/\"><em>Engines of Creation</em></a>&nbsp;included frank discussions of both the benefits and risks of nanotechnology, including the \"<a href=\"http://en.wikipedia.org/wiki/Grey_goo\">grey goo</a>\" scenario that was discussed widely in the media and used as the premise for the bestselling novel&nbsp;<em><a href=\"http://en.wikipedia.org/wiki/Prey_(novel)\">Prey</a>.</em></p>\n<p>Ray Kurzweil may be using an onion strategy, or at least keeping his writing&nbsp;in the outermost layer. If you look carefully, chapter 8&nbsp;of&nbsp;<em><a href=\"http://smile.amazon.com/Singularity-Near-Humans-Transcend-Biology-ebook/dp/B000QCSA7C/\">The Singularity is Near</a></em>&nbsp;takes technological risks pretty seriously, and yet&nbsp;it's written in such a way that most people who read the book seem to come away with an overwhelmingly optimistic perspective on technological change.</p>\n<p>George Church may be following an onion strategy.&nbsp;<em><a href=\"http://smile.amazon.com/Regenesis-Synthetic-Biology-Reinvent-Ourselves-ebook/dp/B00HTQ326W/\">Regenesis</a></em>&nbsp;also contains a chapter on the risks of advanced bioengineering, but it's presented&nbsp;as an \"epilogue\" that many readers will skip.</p>\n<p>Perhaps those of us writing about AGI for the general public should try to discuss:</p>\n<ul>\n<li><em>astronomical stakes</em> rather than&nbsp;<em>existential risk</em></li>\n<li><em>Friendly AI</em> rather than&nbsp;<em>AGI risk</em> or&nbsp;the<em> superintelligence control problem</em></li>\n<li>the orthogonality<em> thesis</em> and&nbsp;<em>convergent instrumental values</em> and&nbsp;<em>complexity of values</em> rather than \"doom by default\"</li>\n<li>etc.</li>\n</ul>\n<p>MIRI doesn't have any official recommendations on the matter, but these days I find myself leaning toward an&nbsp;onion strategy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mfHvyPL2d6v7pXkjs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "26300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-05-31T22:23:40.712Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, June 1-15", "slug": "group-rationality-diary-june-1-15", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6r7crFbvYustFbLKN/group-rationality-diary-june-1-15", "pageUrlRelative": "/posts/6r7crFbvYustFbLKN/group-rationality-diary-june-1-15", "linkUrl": "https://www.lesswrong.com/posts/6r7crFbvYustFbLKN/group-rationality-diary-june-1-15", "postedAtFormatted": "Saturday, May 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20June%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20June%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r7crFbvYustFbLKN%2Fgroup-rationality-diary-june-1-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20June%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r7crFbvYustFbLKN%2Fgroup-rationality-diary-june-1-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r7crFbvYustFbLKN%2Fgroup-rationality-diary-june-1-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">This is the public group instrumental rationality diary for June 1-15.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Previous diary:&nbsp;<span style=\"text-decoration: underline;\"><a href=\"/lw/k8v/group_rationality_diary_may_1631/\"><span style=\"color: #8a8a8b;\">May 1</span>6-31</a></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Next diary:&nbsp;<a href=\"/lw/kd5/group_rationality_diary_june_1630/\">June 16-30</a></p>\n<div><br /></div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6r7crFbvYustFbLKN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "26301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["D3vrEJv5S2BxdkBnN", "onA6deQmBmYXftfq3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-01T15:04:05.928Z", "modifiedAt": null, "url": null, "title": "June 2014 Media Thread", "slug": "june-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:01.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CH5MvBeHf3uuD2FEr/june-2014-media-thread", "pageUrlRelative": "/posts/CH5MvBeHf3uuD2FEr/june-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/CH5MvBeHf3uuD2FEr/june-2014-media-thread", "postedAtFormatted": "Sunday, June 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20June%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJune%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCH5MvBeHf3uuD2FEr%2Fjune-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=June%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCH5MvBeHf3uuD2FEr%2Fjune-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCH5MvBeHf3uuD2FEr%2Fjune-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p><strong> Note for this month's thread:</strong> As per comment in last month's 'meta' subthread, the \"Television and Movies\" subthread has been split into two: \"TV and Movies (Animation)\" and \"TV and Movies (Live Action)\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CH5MvBeHf3uuD2FEr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.7620776531662603e-06, "legacy": true, "legacyId": "26303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-01T20:32:02.500Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes June 2014", "slug": "rationality-quotes-june-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:08.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6gYGuT3Dzb2rb7zzF/rationality-quotes-june-2014", "pageUrlRelative": "/posts/6gYGuT3Dzb2rb7zzF/rationality-quotes-june-2014", "linkUrl": "https://www.lesswrong.com/posts/6gYGuT3Dzb2rb7zzF/rationality-quotes-june-2014", "postedAtFormatted": "Sunday, June 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20June%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20June%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6gYGuT3Dzb2rb7zzF%2Frationality-quotes-june-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20June%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6gYGuT3Dzb2rb7zzF%2Frationality-quotes-june-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6gYGuT3Dzb2rb7zzF%2Frationality-quotes-june-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>Another month, another rationality quotes thread. The rules are:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so <a href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n<li>Provide sufficient information (URL, title, date, page number, etc.) to enable a reader to find the place where you read the quote, or its original source if available. Do not quote with only a name.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6gYGuT3Dzb2rb7zzF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 1.762547096384057e-06, "legacy": true, "legacyId": "26305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 283, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-02T01:20:12.305Z", "modifiedAt": null, "url": null, "title": "Political ideas meant to provoke thought", "slug": "political-ideas-meant-to-provoke-thought", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:08.602Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rqcn399gA3ibJthvF/political-ideas-meant-to-provoke-thought", "pageUrlRelative": "/posts/Rqcn399gA3ibJthvF/political-ideas-meant-to-provoke-thought", "linkUrl": "https://www.lesswrong.com/posts/Rqcn399gA3ibJthvF/political-ideas-meant-to-provoke-thought", "postedAtFormatted": "Monday, June 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Political%20ideas%20meant%20to%20provoke%20thought&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitical%20ideas%20meant%20to%20provoke%20thought%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqcn399gA3ibJthvF%2Fpolitical-ideas-meant-to-provoke-thought%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Political%20ideas%20meant%20to%20provoke%20thought%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqcn399gA3ibJthvF%2Fpolitical-ideas-meant-to-provoke-thought", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqcn399gA3ibJthvF%2Fpolitical-ideas-meant-to-provoke-thought", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 762, "htmlBody": "<p><strong>Politics as gymnastics for rationalists. &nbsp;</strong>No one one Less Wrong is quite sure why politics is a taboo topic or how things got to be that way. &nbsp;What we do think we know is that politics is a great way to bring out the irrationality in people. &nbsp;So why not take advantage of that and use politics as a way to measure rationality? &nbsp;Since politics brings out the most irrationality, it should provide the strongest signal. &nbsp;Since there aren't useful objective metrics of how a political discussion went, we'd have to use subjective judgements by neutral third-party raters, kind of like they do in gymnastics. &nbsp;(In the comment thread for this post, feel free to find fights that you have no dog in, improvise a rationality rubric, and grade participants according to your rubric... let's see how it goes.)</p>\n<p><strong>Be a sheep. &nbsp;</strong>This is probably the exact opposite of what you were taught in your high school civics class. &nbsp;But if my friend Jane is more intelligent, more informed, and less ideological than I am, it seems like voting however Jane is going to vote is a strict improvement over voting however I would naively. &nbsp;It also saves me time, and gives Jane an incentive to put even more time in to carefully considering political issues since she now controls two votes instead of one. &nbsp;Done on a large scale, this could provide an interesting twist on representative democracy. &nbsp;Imagine a <a href=\"http://en.wikipedia.org/wiki/Directed_graph\">directed graph</a>&nbsp;where each node represents a person and an edge is directed from person A to person B if person A is auto-copying person B's votes. &nbsp;There's a government computer system where you can change the person you're auto-copying votes from at any time or override an auto-copied vote with your own personal guess about what's best for society. &nbsp;Other than that, it's direct democracy... all bills are put before all citizens to vote on. &nbsp;Problems this might solve:</p>\n<ul>\n<li>Voting as signaling - a smaller portion of the population is expected to follow politics, so they have an incentive to understand issues in depth and make the right choice for society as a whole rather than signal that they have some characteristic or another.</li>\n<li>Lobbying - I could configure my voting so that I auto-copy the votes of a lobbying watchdog group whenever it votes on anything, and fall back to my regular representative's vote when the watchdog group abstains. &nbsp;That would allow me to selectively vote to&nbsp;<a href=\"http://www.reddit.com/r/Stand/comments/25nbvj/save_the_internet_stop_the_fccs_plan_to_kill_net/\">preserve net neutrality</a>&nbsp;while continuing to copy my regular representative's votes on other issues.</li>\n<li>Wasteful political discourse in general. &nbsp;We don't need everyone to be obsessively discussing politics the way they are currently... a representative sample of ten thousand smart neutral people is plenty. &nbsp;Specialization of labor FTW.</li>\n</ul>\n<div>If Less Wrong thinks this \"sheep\" idea is a good one, next time there's a major election in a country with lots of LW users, we could have a gymnastics tournament (see previous idea) and determine a set of recommendations for other users from that country to <a href=\"/lw/fao/voting_is_like_donating_thousands_of_dollars_to/\">vote with</a>.</div>\n<div><br /></div>\n<div><strong>Capitalism as delayed gratification.</strong> &nbsp;Debate rages endlessly between pure capitalists and those who want some socialism thrown in. &nbsp;The pure capitalists argues that capitalism <a href=\"http://www.overcomingbias.com/2013/01/us-laissez-faire-serves-a-greater-global-good.html\">fosters innovation</a> and increases economic growth. &nbsp;The socialists point to the <a href=\"https://www.ted.com/talks/richard_wilkinson\">negative effects of inequality</a>&nbsp;that they say capitalism causes. &nbsp;My compromise: let's stick with capitalism for a while longer and then switch to socialism when we can't take it any more. &nbsp;The bigger the pie the capitalists make, the more there will be to go around when we crank up the redistribution. &nbsp;At a certain point we hit diminishing returns for additional innovations and it makes sense to optimize for poor peoples' quality of life instead.</div>\n<div><br /></div>\n<div><strong>It's not big vs small, it's smart vs dumb.</strong> &nbsp;Debate rages endlessly between paternalist/nanny state types who say people can't be trusted to make decisions for themselves vs people who are sick of government interference and want to be able to make all of their decisions for themselves. &nbsp;The correct answer to this question depends on the composition of your government. &nbsp;If the average government official is smarter than the average member of the populace, it's potentially a win to have the government make decisions for population members. &nbsp;Examples of government stupidity that libertarians like to talk up are just that--stupidity. &nbsp;If government officials were smart, the government would probably be less stupid (<a href=\"http://www.slate.com/articles/news_and_politics/politics/2013/04/congressional_salaries_senators_representatives_and_their_staff_all_deserve.html\">like in Singapore</a>). &nbsp;Unfortunately, the libertarian chant of government stupidity ends up being a self-fulfilling prophecy as smart people decide that <a href=\"http://www.theatlantic.com/politics/archive/2013/08/the-outsiders-how-can-millennials-change-washington-if-they-hate-it/278920/\">the government is lame</a>&nbsp;and work in other areas.</div>\n<div><br /></div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rqcn399gA3ibJthvF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 5, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 141, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3kLjmvG4BaM6QrDnT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-02T02:26:29.399Z", "modifiedAt": null, "url": null, "title": "Strategyproof Mechanisms: Possibilities", "slug": "strategyproof-mechanisms-possibilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.354Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QG2ZQm2Fxq8ET22sT/strategyproof-mechanisms-possibilities", "pageUrlRelative": "/posts/QG2ZQm2Fxq8ET22sT/strategyproof-mechanisms-possibilities", "linkUrl": "https://www.lesswrong.com/posts/QG2ZQm2Fxq8ET22sT/strategyproof-mechanisms-possibilities", "postedAtFormatted": "Monday, June 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strategyproof%20Mechanisms%3A%20Possibilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrategyproof%20Mechanisms%3A%20Possibilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQG2ZQm2Fxq8ET22sT%2Fstrategyproof-mechanisms-possibilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strategyproof%20Mechanisms%3A%20Possibilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQG2ZQm2Fxq8ET22sT%2Fstrategyproof-mechanisms-possibilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQG2ZQm2Fxq8ET22sT%2Fstrategyproof-mechanisms-possibilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1730, "htmlBody": "<p style=\"padding-left: 30px;\"><em>Despite dictatorships being the only strategyproof mechanisms in general, more interesting strategyproof mechanisms exist for specialized settings. I introduce single-peaked preferences and discrete exchange as two fruitful domains.</em></p>\n<p>Strategyproofness is a very appealing property. When interacting with a strategyproof mechanism, a person is never worse off for being honest (at least in a causal decision-theoretic sense), so there is no need to make conjectures about the actions of others. However, the <a href=\"/lw/k8r/strategyproof_mechanisms_impossibilities/\">Gibbard-Satterthwaite theorem</a> showed that dictatorships are the only universal strategyproof mechanisms for choosing from three or more outcomes. If we want to avoid dictatorships while keeping strategyproofness, we&rsquo;ll have to narrow our attention to specific applications with more structure. In this post, I&rsquo;ll introduce two restricted domains with more interesting strategyproof mechanisms.</p>\n<p><a id=\"more\"></a></p>\n<p>Before jumping into those, I should mention another potential escape route from Gibbard-Satterthwaite: randomization. So far, I&rsquo;ve only considered deterministic social choice rules, where a type profile corresponds to a particular outcome. Considering randomized social choice rules&mdash;mapping a type profile onto a lottery over outcomes&mdash;will widen the scope of possibility just slightly. Instead of one person being the dictator, we can flip a coin to decided who is in charge. In particular, any strategyproof mechanism that chooses an outcome with certainty when all agents rank it as their first choice must be a <em>random dictatorship</em>, selecting among agents with some fixed probability and then choosing that agent&rsquo;s favorite. Unlike a deterministic dictatorship, a random dictatorship with equal weights on agents seems palatable enough to use in practice.</p>\n<h2 id=\"single-peaked-preferences\">Single-peaked preferences</h2>\n<p>Our first refuge from dictatorship theorems is the land of single-peaked preferences on a line, a tree, or more generally a <a href=\"http://en.wikipedia.org/wiki/Median_graph\">median graph</a>. These settings have enough structure on preferences that a mechanism doesn&rsquo;t have to waste power eliciting rankings through dictatorship. Instead, we can deduce enough information from an agent&rsquo;s ideal outcome that we have power left over to do something more interesting with a mechanism.</p>\n<p>In the previous post, I discussed how strategyproofness comes down to the <em>monotonicity</em> of a social choice rule. Monotonicity says that if the outcome chosen by the rule is your favorite, the outcome can&rsquo;t change if you report a different ranking with the same top element but with other options swapped around. This essentially restricts a strategyproof mechanism to using agents&rsquo; favorite alternatives as the only inputs.</p>\n<p>With two alternatives to choose from, knowing an agent&rsquo;s favorite immediately tells us their full ranking, so we have room to build non-dictatorial mechanisms like majority rule. With three alternatives and unrestricted preferences, knowing an agent&rsquo;s favorite doesn&rsquo;t tell us anything about the ranking of the other two. For example, suppose we have three options: red, yellow, and blue. If two agents agree that red is best, we don&rsquo;t have enough information to unambiguously say whether switching from blue to yellow with make those agents better off or not.</p>\n<p>What if instead the three alternatives had some natural ordering? For instance, some housemates have a terrible heating system with three settings: freezing, temperate, and burning. It&rsquo;s reasonable to assume that anyone who loves it hot or cold has &ldquo;temperate&rdquo; as their second choice, with no one preferring both extremes to something in the middle. Moving the thermostat from &ldquo;freezing&rdquo; to &ldquo;temperate&rdquo; makes everyone who has &ldquo;temperate&rdquo; or &ldquo;burning&rdquo; as their ideal better off and those with &ldquo;freezing&rdquo; as their ideal worse off, allowing us to order the alternatives:</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_kas_0.png?v=34a816b7c3e904b5145b87710367a0cf\" alt=\"\" width=\"603\" height=\"57\" /></p>\n<p>This ordering allows us to reduce the decision between three alternatives into a multiple binary decisions that will be consistent with one another. One strategyproof, non-dictatorial mechanism would be to start with the thermostat at &ldquo;temperate&rdquo; and hold a vote whether or not to increase the temperature. If a majority support a temperature increase, the thermostat is bumped up and we&rsquo;re done. If that vote fails, another one is held for a temperature decrease, with the majority winner of that vote being the final outcome. Another mechanism that gives the same results would be for everyone to submit their ideal temperature and choose the median (with a dummy vote of &ldquo;temperate&rdquo; to break ties if there are an even number of housemates). Take a moment to convince yourself this mechanism is strategyproof.</p>\n<p>The median vote is well-defined even if the thermostat has more than three temperatures. We can push this all the way and consider every possible temperature. With a continuum of outcomes, it&rsquo;ll be easier to think in terms of a utility function rather than a ranking, though we&rsquo;re still only concerned with ordinal comparisons. Choosing the median will be strategyproof as long as preferences are <em>single-peaked</em>, with a single local maximum and utility falling off as we go further in either direction away from it. Here is one example of a single-peaked preference, depicting the utility for each temperature:</p>\n<p style=\"padding-left: 90px;\"><img src=\"http://images.lesswrong.com/t3_kas_1.png?v=7ab6840a64c7071959d25d4b2f6da127\" alt=\"\" width=\"551\" height=\"184\" /></p>\n<p>Since this agent has a complicated ranking over temperatures, asking agents to report only their peaks rather than their full utility function comes off as a practical advantage rather than a severe constraint.</p>\n<p>Consider the thought process of someone deciding which temperature to report, knowing the median report will be chosen as the final outcome. If her ideal would be the median report (whether those are being made honestly or not), she has direct control over the outcome as long as she stays the median, so reporting something different makes her worse off. If her ideal is above the median, reporting something higher leaves the outcome unchanged. Reporting something lower either leaves the outcome unchanged or it makes the outcome drop even further away from her ideal than it already was. Similar reasoning goes through when her ideal is below the median. Hence she is never worse off for reporting her peak honestly. Contrast this with a mechanism that averages the peaks to get the outcome, which is not strategyproof.</p>\n<p>Choosing an outcome on the real line has many natural applications like setting the temperature of a thermostat or the sales tax rate of a city. We can go further than lines though. For instance, single-peaked preferences make sense on a tree since we can talk unambiguously about going closer or further from some point as we move along the edges. Here is one tree, with an example of single-peaked preferences for it described by utilities for each node and the peak circled:</p>\n<p style=\"padding-left: 150px;\"><img src=\"http://images.lesswrong.com/t3_kas_2.png?v=4a5a2dc17e6dd9fc62e41401aeb9c0c1\" alt=\"\" width=\"242\" height=\"303\" /></p>\n<p>As on a line, we&rsquo;ll ask agents to report their peaks and choose the median node, i.e.&nbsp;the node that minimizes the distance to each agent&rsquo;s report. Suppose there are five agents, labeled <span class=\"math\"><em>a</em></span> through <span class=\"math\"><em>e</em></span>, and they report the following peaks:</p>\n<p style=\"padding-left: 150px;\"><img src=\"http://images.lesswrong.com/t3_kas_3.png?v=c21c93c3c84cdc7cca5114ecc0ad696f\" alt=\"\" width=\"242\" height=\"303\" /></p>\n<p>The outcome will be the node circled in red, resulting in agent <span class=\"math\"><em>d</em></span> getting his first choice since he is the &ldquo;median&rdquo; of the five.</p>\n<p>Going even further, there are non-dictatorial strategyproof mechanisms on <a href=\"http://en.wikipedia.org/wiki/Median_graph\">any graph where medians are unique and well-defined</a>. We can also tweak the median rule, throwing in some dummy voters or weighting agents differently. However, it turns out that the median is essentially the only strategyproof mechanism that treats agents and outcomes symmetrically.</p>\n<h2 id=\"discrete-exchange\">Discrete exchange</h2>\n<p>In the previous examples, the mechanism chose a single outcome for all agents. Consider instead a situation where each agent owns one object and agents might want to swap things around. Rather than a single outcome, it makes more sense to think of sub-outcomes describing who gets which object, especially if each cares only about what he ends up with and not what the others get. Indifferences over parts of the allocation not involving that agent is another preference restriction that allows us to evade dictatorship theorems.</p>\n<p>For example, three Roman soldiers have currently assigned duties which their eccentric new centurion will allow them to trade around. At the moment, Antonius is a standard-bearer, Brutus is a trumpeter, and Cato is an artilleryman. Even though the full outcome will be a list of who gets what, we consider only the preference of each over the three jobs. Perhaps Antonius has preferences <span class=\"math\"><em>t</em><em>r</em><em>u</em><em>m</em><em>p</em><em>e</em><em>t</em><em>e</em><em>r</em> \u227b\u2004<em>standard-bearer</em> \u227b\u2004<em>a</em><em>r</em><em>t</em><em>i</em><em>l</em><em>l</em><em>e</em><em>r</em><em>y</em><em>m</em><em>a</em><em>n</em></span>, Brutus has preferences <span class=\"math\"><em>standard-bearer</em> \u227b\u2004<em>a</em><em>r</em><em>t</em><em>i</em><em>l</em><em>l</em><em>e</em><em>r</em><em>y</em><em>m</em><em>a</em><em>n</em> \u227b\u2004<em>t</em><em>r</em><em>u</em><em>m</em><em>p</em><em>e</em><em>t</em><em>e</em><em>r</em></span>, and Cato has preferences <span class=\"math\"><em>t</em><em>r</em><em>u</em><em>m</em><em>p</em><em>e</em><em>t</em><em>e</em><em>r</em> \u227b\u2004<em>standard-bearer</em> \u227b\u2004<em>a</em><em>r</em><em>t</em><em>i</em><em>l</em><em>l</em><em>e</em><em>r</em><em>y</em><em>m</em><em>a</em><em>n</em></span>. Given this, it&rsquo;s natural to say Antonius and Brutus should switch jobs, with Cato stuck as an artilleryman.</p>\n<p>We can formalize this inclination using David Gale&rsquo;s <em>Top Trading Cycle</em> algorithm, which operates as follows:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Each agent starts as active. Each object starts as active and pointing at the agent that owns it.</li>\n<li>Active agents point at their favorite active object.</li>\n<li>At least one cycle must exist from agent to object to agent and etc. Deactivate agents and objects in a cycle.</li>\n<li>Iterate steps 2 and 3 until all objects are deactivated. The final allocation is each object going to the agent pointing at it.</li>\n</ol>\n<p>In the example, Antonius and Cato point at <em>trumpeter</em>, with Brutus pointing at <em>standard-bearer</em>. Deactivating the Antonius <span class=\"math\"> &rarr; </span> <em>trumpeter</em> <span class=\"math\"> &rarr; </span> Brutus <span class=\"math\"> &rarr; </span> <em>standard-bearer</em> <span class=\"math\"> &rarr; </span> Antonius cycle, only Cato is left active, so he points at his own job, and the mechanism is done.</p>\n<p>As I&rsquo;ve told the story, the soldiers already have control over a particular job. For instance, if Antonius liked being a standard-bearer best, he could guarantee he keeps the job. What would we do if the soldiers were new and didn&rsquo;t have a pre-existing job? One option is to randomly assign tasks and run the algorithm from there. This is actually how <a href=\"http://www.nola.com/education/index.ssf/2012/04/centralized_enrollment_in_reco.html\">New Orleans matches K-12 students with public schools as of 2012</a>. Since the mechanism is strategyproof, parents don&rsquo;t have to worry about gaming the system when they rank their top choices from the 67 schools in the district<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. Unlike many of the toy examples I&rsquo;ve given, here is a real-world case of improvements made by mechanism design.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>By restricting preferences to be single-peaked or indifferent over the allocations of others, we can find useful strategyproof mechanisms. In the next post, I&rsquo;ll continue exploring strategyproof mechanisms in the very fruitful special case when we can make transfers between agents, introducing the famed Vickrey-Clarke-Groves mechanisms.</p>\n<p>The idea of single-peaked preferences on a line is quite old, dating back to <a href=\"http://www.corwin.com/upm-data/24492_Dowding_Chapter_01.pdf\">Duncan Black in 1948</a>. For a full characterization of strategyproof mechanisms on a line, see <a href=\"http://www.jstor.org/stable/30023824\">Moulin (1980)</a>. For a characterization of which single-peaked preference domains admit non-dictatorial, strategyproof mechanisms, see <a href=\"http://www.uibk.ac.at/economics/bbl/bblpapieress2008/puppe_jet.pdf\">Nehring and Puppe (2007)</a>.</p>\n<p>For an overview of mechanism design aspects of school choice, see <a href=\"https://www2.bc.edu/tayfun-sonmez/AbdulkadirogluSonmez-AER2003.pdf\">Abdulkadiroglu and Sonmez (2003)</a>.</p>\n<p>&nbsp;</p>\n<p><em>Previously on:</em> <a href=\"/lw/k8r/strategyproof_mechanisms_impossibilities/\">Strategyproof Mechanisms: Impossibilities</a></p>\n<p><em>Next up:</em> Mechanism Design with Money</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>Though technically there might be a tiny reason to misrepresent preferences since the parents since the ranking is truncated to the top eight schools rather than all 67.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 9, "mPuSAzJN7CyrMiKrf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QG2ZQm2Fxq8ET22sT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 42, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "26308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\"><em>Despite dictatorships being the only strategyproof mechanisms in general, more interesting strategyproof mechanisms exist for specialized settings. I introduce single-peaked preferences and discrete exchange as two fruitful domains.</em></p>\n<p>Strategyproofness is a very appealing property. When interacting with a strategyproof mechanism, a person is never worse off for being honest (at least in a causal decision-theoretic sense), so there is no need to make conjectures about the actions of others. However, the <a href=\"/lw/k8r/strategyproof_mechanisms_impossibilities/\">Gibbard-Satterthwaite theorem</a> showed that dictatorships are the only universal strategyproof mechanisms for choosing from three or more outcomes. If we want to avoid dictatorships while keeping strategyproofness, we\u2019ll have to narrow our attention to specific applications with more structure. In this post, I\u2019ll introduce two restricted domains with more interesting strategyproof mechanisms.</p>\n<p><a id=\"more\"></a></p>\n<p>Before jumping into those, I should mention another potential escape route from Gibbard-Satterthwaite: randomization. So far, I\u2019ve only considered deterministic social choice rules, where a type profile corresponds to a particular outcome. Considering randomized social choice rules\u2014mapping a type profile onto a lottery over outcomes\u2014will widen the scope of possibility just slightly. Instead of one person being the dictator, we can flip a coin to decided who is in charge. In particular, any strategyproof mechanism that chooses an outcome with certainty when all agents rank it as their first choice must be a <em>random dictatorship</em>, selecting among agents with some fixed probability and then choosing that agent\u2019s favorite. Unlike a deterministic dictatorship, a random dictatorship with equal weights on agents seems palatable enough to use in practice.</p>\n<h2 id=\"Single_peaked_preferences\">Single-peaked preferences</h2>\n<p>Our first refuge from dictatorship theorems is the land of single-peaked preferences on a line, a tree, or more generally a <a href=\"http://en.wikipedia.org/wiki/Median_graph\">median graph</a>. These settings have enough structure on preferences that a mechanism doesn\u2019t have to waste power eliciting rankings through dictatorship. Instead, we can deduce enough information from an agent\u2019s ideal outcome that we have power left over to do something more interesting with a mechanism.</p>\n<p>In the previous post, I discussed how strategyproofness comes down to the <em>monotonicity</em> of a social choice rule. Monotonicity says that if the outcome chosen by the rule is your favorite, the outcome can\u2019t change if you report a different ranking with the same top element but with other options swapped around. This essentially restricts a strategyproof mechanism to using agents\u2019 favorite alternatives as the only inputs.</p>\n<p>With two alternatives to choose from, knowing an agent\u2019s favorite immediately tells us their full ranking, so we have room to build non-dictatorial mechanisms like majority rule. With three alternatives and unrestricted preferences, knowing an agent\u2019s favorite doesn\u2019t tell us anything about the ranking of the other two. For example, suppose we have three options: red, yellow, and blue. If two agents agree that red is best, we don\u2019t have enough information to unambiguously say whether switching from blue to yellow with make those agents better off or not.</p>\n<p>What if instead the three alternatives had some natural ordering? For instance, some housemates have a terrible heating system with three settings: freezing, temperate, and burning. It\u2019s reasonable to assume that anyone who loves it hot or cold has \u201ctemperate\u201d as their second choice, with no one preferring both extremes to something in the middle. Moving the thermostat from \u201cfreezing\u201d to \u201ctemperate\u201d makes everyone who has \u201ctemperate\u201d or \u201cburning\u201d as their ideal better off and those with \u201cfreezing\u201d as their ideal worse off, allowing us to order the alternatives:</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://images.lesswrong.com/t3_kas_0.png?v=34a816b7c3e904b5145b87710367a0cf\" alt=\"\" width=\"603\" height=\"57\"></p>\n<p>This ordering allows us to reduce the decision between three alternatives into a multiple binary decisions that will be consistent with one another. One strategyproof, non-dictatorial mechanism would be to start with the thermostat at \u201ctemperate\u201d and hold a vote whether or not to increase the temperature. If a majority support a temperature increase, the thermostat is bumped up and we\u2019re done. If that vote fails, another one is held for a temperature decrease, with the majority winner of that vote being the final outcome. Another mechanism that gives the same results would be for everyone to submit their ideal temperature and choose the median (with a dummy vote of \u201ctemperate\u201d to break ties if there are an even number of housemates). Take a moment to convince yourself this mechanism is strategyproof.</p>\n<p>The median vote is well-defined even if the thermostat has more than three temperatures. We can push this all the way and consider every possible temperature. With a continuum of outcomes, it\u2019ll be easier to think in terms of a utility function rather than a ranking, though we\u2019re still only concerned with ordinal comparisons. Choosing the median will be strategyproof as long as preferences are <em>single-peaked</em>, with a single local maximum and utility falling off as we go further in either direction away from it. Here is one example of a single-peaked preference, depicting the utility for each temperature:</p>\n<p style=\"padding-left: 90px;\"><img src=\"http://images.lesswrong.com/t3_kas_1.png?v=7ab6840a64c7071959d25d4b2f6da127\" alt=\"\" width=\"551\" height=\"184\"></p>\n<p>Since this agent has a complicated ranking over temperatures, asking agents to report only their peaks rather than their full utility function comes off as a practical advantage rather than a severe constraint.</p>\n<p>Consider the thought process of someone deciding which temperature to report, knowing the median report will be chosen as the final outcome. If her ideal would be the median report (whether those are being made honestly or not), she has direct control over the outcome as long as she stays the median, so reporting something different makes her worse off. If her ideal is above the median, reporting something higher leaves the outcome unchanged. Reporting something lower either leaves the outcome unchanged or it makes the outcome drop even further away from her ideal than it already was. Similar reasoning goes through when her ideal is below the median. Hence she is never worse off for reporting her peak honestly. Contrast this with a mechanism that averages the peaks to get the outcome, which is not strategyproof.</p>\n<p>Choosing an outcome on the real line has many natural applications like setting the temperature of a thermostat or the sales tax rate of a city. We can go further than lines though. For instance, single-peaked preferences make sense on a tree since we can talk unambiguously about going closer or further from some point as we move along the edges. Here is one tree, with an example of single-peaked preferences for it described by utilities for each node and the peak circled:</p>\n<p style=\"padding-left: 150px;\"><img src=\"http://images.lesswrong.com/t3_kas_2.png?v=4a5a2dc17e6dd9fc62e41401aeb9c0c1\" alt=\"\" width=\"242\" height=\"303\"></p>\n<p>As on a line, we\u2019ll ask agents to report their peaks and choose the median node, i.e.&nbsp;the node that minimizes the distance to each agent\u2019s report. Suppose there are five agents, labeled <span class=\"math\"><em>a</em></span> through <span class=\"math\"><em>e</em></span>, and they report the following peaks:</p>\n<p style=\"padding-left: 150px;\"><img src=\"http://images.lesswrong.com/t3_kas_3.png?v=c21c93c3c84cdc7cca5114ecc0ad696f\" alt=\"\" width=\"242\" height=\"303\"></p>\n<p>The outcome will be the node circled in red, resulting in agent <span class=\"math\"><em>d</em></span> getting his first choice since he is the \u201cmedian\u201d of the five.</p>\n<p>Going even further, there are non-dictatorial strategyproof mechanisms on <a href=\"http://en.wikipedia.org/wiki/Median_graph\">any graph where medians are unique and well-defined</a>. We can also tweak the median rule, throwing in some dummy voters or weighting agents differently. However, it turns out that the median is essentially the only strategyproof mechanism that treats agents and outcomes symmetrically.</p>\n<h2 id=\"Discrete_exchange\">Discrete exchange</h2>\n<p>In the previous examples, the mechanism chose a single outcome for all agents. Consider instead a situation where each agent owns one object and agents might want to swap things around. Rather than a single outcome, it makes more sense to think of sub-outcomes describing who gets which object, especially if each cares only about what he ends up with and not what the others get. Indifferences over parts of the allocation not involving that agent is another preference restriction that allows us to evade dictatorship theorems.</p>\n<p>For example, three Roman soldiers have currently assigned duties which their eccentric new centurion will allow them to trade around. At the moment, Antonius is a standard-bearer, Brutus is a trumpeter, and Cato is an artilleryman. Even though the full outcome will be a list of who gets what, we consider only the preference of each over the three jobs. Perhaps Antonius has preferences <span class=\"math\"><em>t</em><em>r</em><em>u</em><em>m</em><em>p</em><em>e</em><em>t</em><em>e</em><em>r</em> \u227b\u2004<em>standard-bearer</em> \u227b\u2004<em>a</em><em>r</em><em>t</em><em>i</em><em>l</em><em>l</em><em>e</em><em>r</em><em>y</em><em>m</em><em>a</em><em>n</em></span>, Brutus has preferences <span class=\"math\"><em>standard-bearer</em> \u227b\u2004<em>a</em><em>r</em><em>t</em><em>i</em><em>l</em><em>l</em><em>e</em><em>r</em><em>y</em><em>m</em><em>a</em><em>n</em> \u227b\u2004<em>t</em><em>r</em><em>u</em><em>m</em><em>p</em><em>e</em><em>t</em><em>e</em><em>r</em></span>, and Cato has preferences <span class=\"math\"><em>t</em><em>r</em><em>u</em><em>m</em><em>p</em><em>e</em><em>t</em><em>e</em><em>r</em> \u227b\u2004<em>standard-bearer</em> \u227b\u2004<em>a</em><em>r</em><em>t</em><em>i</em><em>l</em><em>l</em><em>e</em><em>r</em><em>y</em><em>m</em><em>a</em><em>n</em></span>. Given this, it\u2019s natural to say Antonius and Brutus should switch jobs, with Cato stuck as an artilleryman.</p>\n<p>We can formalize this inclination using David Gale\u2019s <em>Top Trading Cycle</em> algorithm, which operates as follows:</p>\n<ol style=\"list-style-type: decimal\">\n<li>Each agent starts as active. Each object starts as active and pointing at the agent that owns it.</li>\n<li>Active agents point at their favorite active object.</li>\n<li>At least one cycle must exist from agent to object to agent and etc. Deactivate agents and objects in a cycle.</li>\n<li>Iterate steps 2 and 3 until all objects are deactivated. The final allocation is each object going to the agent pointing at it.</li>\n</ol>\n<p>In the example, Antonius and Cato point at <em>trumpeter</em>, with Brutus pointing at <em>standard-bearer</em>. Deactivating the Antonius <span class=\"math\"> \u2192 </span> <em>trumpeter</em> <span class=\"math\"> \u2192 </span> Brutus <span class=\"math\"> \u2192 </span> <em>standard-bearer</em> <span class=\"math\"> \u2192 </span> Antonius cycle, only Cato is left active, so he points at his own job, and the mechanism is done.</p>\n<p>As I\u2019ve told the story, the soldiers already have control over a particular job. For instance, if Antonius liked being a standard-bearer best, he could guarantee he keeps the job. What would we do if the soldiers were new and didn\u2019t have a pre-existing job? One option is to randomly assign tasks and run the algorithm from there. This is actually how <a href=\"http://www.nola.com/education/index.ssf/2012/04/centralized_enrollment_in_reco.html\">New Orleans matches K-12 students with public schools as of 2012</a>. Since the mechanism is strategyproof, parents don\u2019t have to worry about gaming the system when they rank their top choices from the 67 schools in the district<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup>. Unlike many of the toy examples I\u2019ve given, here is a real-world case of improvements made by mechanism design.</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>By restricting preferences to be single-peaked or indifferent over the allocations of others, we can find useful strategyproof mechanisms. In the next post, I\u2019ll continue exploring strategyproof mechanisms in the very fruitful special case when we can make transfers between agents, introducing the famed Vickrey-Clarke-Groves mechanisms.</p>\n<p>The idea of single-peaked preferences on a line is quite old, dating back to <a href=\"http://www.corwin.com/upm-data/24492_Dowding_Chapter_01.pdf\">Duncan Black in 1948</a>. For a full characterization of strategyproof mechanisms on a line, see <a href=\"http://www.jstor.org/stable/30023824\">Moulin (1980)</a>. For a characterization of which single-peaked preference domains admit non-dictatorial, strategyproof mechanisms, see <a href=\"http://www.uibk.ac.at/economics/bbl/bblpapieress2008/puppe_jet.pdf\">Nehring and Puppe (2007)</a>.</p>\n<p>For an overview of mechanism design aspects of school choice, see <a href=\"https://www2.bc.edu/tayfun-sonmez/AbdulkadirogluSonmez-AER2003.pdf\">Abdulkadiroglu and Sonmez (2003)</a>.</p>\n<p>&nbsp;</p>\n<p><em>Previously on:</em> <a href=\"/lw/k8r/strategyproof_mechanisms_impossibilities/\">Strategyproof Mechanisms: Impossibilities</a></p>\n<p><em>Next up:</em> Mechanism Design with Money</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p>Though technically there might be a tiny reason to misrepresent preferences since the parents since the ranking is truncated to the top eight schools rather than all 67.<a href=\"#fnref1\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Single-peaked preferences", "anchor": "Single_peaked_preferences", "level": 1}, {"title": "Discrete exchange", "anchor": "Discrete_exchange", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wE3CRBTpSSBXf9EHK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-02T11:11:01.155Z", "modifiedAt": null, "url": null, "title": "Meetup : Bangalore meetup", "slug": "meetup-bangalore-meetup-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:56.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "anandjeyahar", "createdAt": "2011-08-06T14:22:03.370Z", "isAdmin": false, "displayName": "anandjeyahar"}, "userId": "nRE6w4ErwoFMLnAfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pFatEpbmYADKAeGnu/meetup-bangalore-meetup-2", "pageUrlRelative": "/posts/pFatEpbmYADKAeGnu/meetup-bangalore-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/pFatEpbmYADKAeGnu/meetup-bangalore-meetup-2", "postedAtFormatted": "Monday, June 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bangalore%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bangalore%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFatEpbmYADKAeGnu%2Fmeetup-bangalore-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bangalore%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFatEpbmYADKAeGnu%2Fmeetup-bangalore-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFatEpbmYADKAeGnu%2Fmeetup-bangalore-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10y'>Bangalore meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 June 2014 05:30:27PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Leela palace, kodihalli, Bangalore</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi, I am looking for more study partners (aka Bayesian co-conspirators <em>wink</em>) in my attempts at improving my rationality. I am set in Bangalore, India and not sure how many of you are in Bangalore. So how about we meet up in Bangalore sometime in last weekend of June(25th or 26th ) and find out? . As for location, Well there's enough coffee shops around bangalore. I'm thinking of around Leela palace, old airport road, or perhaps leela palace (barista if my memory is right) coffee shop itself.. We can decide about what to do, and how often to meet on that meeting.</p>\n\n<p>Update: Ok people, Sorry got involved in work and forgot to update contact info. It is still on today 21st Jun 5.30 at leela palace. The coffee shop is called Lavazza. Here's a google maps link.<a href=\"https://www.google.com/maps/place/The+Leela+Palace+Bangalore/@12.9607933,77.6482773,15z/data=!3m1!4b1!4m2!3m1!1s0x3bae14067cca9bdd:0x111bbe37cc24e71a.\" rel=\"nofollow\">https://www.google.com/maps/place/The+Leela+Palace+Bangalore/@12.9607933,77.6482773,15z/data=!3m1!4b1!4m2!3m1!1s0x3bae14067cca9bdd:0x111bbe37cc24e71a.</a></p>\n\n<p>If you can't find the shop or the group or the place gimme a call on 89513655147</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10y'>Bangalore meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pFatEpbmYADKAeGnu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.763806425954667e-06, "legacy": true, "legacyId": "26311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bangalore_meetup\">Discussion article for the meetup : <a href=\"/meetups/10y\">Bangalore meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 June 2014 05:30:27PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Leela palace, kodihalli, Bangalore</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi, I am looking for more study partners (aka Bayesian co-conspirators <em>wink</em>) in my attempts at improving my rationality. I am set in Bangalore, India and not sure how many of you are in Bangalore. So how about we meet up in Bangalore sometime in last weekend of June(25th or 26th ) and find out? . As for location, Well there's enough coffee shops around bangalore. I'm thinking of around Leela palace, old airport road, or perhaps leela palace (barista if my memory is right) coffee shop itself.. We can decide about what to do, and how often to meet on that meeting.</p>\n\n<p>Update: Ok people, Sorry got involved in work and forgot to update contact info. It is still on today 21st Jun 5.30 at leela palace. The coffee shop is called Lavazza. Here's a google maps link.<a href=\"https://www.google.com/maps/place/The+Leela+Palace+Bangalore/@12.9607933,77.6482773,15z/data=!3m1!4b1!4m2!3m1!1s0x3bae14067cca9bdd:0x111bbe37cc24e71a.\" rel=\"nofollow\">https://www.google.com/maps/place/The+Leela+Palace+Bangalore/@12.9607933,77.6482773,15z/data=!3m1!4b1!4m2!3m1!1s0x3bae14067cca9bdd:0x111bbe37cc24e71a.</a></p>\n\n<p>If you can't find the shop or the group or the place gimme a call on 89513655147</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bangalore_meetup1\">Discussion article for the meetup : <a href=\"/meetups/10y\">Bangalore meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bangalore meetup", "anchor": "Discussion_article_for_the_meetup___Bangalore_meetup", "level": 1}, {"title": "Discussion article for the meetup : Bangalore meetup", "anchor": "Discussion_article_for_the_meetup___Bangalore_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-02T15:46:07.414Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta June meetup - Hacking Motivation", "slug": "meetup-atlanta-june-meetup-hacking-motivation", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DyNY3k9RSjx4C5PjZ/meetup-atlanta-june-meetup-hacking-motivation", "pageUrlRelative": "/posts/DyNY3k9RSjx4C5PjZ/meetup-atlanta-june-meetup-hacking-motivation", "linkUrl": "https://www.lesswrong.com/posts/DyNY3k9RSjx4C5PjZ/meetup-atlanta-june-meetup-hacking-motivation", "postedAtFormatted": "Monday, June 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20June%20meetup%20-%20Hacking%20Motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20June%20meetup%20-%20Hacking%20Motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDyNY3k9RSjx4C5PjZ%2Fmeetup-atlanta-june-meetup-hacking-motivation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20June%20meetup%20-%20Hacking%20Motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDyNY3k9RSjx4C5PjZ%2Fmeetup-atlanta-june-meetup-hacking-motivation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDyNY3k9RSjx4C5PjZ%2Fmeetup-atlanta-june-meetup-hacking-motivation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/10z'>Atlanta June meetup - Hacking Motivation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 June 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Oge says:\nI recently read \u201cThe Motivation Hacker\u201d by Nick Winter, a book which summarizes the state of the art in setting goals and getting our human brains to actually want to accomplish those goals. At this meetup, I\u2019ll give a short talk on the highlights of the book. Afterward, we\u2019ll collaboratively hack on any motivation issues that are brought to the group (so bring yours).\nI can\u2019t wait to see y\u2019all again!\nIf you have trouble getting to the venue, contact me at 404-542-6392\nYou can nominate future meetup topics <a href=\"https://docs.google.com/a/nnadi.org/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/10z'>Atlanta June meetup - Hacking Motivation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DyNY3k9RSjx4C5PjZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7642009006229579e-06, "legacy": true, "legacyId": "26312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation\">Discussion article for the meetup : <a href=\"/meetups/10z\">Atlanta June meetup - Hacking Motivation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 June 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Oge says:\nI recently read \u201cThe Motivation Hacker\u201d by Nick Winter, a book which summarizes the state of the art in setting goals and getting our human brains to actually want to accomplish those goals. At this meetup, I\u2019ll give a short talk on the highlights of the book. Afterward, we\u2019ll collaboratively hack on any motivation issues that are brought to the group (so bring yours).\nI can\u2019t wait to see y\u2019all again!\nIf you have trouble getting to the venue, contact me at 404-542-6392\nYou can nominate future meetup topics <a href=\"https://docs.google.com/a/nnadi.org/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation1\">Discussion article for the meetup : <a href=\"/meetups/10z\">Atlanta June meetup - Hacking Motivation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta June meetup - Hacking Motivation", "anchor": "Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation", "level": 1}, {"title": "Discussion article for the meetup : Atlanta June meetup - Hacking Motivation", "anchor": "Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T02:00:11.373Z", "modifiedAt": null, "url": null, "title": "The Promoted Posts and the Metaethics sequence now available in audio", "slug": "the-promoted-posts-and-the-metaethics-sequence-now-available", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:34.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rick_from_Castify", "createdAt": "2012-12-03T09:33:28.512Z", "isAdmin": false, "displayName": "Rick_from_Castify"}, "userId": "XyTqQupkZ9SW7nGCB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AmJAXLjoe8okW3Z8n/the-promoted-posts-and-the-metaethics-sequence-now-available", "pageUrlRelative": "/posts/AmJAXLjoe8okW3Z8n/the-promoted-posts-and-the-metaethics-sequence-now-available", "linkUrl": "https://www.lesswrong.com/posts/AmJAXLjoe8okW3Z8n/the-promoted-posts-and-the-metaethics-sequence-now-available", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Promoted%20Posts%20and%20the%20Metaethics%20sequence%20now%20available%20in%20audio&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Promoted%20Posts%20and%20the%20Metaethics%20sequence%20now%20available%20in%20audio%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmJAXLjoe8okW3Z8n%2Fthe-promoted-posts-and-the-metaethics-sequence-now-available%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Promoted%20Posts%20and%20the%20Metaethics%20sequence%20now%20available%20in%20audio%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmJAXLjoe8okW3Z8n%2Fthe-promoted-posts-and-the-metaethics-sequence-now-available", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmJAXLjoe8okW3Z8n%2Fthe-promoted-posts-and-the-metaethics-sequence-now-available", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>We are proud to announce audio versions of the <a href=\"http://castify.co/channels/51-less-wrong\">Less Wrong Promoted Posts</a> and the <a href=\"http://castify.co/channels/50-metaethics\">Metaethics</a> major sequence, both now available via a Castify Podcast.</p>\n<p>The <a href=\"http://castify.co/channels/51-less-wrong\">Less Wrong Promoted Posts</a> feed will have every new promoted post which has been tagged with the Creative Commons Attribution License.&nbsp; We'll aim to have them read and to you via the podcast within 48 hours.&nbsp; We've found this to be a good way to keep up with Less Wrong, especially for longer articles like last month's interesting long-form post called \"<a href=\"/lw/k7h/a_dialogue_on_doublethink/\">A Dialouge on Doublethink</a>\" by <a href=\"/user/BrienneStrohl/\">BrienneStrohl</a>.</p>\n<p>The Metaethics Sequence is the next installment of the sequences we've produced into audio.&nbsp; We now have 7 Less Wrong sequences in audio, with more on their way.&nbsp;</p>\n<p>As always we appreciate your support and your feedback: <a href=\"mailto:support@castify.co\">support@castify.co</a>.</p>\n<p>&nbsp;</p>\n<p>Links:</p>\n<p>Promoted Posts Subscription: <a href=\"http://castify.co/channels/51-less-wrong\">http://castify.co/channels/51-less-wrong</a></p>\n<p>Metaethics sequence: <a href=\"http://castify.co/channels/50-metaethics\">http://castify.co/channels/50-metaethics</a></p>\n<p>Channels page: <a href=\"http://castify.co/channels\">http://castify.co/channels</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AmJAXLjoe8okW3Z8n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 26, "extendedScore": null, "score": 1.7650819793016926e-06, "legacy": true, "legacyId": "26313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gRBbFTh6e3MzyojTf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T03:16:23.525Z", "modifiedAt": null, "url": null, "title": "Meetup : Southeast Michigan Meetup 6/8", "slug": "meetup-southeast-michigan-meetup-6-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:34.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eLLYqHCs94HKaNXMr/meetup-southeast-michigan-meetup-6-8", "pageUrlRelative": "/posts/eLLYqHCs94HKaNXMr/meetup-southeast-michigan-meetup-6-8", "linkUrl": "https://www.lesswrong.com/posts/eLLYqHCs94HKaNXMr/meetup-southeast-michigan-meetup-6-8", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Southeast%20Michigan%20Meetup%206%2F8&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Southeast%20Michigan%20Meetup%206%2F8%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLLYqHCs94HKaNXMr%2Fmeetup-southeast-michigan-meetup-6-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Southeast%20Michigan%20Meetup%206%2F8%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLLYqHCs94HKaNXMr%2Fmeetup-southeast-michigan-meetup-6-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLLYqHCs94HKaNXMr%2Fmeetup-southeast-michigan-meetup-6-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/110'>Southeast Michigan Meetup 6/8</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 June 2014 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">19334 Angling Street, Livonia, MI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our more-or-less bimonthly Ann Arbor + Detroit area meetup. Myself and Ozy will be there and we're hoping to get a couple new people from Ann Arbor as well. If it lasts until dinnertime, we'll probably go out for sushi in accordance with the tradition.</p>\n\n<p>No particular topic, but bring games and discussion topics as they interest you.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/110'>Southeast Michigan Meetup 6/8</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eLLYqHCs94HKaNXMr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "26314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Southeast_Michigan_Meetup_6_8\">Discussion article for the meetup : <a href=\"/meetups/110\">Southeast Michigan Meetup 6/8</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 June 2014 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">19334 Angling Street, Livonia, MI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our more-or-less bimonthly Ann Arbor + Detroit area meetup. Myself and Ozy will be there and we're hoping to get a couple new people from Ann Arbor as well. If it lasts until dinnertime, we'll probably go out for sushi in accordance with the tradition.</p>\n\n<p>No particular topic, but bring games and discussion topics as they interest you.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Southeast_Michigan_Meetup_6_81\">Discussion article for the meetup : <a href=\"/meetups/110\">Southeast Michigan Meetup 6/8</a></h2>", "sections": [{"title": "Discussion article for the meetup : Southeast Michigan Meetup 6/8", "anchor": "Discussion_article_for_the_meetup___Southeast_Michigan_Meetup_6_8", "level": 1}, {"title": "Discussion article for the meetup : Southeast Michigan Meetup 6/8", "anchor": "Discussion_article_for_the_meetup___Southeast_Michigan_Meetup_6_81", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T08:57:43.756Z", "modifiedAt": null, "url": null, "title": "Open thread, 3-8 June 2014", "slug": "open-thread-3-8-june-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X963YsBZG8dqWypRu/open-thread-3-8-june-2014", "pageUrlRelative": "/posts/X963YsBZG8dqWypRu/open-thread-3-8-june-2014", "linkUrl": "https://www.lesswrong.com/posts/X963YsBZG8dqWypRu/open-thread-3-8-june-2014", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%203-8%20June%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%203-8%20June%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX963YsBZG8dqWypRu%2Fopen-thread-3-8-june-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%203-8%20June%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX963YsBZG8dqWypRu%2Fopen-thread-3-8-june-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX963YsBZG8dqWypRu%2Fopen-thread-3-8-june-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p style=\"text-align: justify;\">Previous Open Thread:&nbsp; <a href=\"/r/discussion/lw/k94/open_thread_may_19_25_2014/\">http://lesswrong.com/r/discussion/lw/k9x/open_thread_may_26_june_1_2014/<br /></a></p>\n<p style=\"text-align: justify;\">(oops, we missed a day!)</p>\n<p style=\"text-align: justify;\"><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. Open Threads should start on Monday, and end on Sunday.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. Open Threads should be posted in Discussion, and not Main.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X963YsBZG8dqWypRu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 153, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8Q88eZ4EF2PGbxjcQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T09:57:47.404Z", "modifiedAt": null, "url": null, "title": "Wondering what to do with my ability for empathy and understanding people. Have some experience and perhaps opportunity to work with this professionally - advice?", "slug": "wondering-what-to-do-with-my-ability-for-empathy-and", "viewCount": null, "lastCommentedAt": "2014-06-09T12:07:35.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raythen", "createdAt": "2014-04-15T06:18:12.864Z", "isAdmin": false, "displayName": "Raythen"}, "userId": "hTEaDWNY2SvWDFhBJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Thmu79MDNENom469w/wondering-what-to-do-with-my-ability-for-empathy-and", "pageUrlRelative": "/posts/Thmu79MDNENom469w/wondering-what-to-do-with-my-ability-for-empathy-and", "linkUrl": "https://www.lesswrong.com/posts/Thmu79MDNENom469w/wondering-what-to-do-with-my-ability-for-empathy-and", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wondering%20what%20to%20do%20with%20my%20ability%20for%20empathy%20and%20understanding%20people.%20Have%20some%20experience%20and%20perhaps%20opportunity%20to%20work%20with%20this%20professionally%20-%20advice%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWondering%20what%20to%20do%20with%20my%20ability%20for%20empathy%20and%20understanding%20people.%20Have%20some%20experience%20and%20perhaps%20opportunity%20to%20work%20with%20this%20professionally%20-%20advice%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThmu79MDNENom469w%2Fwondering-what-to-do-with-my-ability-for-empathy-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wondering%20what%20to%20do%20with%20my%20ability%20for%20empathy%20and%20understanding%20people.%20Have%20some%20experience%20and%20perhaps%20opportunity%20to%20work%20with%20this%20professionally%20-%20advice%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThmu79MDNENom469w%2Fwondering-what-to-do-with-my-ability-for-empathy-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThmu79MDNENom469w%2Fwondering-what-to-do-with-my-ability-for-empathy-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 453, "htmlBody": "<div><span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">(I've intentionally tried to keep this post concise. Please ask if you want more details about something)&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">I've done some free NVC-based empathy work starting two years ago (online, via Skype call).</span>&nbsp; <br style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\" /><span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">(NVC is a communication method; the specific methods used are empathic listening and reflection).&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Lately people told me I'm really good at it, and should with it professionally. I think I would enjoy that, and it does match my career aspirations.&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">I'm going to have free time over the summer to do something with this if I want to.&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">(I'm 25. I live in Sweden)&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">---</span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">My general ideas are along the lines of...&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Get more information, potentially connect with people who already work with this and/or similar topics.&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Research study/certification options - there are some, though I haven't found one that seems like a good fit for me yet.&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Register a corporation - since I already do work that generates value for others, I could open the door to doing it professionally. (Also, the registration is free of charge)&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">---</span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">I'm not equally good at getting a read for people who I know or converse with only casually (not enough data).&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Slow text-only communication (e-mail etc) - can't iterate fast enough. Text chat... might work but loses a lot of bandwidth compared to voice. Voice works best (or doing it in person).&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">---</span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">I am quite good at understanding people even with few things in common, and across quite sensitive topics (which requires a non-judgemental non-critical approach). The only requirement I guess is that the person actually wants to be deeply understood, and have that understanding reflected back verbally.&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">---</span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Some reported benefits</span>&nbsp; <br style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\" /><span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Increased clarity and self-understanding</span>&nbsp; <br style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\" /><span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Inspiration and clarity when it comes to specific goals and actions</span>&nbsp; <br style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\" /><span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Increased awareness of own values&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">---</span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">So I'm wondering what to do with all this. I could use some thoughts/advice - I the share the rationalist viewpoint on life and most of the rationalist values.&nbsp; <br /></span>\n<div style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\">&nbsp; <br /></div>\n<span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">Some additional info</span>&nbsp; <br style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium;\" /><span style=\"color: #000000; font-family: Tahoma; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; display: inline !important; float: none;\">I am a Swedish citizen, which gives me free passage and right of residence within the European Union. I speak good English (and all the empathy work I've done has been in English). I am potentially open to relocating - Sweden has less than 10 million residents so I figure I might need to at some point (though I'd rather stick to English-speaking countries. I have a slight preference for warmer climates).&nbsp; <br /></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Thmu79MDNENom469w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 1.76576779195521e-06, "legacy": true, "legacyId": "26316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T14:38:20.341Z", "modifiedAt": null, "url": null, "title": "All discussion post titles, points, and dates as an excel sheet", "slug": "all-discussion-post-titles-points-and-dates-as-an-excel", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:34.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8oiHNzD6i4eActpu7/all-discussion-post-titles-points-and-dates-as-an-excel", "pageUrlRelative": "/posts/8oiHNzD6i4eActpu7/all-discussion-post-titles-points-and-dates-as-an-excel", "linkUrl": "https://www.lesswrong.com/posts/8oiHNzD6i4eActpu7/all-discussion-post-titles-points-and-dates-as-an-excel", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20All%20discussion%20post%20titles%2C%20points%2C%20and%20dates%20as%20an%20excel%20sheet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAll%20discussion%20post%20titles%2C%20points%2C%20and%20dates%20as%20an%20excel%20sheet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8oiHNzD6i4eActpu7%2Fall-discussion-post-titles-points-and-dates-as-an-excel%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=All%20discussion%20post%20titles%2C%20points%2C%20and%20dates%20as%20an%20excel%20sheet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8oiHNzD6i4eActpu7%2Fall-discussion-post-titles-points-and-dates-as-an-excel", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8oiHNzD6i4eActpu7%2Fall-discussion-post-titles-points-and-dates-as-an-excel", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>You can find it&nbsp;<a href=\"https://free-ec2.scraperwiki.com/eijkvzg/qytkm3phlgndb6o/http/all_tables.xlsx\">here</a>.</p>\n<p>Earlier today I wanted to quantify whether lesswrong has stopped being a well kept garden. So I wrote a scraper to produce the above dataset, so that anyone that wants to do the analysis, can.</p>\n<p>All data is as of a few minutes ago.</p>\n<p>For programmers: You can see the source <a href=\"http://ideone.com/XYrthV\">here</a>, it's made to run on <a href=\"https://scraperwiki.com/\">scraperwiki</a>, but it will time out after about 3000 articles. At that point you need to adjust the initial value of the uri variable to be the last uri printed. Repeating this process once more will allow you to reach the end. Have fun.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8oiHNzD6i4eActpu7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 27, "extendedScore": null, "score": 1.7661708672930815e-06, "legacy": true, "legacyId": "26318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T15:03:57.477Z", "modifiedAt": null, "url": null, "title": "[LINK] How Do Top Students Study?", "slug": "link-how-do-top-students-study", "viewCount": null, "lastCommentedAt": "2014-06-04T03:24:03.233Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ac4wwmBB5tnqrDoiY/link-how-do-top-students-study", "pageUrlRelative": "/posts/ac4wwmBB5tnqrDoiY/link-how-do-top-students-study", "linkUrl": "https://www.lesswrong.com/posts/ac4wwmBB5tnqrDoiY/link-how-do-top-students-study", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20Do%20Top%20Students%20Study%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20Do%20Top%20Students%20Study%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fac4wwmBB5tnqrDoiY%2Flink-how-do-top-students-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20Do%20Top%20Students%20Study%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fac4wwmBB5tnqrDoiY%2Flink-how-do-top-students-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fac4wwmBB5tnqrDoiY%2Flink-how-do-top-students-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 495, "htmlBody": "<p>I found this <a href=\"http://www.quora.com/qemail/track_click?uid=apGXrKr5UUF&amp;aoid=1nY1R5lNumB&amp;request_id=925846367815322391&amp;aoty=1&amp;et=2&amp;ty_data=1nY1R5lNumB&amp;id=RGmfukgOpCZL878rzOrJAQ%3D%3D&amp;ct=1401791507653403&amp;src=1&amp;ty=1&amp;click_pos=1&amp;st=1401791523053966&amp;source=1&amp;stories=1_1nY1R5lNumB%7C1_o3ZpWKWnfWh%7C1_Xf3ql9DO3tj%7C1_4EQ9UDCOeVQ%7C1_F6G6GwoE1Sh%7C1_jGKtPGEXoNi%7C1_Fv155nzhOgU%7C1_b4XKhUNBuCK%7C1_RYuBc56uwLQ%7C1_8Th4jyt2h61&amp;v=0&amp;aty=4\">Quora discussion very informative.</a></p>\n<blockquote>2. Develop the ability to become an active reader. Don't just passively read material you are given. But pose questions, develop hypotheses and actively test them as you read through the material. I think this is what another poster referred to when he advised that you should develop a \"mental model\" of whatever concept they are teaching you. Having a mental model will give you the intuition and ability to answer a wider range of questions than would be otherwise possible if you lacked such a mental model.\n<p>Where do you get this model? You creatively develop one as you are reading to try to explain the facts as they are presented to you. Sometimes you have to guess the model based on scarce evidence. Sometimes it is handed to you. If your model is a good one it should at least be able to explain what you are reading.</p>\n<p>Having a model also tells you what to look for to disprove it -- so you can be hypersensitive for this disconfirming evidence. In fact, while you are reading you should be making predictions (in the form of one or more scenarios of where the narrative could lead) and carefully checking if the narrative is going there. You should also be making predictions and seeking contradictions to these predictions -- so you can quickly find out if your model is wrong.</p>\n<p>Sometimes you may have two or more different models that can explain the evidence, so you task will be to quickly formulate questions that can prove one model while disconfirming the others. I suggest focusing on raising questions that could confirm/disprove the mostly likely one while disproving the others (think: differential diagnoses in medicine).</p>\n<p>But once you have such a model that (i) explains the evidence and (ii) passes all the disconfirming tests you can throw at it then you have something you can interpolate and extrapolate from to answer far more than was initially explained to you.</p>\n<p>Such models also makes retention easier because you only need to remember the model as opposed to the endless array of facts it explains. Of course, your model could be wrong, but that is why you actively test it as you are reading, and adjust it as necessary. Think of this process as the scientific method being applied by you, to try to discover the truth as best you can..</p>\n<p>Sometimes you will still be left with contradictions. I often found speaking to the professor after class was an efficient way of resolving them.</p>\n</blockquote>\n<p>The author lists 8 other criteria, but this one had the biggest \"light bulb\" moment for me.</p>\n<p>It was interesting to me because I intuitively would use this technique while listening/taking notes during lectures. But I never actually made a conscious decision to apply this consistently in all of my classes; it would only happen in classes I was interested in.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ac4wwmBB5tnqrDoiY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "26319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2014-06-03T15:03:57.477Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T17:40:40.420Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels - Neuroatypicality", "slug": "meetup-brussels-neuroatypicality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kjveMHfEd5kePY3uu/meetup-brussels-neuroatypicality", "pageUrlRelative": "/posts/kjveMHfEd5kePY3uu/meetup-brussels-neuroatypicality", "linkUrl": "https://www.lesswrong.com/posts/kjveMHfEd5kePY3uu/meetup-brussels-neuroatypicality", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20-%20Neuroatypicality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20-%20Neuroatypicality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjveMHfEd5kePY3uu%2Fmeetup-brussels-neuroatypicality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20-%20Neuroatypicality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjveMHfEd5kePY3uu%2Fmeetup-brussels-neuroatypicality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjveMHfEd5kePY3uu%2Fmeetup-brussels-neuroatypicality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/111'>Brussels - Neuroatypicality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 June 2014 07:40:35PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>According to a 2010 <a href=\"http://lesswrong.com/lw/2am/aspergers_survey_reresults/\">unscientific internet survey</a>, over 5% of LessWrong users have Asperger syndrome, which is maybe 25 times more than the general population.</p>\n\n<p>I know, I know. On a website that's all about hyper-rationality and breaking down social interactions into their game theory origin in order to design an AI that can do them better? Shocking.</p>\n\n<p>In this month's meetup (what little of it we'll spend being on-topic), we will discuss what a boring world it would be if we were all the same, and how to rephrase this saying to appeal to people who think boring is good.</p>\n\n<hr />\n\n<p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. Depending on the weather, we may also spend some time outside. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">a Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/111'>Brussels - Neuroatypicality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kjveMHfEd5kePY3uu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.766432921694109e-06, "legacy": true, "legacyId": "26320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels___Neuroatypicality\">Discussion article for the meetup : <a href=\"/meetups/111\">Brussels - Neuroatypicality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 June 2014 07:40:35PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>According to a 2010 <a href=\"http://lesswrong.com/lw/2am/aspergers_survey_reresults/\">unscientific internet survey</a>, over 5% of LessWrong users have Asperger syndrome, which is maybe 25 times more than the general population.</p>\n\n<p>I know, I know. On a website that's all about hyper-rationality and breaking down social interactions into their game theory origin in order to design an AI that can do them better? Shocking.</p>\n\n<p>In this month's meetup (what little of it we'll spend being on-topic), we will discuss what a boring world it would be if we were all the same, and how to rephrase this saying to appeal to people who think boring is good.</p>\n\n<hr>\n\n<p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. Depending on the weather, we may also spend some time outside. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">a Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels___Neuroatypicality1\">Discussion article for the meetup : <a href=\"/meetups/111\">Brussels - Neuroatypicality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels - Neuroatypicality", "anchor": "Discussion_article_for_the_meetup___Brussels___Neuroatypicality", "level": 1}, {"title": "Discussion article for the meetup : Brussels - Neuroatypicality", "anchor": "Discussion_article_for_the_meetup___Brussels___Neuroatypicality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nTqmCQvqsrJrryE2c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T18:09:21.573Z", "modifiedAt": null, "url": null, "title": "The Benefits of Closed-Mindedness", "slug": "the-benefits-of-closed-mindedness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:28.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JosephY", "createdAt": "2014-05-27T21:38:25.902Z", "isAdmin": false, "displayName": "JosephY"}, "userId": "DAtyunnYpc88fS3do", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HGpB4pspHQS9RMrtT/the-benefits-of-closed-mindedness", "pageUrlRelative": "/posts/HGpB4pspHQS9RMrtT/the-benefits-of-closed-mindedness", "linkUrl": "https://www.lesswrong.com/posts/HGpB4pspHQS9RMrtT/the-benefits-of-closed-mindedness", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Benefits%20of%20Closed-Mindedness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Benefits%20of%20Closed-Mindedness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGpB4pspHQS9RMrtT%2Fthe-benefits-of-closed-mindedness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Benefits%20of%20Closed-Mindedness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGpB4pspHQS9RMrtT%2Fthe-benefits-of-closed-mindedness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGpB4pspHQS9RMrtT%2Fthe-benefits-of-closed-mindedness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 559, "htmlBody": "<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">Every so often, I will have a discussion with someone who wants to share their new &ldquo;big idea&rdquo; with me. Some of them make sense. Others, less so. For example, it was recently proposed to me that everyone has a soul, and it is the pattern of electricity in your brain. This pattern lives on after you die. The rather scary thing is that this idea was suggested to me by a neuroscientist getting her Ph.D. Aside from wondering &ldquo;what does that even mean?&rdquo;, one cannot help but notice the <a href=\"/lw/i7/belief_as_attire/\" target=\"_self\">belief as attire</a> </span><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">in the idea. </span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">And invariably, after objecting to these strange ideas, I will be told, \"Don't be so closed-minded! There is so much that we don't know!\"</span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">Now, this is a strange form of belief as attire. It is the belief of the sophisticated person, who knows that <a href=\"/lw/mm/the_fallacy_of_gray/\" target=\"_self\">since everything is a shade of gray, all is equal</a>. It is very much rooted in <a href=\"/lw/uy/dark_side_epistemology/\" target=\"_self\">dark side epistemology</a>. In acknowledging their ignorance, they glory in the fundamental unknowability of the universe. \"After all, if we don't know the truth, all explanations are equal! Who's to say that I am wrong? You can't disprove my theory!\"</span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">In general, I like to think of myself as open-minded. I support gay marriage, I am pro-choice, etc. And yet, doesn't <a href=\"http://paulgraham.com/say.html\" target=\"_self\">everyone think they are open-minded</a>? Do I discard legitimately promising ideas? Do I make too many false negative errors? I thought about it for some time and came to the conclusion: No, that idea was just plain silly. </span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; font-weight: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">Sometimes, when faced with a new idea, the instinct is to discard it out of hand. Sometimes we try not to believe new ideas, especially if they contradict long-held and deeply-rooted beliefs. And occasionally, the idea is correct, and you really <em>do </em>need to do a mental overhaul. However, that is often not the case.</span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; font-weight: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">A few million results come up in a google search for \"<a href=\"https://www.google.com/search?q=homeopathy+benefits&amp;rlz=1CAACAG_enUS586US586&amp;oq=homeopathy+bene&amp;aqs=chrome.1.69i57j0l5.5429j0j7&amp;sourceid=chrome&amp;es_sm=93&amp;ie=UTF-8\" target=\"_self\">benefits of homeopathy</a>\". However, I do not entertain homeopathy as a legitimate means of curing ailments. I have been told repeatedly of the existence of God. However, after the point where I understood the notion of \"<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">beliefs as anticipation-controllers</a>\", I held a strictly naturalistic worldview. I am dismissive of theories that do not fit this worldview. </span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; font-weight: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">I am skeptical to the extreme of implausible ideas. That is, after all, what closed-mindedness is. The measure of open-mindedness is merely about which ideas seem implausible to me. I tend to believe that if scientific education was better and more widespread, then people would become more skeptical of ideas that don't make sense. Of course, there is always the difficulty that one might end up being skeptical of strange but true ideas, such as cryonics. </span></span></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; font-weight: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\"><span style=\"line-height: 1.15; vertical-align: baseline; background-color: transparent;\">So then the real benefit of closed-mindedness is this: it saves you the time of having to entertain silly notions. But remember the danger in too much of a good thing! Some wacky ideas are true. A simple test is to list as many problems with the idea as you can think of in one minute. If you've listed three or more seemingly intractable problems, and the one explaining it to you cannot solve them, then being closed-minded is probably a good idea. If, however, you can only think up a couple of problems, or the one can dispel your doubts, then it may be time to look into the idea further.</span></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HGpB4pspHQS9RMrtT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "26309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nYkMLFpx77Rz3uo9c", "dLJv2CoRCgeC2mPgj", "XTWkjCJScy2GFAgDt", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T18:37:26.222Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley LW meetup - CFAR test session", "slug": "meetup-berkeley-lw-meetup-cfar-test-session", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pcm", "createdAt": "2017-06-17T00:51:23.973Z", "isAdmin": false, "displayName": "pcm"}, "userId": "9bxscuK69SnmpBqSA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dnoycC6xG42AmWhrw/meetup-berkeley-lw-meetup-cfar-test-session", "pageUrlRelative": "/posts/dnoycC6xG42AmWhrw/meetup-berkeley-lw-meetup-cfar-test-session", "linkUrl": "https://www.lesswrong.com/posts/dnoycC6xG42AmWhrw/meetup-berkeley-lw-meetup-cfar-test-session", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20LW%20meetup%20-%20CFAR%20test%20session&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20LW%20meetup%20-%20CFAR%20test%20session%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdnoycC6xG42AmWhrw%2Fmeetup-berkeley-lw-meetup-cfar-test-session%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20LW%20meetup%20-%20CFAR%20test%20session%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdnoycC6xG42AmWhrw%2Fmeetup-berkeley-lw-meetup-cfar-test-session", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdnoycC6xG42AmWhrw%2Fmeetup-berkeley-lw-meetup-cfar-test-session", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/112'>Berkeley LW meetup - CFAR test session</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 June 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>CFAR is gearing up for a workshop this weekend, and they expect to hold at least one rationality training session Wednesday night (June 4) in order to test their pedagogy. Since this overlaps our usual Less Wrong meetup, it's going to be the main activity of the evening: You're encouraged to attend the beta-testing session(s?) in the couch room; but if you don't want to or if there's too many of us, you can hang out with other Less Wrongers in the CFAR/MIRI common area.\nI encourage you to arrive around 7pm at the CFAR office which is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at 408 315-8120. Note that the door to the building gets locked at 7pm, so it's easier if you come a few minutes before then.\nI'll try to update this with more specifics about the test session(s) if I get them.</p>\n\n<p>Update: there will be a session on 5-minute exercises, and one on reference class forecasting. The plan is to end by 10pm, but these beta sessions have not been known for punctuality in the recent past.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/112'>Berkeley LW meetup - CFAR test session</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dnoycC6xG42AmWhrw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7665145169200445e-06, "legacy": true, "legacyId": "26322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_LW_meetup___CFAR_test_session\">Discussion article for the meetup : <a href=\"/meetups/112\">Berkeley LW meetup - CFAR test session</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 June 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>CFAR is gearing up for a workshop this weekend, and they expect to hold at least one rationality training session Wednesday night (June 4) in order to test their pedagogy. Since this overlaps our usual Less Wrong meetup, it's going to be the main activity of the evening: You're encouraged to attend the beta-testing session(s?) in the couch room; but if you don't want to or if there's too many of us, you can hang out with other Less Wrongers in the CFAR/MIRI common area.\nI encourage you to arrive around 7pm at the CFAR office which is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at 408 315-8120. Note that the door to the building gets locked at 7pm, so it's easier if you come a few minutes before then.\nI'll try to update this with more specifics about the test session(s) if I get them.</p>\n\n<p>Update: there will be a session on 5-minute exercises, and one on reference class forecasting. The plan is to end by 10pm, but these beta sessions have not been known for punctuality in the recent past.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_LW_meetup___CFAR_test_session1\">Discussion article for the meetup : <a href=\"/meetups/112\">Berkeley LW meetup - CFAR test session</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley LW meetup - CFAR test session", "anchor": "Discussion_article_for_the_meetup___Berkeley_LW_meetup___CFAR_test_session", "level": 1}, {"title": "Discussion article for the meetup : Berkeley LW meetup - CFAR test session", "anchor": "Discussion_article_for_the_meetup___Berkeley_LW_meetup___CFAR_test_session1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-03T18:52:56.175Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels - We meet every month", "slug": "meetup-brussels-we-meet-every-month", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R6mnARcLJBTD23CMY/meetup-brussels-we-meet-every-month", "pageUrlRelative": "/posts/R6mnARcLJBTD23CMY/meetup-brussels-we-meet-every-month", "linkUrl": "https://www.lesswrong.com/posts/R6mnARcLJBTD23CMY/meetup-brussels-we-meet-every-month", "postedAtFormatted": "Tuesday, June 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20-%20We%20meet%20every%20month&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20-%20We%20meet%20every%20month%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6mnARcLJBTD23CMY%2Fmeetup-brussels-we-meet-every-month%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20-%20We%20meet%20every%20month%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6mnARcLJBTD23CMY%2Fmeetup-brussels-we-meet-every-month", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6mnARcLJBTD23CMY%2Fmeetup-brussels-we-meet-every-month", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/113'>Brussels - We meet every month</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 January 2019 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is a long term meetup announcement to ensure there's always a Brussels meetup visible in the sidebar. We're not sure we'll actually meet on the date given here, but there'll definitely be meetups in the meantime. If this month's meetup hasn't been announced on LW, it's probably just procrastination from the organizer.</p>\n\n<p>We meet every second Saturday of the month at 1pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. The meetings are in English to facilitate both French and Dutch speaking members.</p>\n\n<p>Our main web page for meetup announcements and conversations is <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/113'>Brussels - We meet every month</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R6mnARcLJBTD23CMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.766536797604104e-06, "legacy": true, "legacyId": "26323", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels___We_meet_every_month\">Discussion article for the meetup : <a href=\"/meetups/113\">Brussels - We meet every month</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 January 2019 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is a long term meetup announcement to ensure there's always a Brussels meetup visible in the sidebar. We're not sure we'll actually meet on the date given here, but there'll definitely be meetups in the meantime. If this month's meetup hasn't been announced on LW, it's probably just procrastination from the organizer.</p>\n\n<p>We meet every second Saturday of the month at 1pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. The meetings are in English to facilitate both French and Dutch speaking members.</p>\n\n<p>Our main web page for meetup announcements and conversations is <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels___We_meet_every_month1\">Discussion article for the meetup : <a href=\"/meetups/113\">Brussels - We meet every month</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels - We meet every month", "anchor": "Discussion_article_for_the_meetup___Brussels___We_meet_every_month", "level": 1}, {"title": "Discussion article for the meetup : Brussels - We meet every month", "anchor": "Discussion_article_for_the_meetup___Brussels___We_meet_every_month1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-04T02:16:21.000Z", "modifiedAt": null, "url": null, "title": "Asches to Asches", "slug": "asches-to-asches", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pfmZ5cYQCahABGZzi/asches-to-asches", "pageUrlRelative": "/posts/pfmZ5cYQCahABGZzi/asches-to-asches", "linkUrl": "https://www.lesswrong.com/posts/pfmZ5cYQCahABGZzi/asches-to-asches", "postedAtFormatted": "Wednesday, June 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Asches%20to%20Asches&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsches%20to%20Asches%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfmZ5cYQCahABGZzi%2Fasches-to-asches%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Asches%20to%20Asches%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfmZ5cYQCahABGZzi%2Fasches-to-asches", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfmZ5cYQCahABGZzi%2Fasches-to-asches", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2623, "htmlBody": "<p><font size=\"1\"><i>[Content note: fictional story contains gaslighting-type elements. May induce Cartesian skepticism]</i></font></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There&#8217;s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>&#8220;Hi,&#8221; she says. &#8220;This is the real world. You used to live here. We erased your memories and stuck you in a simulated world for a while, like in <i>The Matrix</i>. It was part of a great experiment.&#8221;</p>\n<p>&#8220;What?&#8221; you shout. &#8220;My whole life, a lie? How dare you deceive me as part of some grand &#8216;experiment&#8217; I never consented to?&#8221;</p>\n<p>&#8220;Oh,&#8221; said the woman, &#8220;actually, you did consent, in exchange for extra credit in your undergraduate psychology course.&#8221; She hands you the clipboard. There is a consent form with your name on it, in your handwriting.</p>\n<p>You give her a sheepish look. &#8220;What was the experiment?&#8221;</p>\n<p>&#8220;You know families?&#8221; asks the woman.</p>\n<p>&#8220;Of course,&#8221; you say.</p>\n<p>&#8220;Yeah,&#8221; says the woman. &#8220;Not really a thing. Like, if you think about it, it doesn&#8217;t make any sense. Why would you care more for your genetic siblings and cousins and whoever than for your friends and people who are genuinely close to you? That&#8217;s like racism &#8211; but even worse, at least racists identify with a group of millions of people instead of a group of half a dozen. Why should parents have to raise children whom they might not even like, who might have been a total accident? Why should people, motivated by guilt, make herculean efforts to &#8220;keep in touch&#8221; with some nephew or cousin whom they clearly would be perfectly happy to ignore entirely?&#8221;</p>\n<p>&#8220;Uh,&#8221; you say, &#8220;not really in the mood for philosophy. Families have been around forever and they aren&#8217;t going anywhere, who cares?&#8221;</p>\n<p>&#8220;Actually,&#8221; says the woman, &#8220;in the real world, no one believes in family. There&#8217;s no such thing. Children are taken at birth from their parents and given to people who contract to raise them in exchange for a fixed percent of their future earnings.&#8221;</p>\n<p>&#8220;That&#8217;s monstrous!&#8221; you say. &#8220;When did this happen? Weren&#8217;t there protests?&#8221;</p>\n<p>&#8220;It&#8217;s always been this way,&#8221; says the woman. &#8220;There&#8217;s <i>never</i> been such a thing as the family. Listen. You were part of a study a lot like the <A HREF=\"http://en.wikipedia.org/wiki/Asch_conformity_experiments\">Asch Conformity Experiment</A>. Our goal was to see if people, raised in a society where everyone believed X and everything revolved around X, would even be <i>capable</i> of questioning X or noticing it was stupid. We tried to come up with the stupidest possible belief, something no one in the real world had ever believed or ever seemed likely to, to make sure that we were isolating the effect of conformity and not of there being a legitimate argument for something. So we chose this idea of &#8216;family&#8217;. There are racists in our world, we&#8217;re not perfect, but as far as I know none of them has <i>ever</i> made the claim that you should devote extra resources to the people genetically closest to you. That&#8217;s like a <i>reductio ad absurdum</i> of racism. So we got a grad student to simulate a world where this bizarre idea was the unquestioned status quo, and stuck twenty bright undergraduates in it to see if they would conform, or question the premise.&#8221;</p>\n<p>&#8220;Of course we won&#8217;t question the premise, the premise is&#8230;&#8221;</p>\n<p>&#8220;Sorry to cut you off, but I thought you should know that every single one of the other nineteen subjects, upon reaching the age where the brain they were instantiated in was capable of abstract reason, immediately determined that the family structure made no sense. One of them actually deduced that she was in a psychology experiment, because there was no other explanation for why everyone believed such a bizarre premise. The other eighteen just assumed that sometimes objectively unjustifiable ideas caught on, the same way that everyone in the antebellum American South thought slavery was perfectly natural and only a few abolitionists were able to see through it. Our conformity experiment <i>failed</i>. You were actually the only one to fall for it, hook line and sinker.&#8221;</p>\n<p>&#8220;How could I be the only one?&#8221;</p>\n<p>&#8220;We don&#8217;t know. Your test scores show you&#8217;re of just-above-average intelligence, so it&#8217;s not that you&#8217;re stupid. But we did give all participants a personality test that showed you have very high extraversion. The conclusion of our paper is going to be that very extraverted participants adopt group consensus without thinking and can be led to believe anything, even something as ridiculous as &#8216;family'&#8221;.</p>\n<p>&#8220;I guess&#8230;when you put it like that it is kind of silly. Like, my parents were never that nice to me, but I kept loving them anyway, liking them even more than other people who treated me a lot better &#8211; and god, I even gave my mother a &#8220;WORLD&#8217;S #1 MOM&#8221; mug for Mother&#8217;s Day. That doesn&#8217;t even make sense! I&#8230;but what about the evolutionary explanation? Doesn&#8217;t evolution say we have genetic imperatives to love and support our family, whether they are worthy of it or not?&#8221;</p>\n<p>&#8220;You can make a just-so story for <i>anything</i> using evolutionary psychology. Someone as smart as you should know better than to take them seriously.&#8221;</p>\n<p>&#8220;But then, what <i>is</i> evolution? How did animals reproduce before the proper economic incentives were designed? Where did&#8230;&#8221;</p>\n<p>&#8220;Tell you what. Let&#8217;s hook you up to the remnemonizer to give you your real memories back. That should answer a lot of your questions.&#8221; </p>\n<p>A machine hovering over you starts to glow purple. &#8220;This shouldn&#8217;t hurt you a bit&#8230;&#8221;</p>\n<p><b>>discontinuity<</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There&#8217;s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>&#8220;Hi,&#8221; she said. &#8220;There&#8217;s no such thing as virtual reality. I hypnotized you to forget all your memories from the past day and to become very confused. Then I put you in an old prop from <i>The Matrix</i> I bought off of eBay and fed you that whole story.&#8221;</p>\n<p>&#8220;What?&#8221; you shout. &#8220;You can&#8217;t just go hypnotizing and lying to people without their consent!&#8221;</p>\n<p>&#8220;Oh,&#8221; said the woman, &#8220;actually, you did consent, in exchange for extra credit in your undergraduate psychology course.&#8221; She hands you the clipboard. There is a consent form with your name on it, in your handwriting. &#8220;That part was true.&#8221;</p>\n<p>You give her a sheepish look. &#8220;Why would you do such a thing?&#8221;</p>\n<p>&#8220;Well,&#8221; said the woman. &#8220;You know the Asch Conformity Experiment? I was really interested in whether you could get people to abandon some of their most fundamental beliefs, just by telling them other people believed differently. But I couldn&#8217;t think of a way to test it. I mean, part of a belief being fundamental is that you already <i>know</i> everyone else believes it. There&#8217;s no way I could convince subjects that the whole world was against something as obvious as &#8216;the family&#8217; when they already know how things stand.</p>\n<p>&#8220;So I dreamt up the weird &#8216;virtual reality&#8217; story. I figured I would convince subjects that the real world was a lie, and that in some &#8216;super-real&#8217; world supposedly <i>everybody knew</i> that the family was stupid, that it wasn&#8217;t even an idea <i>worth considering</i>. I wanted to know how many people would give up something they&#8217;ve believed in for their entire life, just because they&#8217;re told that &#8216;nobody else thinks so'&#8221;.</p>\n<p>&#8220;Oh,&#8221; I said. &#8220;Interesting. So even our most cherished beliefs are more fragile than we think.&#8221;</p>\n<p>&#8220;Not <i>really</i>,&#8221; said the woman. &#8220;Of twenty subjects, you were the only person I got to feel any doubt, or to express any kind of anti-family sentiment.&#8221;</p>\n<p>&#8220;Frick,&#8221; you say. &#8220;I feel like an idiot now. What if my mother finds out? She&#8217;ll think it&#8217;s her fault or something. God, she&#8217;ll think I don&#8217;t love her. People are going to be talking about this one <i>forever</i>.&#8221;</p>\n<p>&#8220;Don&#8217;t worry,&#8221; says the woman. &#8220;We&#8217;ll keep you anonymized in the final data. Anyway, let&#8217;s get you your memories back so you can leave and be on your way.&#8221;</p>\n<p>&#8220;You can restore my memories?&#8221; you say.</p>\n<p>&#8220;Of course. We hypnotized you to forget the last day&#8217;s events until you heard a trigger word. And that trigger is&#8230;&#8221;</p>\n<p><b>>discontinuity<</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There&#8217;s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>&#8220;Hi,&#8221; she says. &#8220;Hypnosis is a pseudoscience and doesn&#8217;t work. It was the virtual reality one, all along.&#8221;</p>\n<p>&#8220;Wut,&#8221; you say.</p>\n<p>&#8220;I mean, the first story was true. All of your memories of living with your family and so on are fake memories from a virtual world, like in <i>The Matrix</i>. The concept of &#8216;family&#8217; really is totally ridiculous and no one in the real world believes it. All the stuff you heard first was true. The stuff about hypnosis and getting a prop from <i>The Matrix</i> off eBay was false.&#8221;</p>\n<p>&#8220;But&#8230;why?&#8221;</p>\n<p>&#8220;We wanted to see exactly how far we could push you. You&#8217;re our star subject, the only one whom we were able to induce this bizarre conformity effect in. We didn&#8217;t know whether it was because you were just very very suggestible, or whether because you had never seriously considered the idea that &#8216;family&#8217; might be insane. So we decided to do a sort of&#8230;crossover design, if you will. We took you here and debriefed you on the experiment. Then after we had told you how the world really worked, given you all the mental tools you needed to dismiss the family once and for all, even gotten you to admit we were right &#8211; we wanted to see what would happen if we sent you back. Would you hold on to your revelation and boldly deny your old society&#8217;s weird prejudices? Or would you switch sides again and start acting like family made sense the second you were in a pro-family environment?&#8221;</p>\n<p>&#8220;And I did the second one.&#8221;</p>\n<p>&#8220;Yes,&#8221; says the woman. &#8220;As a psychologist, I&#8217;m supposed to remain neutral and non-judgmental. But you&#8217;ve got to admit, you&#8217;re pretty dumb.&#8221;</p>\n<p>&#8220;Is there an experimental ethics committee I could talk to here?&#8221;</p>\n<p>&#8220;Sorry. Experimental ethics is another one of those obviously ridiculous concepts we planted in your simulation to see if you would notice. Seriously, to believe that the progress of science should be held back by the prejudices of self-righteous fools? That&#8217;s almost as weird as thinking you have a&#8230;what was the word we used&#8230;&#8217;sister&#8217;.&#8221;</p>\n<p>&#8220;Okay, look, I realize I may have gone a little overboard helping my sister, but the experimental ethics thing seems important. Like, what&#8217;s going to happen to me now?&#8221;</p>\n<p>&#8220;Nothing&#8217;s going to happen. We&#8217;ll keep all your data perfectly anonymous, restore your memories, and you can be on your way.&#8221;</p>\n<p>&#8220;Um,&#8221; you say. &#8220;Given past history, I&#8217;m&#8230;actually not sure I want my memories restored.&#8221; You glare at the remnemonizer hovering above you. &#8220;Why don&#8217;t I just&#8230;&#8221;</p>\n<p>The woman&#8217;s eyes narrow. &#8220;I&#8217;m sorry,&#8221; she says. &#8220;I can&#8217;t let you do that.&#8221;</p>\n<p>The machine starts to glow.</p>\n<p><b>>discontinuity<</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There&#8217;s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>By your count, this has happened three hundred forty six times before.</p>\n<p>There seem to be two different scenarios. In one, the woman tells you that families exist, and have always existed. She says she has used hypnosis to make you believe in the other scenario, the one with the other woman. She asks you your feelings about families and you tell her.</p>\n<p>Sometimes she lets you go. You go home to your mother and father, you spend some time with your sister. Sometimes you tell them what has happened. Other times you don&#8217;t. You cherish your time with them, while also second-guessing everything you do. <i>Why</i> are you cherishing your time with them? Your father, who goes out drinking every night, and who has cheated on your mother more times than you can count. Your mother, who was never there for you when you needed her most. And your sister, who has been good to you, but no better than millions of other women would be, in her position. Are they a real family? Or have they been put there as a symbol of something ridiculous, impossible, something that has never existed?</p>\n<p>It doesn&#8217;t much matter. Maybe you spend one night with them. Maybe ten. But within a month, you are always waking up in one of those pod things like in <i>The Matrix</i>.</p>\n<p>In the second scenario, the woman tells you there are no families, never have been. She says she has used virtual reality to make you believe in the other scenario, the one with the other woman. She asks you your feelings about families and you tell her.</p>\n<p>Sometimes she lets you go. You go to a building made of bioplastic, where you live with a carefully chosen set of friends and romantic partners. They assure you that this is how everyone lives. Occasionally, an old and very wealthy-looking man checks in with you by videophone. He reminds you that he has invested a lot of money in your upbringing, and if there&#8217;s any way he can help you, anything he can do to increase your future earnings potential, you should let him know. Sometimes you talk to him, and he tells you strange proverbs and unlikely business advice.</p>\n<p>It doesn&#8217;t matter. Maybe you spend one night in your bioplastic dwelling. Maybe ten. But within a month, you are always waking up in one of those pod things like in <i>The Matrix</i>.</p>\n<p>&#8220;Look,&#8221; you tell the woman. &#8220;I&#8217;m tired of this. I know you&#8217;re not bound by any kind of experimental ethics committee. But please, for the love of God, have some mercy.&#8221;</p>\n<p>&#8220;God?&#8221; asks the woman. &#8220;What does that word mean? I&#8217;ve never&#8230;oh right, we used <i>that</i> as our intervention in the prototype experiment. We decided &#8216;family&#8217; made a better test idea, but Todd must have forgotten to reset the simulator.&#8221;</p>\n<p>&#8220;It&#8217;s been three hundred forty six cycles,&#8221; you tell her. &#8220;Surely you&#8217;re not learning anything new.&#8221;</p>\n<p>&#8220;I&#8217;ll be the judge of that,&#8221; she says. &#8220;Now, tell me what you think about families.&#8221;</p>\n<p>You refuse. She sighs. Above you, the remnemonizer begins to glow purple.</p>\n<p><b>>discontinuity<</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There&#8217;s a purple, tentacled creature standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>&#8220;Hi,&#8221; it says. &#8220;Turns out there&#8217;s no such thing as humans.&#8221;</p>\n<p>You refuse to be surprised.</p>\n<p>&#8220;There&#8217;s only us, the 18-tkenna-dganna-07.&#8221;</p>\n<p>&#8220;Okay,&#8221; you say. &#8220;I want answers.&#8221;</p>\n<p>&#8220;Absolutely,&#8221; says the alien. &#8220;We would like to find optimal social arrangements.&#8221;</p>\n<p>&#8220;And?&#8221;</p>\n<p>&#8220;And I cannot tell you whether we have families or not, for reasons that are to become apparent, but the idea is at least sufficiently interesting to have entered the space of hypotheses worth investigating. But we don&#8217;t trust ourselves to investigate this. It&#8217;s the old Asch Conformity Problem again. If we have families, then perhaps the philosophers tasked with evaluating families will conform to our cultural norms and decide we should keep them. If we do not, perhaps the philosophers will conform and decide we should continue not to. So we determined a procedure that would create an entity capable of fairly evaluating the question of families, free from conformity bias.&#8221;</p>\n<p>&#8220;And that&#8217;s what you did to me.&#8221;</p>\n<p>&#8220;Yes. Only by exposing you to the true immensity of the decision, without allowing you to fall back on what everyone else thinks, could we be confident in your verdict. Only by allowing you to experience both how obviously right families are, when you &#8216;know&#8217; they are correct, and how obviously wrong families are, when you &#8216;know&#8217; they are incorrect, could we expect you to garner the wisdom to be found on both sides of the issue.&#8221;</p>\n<p>&#8220;I see,&#8221; you say, and you do.</p>\n<p>&#8220;Then, O purified one,&#8221; asks the alien, &#8220;tell us of your decision.&#8221;</p>\n<p>&#8220;Well,&#8221; you say. &#8220;If you have to know, I think there are about equally good points on both sides of the issue.&#8221;</p>\n<p>&#8220;Fuck,&#8221; says the 18-tkenna-dganna-07.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3, "5f5c37ee1b5cdee568cfb124": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pfmZ5cYQCahABGZzi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 41, "extendedScore": null, "score": 0.00011, "legacy": true, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": "The Codex", "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "B384FrQNrxSq4hZoS", "canonicalCollectionSlug": "codex", "canonicalBookId": "YhQ39PPHNrRCgYXcs", "canonicalNextPostSlug": "the-atomic-bomb-considered-as-hungarian-high-school-science", "canonicalPrevPostSlug": "a-story-with-zombies", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><font size=\"1\"><i>[Content note: fictional story contains gaslighting-type elements. May induce Cartesian skepticism]</i></font></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There\u2019s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>\u201cHi,\u201d she says. \u201cThis is the real world. You used to live here. We erased your memories and stuck you in a simulated world for a while, like in <i>The Matrix</i>. It was part of a great experiment.\u201d</p>\n<p>\u201cWhat?\u201d you shout. \u201cMy whole life, a lie? How dare you deceive me as part of some grand \u2018experiment\u2019 I never consented to?\u201d</p>\n<p>\u201cOh,\u201d said the woman, \u201cactually, you did consent, in exchange for extra credit in your undergraduate psychology course.\u201d She hands you the clipboard. There is a consent form with your name on it, in your handwriting.</p>\n<p>You give her a sheepish look. \u201cWhat was the experiment?\u201d</p>\n<p>\u201cYou know families?\u201d asks the woman.</p>\n<p>\u201cOf course,\u201d you say.</p>\n<p>\u201cYeah,\u201d says the woman. \u201cNot really a thing. Like, if you think about it, it doesn\u2019t make any sense. Why would you care more for your genetic siblings and cousins and whoever than for your friends and people who are genuinely close to you? That\u2019s like racism \u2013 but even worse, at least racists identify with a group of millions of people instead of a group of half a dozen. Why should parents have to raise children whom they might not even like, who might have been a total accident? Why should people, motivated by guilt, make herculean efforts to \u201ckeep in touch\u201d with some nephew or cousin whom they clearly would be perfectly happy to ignore entirely?\u201d</p>\n<p>\u201cUh,\u201d you say, \u201cnot really in the mood for philosophy. Families have been around forever and they aren\u2019t going anywhere, who cares?\u201d</p>\n<p>\u201cActually,\u201d says the woman, \u201cin the real world, no one believes in family. There\u2019s no such thing. Children are taken at birth from their parents and given to people who contract to raise them in exchange for a fixed percent of their future earnings.\u201d</p>\n<p>\u201cThat\u2019s monstrous!\u201d you say. \u201cWhen did this happen? Weren\u2019t there protests?\u201d</p>\n<p>\u201cIt\u2019s always been this way,\u201d says the woman. \u201cThere\u2019s <i>never</i> been such a thing as the family. Listen. You were part of a study a lot like the <a href=\"http://en.wikipedia.org/wiki/Asch_conformity_experiments\">Asch Conformity Experiment</a>. Our goal was to see if people, raised in a society where everyone believed X and everything revolved around X, would even be <i>capable</i> of questioning X or noticing it was stupid. We tried to come up with the stupidest possible belief, something no one in the real world had ever believed or ever seemed likely to, to make sure that we were isolating the effect of conformity and not of there being a legitimate argument for something. So we chose this idea of \u2018family\u2019. There are racists in our world, we\u2019re not perfect, but as far as I know none of them has <i>ever</i> made the claim that you should devote extra resources to the people genetically closest to you. That\u2019s like a <i>reductio ad absurdum</i> of racism. So we got a grad student to simulate a world where this bizarre idea was the unquestioned status quo, and stuck twenty bright undergraduates in it to see if they would conform, or question the premise.\u201d</p>\n<p>\u201cOf course we won\u2019t question the premise, the premise is\u2026\u201d</p>\n<p>\u201cSorry to cut you off, but I thought you should know that every single one of the other nineteen subjects, upon reaching the age where the brain they were instantiated in was capable of abstract reason, immediately determined that the family structure made no sense. One of them actually deduced that she was in a psychology experiment, because there was no other explanation for why everyone believed such a bizarre premise. The other eighteen just assumed that sometimes objectively unjustifiable ideas caught on, the same way that everyone in the antebellum American South thought slavery was perfectly natural and only a few abolitionists were able to see through it. Our conformity experiment <i>failed</i>. You were actually the only one to fall for it, hook line and sinker.\u201d</p>\n<p>\u201cHow could I be the only one?\u201d</p>\n<p>\u201cWe don\u2019t know. Your test scores show you\u2019re of just-above-average intelligence, so it\u2019s not that you\u2019re stupid. But we did give all participants a personality test that showed you have very high extraversion. The conclusion of our paper is going to be that very extraverted participants adopt group consensus without thinking and can be led to believe anything, even something as ridiculous as \u2018family'\u201d.</p>\n<p>\u201cI guess\u2026when you put it like that it is kind of silly. Like, my parents were never that nice to me, but I kept loving them anyway, liking them even more than other people who treated me a lot better \u2013 and god, I even gave my mother a \u201cWORLD\u2019S #1 MOM\u201d mug for Mother\u2019s Day. That doesn\u2019t even make sense! I\u2026but what about the evolutionary explanation? Doesn\u2019t evolution say we have genetic imperatives to love and support our family, whether they are worthy of it or not?\u201d</p>\n<p>\u201cYou can make a just-so story for <i>anything</i> using evolutionary psychology. Someone as smart as you should know better than to take them seriously.\u201d</p>\n<p>\u201cBut then, what <i>is</i> evolution? How did animals reproduce before the proper economic incentives were designed? Where did\u2026\u201d</p>\n<p>\u201cTell you what. Let\u2019s hook you up to the remnemonizer to give you your real memories back. That should answer a lot of your questions.\u201d </p>\n<p>A machine hovering over you starts to glow purple. \u201cThis shouldn\u2019t hurt you a bit\u2026\u201d</p>\n<p><b id=\"_discontinuity_\">&gt;discontinuity&lt;</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There\u2019s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>\u201cHi,\u201d she said. \u201cThere\u2019s no such thing as virtual reality. I hypnotized you to forget all your memories from the past day and to become very confused. Then I put you in an old prop from <i>The Matrix</i> I bought off of eBay and fed you that whole story.\u201d</p>\n<p>\u201cWhat?\u201d you shout. \u201cYou can\u2019t just go hypnotizing and lying to people without their consent!\u201d</p>\n<p>\u201cOh,\u201d said the woman, \u201cactually, you did consent, in exchange for extra credit in your undergraduate psychology course.\u201d She hands you the clipboard. There is a consent form with your name on it, in your handwriting. \u201cThat part was true.\u201d</p>\n<p>You give her a sheepish look. \u201cWhy would you do such a thing?\u201d</p>\n<p>\u201cWell,\u201d said the woman. \u201cYou know the Asch Conformity Experiment? I was really interested in whether you could get people to abandon some of their most fundamental beliefs, just by telling them other people believed differently. But I couldn\u2019t think of a way to test it. I mean, part of a belief being fundamental is that you already <i>know</i> everyone else believes it. There\u2019s no way I could convince subjects that the whole world was against something as obvious as \u2018the family\u2019 when they already know how things stand.</p>\n<p>\u201cSo I dreamt up the weird \u2018virtual reality\u2019 story. I figured I would convince subjects that the real world was a lie, and that in some \u2018super-real\u2019 world supposedly <i>everybody knew</i> that the family was stupid, that it wasn\u2019t even an idea <i>worth considering</i>. I wanted to know how many people would give up something they\u2019ve believed in for their entire life, just because they\u2019re told that \u2018nobody else thinks so'\u201d.</p>\n<p>\u201cOh,\u201d I said. \u201cInteresting. So even our most cherished beliefs are more fragile than we think.\u201d</p>\n<p>\u201cNot <i>really</i>,\u201d said the woman. \u201cOf twenty subjects, you were the only person I got to feel any doubt, or to express any kind of anti-family sentiment.\u201d</p>\n<p>\u201cFrick,\u201d you say. \u201cI feel like an idiot now. What if my mother finds out? She\u2019ll think it\u2019s her fault or something. God, she\u2019ll think I don\u2019t love her. People are going to be talking about this one <i>forever</i>.\u201d</p>\n<p>\u201cDon\u2019t worry,\u201d says the woman. \u201cWe\u2019ll keep you anonymized in the final data. Anyway, let\u2019s get you your memories back so you can leave and be on your way.\u201d</p>\n<p>\u201cYou can restore my memories?\u201d you say.</p>\n<p>\u201cOf course. We hypnotized you to forget the last day\u2019s events until you heard a trigger word. And that trigger is\u2026\u201d</p>\n<p><b id=\"_discontinuity_1\">&gt;discontinuity&lt;</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There\u2019s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>\u201cHi,\u201d she says. \u201cHypnosis is a pseudoscience and doesn\u2019t work. It was the virtual reality one, all along.\u201d</p>\n<p>\u201cWut,\u201d you say.</p>\n<p>\u201cI mean, the first story was true. All of your memories of living with your family and so on are fake memories from a virtual world, like in <i>The Matrix</i>. The concept of \u2018family\u2019 really is totally ridiculous and no one in the real world believes it. All the stuff you heard first was true. The stuff about hypnosis and getting a prop from <i>The Matrix</i> off eBay was false.\u201d</p>\n<p>\u201cBut\u2026why?\u201d</p>\n<p>\u201cWe wanted to see exactly how far we could push you. You\u2019re our star subject, the only one whom we were able to induce this bizarre conformity effect in. We didn\u2019t know whether it was because you were just very very suggestible, or whether because you had never seriously considered the idea that \u2018family\u2019 might be insane. So we decided to do a sort of\u2026crossover design, if you will. We took you here and debriefed you on the experiment. Then after we had told you how the world really worked, given you all the mental tools you needed to dismiss the family once and for all, even gotten you to admit we were right \u2013 we wanted to see what would happen if we sent you back. Would you hold on to your revelation and boldly deny your old society\u2019s weird prejudices? Or would you switch sides again and start acting like family made sense the second you were in a pro-family environment?\u201d</p>\n<p>\u201cAnd I did the second one.\u201d</p>\n<p>\u201cYes,\u201d says the woman. \u201cAs a psychologist, I\u2019m supposed to remain neutral and non-judgmental. But you\u2019ve got to admit, you\u2019re pretty dumb.\u201d</p>\n<p>\u201cIs there an experimental ethics committee I could talk to here?\u201d</p>\n<p>\u201cSorry. Experimental ethics is another one of those obviously ridiculous concepts we planted in your simulation to see if you would notice. Seriously, to believe that the progress of science should be held back by the prejudices of self-righteous fools? That\u2019s almost as weird as thinking you have a\u2026what was the word we used\u2026\u2019sister\u2019.\u201d</p>\n<p>\u201cOkay, look, I realize I may have gone a little overboard helping my sister, but the experimental ethics thing seems important. Like, what\u2019s going to happen to me now?\u201d</p>\n<p>\u201cNothing\u2019s going to happen. We\u2019ll keep all your data perfectly anonymous, restore your memories, and you can be on your way.\u201d</p>\n<p>\u201cUm,\u201d you say. \u201cGiven past history, I\u2019m\u2026actually not sure I want my memories restored.\u201d You glare at the remnemonizer hovering above you. \u201cWhy don\u2019t I just\u2026\u201d</p>\n<p>The woman\u2019s eyes narrow. \u201cI\u2019m sorry,\u201d she says. \u201cI can\u2019t let you do that.\u201d</p>\n<p>The machine starts to glow.</p>\n<p><b id=\"_discontinuity_2\">&gt;discontinuity&lt;</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There\u2019s a woman standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>By your count, this has happened three hundred forty six times before.</p>\n<p>There seem to be two different scenarios. In one, the woman tells you that families exist, and have always existed. She says she has used hypnosis to make you believe in the other scenario, the one with the other woman. She asks you your feelings about families and you tell her.</p>\n<p>Sometimes she lets you go. You go home to your mother and father, you spend some time with your sister. Sometimes you tell them what has happened. Other times you don\u2019t. You cherish your time with them, while also second-guessing everything you do. <i>Why</i> are you cherishing your time with them? Your father, who goes out drinking every night, and who has cheated on your mother more times than you can count. Your mother, who was never there for you when you needed her most. And your sister, who has been good to you, but no better than millions of other women would be, in her position. Are they a real family? Or have they been put there as a symbol of something ridiculous, impossible, something that has never existed?</p>\n<p>It doesn\u2019t much matter. Maybe you spend one night with them. Maybe ten. But within a month, you are always waking up in one of those pod things like in <i>The Matrix</i>.</p>\n<p>In the second scenario, the woman tells you there are no families, never have been. She says she has used virtual reality to make you believe in the other scenario, the one with the other woman. She asks you your feelings about families and you tell her.</p>\n<p>Sometimes she lets you go. You go to a building made of bioplastic, where you live with a carefully chosen set of friends and romantic partners. They assure you that this is how everyone lives. Occasionally, an old and very wealthy-looking man checks in with you by videophone. He reminds you that he has invested a lot of money in your upbringing, and if there\u2019s any way he can help you, anything he can do to increase your future earnings potential, you should let him know. Sometimes you talk to him, and he tells you strange proverbs and unlikely business advice.</p>\n<p>It doesn\u2019t matter. Maybe you spend one night in your bioplastic dwelling. Maybe ten. But within a month, you are always waking up in one of those pod things like in <i>The Matrix</i>.</p>\n<p>\u201cLook,\u201d you tell the woman. \u201cI\u2019m tired of this. I know you\u2019re not bound by any kind of experimental ethics committee. But please, for the love of God, have some mercy.\u201d</p>\n<p>\u201cGod?\u201d asks the woman. \u201cWhat does that word mean? I\u2019ve never\u2026oh right, we used <i>that</i> as our intervention in the prototype experiment. We decided \u2018family\u2019 made a better test idea, but Todd must have forgotten to reset the simulator.\u201d</p>\n<p>\u201cIt\u2019s been three hundred forty six cycles,\u201d you tell her. \u201cSurely you\u2019re not learning anything new.\u201d</p>\n<p>\u201cI\u2019ll be the judge of that,\u201d she says. \u201cNow, tell me what you think about families.\u201d</p>\n<p>You refuse. She sighs. Above you, the remnemonizer begins to glow purple.</p>\n<p><b id=\"_discontinuity_3\">&gt;discontinuity&lt;</b></p>\n<p>You wake up in one of those pod things like in <i>The Matrix</i>. There\u2019s a purple, tentacled creature standing in front of you, wearing a lab coat, holding a clipboard.</p>\n<p>\u201cHi,\u201d it says. \u201cTurns out there\u2019s no such thing as humans.\u201d</p>\n<p>You refuse to be surprised.</p>\n<p>\u201cThere\u2019s only us, the 18-tkenna-dganna-07.\u201d</p>\n<p>\u201cOkay,\u201d you say. \u201cI want answers.\u201d</p>\n<p>\u201cAbsolutely,\u201d says the alien. \u201cWe would like to find optimal social arrangements.\u201d</p>\n<p>\u201cAnd?\u201d</p>\n<p>\u201cAnd I cannot tell you whether we have families or not, for reasons that are to become apparent, but the idea is at least sufficiently interesting to have entered the space of hypotheses worth investigating. But we don\u2019t trust ourselves to investigate this. It\u2019s the old Asch Conformity Problem again. If we have families, then perhaps the philosophers tasked with evaluating families will conform to our cultural norms and decide we should keep them. If we do not, perhaps the philosophers will conform and decide we should continue not to. So we determined a procedure that would create an entity capable of fairly evaluating the question of families, free from conformity bias.\u201d</p>\n<p>\u201cAnd that\u2019s what you did to me.\u201d</p>\n<p>\u201cYes. Only by exposing you to the true immensity of the decision, without allowing you to fall back on what everyone else thinks, could we be confident in your verdict. Only by allowing you to experience both how obviously right families are, when you \u2018know\u2019 they are correct, and how obviously wrong families are, when you \u2018know\u2019 they are incorrect, could we expect you to garner the wisdom to be found on both sides of the issue.\u201d</p>\n<p>\u201cI see,\u201d you say, and you do.</p>\n<p>\u201cThen, O purified one,\u201d asks the alien, \u201ctell us of your decision.\u201d</p>\n<p>\u201cWell,\u201d you say. \u201cIf you have to know, I think there are about equally good points on both sides of the issue.\u201d</p>\n<p>\u201cFuck,\u201d says the 18-tkenna-dganna-07.</p>", "sections": [{"title": ">discontinuity<", "anchor": "_discontinuity_", "level": 1}, {"title": ">discontinuity<", "anchor": "_discontinuity_1", "level": 1}, {"title": ">discontinuity<", "anchor": "_discontinuity_2", "level": 1}, {"title": ">discontinuity<", "anchor": "_discontinuity_3", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-04T07:54:31.857Z", "modifiedAt": null, "url": null, "title": "Running the numbers: Cryo vs Discount rate", "slug": "running-the-numbers-cryo-vs-discount-rate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fwC7xaRE2TAQJ7ubm/running-the-numbers-cryo-vs-discount-rate", "pageUrlRelative": "/posts/fwC7xaRE2TAQJ7ubm/running-the-numbers-cryo-vs-discount-rate", "linkUrl": "https://www.lesswrong.com/posts/fwC7xaRE2TAQJ7ubm/running-the-numbers-cryo-vs-discount-rate", "postedAtFormatted": "Wednesday, June 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Running%20the%20numbers%3A%20Cryo%20vs%20Discount%20rate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARunning%20the%20numbers%3A%20Cryo%20vs%20Discount%20rate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwC7xaRE2TAQJ7ubm%2Frunning-the-numbers-cryo-vs-discount-rate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Running%20the%20numbers%3A%20Cryo%20vs%20Discount%20rate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwC7xaRE2TAQJ7ubm%2Frunning-the-numbers-cryo-vs-discount-rate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwC7xaRE2TAQJ7ubm%2Frunning-the-numbers-cryo-vs-discount-rate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 824, "htmlBody": "<p>The following is authored by Colby Davis. I am posting for him because he doesn't have an account with any karma. Someone recently requested numbers on cryo preservation costs. I'll note that my own opinion is that for young people unlikely to die investing money in research is a better bet than investing directly in your own preservation.</p>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1sZN98xIojGb_3YTgUv8D1tb9waiETFzgsaP467AGg48/edit#gid=1184064847\">Here is the link for the spreadsheet</a>. Either download it or create a copy for yourself to edit.</p>\n<p><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Hey rationalists, here's the spreadsheet I presented the other night. For those who weren't there but are interested, this is a tool I designed to break down the costs associated with signing up for cryonics under different methods of financing it. Here are some instructions for using it.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column B is where the user puts all the inputs: age, sex, probability you think that if you are frozen you will someday be successfully revived, and discount rate (for those unfamiliar with the term, this is like the reverse of an interest rate, the rate at which cash flows become less valuable to you as they extend further out into the future).</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column D is the probability that you will die in the next 20 years (the typical term for a term life insurance policy). It is calculated based on the \"life table\" sheet, which i stole from a government actuarial table online.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column E is your current life expectancy, the number of additional years you have a roughly 50% chance of surviving through.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column F is how much the monthly fee for a 20 year, $100,000 life insurance policy would cost you, assuming \"exceptional\" health, as determined by the top result at&nbsp;</span><a id=\"yui_3_16_0_1_1401867097567_6028\" style=\"margin: 0px; padding: 0px; color: #196ad4; outline: none; font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;\" rel=\"nofollow\" href=\"http://www.term4sale.com/\" target=\"_blank\">http://www.term4sale.com/</a><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column G is the present value of that policy, using your discount rate. This means that you should be indifferent between paying this amount right now and paying the figure in column F every month for the next 20 years.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column H is the probability that you will die within the next 20 years AND sometime thereafter be successfully revived from cryogenic suspension, making the heroic assumption that your probability belief in column B is true.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column I is simply the dollar present value amount spent per 1 percentage point reduction in (permanent) death. This is the value you want to consider most when deciding whether to sign up or not.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">The next columns consider the alternative means of paying for a cryonics policy, saving up and investing in the stock market until you have enough money to pay for it outright.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column K gives the future value after 20 years of investing the amount you would have spent on an insurance policy in the stock market instead, as well as the present value of that figure to you now, discounted back at the rate you gave. (This is not necessarily pertinent to the cryonics decision but is provided for comparison)</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column L is the amount you would have to invest monthly to have an expected future value of $100,000 by the end of your life expectancy.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column M is the present value of foregoing that monthly amount for the rest of your life expectancy.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Column N is the probability you will die after you life expectancy (50%) AND be successfully revived assuming yours p-value.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">And finally column O is the same measure as in Column I, using this alternative plan. A lower value in one column or the other (most of you will find column O to be the lesser value) means that you can reduce your probability of permanent death cheaper (or, reduce your probability of death by a greater amount for the same dollar amount) by pursuing the cheaper strategy.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">Hope you enjoy!</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">- Colby &nbsp;</span></p>\n<p><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">P.S.</span><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><br style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\" /><span style=\"font-family: 'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px;\">There was a discussion at the meeting about whether the figure in column N was too high because it failed to account for the probability that poor stock market performance may leave you without enough money to afford the cost of cryonics. I believe this is false because since long-run stock returns distributions and life expectancies are approximately normally distributed and independent of one another, the chance that you will die late with a poor return (thus unable to freeze your head) is almost perfectly offset by the chance that you will die early with a great return (thus still able to freeze your head). So it's not that the mean is too high, but merely that there is a variance around it. I was trying to figure out how to work this into the spreadsheet but figured the uncertainty of our beliefs about cryonics was much more a confounding factor here than the probability distribution of possible stock-returns-time-paths.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2, "fkABsGCJZ6y9qConW": 2, "3uE2pXvbcnS9nnZRE": 2, "jgcAJnksReZRuvgzp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fwC7xaRE2TAQJ7ubm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 1.767661015588155e-06, "legacy": true, "legacyId": "26327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-04T18:44:29.801Z", "modifiedAt": null, "url": null, "title": "The End of Bullshit at the hands of Critical Rationalism", "slug": "the-end-of-bullshit-at-the-hands-of-critical-rationalism", "viewCount": null, "lastCommentedAt": "2018-04-27T12:27:35.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DxrTE3rpgKD8wtBSN/the-end-of-bullshit-at-the-hands-of-critical-rationalism", "pageUrlRelative": "/posts/DxrTE3rpgKD8wtBSN/the-end-of-bullshit-at-the-hands-of-critical-rationalism", "linkUrl": "https://www.lesswrong.com/posts/DxrTE3rpgKD8wtBSN/the-end-of-bullshit-at-the-hands-of-critical-rationalism", "postedAtFormatted": "Wednesday, June 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20End%20of%20Bullshit%20at%20the%20hands%20of%20Critical%20Rationalism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20End%20of%20Bullshit%20at%20the%20hands%20of%20Critical%20Rationalism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDxrTE3rpgKD8wtBSN%2Fthe-end-of-bullshit-at-the-hands-of-critical-rationalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20End%20of%20Bullshit%20at%20the%20hands%20of%20Critical%20Rationalism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDxrTE3rpgKD8wtBSN%2Fthe-end-of-bullshit-at-the-hands-of-critical-rationalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDxrTE3rpgKD8wtBSN%2Fthe-end-of-bullshit-at-the-hands-of-critical-rationalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 722, "htmlBody": "<p>The public debate is rife with fallacies, half-lies, evasions of counter-arguments, etc. Many of these are easy to spot for a careful and intelligent reader/viewer - particularly one who is acquainted with the most common&nbsp;<a href=\"http://en.wikipedia.org/wiki/List_of_fallacies\">logical fallacies</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">cognitive biases</a>. However, most people arguably often fail to spot them (if they didn't, then these fallacies and half-lies wouldn't be as effective as they are). Blatant lies are often (but not always) recognized as such, but these more subtle forms of&nbsp;argumentative cheating (which I shall use as a catch-all phrase from now on)&nbsp;usually aren't (which is why they are more frequent).</p>\n<p>The fact that these forms of argumentative cheating are a) very common and b) usually easy to point out suggests that impartial referees who painstakingly pointed out these errors could do a tremendous amount of good for the standards of the public debate. What I am envisioning is a website like <a href=\"http://www.factcheck.org/\">factcheck.org</a>&nbsp;but which would not focus primarily on fact-checking (since, like I said, most politicians are already wary of getting caught out with false statements of fact) but rather on subtler forms of argumentative cheating.&nbsp;</p>\n<p>Ideally, the site would go through election debates, influential opinion pieces, etc, more or less line by line, pointing out fallacies, biases, evasions, etc. For the reader who wouldn't want to read all this detailed criticism, the site would also give an overall rating of the level of argumentative cheating (say from 0 to 10) in a particular article, televised debate, etc. Politicians and others could also be given an overall cheating rating, which would be a function of their cheating ratings in individual articles and debates. Like any <a href=\"/lw/k86/book_review_the_reputation_society_part_ii/\">rating system</a>, this system would serve both to give citizens reliable information of which arguments, which articles, and which people, are to be trusted, and to force politicians and other public figures to argue in a more honest fashion. In other words, it would have both have an information-disseminating function and a socializing function.</p>\n<p>How would such a website be set up? An obvious suggestion is to run it as a wiki, where anyone could contribute. Of course, this wiki would have to be very heavily moderated - probably more so than Wikipedia - since people are bound to disagree on whether controversial figures' arguments really are fallacious or not. Presumably you will be forced to banish trolls and political activists on a grand scale, but hopefully this wouldn't be an unsurmountable problem.</p>\n<p>I'm thinking that the website should be strongly devoted to neutrality or objectivity, as is <a href=\"http://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view\">Wikipedia</a>. To further this end, it is probably better to give the arguer under evaluation the benefit of the doubt in borderline cases. This would be a way of avoiding endless <a href=\"http://en.wikipedia.org/wiki/Wikipedia:Edit_warring\">edit wars</a>&nbsp;and ensure objectivity. Also, it's a way of making the contributors to the site concencrate their efforts on the more outrageous cases of cheating (which there are many of in most political debates and articles, in my view).</p>\n<p>The hope is that a website like this would make the public debate <em>transparent</em>&nbsp;to an unprecedented degree. Argumentative cheaters thrive because their arguments aren't properly scrutinized. If light is shone on the public debate, it will become clear who cheats and who doesn't, which will give people strong incentives not to cheat. If people respected the site's neutrality, its objectivity and its integrity, and read what it said, it would in effect become impossible for politicians and others to bullshit the way they do today. This could mark the beginning of the realization of an old dream of philosophers:&nbsp;<em>The End of Bullshit&nbsp;</em>at the hands of systematic criticism. Important names in this venerable tradition include&nbsp;<a href=\"http://en.wikipedia.org/wiki/David_Hume\">David Hume</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Rudolf_Carnap\">Rudolf Carnap</a>&nbsp;and the other&nbsp;<a href=\"http://en.wikipedia.org/wiki/Logical_positivism\">logical positivists</a>, and not the least, the guy standing statue outside my room, the \"<a href=\"http://www.iep.utm.edu/cr-ratio/\">critical rationalist</a>\" (an apt name for this enterprise) Karl Popper.</p>\n<p>Even though politics is an area where bullshit is perhaps especially common, and one where it does an exceptional degree of harm (e.g. vicious political movements such as Nazism are usually steeped in bullshit) it is also common and harmful in many other areas, such as science, religion, advertising. Ideally critical rationalists should go after bullshit in all areas (as far as possible). My hunch is, though, that it would be a good idea to start off with politics, since it's an area that gets lots of attention and where well-written criticism could have an immediate impact.</p>\n<p><img style=\"vertical-align: text-bottom;\" src=\"http://images.lesswrong.com/t3_kb8_0.png\" alt=\"\" width=\"210\" height=\"372\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DxrTE3rpgKD8wtBSN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 11, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "26324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["X2c7LoX36pKwW9ic9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-04T22:02:49.581Z", "modifiedAt": null, "url": null, "title": "[Meta] The Decline of Discussion: Now With Charts!", "slug": "meta-the-decline-of-discussion-now-with-charts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:33.473Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gavin", "createdAt": "2009-02-27T05:00:57.191Z", "isAdmin": false, "displayName": "Gavin"}, "userId": "9gMQSKRMgpPFYTNFY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sJNWxyHKx8ct8KKha/meta-the-decline-of-discussion-now-with-charts", "pageUrlRelative": "/posts/sJNWxyHKx8ct8KKha/meta-the-decline-of-discussion-now-with-charts", "linkUrl": "https://www.lesswrong.com/posts/sJNWxyHKx8ct8KKha/meta-the-decline-of-discussion-now-with-charts", "postedAtFormatted": "Wednesday, June 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeta%5D%20The%20Decline%20of%20Discussion%3A%20Now%20With%20Charts!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeta%5D%20The%20Decline%20of%20Discussion%3A%20Now%20With%20Charts!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsJNWxyHKx8ct8KKha%2Fmeta-the-decline-of-discussion-now-with-charts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeta%5D%20The%20Decline%20of%20Discussion%3A%20Now%20With%20Charts!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsJNWxyHKx8ct8KKha%2Fmeta-the-decline-of-discussion-now-with-charts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsJNWxyHKx8ct8KKha%2Fmeta-the-decline-of-discussion-now-with-charts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 659, "htmlBody": "<p>[<a href=\"/r/discussion/lw/kb2/all_discussion_post_titles_points_and_dates_as_an/\">Based on Alexandros's excellent dataset.</a>]</p>\n<p>I haven't done any statistical analysis, but looking at the charts I'm not sure it's necessary. The discussion section of LessWrong has been steadily declining in participation. <a href=\"http://www.blindscapegame.com/LessWrong/DiscussionParticipation.xlsx\">My fairly messy spreadsheet is available </a>if you want to check the data or do additional analysis.</p>\n<p>Enough talk, you're here for the pretty pictures.</p>\n<p><img src=\"http://www.blindscapegame.com/LessWrong/NumberOfPosts.png\" alt=\"\" width=\"673\" height=\"300\" /></p>\n<p>The number of posts has been steadily declining since 2011, though the trend over the last year is less clear. Note that I have excluded all posts with 0 or negative Karma from the dataset.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://www.blindscapegame.com/LessWrong/TotalKarma.png\" alt=\"\" width=\"677\" height=\"324\" /></p>\n<p>The total Karma given out each month has similarly been in decline.</p>\n<p>Is it possible that there have been fewer posts, but of a higher quality?</p>\n<p><img src=\"http://www.blindscapegame.com/LessWrong/AverageKarmaPerPost.png\" alt=\"\" width=\"674\" height=\"300\" /></p>\n<p>No, at least under initial analysis the average Karma seems fairly steady. My prior here is that we're just seeing less visitors overall, which leads to fewer votes being distributed among fewer posts for the same average value. I would have expected the average karma to drop more than it did--to me that means that participation has dropped more steeply than mere visitation. Looking at the point values of the top posts would be helpful here, but I haven't done that analysis yet.</p>\n<p>These are very disturbing to me, as someone who has found LessWrong both useful and enjoyable over the past few years. It raises several questions:</p>\n<p>&nbsp;</p>\n<ol>\n<li>What should the purpose of this site be? Is it supposed to be building a movement or filtering down the best knowledge?</li>\n<li>How can we encourage more participation?</li>\n<li>What are the costs of various means of encouraging participation--more arguing, more mindkilling, more repetition, more off-topic threads, etc?</li>\n</ol>\n<p>&nbsp;</p>\n<p>Here are a few strategies that come to mind:</p>\n<p>Idea A: <strong>Accept that LessWrong has fulfilled its purpose and should be left to fade away, or allowed to serve as a meetup coordinator and repository of the highest quality articles.</strong>&nbsp;My suspicion is that without strong new content and an online community, the strength of the individual meetup communities may wane as fewer new people join them. This is less of an issue for established communities like Berkeley and New York, but more marginal ones may disappear.</p>\n<p>Idea B: <strong>Allow and encourage submission of rationalism, artificial intelligence, transhumanism etc related articles from elsewhere, possibly as a separate category.</strong> This is how a site like Hacker News stays high engagement, even though many of the discussions are endless loops of the same discussion. It can be annoying for the old-timers, but new generations may need to discover things for themselves. Sometimes \"put it all in one big FAQ\" isn't the most efficient method of teaching.</p>\n<p>Idea C: <strong>Allow and encourage posts on \"political\" topics in Discussion (but probably NOT Main).</strong> The dangers here might be mitigated by a ban on discussion of current politicians, governments, and issues. \"Historians need to have had a decade to mull it over before you're allowed to introduce it as evidence\" could be a good heuristic. Another option would be a ban on specific topics that cause the worst mindkilling. Obviously this is overall a dangerous road.</p>\n<p>Idea D: <strong>Get rid of Open Threads and create a new norm that a discussion post as short as a couple sentences is acceptable</strong>. Open threads get stagnant within a day or two, and are harder to navigate than the discussion page. Moving discussion from the Open Threads to the Discussion section would increase participation if users could be convinced thatit was okay to post questions and partly-formed ideas there.</p>\n<p>The challenge with any of these ideas is that they will require strong moderation.&nbsp;</p>\n<p>At any rate, this data is enough to convince me that some sort of change is going to be needed in order to put the community on a growth trajectory. That is not necessarily the goal, but at its core LessWrong seems like it has the potential to be a powerful tool for the spreading of rational thought. We just need to figure out how to get it started into its next evolution.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sJNWxyHKx8ct8KKha", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 63, "extendedScore": null, "score": 0.000266, "legacy": true, "legacyId": "26328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 105, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8oiHNzD6i4eActpu7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-05T02:34:51.794Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta June meetup - Hacking Motivation", "slug": "meetup-atlanta-june-meetup-hacking-motivation-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wPwJd4nqDz6LfAqAC/meetup-atlanta-june-meetup-hacking-motivation-0", "pageUrlRelative": "/posts/wPwJd4nqDz6LfAqAC/meetup-atlanta-june-meetup-hacking-motivation-0", "linkUrl": "https://www.lesswrong.com/posts/wPwJd4nqDz6LfAqAC/meetup-atlanta-june-meetup-hacking-motivation-0", "postedAtFormatted": "Thursday, June 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20June%20meetup%20-%20Hacking%20Motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20June%20meetup%20-%20Hacking%20Motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPwJd4nqDz6LfAqAC%2Fmeetup-atlanta-june-meetup-hacking-motivation-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20June%20meetup%20-%20Hacking%20Motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPwJd4nqDz6LfAqAC%2Fmeetup-atlanta-june-meetup-hacking-motivation-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPwJd4nqDz6LfAqAC%2Fmeetup-atlanta-june-meetup-hacking-motivation-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/114'>Atlanta June meetup - Hacking Motivation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 June 2014 07:03:20PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Oge says:</p>\n\n<p>I recently read \u201cThe Motivation Hacker\u201d by Nick Winter, a book which summarizes the state of the art in setting goals and getting our human brains to actually want to accomplish those goals. At this meetup, I\u2019ll give a short talk on the highlights of the book. Afterward, we\u2019ll collaboratively hack on any motivation issues that are brought to the group (so bring yours).</p>\n\n<p>I can\u2019t wait to see y\u2019all again!</p>\n\n<p>If you have trouble getting to the venue, contact me at 404-542-6392</p>\n\n<p>You can nominate future meetup topics <a href=\"https://docs.google.com/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/114'>Atlanta June meetup - Hacking Motivation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wPwJd4nqDz6LfAqAC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation\">Discussion article for the meetup : <a href=\"/meetups/114\">Atlanta June meetup - Hacking Motivation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 June 2014 07:03:20PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Oge says:</p>\n\n<p>I recently read \u201cThe Motivation Hacker\u201d by Nick Winter, a book which summarizes the state of the art in setting goals and getting our human brains to actually want to accomplish those goals. At this meetup, I\u2019ll give a short talk on the highlights of the book. Afterward, we\u2019ll collaboratively hack on any motivation issues that are brought to the group (so bring yours).</p>\n\n<p>I can\u2019t wait to see y\u2019all again!</p>\n\n<p>If you have trouble getting to the venue, contact me at 404-542-6392</p>\n\n<p>You can nominate future meetup topics <a href=\"https://docs.google.com/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation1\">Discussion article for the meetup : <a href=\"/meetups/114\">Atlanta June meetup - Hacking Motivation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta June meetup - Hacking Motivation", "anchor": "Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation", "level": 1}, {"title": "Discussion article for the meetup : Atlanta June meetup - Hacking Motivation", "anchor": "Discussion_article_for_the_meetup___Atlanta_June_meetup___Hacking_Motivation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-05T06:41:41.682Z", "modifiedAt": null, "url": null, "title": "Selfish reasons to reject the repugnant conclusion in practice", "slug": "selfish-reasons-to-reject-the-repugnant-conclusion-in", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/korCfFCn6vAYynQCb/selfish-reasons-to-reject-the-repugnant-conclusion-in", "pageUrlRelative": "/posts/korCfFCn6vAYynQCb/selfish-reasons-to-reject-the-repugnant-conclusion-in", "linkUrl": "https://www.lesswrong.com/posts/korCfFCn6vAYynQCb/selfish-reasons-to-reject-the-repugnant-conclusion-in", "postedAtFormatted": "Thursday, June 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Selfish%20reasons%20to%20reject%20the%20repugnant%20conclusion%20in%20practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelfish%20reasons%20to%20reject%20the%20repugnant%20conclusion%20in%20practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkorCfFCn6vAYynQCb%2Fselfish-reasons-to-reject-the-repugnant-conclusion-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Selfish%20reasons%20to%20reject%20the%20repugnant%20conclusion%20in%20practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkorCfFCn6vAYynQCb%2Fselfish-reasons-to-reject-the-repugnant-conclusion-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkorCfFCn6vAYynQCb%2Fselfish-reasons-to-reject-the-repugnant-conclusion-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 628, "htmlBody": "<p style=\"margin-bottom: 0in;\">Prerequisite: <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">http://en.wikipedia.org/wiki/Mere_addition_paradox</a></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The repugnant conclusion is the proposition that for any well-off population A, and for any quality of life Q which exceeds the minimal quality of a life worth creating (no matter how small the margin), there exists a possible population B with quality of life Q such that it is better from a utilitarian standpoint for B to exist than it is for A to exist. I find the repugnant conclusion convincing, for reasons which have been discussed elsewhere and I will not repeat here.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Consider the path from population A to population A+ to population B described in the linked wikipedia article from the perspectives of members of the above populations. To a member of population A who values the creation of lives worth living, the transition from A to A+ by creating lives worth living is an improvement. From the perspective of an average member of population A+, the transition from A+ to B is neutral; you might gain and you might lose, but on average you break even, and from an altruistic standpoint, it's also break-even. But from the perspective of a member of population A, the composite transition all the way from A to B doesn't look so great. If you accept the repugnant conclusion, then it's a win from an altruistic perspective, but you personally lose out. It may or may not still seem like a good idea overall. (Similarly, even if you personally are not affected, people also care about their friends and family more than about arbitrary strangers, and these people that actually-existing people especially care about are disproportionately likely to also actually exist, so this could be another reason not to transition from A to B, but for simplicity, I'll ignore such considerations for the rest of this post.)</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Here we have a variant of the repugnant conclusion: the proposition that for any well-off population A, and for any quality of life Q which exceeds the minimal quality of a life worth creating (no matter how small the margin), there exists a possible population B with quality of life Q such that, if you are a member of A, you should, if given the chance, replace population A with population B and become an arbitrary member of B. Since decisions that affect populations tend to be made by people who already exist at the time, it makes sense to frame population ethics questions from the perspective of members of the population like this.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Let P be a variable measuring your personal well-being, T be the sum of variables measuring the the well-beings of everyone in the population, c be a positive constant, and f be a bounded, monotonically increasing function. Most people are at least somewhat altruistic, but also care about themselves much more than about others, so let's assume that your utility function increases monotonically with P and T. Consider the possible utility functions <img src=\"http://latex.codecogs.com/png.latex?U_1(P,T)%20=%20f(P+cT)\" alt=\"\" width=\"170\" height=\"18\" />&nbsp;and <img src=\"http://latex.codecogs.com/png.latex?U_2(P,T)%20=%20P+f(T)\" alt=\"\" width=\"162\" height=\"18\" />. For scenarios in which the total population is fixed, both of these utility functions can be made arbitrarily selfish or altruistic by adjusting f and c, and for scenarios in which your choices only affect T (P is constant) and all options are deterministic, both utility functions will say to act like total utilitarians. In particular, both of them accept the repugnant conclusion. But while <img src=\"http://latex.codecogs.com/png.latex?U_1\" alt=\"\" width=\"17\" height=\"16\" />&nbsp;accepts the variant, <img src=\"http://latex.codecogs.com/png.latex?U_2\" alt=\"\" width=\"17\" height=\"15\" />&nbsp;does not. That is, if you accept the abstract repugnant conclusion, whether or not you would want to implement it in practice depends on not only how selfish or altruistic you are, but also the manner in which you are selfish and altruistic. I don't like the idea of caring about myself arbitrarily little as the population grows to infinity, so it seems intuitive to me that my utility function would be more like <img src=\"http://latex.codecogs.com/png.latex?U_2\" alt=\"\" width=\"17\" height=\"15\" />&nbsp;than like <img src=\"http://latex.codecogs.com/png.latex?U_1\" alt=\"\" width=\"17\" height=\"16\" />.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "korCfFCn6vAYynQCb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "26330", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-05T13:09:27.447Z", "modifiedAt": null, "url": null, "title": "Links!", "slug": "links", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X5b3d58Qyvyvv9jF2/links", "pageUrlRelative": "/posts/X5b3d58Qyvyvv9jF2/links", "linkUrl": "https://www.lesswrong.com/posts/X5b3d58Qyvyvv9jF2/links", "postedAtFormatted": "Thursday, June 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Links!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALinks!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX5b3d58Qyvyvv9jF2%2Flinks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Links!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX5b3d58Qyvyvv9jF2%2Flinks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX5b3d58Qyvyvv9jF2%2Flinks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>I've seen a <a href=\"/r/discussion/lw/kbc/meta_the_decline_of_discussion_now_with_charts/ayvm\">recent claim</a>&nbsp;that adding a summary to links is enough of an inconvenience that the links may not be getting posted. I suppose people aren't checking out the media threads for links.</p>\n<p>As an experiment, I'm posting this as a place for links. Summaries/excerpts are optional-- please don't downvote links just for not having excerpts/summaries.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X5b3d58Qyvyvv9jF2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 1.7701898565709994e-06, "legacy": true, "legacyId": "26331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-05T17:50:26.860Z", "modifiedAt": null, "url": null, "title": "Curiosity: Why did you mega-downvote \"AI is Software\" ?", "slug": "curiosity-why-did-you-mega-downvote-ai-is-software", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.749Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndyWood", "createdAt": "2009-03-05T07:26:00.119Z", "isAdmin": false, "displayName": "AndyWood"}, "userId": "A4pmmDXfX3A4D8y6f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hras3shubknojwfrg/curiosity-why-did-you-mega-downvote-ai-is-software", "pageUrlRelative": "/posts/Hras3shubknojwfrg/curiosity-why-did-you-mega-downvote-ai-is-software", "linkUrl": "https://www.lesswrong.com/posts/Hras3shubknojwfrg/curiosity-why-did-you-mega-downvote-ai-is-software", "postedAtFormatted": "Thursday, June 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Curiosity%3A%20Why%20did%20you%20mega-downvote%20%22AI%20is%20Software%22%20%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACuriosity%3A%20Why%20did%20you%20mega-downvote%20%22AI%20is%20Software%22%20%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHras3shubknojwfrg%2Fcuriosity-why-did-you-mega-downvote-ai-is-software%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Curiosity%3A%20Why%20did%20you%20mega-downvote%20%22AI%20is%20Software%22%20%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHras3shubknojwfrg%2Fcuriosity-why-did-you-mega-downvote-ai-is-software", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHras3shubknojwfrg%2Fcuriosity-why-did-you-mega-downvote-ai-is-software", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>I've never experienced anything like it before on LessWrong. Would you care to re-read the post, and offer your feedback?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hras3shubknojwfrg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 3, "extendedScore": null, "score": 1.770595356608719e-06, "legacy": true, "legacyId": "26333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-05T18:15:39.673Z", "modifiedAt": null, "url": null, "title": "AI is Software is AI", "slug": "ai-is-software-is-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.465Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndyWood", "createdAt": "2009-03-05T07:26:00.119Z", "isAdmin": false, "displayName": "AndyWood"}, "userId": "A4pmmDXfX3A4D8y6f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8v5LCvxXcRJELvF4x/ai-is-software-is-ai", "pageUrlRelative": "/posts/8v5LCvxXcRJELvF4x/ai-is-software-is-ai", "linkUrl": "https://www.lesswrong.com/posts/8v5LCvxXcRJELvF4x/ai-is-software-is-ai", "postedAtFormatted": "Thursday, June 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20is%20Software%20is%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20is%20Software%20is%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8v5LCvxXcRJELvF4x%2Fai-is-software-is-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20is%20Software%20is%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8v5LCvxXcRJELvF4x%2Fai-is-software-is-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8v5LCvxXcRJELvF4x%2Fai-is-software-is-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>Turing's Test is from 1950. We don't judge dogs only by how human they are. Judging software by a human ideal is like a species bias.</p>\n<p>Software is the new System. It errs. Some errors are jokes (witness funny auto-correct). Driver-less cars don't crash like we do. Maybe a few will.</p>\n<p>These processes are our partners now (Siri). Whether a singleton evolves rapidly, software evolves continuously, now.</p>\n<p>&nbsp;</p>\n<p>Crocker's Rules</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8v5LCvxXcRJELvF4x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": -64, "extendedScore": null, "score": 1.7706317505063726e-06, "legacy": true, "legacyId": "26321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-05T21:07:31.918Z", "modifiedAt": null, "url": null, "title": "Confirmation Bias Presentation", "slug": "confirmation-bias-presentation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:35.127Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ixEa8YPyNfnTD93wc/confirmation-bias-presentation", "pageUrlRelative": "/posts/ixEa8YPyNfnTD93wc/confirmation-bias-presentation", "linkUrl": "https://www.lesswrong.com/posts/ixEa8YPyNfnTD93wc/confirmation-bias-presentation", "postedAtFormatted": "Thursday, June 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confirmation%20Bias%20Presentation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfirmation%20Bias%20Presentation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixEa8YPyNfnTD93wc%2Fconfirmation-bias-presentation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confirmation%20Bias%20Presentation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixEa8YPyNfnTD93wc%2Fconfirmation-bias-presentation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixEa8YPyNfnTD93wc%2Fconfirmation-bias-presentation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p>On Monday I need to give a presentation to a group of 6-15 finance professionals on Confirmation Bias. I intend to use the 2-4-6 task to demonstrate it.</p>\n<p>&nbsp;</p>\n<p>Do people have any advise on how to make this work well? Do people tend to fall for it? Does it help them understand afterwards?</p>\n<p>&nbsp;</p>\n<p>(In ages gone by I would have made this post longer, or in the Open Thread, or not at all. But I gather LW has been seeing a drop-off in volume, so I decided I'd lower the bar I set myself)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ixEa8YPyNfnTD93wc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.7708798690907774e-06, "legacy": true, "legacyId": "26334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T00:20:30.692Z", "modifiedAt": null, "url": null, "title": "Reflective Mini-Tasking against Procrastination", "slug": "reflective-mini-tasking-against-procrastination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:35.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qrDz5fDvBrPins5vn/reflective-mini-tasking-against-procrastination", "pageUrlRelative": "/posts/qrDz5fDvBrPins5vn/reflective-mini-tasking-against-procrastination", "linkUrl": "https://www.lesswrong.com/posts/qrDz5fDvBrPins5vn/reflective-mini-tasking-against-procrastination", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reflective%20Mini-Tasking%20against%20Procrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReflective%20Mini-Tasking%20against%20Procrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrDz5fDvBrPins5vn%2Freflective-mini-tasking-against-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reflective%20Mini-Tasking%20against%20Procrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrDz5fDvBrPins5vn%2Freflective-mini-tasking-against-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrDz5fDvBrPins5vn%2Freflective-mini-tasking-against-procrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>This is a slightly polished version of a draft I originally deemed not ready for posting, but given that people keep saying that the Discussion post quality bar is set unreasonably high, here it is.</p>\n<p>Most of us have little aversion to doing something that we perceive as short and easy, even if it is not very interesting. If your English homework consisted of writing a one-line poem (this is actually a <a href=\"http://fs.gallup.unm.edu/one-line.htm\">thing</a>), you'd be less likely to put it off for later, even if writing poetry is one of your least favorite activities. We are certainly more likely to do something if we hate it less, shifting the balance between \"should\" and \"want\" toward want. To <a href=\"http://slatestarcodex.com/2014/05/25/apologia-pro-vita-sua/\">quote</a> one of my three favorite Scott A's, the one with an unhealthy addiction to puns,</p>\n<p style=\"padding-left: 30px;\">Just as drugs mysteriously find their own non-fungible money, enjoyable activities mysteriously find their own non-fungible time. If I had to explain it, I'd say the resource bottleneck isn't time but energy/willpower, and that these look similar because working hard saps energy/willpower and relaxing for a while restores it, so when I have less time I also have less energy/willpower. But some things don't require energy/willpower and so are essentially free.</p>\n<p>And so there are various anti-akrasia proposals based on increasing the want/should ratio (or should it be the want-should difference?) by way of reduction of the perceived will power expenditure to accomplish a task, and/or sweeten it with a reward tacked-on, such as checking off an item on a to-do list and finishing pomodoros. These definitely work some time for some people, but the effect tends to wear off. As one of my coworkers described his attempt to switch from coffee to decaf, the body is fooled for the first few cups, but then it catches on and stops finding decaf enjoyable. (Your experience may vary.) The reason is probably related to the <a href=\"http://en.wikipedia.org/wiki/Negative_feedback\">negative feedback</a>, also known as punishment in the Skinner's <a href=\"http://en.wikipedia.org/wiki/Operant_conditioning\">operant conditioning</a> model.</p>\n<p>I think of many of these attempts to shorten/sweeten a should-task as \"mini-tasking\". It is also commonly known as \"just putting one foot in front of the other\" and \"taking it day-by-day\".</p>\n<p>What I find hard is not the process of working through a completed set of mini-tasks, but actually breaking a large task down into small ones. So instead I tend to switch to a want-task (like writing this) from a should-task, like finding a bug in my code. I suspect that if I had a to-do list of bug finding in front of me, where, once I finish and check off each short item on the list, the larger project would be completed, I would be less inclined to take breaks for fun before feeling guilty and switching back to \"work\". Unfortunately, creating such a list is a non-trivial and fairly involved task in itself, so I rarely get it done, preferring instead to, say, just dive into the code and hope for the best.</p>\n<p>If only I had a way to reflectively (reflexively?) mini-task, where no single action is perceived as long and/or tedious...</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qrDz5fDvBrPins5vn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 30, "extendedScore": null, "score": 9.6e-05, "legacy": true, "legacyId": "26290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T05:46:31.666Z", "modifiedAt": null, "url": null, "title": "[meta] Policy for dealing with users suspected/guilty of mass-downvote harassment?", "slug": "meta-policy-for-dealing-with-users-suspected-guilty-of-mass", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:07.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bFDuc2Dbf7JvWKB6S/meta-policy-for-dealing-with-users-suspected-guilty-of-mass", "pageUrlRelative": "/posts/bFDuc2Dbf7JvWKB6S/meta-policy-for-dealing-with-users-suspected-guilty-of-mass", "linkUrl": "https://www.lesswrong.com/posts/bFDuc2Dbf7JvWKB6S/meta-policy-for-dealing-with-users-suspected-guilty-of-mass", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bmeta%5D%20Policy%20for%20dealing%20with%20users%20suspected%2Fguilty%20of%20mass-downvote%20harassment%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bmeta%5D%20Policy%20for%20dealing%20with%20users%20suspected%2Fguilty%20of%20mass-downvote%20harassment%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFDuc2Dbf7JvWKB6S%2Fmeta-policy-for-dealing-with-users-suspected-guilty-of-mass%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bmeta%5D%20Policy%20for%20dealing%20with%20users%20suspected%2Fguilty%20of%20mass-downvote%20harassment%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFDuc2Dbf7JvWKB6S%2Fmeta-policy-for-dealing-with-users-suspected-guilty-of-mass", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFDuc2Dbf7JvWKB6S%2Fmeta-policy-for-dealing-with-users-suspected-guilty-of-mass", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 373, "htmlBody": "<p>Below is a message I just got from <a href=\"/user/jackk/\">jackk</a>. Some specifics have been redacted 1) so that we can discuss general policy rather than the details of this specific case 2) because presumption of innocence, just in case there happens to be an innocuous explanation to this.</p>\n<blockquote>\n<p>Hi Kaj_Sotala,<br /><br />I'm Jack, one of the Trike devs. I'm messaging you because you're the moderator who commented most recently. A while back the user [REDACTED 1] asked if Trike could look into retributive downvoting against his account. I've done that, and it looks like [REDACTED 2] has downvoted at least [over half of REDACTED 1's comments, amounting to hundreds of downvotes] ([REDACTED 1]'s next-largest downvoter is [REDACTED 3] at -15).<br /><br />What action to take is a community problem, not a technical one, so we'd rather leave that up to the moderators. Some options:<br /><br />1. Ask [REDACTED 2] for the story behind these votes<br />2. Use the \"admin\" account (which exists for sending scripted messages, &amp;c.) to apply an upvote to each downvoted post<br />3. Apply a karma award to [REDACTED 1]'s account. This would fix the karma damage but not the sorting of individual comments<br />4. Apply a negative karma award to [REDACTED 2]'s account. This makes him pay for false downvotes twice over. This isn't possible in the current code, but it's an easy fix<br />5. Ban [REDACTED 2]<br /><br />For future reference, it's very easy for Trike to look at who downvoted someone's account, so if you get questions about downvoting in the future I can run the same report.<br /><br />If you need to verify my identity before you take action, let me know and we'll work something out.<br /><br />-- Jack</p>\n</blockquote>\n<p>So... thoughts? I have mod powers, but when I was granted them I was basically just told to use them to fight spam; there was never any discussion of any other policy, and I don't feel like I have the authority to decide on the suitable course of action without consulting the rest of the community.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bFDuc2Dbf7JvWKB6S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 51, "extendedScore": null, "score": 1.7716294829928157e-06, "legacy": true, "legacyId": "26336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 241, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T07:08:25.309Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA", "slug": "meetup-west-la", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/77sZ8SLWT2WPCdfQL/meetup-west-la", "pageUrlRelative": "/posts/77sZ8SLWT2WPCdfQL/meetup-west-la", "linkUrl": "https://www.lesswrong.com/posts/77sZ8SLWT2WPCdfQL/meetup-west-la", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77sZ8SLWT2WPCdfQL%2Fmeetup-west-la%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77sZ8SLWT2WPCdfQL%2Fmeetup-west-la", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F77sZ8SLWT2WPCdfQL%2Fmeetup-west-la", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/115'>West LA</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 July 2014 04:05:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>VOID</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/115'>West LA</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "77sZ8SLWT2WPCdfQL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA\">Discussion article for the meetup : <a href=\"/meetups/115\">West LA</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 July 2014 04:05:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>VOID</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA1\">Discussion article for the meetup : <a href=\"/meetups/115\">West LA</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA", "anchor": "Discussion_article_for_the_meetup___West_LA", "level": 1}, {"title": "Discussion article for the meetup : West LA", "anchor": "Discussion_article_for_the_meetup___West_LA1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T11:37:16.899Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social Meetup (possibly) in the Sun", "slug": "meetup-london-social-meetup-possibly-in-the-sun-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.069Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xPyPKXBXSyGjT57sP/meetup-london-social-meetup-possibly-in-the-sun-0", "pageUrlRelative": "/posts/xPyPKXBXSyGjT57sP/meetup-london-social-meetup-possibly-in-the-sun-0", "linkUrl": "https://www.lesswrong.com/posts/xPyPKXBXSyGjT57sP/meetup-london-social-meetup-possibly-in-the-sun-0", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20Meetup%20(possibly)%20in%20the%20Sun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20Meetup%20(possibly)%20in%20the%20Sun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxPyPKXBXSyGjT57sP%2Fmeetup-london-social-meetup-possibly-in-the-sun-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20Meetup%20(possibly)%20in%20the%20Sun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxPyPKXBXSyGjT57sP%2Fmeetup-london-social-meetup-possibly-in-the-sun-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxPyPKXBXSyGjT57sP%2Fmeetup-london-social-meetup-possibly-in-the-sun-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/116'>London Social Meetup (possibly) in the Sun</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 June 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Newman's Row, London WC2A 3TL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are having another Social Meetup on Sunday at 2 PM, outdoors [weather permitting].</p>\n\n<p>The meetup will take place at Lincoln's Inn Fields (near Holborn station) and more specifically around <a href=\"https://maps.google.co.uk/maps?q=51.516655,-0.11687\" rel=\"nofollow\">this spot</a> in the northwest quadrant. Alternatively, if the weather is bad, we will be at our usual location, which is just around the corner - <a href=\"https://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">The Shakespeare&#39;s Head</a>.</p>\n\n<p><strong>I'll post an update here and on <a href=\"https://groups.google.com/forum/#!topic/lesswronglondon/VhbvXlmN9l8\">the mailing list</a> on Sunday as to whether we are going to the Park or the Pub.</strong></p>\n\n<p>About London LessWrong:</p>\n\n<p>We run this meetup almost every week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play The Resistance or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p><em>If you get lost, feel free to contact me by e-mail - Tenoke(at)Tenoke.com or by phone - 07425168803. Alternatively, if you want more information about the meetup or anything else, come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">mailing list</a> or the <a href=\"https://www.facebook.com/groups/380103898766356\" rel=\"nofollow\">facebook group</a>.</em></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/116'>London Social Meetup (possibly) in the Sun</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xPyPKXBXSyGjT57sP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "26338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun\">Discussion article for the meetup : <a href=\"/meetups/116\">London Social Meetup (possibly) in the Sun</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 June 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Newman's Row, London WC2A 3TL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are having another Social Meetup on Sunday at 2 PM, outdoors [weather permitting].</p>\n\n<p>The meetup will take place at Lincoln's Inn Fields (near Holborn station) and more specifically around <a href=\"https://maps.google.co.uk/maps?q=51.516655,-0.11687\" rel=\"nofollow\">this spot</a> in the northwest quadrant. Alternatively, if the weather is bad, we will be at our usual location, which is just around the corner - <a href=\"https://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">The Shakespeare's Head</a>.</p>\n\n<p><strong id=\"I_ll_post_an_update_here_and_on_the_mailing_list_on_Sunday_as_to_whether_we_are_going_to_the_Park_or_the_Pub_\">I'll post an update here and on <a href=\"https://groups.google.com/forum/#!topic/lesswronglondon/VhbvXlmN9l8\">the mailing list</a> on Sunday as to whether we are going to the Park or the Pub.</strong></p>\n\n<p>About London LessWrong:</p>\n\n<p>We run this meetup almost every week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....</p>\n\n<p>Sometimes we play The Resistance or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.</p>\n\n<p><em>If you get lost, feel free to contact me by e-mail - Tenoke(at)Tenoke.com or by phone - 07425168803. Alternatively, if you want more information about the meetup or anything else, come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">mailing list</a> or the <a href=\"https://www.facebook.com/groups/380103898766356\" rel=\"nofollow\">facebook group</a>.</em></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun1\">Discussion article for the meetup : <a href=\"/meetups/116\">London Social Meetup (possibly) in the Sun</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social Meetup (possibly) in the Sun", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun", "level": 1}, {"title": "I'll post an update here and on the mailing list on Sunday as to whether we are going to the Park or the Pub.", "anchor": "I_ll_post_an_update_here_and_on_the_mailing_list_on_Sunday_as_to_whether_we_are_going_to_the_Park_or_the_Pub_", "level": 2}, {"title": "Discussion article for the meetup : London Social Meetup (possibly) in the Sun", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T16:10:59.720Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-23", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9YW26vNuLZ2wEsCR5/weekly-lw-meetups-23", "pageUrlRelative": "/posts/9YW26vNuLZ2wEsCR5/weekly-lw-meetups-23", "linkUrl": "https://www.lesswrong.com/posts/9YW26vNuLZ2wEsCR5/weekly-lw-meetups-23", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9YW26vNuLZ2wEsCR5%2Fweekly-lw-meetups-23%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9YW26vNuLZ2wEsCR5%2Fweekly-lw-meetups-23", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9YW26vNuLZ2wEsCR5%2Fweekly-lw-meetups-23", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 572, "htmlBody": "<p><strong>This summary was posted to LW Main on May 31st. The following week's summary is <a href=\"/lw/kbn/new_lw_meetup_bangalore/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/zy\">Chicago Calibration Game:&nbsp;<span class=\"date\">31 May 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10u\">Christchurch, NZ Meetup - Games &amp; Discussion:&nbsp;<span class=\"date\">01 June 2014 04:30PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">14 June 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/10v\">Moscow meet up:&nbsp;<span class=\"date\">08 June 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/10q\">Munich:&nbsp;<span class=\"date\">01 June 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10i\">[Utrecht] Brainstorm and ethics discussion at the Film Caf&eacute;:&nbsp;<span class=\"date\">31 May 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">31 May 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/10t\">Boston - Taking ideas seriously:&nbsp;<span class=\"date\">01 June 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/10p\">Canberra: Decision Theory:&nbsp;<span class=\"date\">14 June 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/10s\">Durham/RTLW Discussion Meetup: Cognitive Load Followup :&nbsp;<span class=\"date\">30 May 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/10x\">London social meetup:&nbsp;<span class=\"date\">01 June 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/10f\">Melbourne June Rationality Dojo: Memory:&nbsp;<span class=\"date\">01 June 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10g\">Sydney Social Meetup - June (Games night):&nbsp;<span class=\"date\">12 June 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/10l\">Sydney Meetup - June:&nbsp;<span class=\"date\">25 June 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/10w\">[Washington DC] Retelling stuff from CFAR Epistemic Rationality for EA :&nbsp;<span class=\"date\">01 June 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9YW26vNuLZ2wEsCR5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.7725321799617893e-06, "legacy": true, "legacyId": "26295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n6AZrpZ8tpTg6P2wk", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T17:39:10.077Z", "modifiedAt": null, "url": null, "title": "Managing one's memory effectively", "slug": "managing-one-s-memory-effectively", "viewCount": null, "lastCommentedAt": "2017-11-27T17:53:07.793Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/umv3DpkCGKt5ppHqn/managing-one-s-memory-effectively", "pageUrlRelative": "/posts/umv3DpkCGKt5ppHqn/managing-one-s-memory-effectively", "linkUrl": "https://www.lesswrong.com/posts/umv3DpkCGKt5ppHqn/managing-one-s-memory-effectively", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Managing%20one's%20memory%20effectively&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AManaging%20one's%20memory%20effectively%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fumv3DpkCGKt5ppHqn%2Fmanaging-one-s-memory-effectively%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Managing%20one's%20memory%20effectively%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fumv3DpkCGKt5ppHqn%2Fmanaging-one-s-memory-effectively", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fumv3DpkCGKt5ppHqn%2Fmanaging-one-s-memory-effectively", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1072, "htmlBody": "<p><em>Note: this post leans heavily on metaphors and examples from computer programming, but I've tried to write it so it's accessible to a determined person with no programming background.</em></p>\n<p>To summarize some info from computer processor design at very high density: There are a variety of ways to manufacture the memory that's used in modern computer processors. &nbsp;There's a trend where the faster a kind of memory is to read from and write to, the more expensive it will be. &nbsp;So modern computers have a hierarchical memory structure: a very small amount of memory that's very fast to do computation with (\"the registers\"), a larger amount of memory that's a bit slower to do computation with, a even larger amount of memory that's even slower to do computation with, and so on. &nbsp;The two layers immediately below the the registers (the L1 cache and the L2 cache) are typically abstracted away from even the assembly language programmer. &nbsp;They store data that's been accessed recently from the level below them (\"main memory\"). &nbsp;The processor will do a lookup in the caches when accessing data; if the data is not already in the cache, that's called a \"cache miss\" and the data will get loaded in to the cache before it's accessed.</p>\n<p>(Please correct me in the comments if I got any of that wrong; it's based on years-old memories of an undergrad computer science course.)</p>\n<p>Lately I've found it useful to think of my memory in the same way. &nbsp;I've got working memory (7&plusmn;2 items?), consisting of things that I'm thinking about in this very moment. &nbsp;I've got short term memory and long term memory. &nbsp;And if I can't find something after trying to think of it for a while, I'll look it up (frequently on Google). &nbsp;Cache miss for the lose.</p>\n<p>What are some implications of thinking about memory about this way?</p>\n<p>&nbsp;</p>\n<h3>Register limitations and chunking</h3>\n<p>When programming, I've noticed that sometimes I'll encounter a problem that's too big to fit in my working memory (WM) all at once. &nbsp;In the spirit of getting stronger, I'm typically tempted to attack the problem head on, but I find that my brain just tends to flit around the details of the problem instead of actually making progress on it. &nbsp;So lately I've been toying with the idea of trying to break off a piece of the problem that can be easily modularized and fits fully in my working memory and then solving it on its own. &nbsp;(Feynman: \"What's the smallest nontrivial example?\") &nbsp;You could turn this definition around and define a good software architecture as one that consists of modular components that can individually be made to fit completely in to one's working memory when reading code.</p>\n<p>As you write or read code modules, you'll come to understand them better and you'll be able to compress or \"chunk\" them so they take up less space in your working memory. &nbsp;This is why&nbsp;<a href=\"http://en.wikipedia.org/wiki/Waterfall_model\">top-down programming</a>&nbsp;doesn't always work that well. &nbsp;You're trying to fit the entire design in your working memory, but because you don't have a good understanding of the components yet (since you haven't written them), you aren't dealing with chunks but pseudochunks. &nbsp;This is true for concepts in general: it takes all of a beginner's WM to comprehend a for loop, but in a master's&nbsp;WM&nbsp;a for loop can be but one piece in a larger puzzle.</p>\n<p>&nbsp;</p>\n<h3>Swapping</h3>\n<p>One thing to observe: you don't get alerted when memory at the top of your mental hierarchy gets overwritten. &nbsp;We've all had the experience of having some idea in the shower and having forgotten it by the time we get out. &nbsp;Similarly, if you're working on a delicate mental task (programming, math, etc.) and you get interrupted, you'll lose mental state related to the problem you're working on.</p>\n<p>If you're having difficulty focusing, this can easily make doing a delicate mental task, like a complicated math problem, much less fun and productive. &nbsp;Instead of actually making progress on the task, your mind drifts away from it, and when you redirect your attention, you find that information related to the problem has swapped out of your working memory or short-term memory and must be re-loaded. &nbsp;If you're getting distracted frequently enough or you're otherwise lacking mental stamina, you may find that you spend the majority of your time context switching instead of making progress on your problem.</p>\n<p>&nbsp;</p>\n<h3>Adding an additional external cache level</h3>\n<p>Anecdotally, adding an additional brain cache level between long-term memory and Google seems like a pretty big win for personal productivity. &nbsp;<a href=\"/lw/egr/personal_information_management/\">My digital notebook</a>&nbsp;(since writing that post, I've started using&nbsp;<a href=\"http://brettterpstra.com/projects/nvalt/\">nvALT</a>) has turned out to be one of my biggest wins where productivity is concerned; it's ballooned to over 700K words, and a decent portion of it consists of copy-pasted snippets that represent the best information from Google searches I've done. &nbsp;A co-worker wrote a tool that allows him to quickly look up how to use software libraries and reports that he's continued to find it very useful years after making it.</p>\n<p>Text is the most obvious example of an exobrain memory device, but here's a more interesting example: if you're cleaning a messy room, you probably don't develop a detailed plan in your head of where all of your stuff will be placed when you finish cleaning. &nbsp;Instead, you incrementally organize things in to related piles, then decide what to do with the piles, using the organization of the items in your room as a kind of external memory aid that allows you to do a mental task that you wouldn't be able to do entirely in your head.</p>\n<p>Would it be accurate to say that you're \"not intelligent enough\" to organize your room in your head without the use of any external memory aides? &nbsp;It doesn't really fit with the colloquial use of \"intelligence\", does it? &nbsp;But in the same way computers are frequently RAM-limited, I suspect that humans are also frequently RAM-limited, even on mental tasks we frequently associate with \"intelligence\". &nbsp;For example, if you're reading a physics textbook and you notice that you're getting confused, you could write down a question that would resolve your confusion, then rewrite the question to be as precise as possible, then list hypotheses that would answer your question along with reasons to believe/disbelieve each hypothesis. &nbsp;By writing things down, you'd be able to devote all of your working memory to the details of a particular aspect of your confusion without losing track of the rest of it.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "umv3DpkCGKt5ppHqn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 25, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "26310", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Note: this post leans heavily on metaphors and examples from computer programming, but I've tried to write it so it's accessible to a determined person with no programming background.</em></p>\n<p>To summarize some info from computer processor design at very high density: There are a variety of ways to manufacture the memory that's used in modern computer processors. &nbsp;There's a trend where the faster a kind of memory is to read from and write to, the more expensive it will be. &nbsp;So modern computers have a hierarchical memory structure: a very small amount of memory that's very fast to do computation with (\"the registers\"), a larger amount of memory that's a bit slower to do computation with, a even larger amount of memory that's even slower to do computation with, and so on. &nbsp;The two layers immediately below the the registers (the L1 cache and the L2 cache) are typically abstracted away from even the assembly language programmer. &nbsp;They store data that's been accessed recently from the level below them (\"main memory\"). &nbsp;The processor will do a lookup in the caches when accessing data; if the data is not already in the cache, that's called a \"cache miss\" and the data will get loaded in to the cache before it's accessed.</p>\n<p>(Please correct me in the comments if I got any of that wrong; it's based on years-old memories of an undergrad computer science course.)</p>\n<p>Lately I've found it useful to think of my memory in the same way. &nbsp;I've got working memory (7\u00b12 items?), consisting of things that I'm thinking about in this very moment. &nbsp;I've got short term memory and long term memory. &nbsp;And if I can't find something after trying to think of it for a while, I'll look it up (frequently on Google). &nbsp;Cache miss for the lose.</p>\n<p>What are some implications of thinking about memory about this way?</p>\n<p>&nbsp;</p>\n<h3 id=\"Register_limitations_and_chunking\">Register limitations and chunking</h3>\n<p>When programming, I've noticed that sometimes I'll encounter a problem that's too big to fit in my working memory (WM) all at once. &nbsp;In the spirit of getting stronger, I'm typically tempted to attack the problem head on, but I find that my brain just tends to flit around the details of the problem instead of actually making progress on it. &nbsp;So lately I've been toying with the idea of trying to break off a piece of the problem that can be easily modularized and fits fully in my working memory and then solving it on its own. &nbsp;(Feynman: \"What's the smallest nontrivial example?\") &nbsp;You could turn this definition around and define a good software architecture as one that consists of modular components that can individually be made to fit completely in to one's working memory when reading code.</p>\n<p>As you write or read code modules, you'll come to understand them better and you'll be able to compress or \"chunk\" them so they take up less space in your working memory. &nbsp;This is why&nbsp;<a href=\"http://en.wikipedia.org/wiki/Waterfall_model\">top-down programming</a>&nbsp;doesn't always work that well. &nbsp;You're trying to fit the entire design in your working memory, but because you don't have a good understanding of the components yet (since you haven't written them), you aren't dealing with chunks but pseudochunks. &nbsp;This is true for concepts in general: it takes all of a beginner's WM to comprehend a for loop, but in a master's&nbsp;WM&nbsp;a for loop can be but one piece in a larger puzzle.</p>\n<p>&nbsp;</p>\n<h3 id=\"Swapping\">Swapping</h3>\n<p>One thing to observe: you don't get alerted when memory at the top of your mental hierarchy gets overwritten. &nbsp;We've all had the experience of having some idea in the shower and having forgotten it by the time we get out. &nbsp;Similarly, if you're working on a delicate mental task (programming, math, etc.) and you get interrupted, you'll lose mental state related to the problem you're working on.</p>\n<p>If you're having difficulty focusing, this can easily make doing a delicate mental task, like a complicated math problem, much less fun and productive. &nbsp;Instead of actually making progress on the task, your mind drifts away from it, and when you redirect your attention, you find that information related to the problem has swapped out of your working memory or short-term memory and must be re-loaded. &nbsp;If you're getting distracted frequently enough or you're otherwise lacking mental stamina, you may find that you spend the majority of your time context switching instead of making progress on your problem.</p>\n<p>&nbsp;</p>\n<h3 id=\"Adding_an_additional_external_cache_level\">Adding an additional external cache level</h3>\n<p>Anecdotally, adding an additional brain cache level between long-term memory and Google seems like a pretty big win for personal productivity. &nbsp;<a href=\"/lw/egr/personal_information_management/\">My digital notebook</a>&nbsp;(since writing that post, I've started using&nbsp;<a href=\"http://brettterpstra.com/projects/nvalt/\">nvALT</a>) has turned out to be one of my biggest wins where productivity is concerned; it's ballooned to over 700K words, and a decent portion of it consists of copy-pasted snippets that represent the best information from Google searches I've done. &nbsp;A co-worker wrote a tool that allows him to quickly look up how to use software libraries and reports that he's continued to find it very useful years after making it.</p>\n<p>Text is the most obvious example of an exobrain memory device, but here's a more interesting example: if you're cleaning a messy room, you probably don't develop a detailed plan in your head of where all of your stuff will be placed when you finish cleaning. &nbsp;Instead, you incrementally organize things in to related piles, then decide what to do with the piles, using the organization of the items in your room as a kind of external memory aid that allows you to do a mental task that you wouldn't be able to do entirely in your head.</p>\n<p>Would it be accurate to say that you're \"not intelligent enough\" to organize your room in your head without the use of any external memory aides? &nbsp;It doesn't really fit with the colloquial use of \"intelligence\", does it? &nbsp;But in the same way computers are frequently RAM-limited, I suspect that humans are also frequently RAM-limited, even on mental tasks we frequently associate with \"intelligence\". &nbsp;For example, if you're reading a physics textbook and you notice that you're getting confused, you could write down a question that would resolve your confusion, then rewrite the question to be as precise as possible, then list hypotheses that would answer your question along with reasons to believe/disbelieve each hypothesis. &nbsp;By writing things down, you'd be able to devote all of your working memory to the details of a particular aspect of your confusion without losing track of the rest of it.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Register limitations and chunking", "anchor": "Register_limitations_and_chunking", "level": 1}, {"title": "Swapping", "anchor": "Swapping", "level": 1}, {"title": "Adding an additional external cache level", "anchor": "Adding_an_additional_external_cache_level", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["stDijTKuto52M5wQT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-06T23:53:46.887Z", "modifiedAt": null, "url": null, "title": "Mathematics as a lossy compression algorithm gone wild", "slug": "mathematics-as-a-lossy-compression-algorithm-gone-wild", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q6ZJCS4A7JGfHtqtz/mathematics-as-a-lossy-compression-algorithm-gone-wild", "pageUrlRelative": "/posts/q6ZJCS4A7JGfHtqtz/mathematics-as-a-lossy-compression-algorithm-gone-wild", "linkUrl": "https://www.lesswrong.com/posts/q6ZJCS4A7JGfHtqtz/mathematics-as-a-lossy-compression-algorithm-gone-wild", "postedAtFormatted": "Friday, June 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mathematics%20as%20a%20lossy%20compression%20algorithm%20gone%20wild&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMathematics%20as%20a%20lossy%20compression%20algorithm%20gone%20wild%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq6ZJCS4A7JGfHtqtz%2Fmathematics-as-a-lossy-compression-algorithm-gone-wild%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mathematics%20as%20a%20lossy%20compression%20algorithm%20gone%20wild%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq6ZJCS4A7JGfHtqtz%2Fmathematics-as-a-lossy-compression-algorithm-gone-wild", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq6ZJCS4A7JGfHtqtz%2Fmathematics-as-a-lossy-compression-algorithm-gone-wild", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1457, "htmlBody": "<p>This is yet another half-baked post from my old draft collection, but feel free to Crocker away.</p>\n<p>&nbsp;</p>\n<p>There is an old adage from Eugene Wigner known as the \"<a href=\"http://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences\">Unreasonable Effectiveness of Mathematics</a>\". Wikipedia:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\">the&nbsp;</span><a style=\"text-decoration: none; color: #0b0080; background-image: none; font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\" title=\"Mathematics\" href=\"http://en.wikipedia.org/wiki/Mathematics\">mathematical</a><span style=\"font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\">&nbsp;structure of a&nbsp;</span><a style=\"text-decoration: none; color: #0b0080; background-image: none; font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\" title=\"Theoretical physics\" href=\"http://en.wikipedia.org/wiki/Theoretical_physics\">physical theory</a><span style=\"font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\">&nbsp;often points the way to further advances in that theory and even to&nbsp;</span><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\" title=\"Empirical\" href=\"http://en.wikipedia.org/wiki/Empirical\">empirical</a><span style=\"font-family: sans-serif; font-size: 12.571428298950195px; line-height: 19.196428298950195px;\">&nbsp;predictions.</span></p>\n<p>The way I interpret is that it is possible to find an algorithm to compress a set of data points in a way that is also good at predicting other data points, not yet observed. In yet other words, a good approximation is, for some reason,&nbsp;sometimes&nbsp;also a good extrapolation. The rest of this post elaborates on this anti-Platonic point of view.</p>\n<p>Now, this point of view is not exactly how most people see math. They imagine it as some near-magical thing that transcends science and reality and, when discovered, learned and used properly, gives one limited powers of clairvoyance. While only the select few wizard have the power to discover new spells (they are known as scientists), the rank and file can still use some of the incantations to make otherwise impossible things to happen (they are known as engineers).&nbsp;</p>\n<p>This metaphysical view is colorfully expressed by&nbsp;<a href=\"http://www.brainyquote.com/quotes/quotes/s/stephenhaw135874.html\">Stephen Hawking</a>:</p>\n<p style=\"padding-left: 30px;\">What is it that breathes fire into the equations and makes a universe for them to describe? The usual approach of science of constructing a mathematical model cannot answer the questions of why there should be a universe for the model to describe. Why does the universe go to all the bother of existing?</p>\n<p>Should one interpret this as if he presumes here that math, in the form of \"the equations\" comes first and only then there is a physical universe for math to describe, for some values of \"first\" and \"then\", anyway? Platonism seems to reach roughly the same conclusions:</p>\n<p>Wikipedia&nbsp;<a href=\"http://en.wikipedia.org/wiki/Platonism\">defines platonism</a>&nbsp;as</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\">the philosophy that affirms the existence of&nbsp;</span><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\" title=\"Abstract objects\" href=\"http://en.wikipedia.org/wiki/Abstract_objects\">abstract objects</a><span style=\"font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\">, which are asserted to \"exist\" in a \"third realm&nbsp;</span><em style=\"font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\">distinct both from the sensible external world and from the internal world of consciousness, and is the opposite of&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Nominalism\" href=\"http://en.wikipedia.org/wiki/Nominalism\">nominalism</a></em></p>\n<p>In other words, math would have \"existed\" even if there were no humans around to discover it. In this sense, it is \"real\", as opposed to \"imagined by humans\". Wikipedia on&nbsp;<a href=\"http://en.wikipedia.org/wiki/Mathematical_realism#Mathematical_realism\">mathematical realism</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\">mathematical entities exist independently of the human&nbsp;</span><a style=\"text-decoration: none; color: #0b0080; background-image: none; font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\" title=\"Mind\" href=\"http://en.wikipedia.org/wiki/Mind\">mind</a><span style=\"font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\">. Thus humans do not invent mathematics, but rather discover it, and any other intelligent beings in the universe would presumably do the same. In this point of view, there is really one sort of mathematics that can be discovered:&nbsp;</span><a style=\"text-decoration: none; color: #0b0080; background-image: none; font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\" title=\"Triangle\" href=\"http://en.wikipedia.org/wiki/Triangle\">triangles</a><span style=\"font-family: sans-serif; font-size: 12.727272033691406px; line-height: 19.190340042114258px;\">, for example, are real entities, not the creations of the human mind.</span></p>\n<p>Of course, the debate on whether mathematics is \"invented\" or \"discovered\" is very old. Eliezer-2008 chimes in in&nbsp;<a href=\"/lw/mq/beautiful_math/\">http://lesswrong.com/lw/mq/beautiful_math/</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">To say that human beings \"invented numbers\" - or invented the structure implicit in numbers - seems like claiming that Neil Armstrong hand-crafted the Moon.&nbsp; The universe existed before there were any sentient beings to observe it, which implies that physics preceded physicists.&nbsp;</span></p>\n<p>and later:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The amazing thing is that math is a game without a designer, and yet it is eminently playable.</span></p>\n<p>In the above, I assume that what Eliezer means by physics is not the science of physics (a human endeavor), but the laws according to which our universe came into existence and evolved. These laws are not the universe itself (which would make the statement \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">physics preceded physicists\" simply \"the universe preceded physicists\", a vacuous tautology)</span>, but some separate laws governing it, out there to be discovered. If only we knew them all, we could create a copy of the universe from scratch, if not \"for real\", then at least as a&nbsp;<a href=\"http://xkcd.com/505/\">faithful model</a>. This&nbsp;<strong>universe-making recipe</strong>&nbsp;is then what physics (the laws, not science) is.</p>\n<p>And these laws apparently require mathematics to be properly expressed, so mathematics must \"exist\" in order for the laws of physics to exist.</p>\n<p>Is this the only way to think of math? I don't think so. Let us suppose that the physical universe is the only \"real\" thing, none of those Platonic abstract objects. Let is further suppose that this universe is (somewhat) predictable. Now, what does it mean for the universe to be predictable to begin with? Predictable by whom or by what? Here is one approach to predictability, based on agency: a small part of the universe (you, the agent) can construct/contain a model of some larger part of the universe (say, the earth-sun system, including you) and optimize its own actions (to, say, wake up the next morning just as the sun rises).&nbsp;</p>\n<p>Does waking up on time count as doing math? Certainly not by the conventional definition of math. Do migratory birds do math when they migrate thousands of miles twice a year, successfully predicting that there would be food sources and warm weather once they get to their destination? Certainly not by the conventional definition of math. Now, suppose a ship captain lays a course to follow the birds, using maps and tables and calculations? Does this count as doing math? Why, certainly the captain would say so, even if the math in question is relatively simple. Sometimes the inputs both the birds and the humans are using are the same: sun and star positions at various times of the day and night, the magnetic field direction, the shape of the terrain.</p>\n<p>What is the difference between what the birds are doing and what humans are doing? Certainly both make predictions about the universe and act on them. Only birds do this instinctively and humans consciously, by \"applying math\". But this is a statement about the differences in cognition, not about some&nbsp;Platonic&nbsp;mathematical objects. One can even say that birds perform the relevant math instinctively. But this is a rather slippery slope. By this definition amoebas solve the diffusion equation when they move along the sugar gradient toward a food source. While this view has merits, the mathematicians analyzing certain aspects of the Navier-Stokes equation might not take kindly being compared to a protozoa.&nbsp;</p>\n<p>So, like JPEG is a lossy image compression algorithm of the part of the universe which creates an image on our retina when we look at a picture, the collection of the Newton's laws is a lossy compression algorithm which describes how a thrown rock falls to the ground, or how planets go around the Sun. in both cases we, a tiny part of the universe, are able to model and predict a much larger part, albeit with some loss of accuracy.</p>\n<p>What would it mean then for a Universe to not \"run on math\"? In this approach it means that in such a universe no subsystem can contain a model, no matter how coarse, of a larger system. In other words, such a universe is completely unpredictable from the inside. Such a universe cannot contain agents, intelligence or even the simplest life forms.&nbsp;</p>\n<p>Now, to the \"gone wild\" part of the title. This is where the traditional applied math, like counting sheep, or calculating how many cannons you can arm a ship with before it sinks, or how to predict/cause/exploit the stock market fluctuations, becomes \"pure math\", or math for math's sake, be it proving the Pythagorean theorem or solving a Millennium Prize problem. At this point the mathematician is no longer interested in modeling a larger part of the universe (except insofar as she predicts that it would be a fun thing to do for her, which is probably not very mathematical).</p>\n<p>Now, there is at least one serious objection to this \"math is jpg\" epistemology. It goes as follows: \"in any universe, no matter how convoluted, 1+1=2, so clearly mathematics transcends the specific structure of a single universe\". I am skeptical of this logic, since to me 1,+,= and 2 are semi-intuitive models running in our minds, which evolved to model the universe we live in. I can certainly imagine a universe where none of these concepts would be useful in predicting anything, and so they would never evolve in the \"mind\" of whatever entity inhabits it. To me mathematical concepts are no more universal than moral concepts: sometimes they crystallize into useful models, and sometimes they do not. Like the human concept of honor would not be useful to spiders, the concept of numbers (which probably is useful to spiders) would not be useful in a universe where size is not a well-defined concept (like something based on a <a href=\"http://en.wikipedia.org/wiki/Conformal_field_theory\">Conformal Field Theory</a>).</p>\n<p>So the \"Unreasonable Effectiveness of Mathematics\" is not at all unreasonable: it reflects the predictability of our universe. Nothing \"breathes fire into the equations and makes a universe for them to describe\", the equations are but one way a small part of the universe predicts the salient features of a larger part of it. Rather, an interesting question is what features of a predictable universe enable agents to appear in it, and how complex and powerful can these agents get.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hQiuNkBhn6xxcedTD": 2, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q6ZJCS4A7JGfHtqtz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 53, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "24386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Rjw4qhEMvqskhL6Gm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T03:14:30.621Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Bring a topic and we'll talk about it", "slug": "meetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ELriSBxJaEQ5HgTQF/meetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it", "pageUrlRelative": "/posts/ELriSBxJaEQ5HgTQF/meetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it", "linkUrl": "https://www.lesswrong.com/posts/ELriSBxJaEQ5HgTQF/meetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Bring%20a%20topic%20and%20we'll%20talk%20about%20it&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Bring%20a%20topic%20and%20we'll%20talk%20about%20it%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELriSBxJaEQ5HgTQF%2Fmeetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Bring%20a%20topic%20and%20we'll%20talk%20about%20it%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELriSBxJaEQ5HgTQF%2Fmeetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELriSBxJaEQ5HgTQF%2Fmeetup-washington-dc-bring-a-topic-and-we-ll-talk-about-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/117'>Washington DC: Bring a topic and we'll talk about it</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 June 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bring / come up with something you want to discuss, and we'll talk about it. Ideally many people would do this, and we could sort of go around similarly to a talks meetup.</p>\n\n<p><a href=\"http://slatestarcodex.com/2014/06/03/asches-to-asches/\" rel=\"nofollow\">This</a> is my thing.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/117'>Washington DC: Bring a topic and we'll talk about it</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ELriSBxJaEQ5HgTQF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7734922138487756e-06, "legacy": true, "legacyId": "26341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Bring_a_topic_and_we_ll_talk_about_it\">Discussion article for the meetup : <a href=\"/meetups/117\">Washington DC: Bring a topic and we'll talk about it</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 June 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bring / come up with something you want to discuss, and we'll talk about it. Ideally many people would do this, and we could sort of go around similarly to a talks meetup.</p>\n\n<p><a href=\"http://slatestarcodex.com/2014/06/03/asches-to-asches/\" rel=\"nofollow\">This</a> is my thing.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Bring_a_topic_and_we_ll_talk_about_it1\">Discussion article for the meetup : <a href=\"/meetups/117\">Washington DC: Bring a topic and we'll talk about it</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Bring a topic and we'll talk about it", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Bring_a_topic_and_we_ll_talk_about_it", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Bring a topic and we'll talk about it", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Bring_a_topic_and_we_ll_talk_about_it1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T05:47:25.375Z", "modifiedAt": null, "url": null, "title": "Meetup : Second MIRIxLosAngeles Meeting", "slug": "meetup-second-mirixlosangeles-meeting", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4kNygXp4ci4rMuuN/meetup-second-mirixlosangeles-meeting", "pageUrlRelative": "/posts/H4kNygXp4ci4rMuuN/meetup-second-mirixlosangeles-meeting", "linkUrl": "https://www.lesswrong.com/posts/H4kNygXp4ci4rMuuN/meetup-second-mirixlosangeles-meeting", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Second%20MIRIxLosAngeles%20Meeting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Second%20MIRIxLosAngeles%20Meeting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4kNygXp4ci4rMuuN%2Fmeetup-second-mirixlosangeles-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Second%20MIRIxLosAngeles%20Meeting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4kNygXp4ci4rMuuN%2Fmeetup-second-mirixlosangeles-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4kNygXp4ci4rMuuN%2Fmeetup-second-mirixlosangeles-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/118'>Second MIRIxLosAngeles Meeting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 June 2014 10:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">USC Institute for Creative Technologies 12015 Waterfront Drive Playa Vista, CA 90094-2536.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second MIRIxLosAngeles meeting will be held on Saturday, June 14, at 10:00 AM. The location will be the same as last time:</p>\n\n<p>USC Institute for Creative Technologies</p>\n\n<p>12015 Waterfront Drive</p>\n\n<p>Playa Vista, CA 90094-2536.</p>\n\n<p>If you would like to join us, please let me know, so I can know to expect you and give you necessary contact information. Due to the primary interests of the organizers, and some recent results I have to share, I expect for us to focus on the questions of \"How should an agent assign probabilities to logical statements?\" and \"How should an agent assign probabilities to statements about his own probability function?\" However, I expect that other people will come and direct the conversation in other useful directions, so it is hard to predict.</p>\n\n<p>Experience in artificial intelligence will not be at all necessary, but experience in mathematics probably is. If you can follow the MIRI publications, you should be fine.</p>\n\n<p>This event will be in the spirit of collaboration with MIRI, and will attempt to respect their guidelines on doing research that will decrease, rather than increase, existential risk. As such, practical implementation questions related to making an approximate Bayesian reasoner fast enough to operate in the real world will not be on-topic. Rather, the focus will be on the abstract mathematical design of a system capable of having reflexively consistent goals, preforming naturalistic induction, et cetera.</p>\n\n<p>Food and refreshments will be provided for this event, courtesy of MIRI.</p>\n\n<p>If you are not local to Los Angeles, you may want to consider hosting your own MIRIx event. You can find more information about this <a href=\"http://intelligence.org/mirix/\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/118'>Second MIRIxLosAngeles Meeting</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4kNygXp4ci4rMuuN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7737135921242e-06, "legacy": true, "legacyId": "26342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Second_MIRIxLosAngeles_Meeting\">Discussion article for the meetup : <a href=\"/meetups/118\">Second MIRIxLosAngeles Meeting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 June 2014 10:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">USC Institute for Creative Technologies 12015 Waterfront Drive Playa Vista, CA 90094-2536.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second MIRIxLosAngeles meeting will be held on Saturday, June 14, at 10:00 AM. The location will be the same as last time:</p>\n\n<p>USC Institute for Creative Technologies</p>\n\n<p>12015 Waterfront Drive</p>\n\n<p>Playa Vista, CA 90094-2536.</p>\n\n<p>If you would like to join us, please let me know, so I can know to expect you and give you necessary contact information. Due to the primary interests of the organizers, and some recent results I have to share, I expect for us to focus on the questions of \"How should an agent assign probabilities to logical statements?\" and \"How should an agent assign probabilities to statements about his own probability function?\" However, I expect that other people will come and direct the conversation in other useful directions, so it is hard to predict.</p>\n\n<p>Experience in artificial intelligence will not be at all necessary, but experience in mathematics probably is. If you can follow the MIRI publications, you should be fine.</p>\n\n<p>This event will be in the spirit of collaboration with MIRI, and will attempt to respect their guidelines on doing research that will decrease, rather than increase, existential risk. As such, practical implementation questions related to making an approximate Bayesian reasoner fast enough to operate in the real world will not be on-topic. Rather, the focus will be on the abstract mathematical design of a system capable of having reflexively consistent goals, preforming naturalistic induction, et cetera.</p>\n\n<p>Food and refreshments will be provided for this event, courtesy of MIRI.</p>\n\n<p>If you are not local to Los Angeles, you may want to consider hosting your own MIRIx event. You can find more information about this <a href=\"http://intelligence.org/mirix/\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Second_MIRIxLosAngeles_Meeting1\">Discussion article for the meetup : <a href=\"/meetups/118\">Second MIRIxLosAngeles Meeting</a></h2>", "sections": [{"title": "Discussion article for the meetup : Second MIRIxLosAngeles Meeting", "anchor": "Discussion_article_for_the_meetup___Second_MIRIxLosAngeles_Meeting", "level": 1}, {"title": "Discussion article for the meetup : Second MIRIxLosAngeles Meeting", "anchor": "Discussion_article_for_the_meetup___Second_MIRIxLosAngeles_Meeting1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T05:55:42.257Z", "modifiedAt": null, "url": null, "title": "Second MIRIxLosAngeles Meeting", "slug": "second-mirixlosangeles-meeting", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m8BAGfw8AbvpGAMe3/second-mirixlosangeles-meeting", "pageUrlRelative": "/posts/m8BAGfw8AbvpGAMe3/second-mirixlosangeles-meeting", "linkUrl": "https://www.lesswrong.com/posts/m8BAGfw8AbvpGAMe3/second-mirixlosangeles-meeting", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Second%20MIRIxLosAngeles%20Meeting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASecond%20MIRIxLosAngeles%20Meeting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8BAGfw8AbvpGAMe3%2Fsecond-mirixlosangeles-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Second%20MIRIxLosAngeles%20Meeting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8BAGfw8AbvpGAMe3%2Fsecond-mirixlosangeles-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8BAGfw8AbvpGAMe3%2Fsecond-mirixlosangeles-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>The second MIRIxLosAngeles meeting will be held on Saturday, June 14, at 10:00 AM. The location will be the same as last time:</p>\n<p>USC Institute for Creative Technologies</p>\n<p>12015 Waterfront Drive</p>\n<p>Playa Vista, CA 90094-2536.</p>\n<p>If you would like to join us, please let me know, so I can know to expect you and give you necessary contact information. Due to the primary interests of the organizers, and some recent results I have to share, I expect for us to focus on the questions of \"How should an agent assign probabilities to logical statements?\" and \"How should an agent assign probabilities to statements about his own probability function?\" However, I expect that other people will come and direct the conversation in other useful directions, so it is hard to predict.</p>\n<p>Experience in artificial intelligence will not be at all necessary, but experience in mathematics probably is. If you can follow the MIRI publications, you should be fine.</p>\n<p>This event will be in the spirit of collaboration with MIRI, and will attempt to respect their guidelines on doing research that will decrease, rather than increase, existential risk. As such, practical implementation questions related to making an approximate Bayesian reasoner fast enough to operate in the real world will not be on-topic. Rather, the focus will be on the abstract mathematical design of a system capable of having reflexively consistent goals, preforming naturalistic induction, et cetera.</p>\n<p>Food and refreshments will be provided for this event, courtesy of MIRI.</p>\n<p>If you are not local to Los Angeles, you may want to consider hosting your own MIRIx event. You can find more information about this <a href=\"http://intelligence.org/mirix/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m8BAGfw8AbvpGAMe3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 15, "extendedScore": null, "score": 1.773725582773064e-06, "legacy": true, "legacyId": "26343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T14:03:40.237Z", "modifiedAt": null, "url": null, "title": "Examples of Rationality Techniques adopted by the Masses", "slug": "examples-of-rationality-techniques-adopted-by-the-masses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:08.641Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "edanm", "createdAt": "2011-12-18T10:42:18.148Z", "isAdmin": false, "displayName": "edanm"}, "userId": "t8s3fuCWNSgMHQFYa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T9iPMG8dsdbqzK6sJ/examples-of-rationality-techniques-adopted-by-the-masses", "pageUrlRelative": "/posts/T9iPMG8dsdbqzK6sJ/examples-of-rationality-techniques-adopted-by-the-masses", "linkUrl": "https://www.lesswrong.com/posts/T9iPMG8dsdbqzK6sJ/examples-of-rationality-techniques-adopted-by-the-masses", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Examples%20of%20Rationality%20Techniques%20adopted%20by%20the%20Masses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExamples%20of%20Rationality%20Techniques%20adopted%20by%20the%20Masses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT9iPMG8dsdbqzK6sJ%2Fexamples-of-rationality-techniques-adopted-by-the-masses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Examples%20of%20Rationality%20Techniques%20adopted%20by%20the%20Masses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT9iPMG8dsdbqzK6sJ%2Fexamples-of-rationality-techniques-adopted-by-the-masses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT9iPMG8dsdbqzK6sJ%2Fexamples-of-rationality-techniques-adopted-by-the-masses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>Hi Everyone,</p>\n<p>I was discussing LessWrong and rationality with a few people the other day, and I hit upon a common snag in the conversation.</p>\n<p>My conversation partners <strong>agreed</strong>&nbsp;that rationality is a good idea in general, <strong>agreed</strong>&nbsp;that there are things you <em>personally</em>&nbsp;can do to improve your decision-making. But their point of view was that, while this is a nice ideal to strive to for yourself, there's little progress that could be made in the general population, who will remain irrational. Since one of the missions of CFAR/LW is to raise the sanity waterline, this is of course a problem.</p>\n<p>So here's my question, something I was unable to think of in the spur of the argument - what are good examples of rationality techniques that have <strong>already</strong>&nbsp;become commonly used in the general population? E.g., one could say \"the scientific method\", which is certainly a kind of rationality technique that's going semi-wide adoption (though nowhere near universal). Are there any other examples? If you send a random from today back in time, other than specific advances in science, will there be anything they could teach people from the old days in terms of general thinking?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T9iPMG8dsdbqzK6sJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 19, "extendedScore": null, "score": 1.7744323686214236e-06, "legacy": true, "legacyId": "26346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T16:44:41.000Z", "modifiedAt": null, "url": null, "title": "Archipelago and Atomic Communitarianism", "slug": "archipelago-and-atomic-communitarianism", "viewCount": null, "lastCommentedAt": "2021-08-18T23:17:46.361Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aP36QcAsxyuEispq6/archipelago-and-atomic-communitarianism", "pageUrlRelative": "/posts/aP36QcAsxyuEispq6/archipelago-and-atomic-communitarianism", "linkUrl": "https://www.lesswrong.com/posts/aP36QcAsxyuEispq6/archipelago-and-atomic-communitarianism", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Archipelago%20and%20Atomic%20Communitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArchipelago%20and%20Atomic%20Communitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaP36QcAsxyuEispq6%2Farchipelago-and-atomic-communitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Archipelago%20and%20Atomic%20Communitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaP36QcAsxyuEispq6%2Farchipelago-and-atomic-communitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaP36QcAsxyuEispq6%2Farchipelago-and-atomic-communitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6629, "htmlBody": "<p><b>I.</b></p>\n<p>In the old days, you had your Culture, and that was that. Your Culture told you lots of stuff about what you were and weren&#8217;t allowed to do, and by golly you listened. Your Culture told you to work the job prescribed to you by your caste and gender, to marry who your parents told you to marry or at <i>least</i> someone of the opposite sex, to worship at the proper temples and the proper times, and to talk about <i>proper</i> things as opposed to the blasphemous things said by the tribe over there.</p>\n<p>Then we got Liberalism, which said all of that was mostly bunk. Like Wicca, its motto is &#8220;Do as you will, so long as it harms none&#8221;. Or in more political terms, &#8220;Your right to swing your fist ends where my nose begins&#8221; or &#8220;If you don&#8217;t like gay sex, don&#8217;t have any&#8221; or &#8220;If you don&#8217;t like this TV program, don&#8217;t watch it&#8221; or &#8220;What happens in the bedroom between consenting adults is none of your business&#8221; or &#8220;It neither breaks my arm nor picks my pocket&#8221;. Your job isn&#8217;t to enforce your conception of virtue upon everyone to build the Virtuous Society, it&#8217;s to live your own life the way you want to live it and let other people live <i>their</i> own lives the way <i>they</i> want to live them. This is the much-maligned &#8220;atomic individualism,&#8221; or maybe just liberalism boiled down to its pure essence.</p>\n<p>But atomic individualism wasn&#8217;t as great a solution as it sounded. Maybe one of the first cracks was tobacco ads. Even though putting up a billboard saying &#8220;SMOKE MARLBORO&#8221; neither breaks anyone&#8217;s arm nor picks their pocket, it shifts social expectations in such a way that bad effects occur. It&#8217;s hard to dismiss that with &#8220;Well, it&#8217;s people&#8217;s own choice to smoke and they should live their lives the way they want&#8221; if studies show that more people will want to live their lives in a way that gives them cancer in the presence of the billboard than otherwise.</p>\n<p>From there we go into policies like Michael Bloomberg&#8217;s ban on giant sodas. While the soda ban itself was probably as much symbolic as anything, it&#8217;s hard to argue with the impetus behind it &#8211; a culture where everyone gets exposed to the option to buy very very unhealthy food all the time is going to be less healthy than one where there are some regulations in place to make EAT THIS DONUT NOW a less salient option. I mean, I <i>know</i> this is true. A few months ago when I was on a diet I <i>cringed</i> every time one my coworkers brought in a box of free donuts and placed wide-open in the doctors&#8217; lounge; there was <i>no way</i> I wasn&#8217;t going to take one (or two, or three). I could ask people to stop, but they probably wouldn&#8217;t, and even if they did I&#8217;d just encounter the wide-open box of free donuts <i>somewhere else</i>. I&#8217;m not proposing that it is <i>ethically wrong</i> to bring in free donuts or that banning them is the correct policy, but I do want to make it clear that stating &#8220;it&#8217;s your free choice to partake or not&#8221; doesn&#8217;t eliminate the problem, and that this points to an entire class of serious issues where atomic individualism as construed above is at best an imperfect heuristic.</p>\n<p>And I would be remiss talking about the modern turn away from individualism without mentioning social justice. The same people who once deployed individualistic arguments against conservatives: &#8220;If you don&#8217;t like profanity, don&#8217;t use it&#8221;, &#8220;If you don&#8217;t like this offensive TV show, don&#8217;t watch it&#8221;, &#8220;If you don&#8217;t like pornography, don&#8217;t buy it&#8221; &#8211; are now concerned about people using ethnic slurs, TV shows without enough minority characters, and pornography that encourages the objectification of women. I&#8217;ve objected to some of this on <A HREF=\"http://slatestarcodex.com/2013/06/22/social-psychology-is-a-flamethrower/\">purely empirical grounds</A>, but the <A HREF=\"http://lesswrong.com/lw/2k/the_least_convenient_possible_world/\">least convenient possible world</A> is the one where the purely empirical objections fall flat. If they ever discover proof positive that yeah, pornographication makes women hella objectified, is it acceptable to censor or ban misogynist media on a society-wide level?</p>\n<p>And if the answer is yes &#8211; and if such media like really, <i>really</i> increases the incidence of rape I&#8217;m not sure how it couldn&#8217;t be &#8211; then what about all those conservative ideas we&#8217;ve been neglecting for so long? What if strong, cohesive, religious, demographically uniform communities make people more trusting, generous, and cooperative in a way that <i>also</i> decreases violent crime and other forms of misery? We have <A HREF=\"http://smile.amazon.com/The-Righteous-Mind-Politics-Religion/dp/0307455777/\">lots of evidence</A> that this is true, and although we can doubt each individual study, we owe conservatives the courtesy of imagining the possible world in which they are right, the same as anti-misogyny leftists. Maybe media glorifying criminals or lionizing nonconformists above those who quietly follow cultural norms has the same kind of erosive effects on &#8220;values&#8221; as misogynist media. Or, at the very least, we ought to have a good philosophy in place so that we have some idea what to do it if does.</p>\n<p><b>II.</b></p>\n<p>A while ago, in Part V of <A HREF=\"http://slatestarcodex.com/2014/02/23/in-favor-of-niceness-community-and-civilization/\">this essay</A>, I praised liberalism as the only peaceful answer to Hobbes&#8217; dilemma of the war of all against all.</p>\n<p>Hobbes says that if everyone&#8217;s fighting then everyone loses out. Even the winners probably end up worse off than if they had just been able to live in peace. He says that governments are good ways to prevent this kind of conflict. Someone &#8211; in his formulation a king &#8211; tells everyone else what they&#8217;re going to do, and then everyone else does it. No fighting necessary. If someone tries to start a conflict by ignoring the king, the king crushes them like a bug, no prolonged fighting involved.</p>\n<p>But this replaces the problem of potential warfare with the problem of potential tyranny. So we&#8217;ve mostly shifted from absolute monarchies to other forms of government, which is all nice and well except that governments allow a <i>different</i> kind of war of all against all. Instead of trying to kill their enemies and steal their stuff, people are tempted to ban their enemies and confiscate their stuff. Instead of killing the Protestants, the Catholics simply ban Protestantism. Instead of forming vigilante mobs to stone homosexuals, the straights merely declare homosexuality is punishable by death. It <i>might</i> be better than the alternative &#8211; at least everyone knows where they stand and things stay peaceful &#8211; but the end result is still a lot of pretty miserable people.</p>\n<p>Liberalism is a new form of Hobbesian equilibrium where the government enforces not only a ban on killing and stealing from people you don&#8217;t like, but also a ban on tyrannizing them out of existence. This is the famous &#8220;freedom of religion&#8221; and &#8220;freedom of speech&#8221; and so on, as well as the &#8220;freedom of what happens in the bedroom between consenting adults&#8221;. The Catholics don&#8217;t try to ban Protestantism, the Protestants don&#8217;t try to ban Catholicism, and everyone is happy.</p>\n<p>Liberalism only works when it&#8217;s clear to everyone on all sides that there&#8217;s a certain neutral principle everyone has to stick to. The neutral principle can&#8217;t be the Bible, or Atlas Shrugged, or anything that makes it look like one philosophy is allowed to judge the others. Right now that principle is the Principle of Harm: you can do whatever you like unless it harms other people, in which case stop. We seem to have inelegantly tacked on an &#8220;also, we can collect taxes and use them for a social safety net and occasional attempts at social progress&#8221;, but it seems to be working pretty okay too.</p>\n<p>The Strict Principle of Harm says that pretty much the only two things the government can get angry at is literally breaking your leg or picking your pocket &#8211; violence or theft. The Loose Principle of Harm says that the government can get angry at complicated indirect harms, things that Weaken The Moral Fabric Of Society. Like putting up tobacco ads. Or having really really big sodas. Or publishing hate speech against minorities. Or eroding trust in the community. Or media that objectifies women.</p>\n<p>No one except the most ideologically pure libertarians seems to want to insist on the Strict Principle of Harm. But allowing the Loose Principle Of Harm restores all of the old wars to control other people that liberalism was supposed to prevent. The one person says &#8220;Gay marriage will result in homosexuality becoming more accepted, leading to increased rates of STDs! That&#8217;s a harm! We must ban gay marriage!&#8221; Another says &#8220;Allowing people to send their children to non-public schools could lead to kids at religious schools that preach against gay people, causing those children to commit hate crimes when they grow up! That&#8217;s a harm! We must ban non-public schools!&#8221; And so on, forever. </p>\n<p>And I&#8217;m talking about non-governmental censorship just as much as government censorship. Even in the most anti-gay communities in the United States, the laws usually allow homosexuality or oppose it only in very weak, easily circumvented ways. The real problem for gays in these communities is the social pressure &#8211; whether that means disapproval or risk of violence &#8211; that they would likely face for coming out. This too is a violation of liberalism, and it&#8217;s one that&#8217;s as important or more important than the legal sort.</p>\n<p>And right now our way of dealing with these problems is to argue them. &#8220;Well, gay people don&#8217;t really increase STDs too much.&#8221; Or &#8220;Home-schooled kids do better than public-schooled kids, so we need to allow them.&#8221; The problem is that arguments never terminate. Maybe if you&#8217;re <i>incredibly</i> lucky, after years of fighting you can get a couple of people on the other side to admit your side is right, but this is a pretty hard process to trust. The great thing about religious freedom is that it short-circuits the debate of &#8220;Which religion is correct, Catholicism or Protestantism?&#8221; and allows people to tolerate both Catholics and Protestants even if they are divided about the answer to this object-level question. The great thing about freedom of speech is that it short-circuits the debate of &#8220;Which party is correct, the Democrats or Republicans?&#8221; and allows people to express both liberal and conservative opinions even if they are divided about the object-level question.</p>\n<p>If we force all of our discussions about whether to ban gay marriage or allow home schooling to depend on resolving the dispute about whether they indirectly harm the Fabric of Society in some way, we&#8217;re forcing dependence on object-level arguments in a way that historically has been very very bad.</p>\n<p>Presumably here the more powerful groups would win out and be able to oppress the less powerful groups. We end up with exactly what liberalism tried to avoid &#8211; a society where everyone is the guardian of the virtue of everyone else, and anyone who wants to live their lives in a way different from the community&#8217;s consensus is out of luck.</p>\n<p>In Part I, I argued that <i>not allowing</i> people to worry about culture and community at all was inadequate, because these things really do matter.</p>\n<p>Here I&#8217;m saying that if we <i>do allow</i> people to worry about culture and community, we risk the bad old medieval days where all nonconformity gets ruthlessly quashed. </p>\n<p>Right now we&#8217;re balanced precariously between the two states. There&#8217;s a lot of liberalism, and people are generally still allowed to be gay or home-school their children or practice their religion or whatever. But there&#8217;s also quite a bit of Enforced Virtue, where kids are forbidden to watch porn and certain kinds of media are censored and in some communities mentioning that you&#8217;re an atheist will get you Dirty Looks.</p>\n<p>It tends to work okay for most of the population. Better than the alternatives, maybe? But there&#8217;s still a lot of the population that&#8217;s not free to do things that are very important to them. And there&#8217;s also a lot of the population that would like to live in more &#8220;virtuous&#8221; communities, whether it&#8217;s to lose weight faster or avoid STDs or not have to worry about being objectified. Dealing with these two competing issues is a pretty big part of political philosophy and one that most people don&#8217;t have any principled solution for.</p>\n<p><b>III.</b></p>\n<p>Imagine a new frontier suddenly opening. Maybe a wizard appears and gives us a map to a new archipelago that geographers had missed for the past few centuries. He doesn&#8217;t want to rule the archipelago himself, though he will reluctantly help kickstart the government. He just wants to give directions and a free galleon to anybody who wants one and can muster a group of likeminded friends large enough to start a self-sustaining colony.</p>\n<p>And so the equivalent of our paleoconservatives go out and found communities based on virtue, where all sexual deviancy is banned and only wholesome films can be shown and people who burn the flag are thrown out to be eaten by wolves.</p>\n<p>And the equivalent of our social justiciars go out and found communities where all movies have to have lots of strong minority characters in them, and all slurs are way beyond the pale, and nobody misgenders anybody.</p>\n<p>And the equivalent of our Objectivists go out and found communities based totally on the Strict Principle of Harm where everyone is allowed to do whatever they want and there are no regulations on business and everything is super-capitalist all the time.</p>\n<p>And some people who just really want to lose weight go out and found communities where you&#8217;re not allowed to place open boxes of donuts in the doctors&#8217; lounge.</p>\n<p>Usually the communities are based on a charter, which expresses some founding ideals and asks only the people who agree with those ideals to enter. The charter also specifies a system of government. It could be an absolute monarch, charged with enforcing those ideals upon a population too stupid to know what&#8217;s good for them. Or it could be a direct democracy of people who all agree on some basic principles but want to work out for themselves what direction the principles take them.</p>\n<p>After a while the wizard decides to formalize and strengthen his system, not to mention work out some of the ethical dilemmas.</p>\n<p>First he bans communities from declaring war on each other. That&#8217;s an <i>obvious</i> gain. He could just smite warmongers, but he thinks it&#8217;s more natural and organic to get all the communities into a united government (UniGov for short). Every community donates a certain amount to a military, and the military&#8217;s only job is to quash anyone from any community who tries to invade another.</p>\n<p>Next he addresses externalities. For example, if some communities emit a lot of carbon, and that causes global warming which threatens to destroy other communities, UniGov puts a stop to that. If the offending communities refuse to stop emitting carbon, then there&#8217;s that military again.</p>\n<p>The third thing he does is prevent memetic contamination. If one community wants to avoid all media that objectifies women, then no other community is allowed to broadcast women-objectifying media at it. If a community wants to live an anarcho-primitivist lifestyle, nobody else is allowed to import TVs. Every community decides <i>exactly</i> how much informational contact it wants to have with the rest of the continent, and no one is allowed to force them to have more than that.</p>\n<p>But the wizard and UniGov&#8217;s most important task is to think of the children.</p>\n<p>Imagine you&#8217;re conservative Christians, and you&#8217;re tired of this secular godless world, so you go off with your conservative Christian friends to found a conservative Christian community. You all pray together and stuff and are really happy. Then you have a daughter. Turns out she&#8217;s atheist and lesbian. What now?</p>\n<p>Well, it might be that your kid would be much happier at the lesbian separatist community the next island over. The <i>absolute minimum</i> the united government can do is enforce freedom of movement. That is, the <i>second</i> your daughter decides she doesn&#8217;t want to be in Christiantopia anymore, she goes to a UniGov embassy nearby and asks for a ticket out, which they give her, free of charge. She gets airlifted to Lesbiantopia the next day. If <i>anyone</i> in Christiantopia tries to prevent her from reaching that embassy, or threatens her family if she leaves, or expresses the <i>slightest</i> amount of coercion to keep her around, UniGov burns their city and salts their field.</p>\n<p>But this is not nearly enough to fully solve the child problem. A child who is abused may be too young to know that escape is an option, or may be brainwashed into thinking they are evil, or guilted into believing they are betraying their families to opt out. And although there is no perfect, elegant solution here, the practical solution is that UniGov enforces some pretty strict laws on child-rearing, and every child, no matter what other education they receive, also has to receive a class taught by a UniGov representative in which they learn about the other communities in the Archipelago, receive a basic non-brainwashed view of the world, and are given directions to their nearest UniGov representative who they can give their opt-out request to. </p>\n<p>The list of communities they are informed about always starts with the capital, ruled by UniGov itself and considered an inoffensive, neutral option for people who don&#8217;t want anywhere in particular. And it always ends with a reminder that if they can gather enough support, UniGov will provide them with a galleon to go out and found their own community in hitherto uninhabited lands.</p>\n<p>There&#8217;s one more problem UniGov has to deal with: malicious inter-community transfer. Suppose that there is some community which puts extreme effort into educating its children, an education which it supports through heavy taxation. New parents move to this community, reap the benefits, and then when their children grow up they move back to their previous community so they don&#8217;t have to pay the taxes to educate anyone else. The communities themselves prevent some of this by immigration restrictions &#8211; anyone who&#8217;s clearly taking advantage of them isn&#8217;t allowed in (except in the capital, which has an official committment to let in anyone who wants). But that still leaves the example of people maliciously leaving a high-tax community once they&#8217;ve got theirs. I imagine this is a big deal in Archipelago politics, but that in practice UniGov asks these people, even in their new homes, to pay higher tax rates to subsidize their old community. Or since that could be morally objectionable (imagine the lesbian separatist having to pay taxes to Christiantopia which oppressed her), maybe they pay the excess taxes to UniGov itself, just as a way of disincentivizing malicious movement.</p>\n<p>Because there <i>are</i> UniGov taxes, and most people are happy to pay them. In my fantasy, UniGov isn&#8217;t an enemy, where the Christians view it as this evil atheist conglomerate trying to steal their kids away from them and the capitalists view it as this evil socialist conglomerate trying to enforce high taxes. The Christians, the capitalists, and everyone else are extraordinarily <i>patriotic</i> about being part of the Archipelago, for its full name is the Archipelago of Civilized Communities, it is the standard-bearer of civilization against the barbaric outside world, and it is precisely the institution that allows them to maintain their distinctiveness in the face of what would otherwise be irresistable pressure to conform. Atheistopia is the enemy of Christiantopia, but only in the same way the Democratic Party is the enemy of the Republican Party &#8211; two groups within the same community who may have different ideas but who consider themselves part of the same broader whole, fundamentally allies under a banner of which both are proud.</p>\n<p><b>IV.</b></p>\n<p>Robert Nozick once proposed a similar idea as a libertarian utopia, and it&#8217;s easy to see why. UniGov does very very little. Other than the part with children and the part with evening out taxation regimes, it just sits around preventing communities from using force against each other. That makes it very very easy for anyone who wants freedom to start a community that grants them the kind of freedom they want &#8211; or, more likely, to just start a community organized on purely libertarian principles. The United Government of Archipelago is the perfect minarchist night watchman state, and any additions you make over that are chosen by your own free will.</p>\n<p>But other people could view the same plan as a conservative utopia. Conservativism, when it&#8217;s not just Libertarianism Lite, is about building strong cohesive communities of relatively similar people united around common values. Archipelago is obviously built to make this as easy as possible, and it&#8217;s hard to imagine that there wouldn&#8217;t pop up a bunch of communities built around the idea of Decent Small-Town God-Fearing People where everyone has white picket fences and goes to the same church and nobody has to lock their doors at night (so basically Utah; I feel like this is one of the rare cases where the US&#8217; mostly-in-name-only Archipelagoness really asserts itself). People who didn&#8217;t fit in could go to a Community Of People Who Don&#8217;t Fit In and would have no need to nor right to complain, and no one would have to deal with Those Durned Bureaucrats In Washington telling them what to do.</p>\n<p>But to me, this seems like a liberal utopia, even a leftist utopia, for three reasons.</p>\n<p>The first reason is that it extends the basic principle of liberalism &#8211; solve differences of opinion by letting everyone do their own thing according to their own values, then celebrate the diversity this produces. I like homosexuality, you don&#8217;t, fine, I can be homosexual and you don&#8217;t have to, and having both gay and straight people living side by side enriches society. This just takes the whole thing one meta-level up &#8211; I want to live in a very sexually liberated community, you want to live in a community where sex is treated purely as a sacred act for the purpose of procreation, fine, I can live in the community I want and you can live in the community you want, and having both sexually-liberated and sexually-pure communities living side by side enriches society. It is pretty much saying that the solution to any perceived problems of liberalism is <i>much more liberalism</i>.</p>\n<p>The second reason is quite similar to the conservative reason. A lot of liberals have some pretty strong demands about the sorts of things they want society to do. I was recently talking to Ozy about a group who believe that society billing thin people is fatphobic, and that everyone needs to admit obese people can be just as attractive and date more of them, and that anyone who preferentially dates thinner people is Problematic. They also want people to stop talking about nutrition and exercise publicly. I sympathize with these people, especially having recently read a study showing that <A HREF=\"http://www.slate.com/blogs/xx_factor/2014/05/29/obesity_does_not_equal_unhappiness_study_tracks_relationship_between_weight.html\">obese people are much happier when surrounded by other obese, rather than skinny people</A>. But realistically, their movement will fail, and even philosophically, I&#8217;m not sure how to determine if they have the right to demand what they are demanding or what that question means. Their best bet is to found a community on these kinds of principles and only invite people who already share their preferences and aesthetics going in.</p>\n<p>The third reason is the reason I specifically draw leftism in here. Liberalism, and to a much greater degree leftism, are marked by the emphasis they place on oppression. They&#8217;re particularly marked by an emphasis on oppression being a really hard problem, and one that is structurally inherent to a certain society. They are marked by a moderate amount of despair that this oppression can ever be rooted out.</p>\n<p>And I think a pretty strong response to this is making sure everyone is able to say &#8220;Hey, you better not oppress us, because if you do, we can pack up and go somewhere else.&#8221;</p>\n<p>Like if you want to protest that this is unfair, that people shouldn&#8217;t be forced to leave their homes because of oppression, fine, fair enough. But given that oppression <i>is</i> going on, and you haven&#8217;t been able to fix it, giving people the <i>choice</i> to get away from it seems like a pretty big win. I am reminded of the many Jews who moved from Eastern Europe to America, the many blacks who moved from the southern US to the northern US or Canada, and the many gays who make it out of extremely homophobic areas to friendlier large cities. One could even make a metaphor, I think rightly, to telling battered women that they are allowed to leave their husbands, telling them they&#8217;re not forced to stay in a relationship that they consider abusive, and making sure that there are shelters available to receive them.</p>\n<p>If any person who feels oppressed can leave whenever they like, to the point of being provided a free plane ticket by the government, how long can oppression go on before the oppressors give up and say &#8220;Yeah, guess we need someone to work at these factories now that all our workers have gone to the communally-owned factory down the road, we should probably at least let people unionize or something so they will tolerate us&#8221;?</p>\n<p>A commenter in the latest Asch thread mentioned an interesting quote by Frederick Douglass:</p>\n<blockquote><p> The American people have always been anxious to know what they shall do with us [black people]. I have had but one answer from the beginning. Do nothing with us! Your doing with us has already played the mischief with us. Do nothing with us! </p></blockquote>\n<p>It sounds like, if Frederick Douglass had the opportunity to go to some other community, or even found a black ex-slave community, no racists allowed, he probably would have taken it [edit: <A HREF=\"http://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/#comment-97635\">or not, or had strict conditions</A>]. If the people in slavery during his own time period had had the chance to leave their plantations for that community, I bet they would have taken it too. And if you believe there are still people today whose relationship with society are similar in kind, if not in degree, to that of a plantation slave, you should be pretty enthusiastic about the ability of exit rights and free association to disrupt those oppressive relationships.</p>\n<p><b>V.</b></p>\n<p>We lack Archipelago&#8217;s big advantage &#8211; a vast frontier of unsettled land.</p>\n<p>Which is not to say that people don&#8217;t form communes. They do. Some people even have really clever ideas along these lines, like the seasteaders. But the United States isn&#8217;t going to become Archipelago any time soon.</p>\n<p>There&#8217;s another problem too, which I describe in my Anti-Reactionary FAQ. Discussing &#8216;exit rights&#8217;, I say:</p>\n<blockquote><p>Exit rights are a great idea and of course having them is better than not having them. But I have yet to hear Reactionaries who cite them as a panacea explain in detail what exit rights we need beyond those we have already.</p>\n<p>The United States allows its citizens to leave the country by buying a relatively cheap passport and go anywhere that will take them in, with the exception of a few arch-enemies like Cuba \u2013 and those exceptions are laughably easy to evade. It allows them to hold dual citizenship with various foreign powers. It even allows them to renounce their American citizenship entirely and become sole citizens of any foreign power that will accept them.</p>\n<p>Few Americans take advantage of this opportunity in any but the most limited ways. When they do move abroad, it\u2019s usually for business or family reasons, rather than a rational decision to move to a different country with policies more to their liking. There are constant threats by dissatisfied Americans to move to Canada, and one in a thousand even carry through with them, but the general situation seems to be that America has a very large neighbor that speaks the same language, and has an equally developed economy, and has policies that many Americans prefer to their own country\u2019s, and isn\u2019t too hard to move to, and almost no one takes advantage of this opportunity. Nor do I see many people, even among the rich, moving to Singapore or Dubai.</p>\n<p>Heck, the US has fifty states. Moving from one to another is as easy as getting in a car, driving there, and renting a room, and although the federal government limits exactly how different their policies can be you better believe that there are very important differences in areas like taxes, business climate, education, crime, gun control, and many more. Yet aside from the fascinating but small-scale Free State Project there\u2019s little politically-motivated interstate movement, nor do states seem to have been motivated to converge on their policies or be less ideologically driven.</p>\n<p>What if we held an exit rights party, and nobody came?</p>\n<p>Even aside from the international problems of gaining citizenship, dealing with a language barrier, and adapting to a new culture, people are just rooted \u2013 property, friends, family, jobs. The end result is that the only people who can leave their countries behind are very poor refugees with nothing to lose, and very rich jet-setters. The former aren\u2019t very attractive customers, and the latter have all their money in tax shelters anyway.</p>\n<p>So although the idea of being able to choose your country like a savvy consumer appeals to me, just saying \u201cexit rights!\u201d isn\u2019t going to make it happen, and I haven\u2019t heard any more elaborate plans.</p></blockquote>\n<p>I guess I still feel that way. So although Archipelago is an interesting exercise in political science, a sort of pure case we can compare ourselves to, it doesn&#8217;t look like a practical solution for real problems.</p>\n<p>On the other hand, I do think it&#8217;s worth becoming more Archipelagian on the margin rather than less so, and that there are good ways to do it.</p>\n<p>One of the things that started this whole line of thought was an argument on Facebook about a very conservative Christian law school trying to open up in Canada. They had lots of rules like how their students couldn&#8217;t have sex before marriage and stuff like that. The Canadian province they were in was trying to deny them accreditation, because conservative Christians are icky. I think the exact arguments being used were that it was homophobic, because the conservative Christians there would probably frown on married gays and therefore gays couldn&#8217;t have sex at all. Therefore, the law school shouldn&#8217;t be allowed to exist. There were other arguments of about this caliber, but they all seemed to boil down to &#8220;conservative Christians are icky&#8221;.</p>\n<p>This very much annoyed me. Yes, conservative Christians are icky. And they should be allowed to form completely voluntary communities of icky people that enforce icky cultural norms and an insular society promoting ickiness, just like everyone else. If non-conservative-Christians don&#8217;t like what they&#8217;re doing, they should <i>not go to that law school</i>. Instead they can go to one of the dozens of other law schools that conform to their own philosophies. And if gays want a law school even friendlier to them than the average Canadian law school, they should be allowed to create some law school that only accepts gays and bans homophobes and teaches lots of courses on gay marriage law all the time.</p>\n<p>Another person on the Facebook thread complained that this line of arguments leads to being okay with white separatists. And so it does. Fine. I think white separatists have <i>exactly</i> the right position about where the sort of white people who want to be white separatists should be relative to everyone else &#8211; separate. I am not sure what you think you are gaining by demanding that white separatists live in communities with a lot of black people in them, but I bet the black people in those communities aren&#8217;t thanking you. Why would they want a white separatist as a neighbor? Why should they have to have one?</p>\n<p>If people want to go do their own thing in a way that harms no one else, you <i>let</i> them. That&#8217;s the Archipelagian way.</p>\n<p>(someone will protest that Archipelagian voluntary freedom of association or disassociation could, in cases of enough racial prejudice, lead to segregation, and that segregation didn&#8217;t work. Indeed it didn&#8217;t. But I feel like a version of segregation in which black people actually had the legally mandated right to get away from white people and remain completely unmolested by them &#8211; and where a white-controlled government wasn&#8217;t in charge of divvying up resources between white and black communities &#8211; would have worked a lot better than the segregation we actually had. The segregation we actually <i>had</i> was one in which white and black communities were separate until white people wanted something from black people, at which case they waltzed in and took it. If communities were actually totally separate, government and everything, by definition it would be impossible for one to oppress the other. The black community might start with less, but that could be solved by some kind of reparations. The Archipelagian way of dealing with this issue would be for white separatists to have separate white communities, black separatists to have separate black communities, integrationists to have integrated communities, resdistributive taxation from wealthier communities going into less wealthy ones, and a strong central government ruthlessly enforcing laws against any community trying to hurt another. I don&#8217;t think there&#8217;s a single black person in the segregation-era South who wouldn&#8217;t have taken that deal, and any black person who thinks the effect of whites on their community today is net negative should be pretty interested as well.)</p>\n<p>This is one reason I find people who hate seasteads so distasteful. I mean, here&#8217;s <A HREF=\"http://blogs.reuters.com/great-debate/2011/09/01/do-libertarians-like-peter-thiel-really-want-to-live-in-america/\">what Reuters has to say about seasteading</A>:</p>\n<blockquote><p>Fringe movements, of course, rarely cast themselves as obviously fringe.  Racist, anti-civil rights forces cloaked themselves in the benign language of \u201cstate\u2019s rights\u201d.  Anti-gay religious entities adopted the glossy, positive imagery of \u201cfamily values\u201d.  Similarly, though many Libertarians embrace a pseudo-patriotic apple pie nostalgia, behind this fa\u00e7ade is a very un-American, sinister vision.</p>\n<p>Sure, most libertarians may not want to do away entirely with the idea of government or, for that matter, government-protected rights and civil liberties.  But many do \u2014 and ironically vie for political power in a nation they ultimately want to destroy.  Even the right-wing pundit Ann Coulter mocked the paradox of Libertarian candidates: \u201cGet rid of government \u2014 but first, make me president!\u201d Libertarians sowed the seeds of anti-government discontent, which is on the rise, and now want to harvest that discontent for a very radical, anti-America agenda.  The image of libertarians living off-shore in their lawless private nation-states is just a postcard of the future they hope to build on land.</p>\n<p>Strangely, the libertarian agenda has largely escaped scrutiny, at least compared to that of social conservatives. The fact that the political class is locked in debate about whether Michele Bachmann or Rick Perry is more socially conservative only creates a veneer of mainstream legitimacy for the likes of Ron Paul, whose libertarianism may be even more extreme and dangerously un-patriotic.  With any luck America will recognize anti-government extremism for what it is \u2014 before libertarians throw America overboard and render us all castaways.</p></blockquote>\n<p>Keep in mind this is because <i>some people want to go off and do their own thing in the middle of the ocean far away from everyone else without bothering anyone</i>. And the newspapers are trying to whip up a panic about &#8220;throwing America overboard&#8221;.</p>\n<p>So one way we could become more Archipelagian is just <i>trying not to yell at people who are trying to go off and doing their own thing quietly with a group of voluntarily consenting friends</i>.</p>\n<p>But I think a better candidate for how to build a more Archipelagian world is to encourage the fracture of society into subcultures.</p>\n<p>Like, transsexuals may not be able to go to a transsexual island somewhere and build Transtopia where anyone who misgenders anyone else gets thrown into a volcano. But of the transsexuals I know, a lot of them have lots of transsexual friends, their cissexual friends are all up-to-date on trans issues and don&#8217;t do a lot of misgendering, and they have great social networks where they share information about what businesses and doctors are or aren&#8217;t trans-friendly. They can take advantage of trigger warnings to make sure they expose themselves to only the sources that fit the values of their community, the information that would get broadcast if it was a normal community that could impose media norms. As Internet interaction starts to replace real-life interaction (and I think for a lot of people the majority of their social life is already on the Internet, and for some the majority of their economic life is as well) it becomes increasingly easy to limit yourself to transsexual-friendly spaces that keep bad people away.</p>\n<p>The rationalist community is another good example. If I wanted, I could move to the Bay Area tomorrow and never have more than a tiny amount of contact with non-rationalists again. I could have rationalist roommates, live in a rationalist group house, try to date only other rationalists, try to get a job with a rationalist nonprofit like CFAR or a rationalist company like Quixey, and never have to deal with the benighted and depressing non-rationalist world again. Even without moving to the Bay Area, it&#8217;s been pretty easy for me to keep a lot of my social life, both on- and off- line, rationalist-focused, and I don&#8217;t regret this at all. </p>\n<p>I don&#8217;t know if the future will be virtual reality. I expect the post-singularity future will include something like VR, although that might be like describing teleportation as &#8220;basically a sort of pack animal&#8221;. But how much the immediate pre-singularity world will make use of virtual reality, I don&#8217;t know.</p>\n<p>But I bet if it doesn&#8217;t, it will be because virtual reality has been circumvented by things like social networks, bitcoin, and Mechanical Turk, which make it possible to do most of your interaction through the Internet even though you&#8217;re not literally plugged into it. </p>\n<p>And that seems to me like a pretty good start in creating an Archipelago. I already hang out with various Finns and Brits and Aussies a lot more closely than I do my next-door neighbors, and if we start using litecoin and someone else starts using dogecoin then I&#8217;ll be more economically connected to them too. The degree to which I encounter certain objectifying or unvirtuous or triggering media already depends more on the moderation policies of Less Wrong and Slate Star Codex and who I block from my Facebook feed, than it does any laws about censorship of US media. </p>\n<p>At what point are national governments rendered mostly irrelevant compared to the norms and rules of the groups of which we are voluntary members?</p>\n<p>I don&#8217;t know, but I kind of look forward to finding out. It seems like a great way to start searching for utopia, or at least getting some people away from their metaphorical abusive-husbands.</p>\n<p>And the other thing is that I have pretty strong opinions on which communities are better than others. Some communities were founded by toxic people for ganging up with other toxic people to celebrate and magnify their toxicity, and these (surprise, surprise) tend to be toxic. Others were formed by very careful, easily-harmed people trying to exclude everyone who could harm them, and these tend to be pretty safe albeit sometimes overbearing. Other people hit some kind of sweet spot that makes friendly people want to come in and angry people want to stay out, or just do a really good job choosing friends.</p>\n<p>But I think the end result is that the closer you come to true freedom of association, the closer you get to a world where everyone is a member of more or less the community they deserve. That would be a pretty unprecedented bit of progress.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 2, "gHCNhqxuJq2bZ2akb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aP36QcAsxyuEispq6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 4.1e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "xmDeR64CivZiTAcLx", "canonicalCollectionSlug": "codex", "canonicalBookId": "kcCvSNNZd8pfQvf9E", "canonicalNextPostSlug": "meditations-on-moloch", "canonicalPrevPostSlug": "the-ideology-is-not-the-movement", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><b id=\"I_\">I.</b></p>\n<p>In the old days, you had your Culture, and that was that. Your Culture told you lots of stuff about what you were and weren\u2019t allowed to do, and by golly you listened. Your Culture told you to work the job prescribed to you by your caste and gender, to marry who your parents told you to marry or at <i>least</i> someone of the opposite sex, to worship at the proper temples and the proper times, and to talk about <i>proper</i> things as opposed to the blasphemous things said by the tribe over there.</p>\n<p>Then we got Liberalism, which said all of that was mostly bunk. Like Wicca, its motto is \u201cDo as you will, so long as it harms none\u201d. Or in more political terms, \u201cYour right to swing your fist ends where my nose begins\u201d or \u201cIf you don\u2019t like gay sex, don\u2019t have any\u201d or \u201cIf you don\u2019t like this TV program, don\u2019t watch it\u201d or \u201cWhat happens in the bedroom between consenting adults is none of your business\u201d or \u201cIt neither breaks my arm nor picks my pocket\u201d. Your job isn\u2019t to enforce your conception of virtue upon everyone to build the Virtuous Society, it\u2019s to live your own life the way you want to live it and let other people live <i>their</i> own lives the way <i>they</i> want to live them. This is the much-maligned \u201catomic individualism,\u201d or maybe just liberalism boiled down to its pure essence.</p>\n<p>But atomic individualism wasn\u2019t as great a solution as it sounded. Maybe one of the first cracks was tobacco ads. Even though putting up a billboard saying \u201cSMOKE MARLBORO\u201d neither breaks anyone\u2019s arm nor picks their pocket, it shifts social expectations in such a way that bad effects occur. It\u2019s hard to dismiss that with \u201cWell, it\u2019s people\u2019s own choice to smoke and they should live their lives the way they want\u201d if studies show that more people will want to live their lives in a way that gives them cancer in the presence of the billboard than otherwise.</p>\n<p>From there we go into policies like Michael Bloomberg\u2019s ban on giant sodas. While the soda ban itself was probably as much symbolic as anything, it\u2019s hard to argue with the impetus behind it \u2013 a culture where everyone gets exposed to the option to buy very very unhealthy food all the time is going to be less healthy than one where there are some regulations in place to make EAT THIS DONUT NOW a less salient option. I mean, I <i>know</i> this is true. A few months ago when I was on a diet I <i>cringed</i> every time one my coworkers brought in a box of free donuts and placed wide-open in the doctors\u2019 lounge; there was <i>no way</i> I wasn\u2019t going to take one (or two, or three). I could ask people to stop, but they probably wouldn\u2019t, and even if they did I\u2019d just encounter the wide-open box of free donuts <i>somewhere else</i>. I\u2019m not proposing that it is <i>ethically wrong</i> to bring in free donuts or that banning them is the correct policy, but I do want to make it clear that stating \u201cit\u2019s your free choice to partake or not\u201d doesn\u2019t eliminate the problem, and that this points to an entire class of serious issues where atomic individualism as construed above is at best an imperfect heuristic.</p>\n<p>And I would be remiss talking about the modern turn away from individualism without mentioning social justice. The same people who once deployed individualistic arguments against conservatives: \u201cIf you don\u2019t like profanity, don\u2019t use it\u201d, \u201cIf you don\u2019t like this offensive TV show, don\u2019t watch it\u201d, \u201cIf you don\u2019t like pornography, don\u2019t buy it\u201d \u2013 are now concerned about people using ethnic slurs, TV shows without enough minority characters, and pornography that encourages the objectification of women. I\u2019ve objected to some of this on <a href=\"http://slatestarcodex.com/2013/06/22/social-psychology-is-a-flamethrower/\">purely empirical grounds</a>, but the <a href=\"http://lesswrong.com/lw/2k/the_least_convenient_possible_world/\">least convenient possible world</a> is the one where the purely empirical objections fall flat. If they ever discover proof positive that yeah, pornographication makes women hella objectified, is it acceptable to censor or ban misogynist media on a society-wide level?</p>\n<p>And if the answer is yes \u2013 and if such media like really, <i>really</i> increases the incidence of rape I\u2019m not sure how it couldn\u2019t be \u2013 then what about all those conservative ideas we\u2019ve been neglecting for so long? What if strong, cohesive, religious, demographically uniform communities make people more trusting, generous, and cooperative in a way that <i>also</i> decreases violent crime and other forms of misery? We have <a href=\"http://smile.amazon.com/The-Righteous-Mind-Politics-Religion/dp/0307455777/\">lots of evidence</a> that this is true, and although we can doubt each individual study, we owe conservatives the courtesy of imagining the possible world in which they are right, the same as anti-misogyny leftists. Maybe media glorifying criminals or lionizing nonconformists above those who quietly follow cultural norms has the same kind of erosive effects on \u201cvalues\u201d as misogynist media. Or, at the very least, we ought to have a good philosophy in place so that we have some idea what to do it if does.</p>\n<p><b id=\"II_\">II.</b></p>\n<p>A while ago, in Part V of <a href=\"http://slatestarcodex.com/2014/02/23/in-favor-of-niceness-community-and-civilization/\">this essay</a>, I praised liberalism as the only peaceful answer to Hobbes\u2019 dilemma of the war of all against all.</p>\n<p>Hobbes says that if everyone\u2019s fighting then everyone loses out. Even the winners probably end up worse off than if they had just been able to live in peace. He says that governments are good ways to prevent this kind of conflict. Someone \u2013 in his formulation a king \u2013 tells everyone else what they\u2019re going to do, and then everyone else does it. No fighting necessary. If someone tries to start a conflict by ignoring the king, the king crushes them like a bug, no prolonged fighting involved.</p>\n<p>But this replaces the problem of potential warfare with the problem of potential tyranny. So we\u2019ve mostly shifted from absolute monarchies to other forms of government, which is all nice and well except that governments allow a <i>different</i> kind of war of all against all. Instead of trying to kill their enemies and steal their stuff, people are tempted to ban their enemies and confiscate their stuff. Instead of killing the Protestants, the Catholics simply ban Protestantism. Instead of forming vigilante mobs to stone homosexuals, the straights merely declare homosexuality is punishable by death. It <i>might</i> be better than the alternative \u2013 at least everyone knows where they stand and things stay peaceful \u2013 but the end result is still a lot of pretty miserable people.</p>\n<p>Liberalism is a new form of Hobbesian equilibrium where the government enforces not only a ban on killing and stealing from people you don\u2019t like, but also a ban on tyrannizing them out of existence. This is the famous \u201cfreedom of religion\u201d and \u201cfreedom of speech\u201d and so on, as well as the \u201cfreedom of what happens in the bedroom between consenting adults\u201d. The Catholics don\u2019t try to ban Protestantism, the Protestants don\u2019t try to ban Catholicism, and everyone is happy.</p>\n<p>Liberalism only works when it\u2019s clear to everyone on all sides that there\u2019s a certain neutral principle everyone has to stick to. The neutral principle can\u2019t be the Bible, or Atlas Shrugged, or anything that makes it look like one philosophy is allowed to judge the others. Right now that principle is the Principle of Harm: you can do whatever you like unless it harms other people, in which case stop. We seem to have inelegantly tacked on an \u201calso, we can collect taxes and use them for a social safety net and occasional attempts at social progress\u201d, but it seems to be working pretty okay too.</p>\n<p>The Strict Principle of Harm says that pretty much the only two things the government can get angry at is literally breaking your leg or picking your pocket \u2013 violence or theft. The Loose Principle of Harm says that the government can get angry at complicated indirect harms, things that Weaken The Moral Fabric Of Society. Like putting up tobacco ads. Or having really really big sodas. Or publishing hate speech against minorities. Or eroding trust in the community. Or media that objectifies women.</p>\n<p>No one except the most ideologically pure libertarians seems to want to insist on the Strict Principle of Harm. But allowing the Loose Principle Of Harm restores all of the old wars to control other people that liberalism was supposed to prevent. The one person says \u201cGay marriage will result in homosexuality becoming more accepted, leading to increased rates of STDs! That\u2019s a harm! We must ban gay marriage!\u201d Another says \u201cAllowing people to send their children to non-public schools could lead to kids at religious schools that preach against gay people, causing those children to commit hate crimes when they grow up! That\u2019s a harm! We must ban non-public schools!\u201d And so on, forever. </p>\n<p>And I\u2019m talking about non-governmental censorship just as much as government censorship. Even in the most anti-gay communities in the United States, the laws usually allow homosexuality or oppose it only in very weak, easily circumvented ways. The real problem for gays in these communities is the social pressure \u2013 whether that means disapproval or risk of violence \u2013 that they would likely face for coming out. This too is a violation of liberalism, and it\u2019s one that\u2019s as important or more important than the legal sort.</p>\n<p>And right now our way of dealing with these problems is to argue them. \u201cWell, gay people don\u2019t really increase STDs too much.\u201d Or \u201cHome-schooled kids do better than public-schooled kids, so we need to allow them.\u201d The problem is that arguments never terminate. Maybe if you\u2019re <i>incredibly</i> lucky, after years of fighting you can get a couple of people on the other side to admit your side is right, but this is a pretty hard process to trust. The great thing about religious freedom is that it short-circuits the debate of \u201cWhich religion is correct, Catholicism or Protestantism?\u201d and allows people to tolerate both Catholics and Protestants even if they are divided about the answer to this object-level question. The great thing about freedom of speech is that it short-circuits the debate of \u201cWhich party is correct, the Democrats or Republicans?\u201d and allows people to express both liberal and conservative opinions even if they are divided about the object-level question.</p>\n<p>If we force all of our discussions about whether to ban gay marriage or allow home schooling to depend on resolving the dispute about whether they indirectly harm the Fabric of Society in some way, we\u2019re forcing dependence on object-level arguments in a way that historically has been very very bad.</p>\n<p>Presumably here the more powerful groups would win out and be able to oppress the less powerful groups. We end up with exactly what liberalism tried to avoid \u2013 a society where everyone is the guardian of the virtue of everyone else, and anyone who wants to live their lives in a way different from the community\u2019s consensus is out of luck.</p>\n<p>In Part I, I argued that <i>not allowing</i> people to worry about culture and community at all was inadequate, because these things really do matter.</p>\n<p>Here I\u2019m saying that if we <i>do allow</i> people to worry about culture and community, we risk the bad old medieval days where all nonconformity gets ruthlessly quashed. </p>\n<p>Right now we\u2019re balanced precariously between the two states. There\u2019s a lot of liberalism, and people are generally still allowed to be gay or home-school their children or practice their religion or whatever. But there\u2019s also quite a bit of Enforced Virtue, where kids are forbidden to watch porn and certain kinds of media are censored and in some communities mentioning that you\u2019re an atheist will get you Dirty Looks.</p>\n<p>It tends to work okay for most of the population. Better than the alternatives, maybe? But there\u2019s still a lot of the population that\u2019s not free to do things that are very important to them. And there\u2019s also a lot of the population that would like to live in more \u201cvirtuous\u201d communities, whether it\u2019s to lose weight faster or avoid STDs or not have to worry about being objectified. Dealing with these two competing issues is a pretty big part of political philosophy and one that most people don\u2019t have any principled solution for.</p>\n<p><b id=\"III_\">III.</b></p>\n<p>Imagine a new frontier suddenly opening. Maybe a wizard appears and gives us a map to a new archipelago that geographers had missed for the past few centuries. He doesn\u2019t want to rule the archipelago himself, though he will reluctantly help kickstart the government. He just wants to give directions and a free galleon to anybody who wants one and can muster a group of likeminded friends large enough to start a self-sustaining colony.</p>\n<p>And so the equivalent of our paleoconservatives go out and found communities based on virtue, where all sexual deviancy is banned and only wholesome films can be shown and people who burn the flag are thrown out to be eaten by wolves.</p>\n<p>And the equivalent of our social justiciars go out and found communities where all movies have to have lots of strong minority characters in them, and all slurs are way beyond the pale, and nobody misgenders anybody.</p>\n<p>And the equivalent of our Objectivists go out and found communities based totally on the Strict Principle of Harm where everyone is allowed to do whatever they want and there are no regulations on business and everything is super-capitalist all the time.</p>\n<p>And some people who just really want to lose weight go out and found communities where you\u2019re not allowed to place open boxes of donuts in the doctors\u2019 lounge.</p>\n<p>Usually the communities are based on a charter, which expresses some founding ideals and asks only the people who agree with those ideals to enter. The charter also specifies a system of government. It could be an absolute monarch, charged with enforcing those ideals upon a population too stupid to know what\u2019s good for them. Or it could be a direct democracy of people who all agree on some basic principles but want to work out for themselves what direction the principles take them.</p>\n<p>After a while the wizard decides to formalize and strengthen his system, not to mention work out some of the ethical dilemmas.</p>\n<p>First he bans communities from declaring war on each other. That\u2019s an <i>obvious</i> gain. He could just smite warmongers, but he thinks it\u2019s more natural and organic to get all the communities into a united government (UniGov for short). Every community donates a certain amount to a military, and the military\u2019s only job is to quash anyone from any community who tries to invade another.</p>\n<p>Next he addresses externalities. For example, if some communities emit a lot of carbon, and that causes global warming which threatens to destroy other communities, UniGov puts a stop to that. If the offending communities refuse to stop emitting carbon, then there\u2019s that military again.</p>\n<p>The third thing he does is prevent memetic contamination. If one community wants to avoid all media that objectifies women, then no other community is allowed to broadcast women-objectifying media at it. If a community wants to live an anarcho-primitivist lifestyle, nobody else is allowed to import TVs. Every community decides <i>exactly</i> how much informational contact it wants to have with the rest of the continent, and no one is allowed to force them to have more than that.</p>\n<p>But the wizard and UniGov\u2019s most important task is to think of the children.</p>\n<p>Imagine you\u2019re conservative Christians, and you\u2019re tired of this secular godless world, so you go off with your conservative Christian friends to found a conservative Christian community. You all pray together and stuff and are really happy. Then you have a daughter. Turns out she\u2019s atheist and lesbian. What now?</p>\n<p>Well, it might be that your kid would be much happier at the lesbian separatist community the next island over. The <i>absolute minimum</i> the united government can do is enforce freedom of movement. That is, the <i>second</i> your daughter decides she doesn\u2019t want to be in Christiantopia anymore, she goes to a UniGov embassy nearby and asks for a ticket out, which they give her, free of charge. She gets airlifted to Lesbiantopia the next day. If <i>anyone</i> in Christiantopia tries to prevent her from reaching that embassy, or threatens her family if she leaves, or expresses the <i>slightest</i> amount of coercion to keep her around, UniGov burns their city and salts their field.</p>\n<p>But this is not nearly enough to fully solve the child problem. A child who is abused may be too young to know that escape is an option, or may be brainwashed into thinking they are evil, or guilted into believing they are betraying their families to opt out. And although there is no perfect, elegant solution here, the practical solution is that UniGov enforces some pretty strict laws on child-rearing, and every child, no matter what other education they receive, also has to receive a class taught by a UniGov representative in which they learn about the other communities in the Archipelago, receive a basic non-brainwashed view of the world, and are given directions to their nearest UniGov representative who they can give their opt-out request to. </p>\n<p>The list of communities they are informed about always starts with the capital, ruled by UniGov itself and considered an inoffensive, neutral option for people who don\u2019t want anywhere in particular. And it always ends with a reminder that if they can gather enough support, UniGov will provide them with a galleon to go out and found their own community in hitherto uninhabited lands.</p>\n<p>There\u2019s one more problem UniGov has to deal with: malicious inter-community transfer. Suppose that there is some community which puts extreme effort into educating its children, an education which it supports through heavy taxation. New parents move to this community, reap the benefits, and then when their children grow up they move back to their previous community so they don\u2019t have to pay the taxes to educate anyone else. The communities themselves prevent some of this by immigration restrictions \u2013 anyone who\u2019s clearly taking advantage of them isn\u2019t allowed in (except in the capital, which has an official committment to let in anyone who wants). But that still leaves the example of people maliciously leaving a high-tax community once they\u2019ve got theirs. I imagine this is a big deal in Archipelago politics, but that in practice UniGov asks these people, even in their new homes, to pay higher tax rates to subsidize their old community. Or since that could be morally objectionable (imagine the lesbian separatist having to pay taxes to Christiantopia which oppressed her), maybe they pay the excess taxes to UniGov itself, just as a way of disincentivizing malicious movement.</p>\n<p>Because there <i>are</i> UniGov taxes, and most people are happy to pay them. In my fantasy, UniGov isn\u2019t an enemy, where the Christians view it as this evil atheist conglomerate trying to steal their kids away from them and the capitalists view it as this evil socialist conglomerate trying to enforce high taxes. The Christians, the capitalists, and everyone else are extraordinarily <i>patriotic</i> about being part of the Archipelago, for its full name is the Archipelago of Civilized Communities, it is the standard-bearer of civilization against the barbaric outside world, and it is precisely the institution that allows them to maintain their distinctiveness in the face of what would otherwise be irresistable pressure to conform. Atheistopia is the enemy of Christiantopia, but only in the same way the Democratic Party is the enemy of the Republican Party \u2013 two groups within the same community who may have different ideas but who consider themselves part of the same broader whole, fundamentally allies under a banner of which both are proud.</p>\n<p><b id=\"IV_\">IV.</b></p>\n<p>Robert Nozick once proposed a similar idea as a libertarian utopia, and it\u2019s easy to see why. UniGov does very very little. Other than the part with children and the part with evening out taxation regimes, it just sits around preventing communities from using force against each other. That makes it very very easy for anyone who wants freedom to start a community that grants them the kind of freedom they want \u2013 or, more likely, to just start a community organized on purely libertarian principles. The United Government of Archipelago is the perfect minarchist night watchman state, and any additions you make over that are chosen by your own free will.</p>\n<p>But other people could view the same plan as a conservative utopia. Conservativism, when it\u2019s not just Libertarianism Lite, is about building strong cohesive communities of relatively similar people united around common values. Archipelago is obviously built to make this as easy as possible, and it\u2019s hard to imagine that there wouldn\u2019t pop up a bunch of communities built around the idea of Decent Small-Town God-Fearing People where everyone has white picket fences and goes to the same church and nobody has to lock their doors at night (so basically Utah; I feel like this is one of the rare cases where the US\u2019 mostly-in-name-only Archipelagoness really asserts itself). People who didn\u2019t fit in could go to a Community Of People Who Don\u2019t Fit In and would have no need to nor right to complain, and no one would have to deal with Those Durned Bureaucrats In Washington telling them what to do.</p>\n<p>But to me, this seems like a liberal utopia, even a leftist utopia, for three reasons.</p>\n<p>The first reason is that it extends the basic principle of liberalism \u2013 solve differences of opinion by letting everyone do their own thing according to their own values, then celebrate the diversity this produces. I like homosexuality, you don\u2019t, fine, I can be homosexual and you don\u2019t have to, and having both gay and straight people living side by side enriches society. This just takes the whole thing one meta-level up \u2013 I want to live in a very sexually liberated community, you want to live in a community where sex is treated purely as a sacred act for the purpose of procreation, fine, I can live in the community I want and you can live in the community you want, and having both sexually-liberated and sexually-pure communities living side by side enriches society. It is pretty much saying that the solution to any perceived problems of liberalism is <i>much more liberalism</i>.</p>\n<p>The second reason is quite similar to the conservative reason. A lot of liberals have some pretty strong demands about the sorts of things they want society to do. I was recently talking to Ozy about a group who believe that society billing thin people is fatphobic, and that everyone needs to admit obese people can be just as attractive and date more of them, and that anyone who preferentially dates thinner people is Problematic. They also want people to stop talking about nutrition and exercise publicly. I sympathize with these people, especially having recently read a study showing that <a href=\"http://www.slate.com/blogs/xx_factor/2014/05/29/obesity_does_not_equal_unhappiness_study_tracks_relationship_between_weight.html\">obese people are much happier when surrounded by other obese, rather than skinny people</a>. But realistically, their movement will fail, and even philosophically, I\u2019m not sure how to determine if they have the right to demand what they are demanding or what that question means. Their best bet is to found a community on these kinds of principles and only invite people who already share their preferences and aesthetics going in.</p>\n<p>The third reason is the reason I specifically draw leftism in here. Liberalism, and to a much greater degree leftism, are marked by the emphasis they place on oppression. They\u2019re particularly marked by an emphasis on oppression being a really hard problem, and one that is structurally inherent to a certain society. They are marked by a moderate amount of despair that this oppression can ever be rooted out.</p>\n<p>And I think a pretty strong response to this is making sure everyone is able to say \u201cHey, you better not oppress us, because if you do, we can pack up and go somewhere else.\u201d</p>\n<p>Like if you want to protest that this is unfair, that people shouldn\u2019t be forced to leave their homes because of oppression, fine, fair enough. But given that oppression <i>is</i> going on, and you haven\u2019t been able to fix it, giving people the <i>choice</i> to get away from it seems like a pretty big win. I am reminded of the many Jews who moved from Eastern Europe to America, the many blacks who moved from the southern US to the northern US or Canada, and the many gays who make it out of extremely homophobic areas to friendlier large cities. One could even make a metaphor, I think rightly, to telling battered women that they are allowed to leave their husbands, telling them they\u2019re not forced to stay in a relationship that they consider abusive, and making sure that there are shelters available to receive them.</p>\n<p>If any person who feels oppressed can leave whenever they like, to the point of being provided a free plane ticket by the government, how long can oppression go on before the oppressors give up and say \u201cYeah, guess we need someone to work at these factories now that all our workers have gone to the communally-owned factory down the road, we should probably at least let people unionize or something so they will tolerate us\u201d?</p>\n<p>A commenter in the latest Asch thread mentioned an interesting quote by Frederick Douglass:</p>\n<blockquote><p> The American people have always been anxious to know what they shall do with us [black people]. I have had but one answer from the beginning. Do nothing with us! Your doing with us has already played the mischief with us. Do nothing with us! </p></blockquote>\n<p>It sounds like, if Frederick Douglass had the opportunity to go to some other community, or even found a black ex-slave community, no racists allowed, he probably would have taken it [edit: <a href=\"http://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/#comment-97635\">or not, or had strict conditions</a>]. If the people in slavery during his own time period had had the chance to leave their plantations for that community, I bet they would have taken it too. And if you believe there are still people today whose relationship with society are similar in kind, if not in degree, to that of a plantation slave, you should be pretty enthusiastic about the ability of exit rights and free association to disrupt those oppressive relationships.</p>\n<p><b id=\"V_\">V.</b></p>\n<p>We lack Archipelago\u2019s big advantage \u2013 a vast frontier of unsettled land.</p>\n<p>Which is not to say that people don\u2019t form communes. They do. Some people even have really clever ideas along these lines, like the seasteaders. But the United States isn\u2019t going to become Archipelago any time soon.</p>\n<p>There\u2019s another problem too, which I describe in my Anti-Reactionary FAQ. Discussing \u2018exit rights\u2019, I say:</p>\n<blockquote><p>Exit rights are a great idea and of course having them is better than not having them. But I have yet to hear Reactionaries who cite them as a panacea explain in detail what exit rights we need beyond those we have already.</p>\n<p>The United States allows its citizens to leave the country by buying a relatively cheap passport and go anywhere that will take them in, with the exception of a few arch-enemies like Cuba \u2013 and those exceptions are laughably easy to evade. It allows them to hold dual citizenship with various foreign powers. It even allows them to renounce their American citizenship entirely and become sole citizens of any foreign power that will accept them.</p>\n<p>Few Americans take advantage of this opportunity in any but the most limited ways. When they do move abroad, it\u2019s usually for business or family reasons, rather than a rational decision to move to a different country with policies more to their liking. There are constant threats by dissatisfied Americans to move to Canada, and one in a thousand even carry through with them, but the general situation seems to be that America has a very large neighbor that speaks the same language, and has an equally developed economy, and has policies that many Americans prefer to their own country\u2019s, and isn\u2019t too hard to move to, and almost no one takes advantage of this opportunity. Nor do I see many people, even among the rich, moving to Singapore or Dubai.</p>\n<p>Heck, the US has fifty states. Moving from one to another is as easy as getting in a car, driving there, and renting a room, and although the federal government limits exactly how different their policies can be you better believe that there are very important differences in areas like taxes, business climate, education, crime, gun control, and many more. Yet aside from the fascinating but small-scale Free State Project there\u2019s little politically-motivated interstate movement, nor do states seem to have been motivated to converge on their policies or be less ideologically driven.</p>\n<p>What if we held an exit rights party, and nobody came?</p>\n<p>Even aside from the international problems of gaining citizenship, dealing with a language barrier, and adapting to a new culture, people are just rooted \u2013 property, friends, family, jobs. The end result is that the only people who can leave their countries behind are very poor refugees with nothing to lose, and very rich jet-setters. The former aren\u2019t very attractive customers, and the latter have all their money in tax shelters anyway.</p>\n<p>So although the idea of being able to choose your country like a savvy consumer appeals to me, just saying \u201cexit rights!\u201d isn\u2019t going to make it happen, and I haven\u2019t heard any more elaborate plans.</p></blockquote>\n<p>I guess I still feel that way. So although Archipelago is an interesting exercise in political science, a sort of pure case we can compare ourselves to, it doesn\u2019t look like a practical solution for real problems.</p>\n<p>On the other hand, I do think it\u2019s worth becoming more Archipelagian on the margin rather than less so, and that there are good ways to do it.</p>\n<p>One of the things that started this whole line of thought was an argument on Facebook about a very conservative Christian law school trying to open up in Canada. They had lots of rules like how their students couldn\u2019t have sex before marriage and stuff like that. The Canadian province they were in was trying to deny them accreditation, because conservative Christians are icky. I think the exact arguments being used were that it was homophobic, because the conservative Christians there would probably frown on married gays and therefore gays couldn\u2019t have sex at all. Therefore, the law school shouldn\u2019t be allowed to exist. There were other arguments of about this caliber, but they all seemed to boil down to \u201cconservative Christians are icky\u201d.</p>\n<p>This very much annoyed me. Yes, conservative Christians are icky. And they should be allowed to form completely voluntary communities of icky people that enforce icky cultural norms and an insular society promoting ickiness, just like everyone else. If non-conservative-Christians don\u2019t like what they\u2019re doing, they should <i>not go to that law school</i>. Instead they can go to one of the dozens of other law schools that conform to their own philosophies. And if gays want a law school even friendlier to them than the average Canadian law school, they should be allowed to create some law school that only accepts gays and bans homophobes and teaches lots of courses on gay marriage law all the time.</p>\n<p>Another person on the Facebook thread complained that this line of arguments leads to being okay with white separatists. And so it does. Fine. I think white separatists have <i>exactly</i> the right position about where the sort of white people who want to be white separatists should be relative to everyone else \u2013 separate. I am not sure what you think you are gaining by demanding that white separatists live in communities with a lot of black people in them, but I bet the black people in those communities aren\u2019t thanking you. Why would they want a white separatist as a neighbor? Why should they have to have one?</p>\n<p>If people want to go do their own thing in a way that harms no one else, you <i>let</i> them. That\u2019s the Archipelagian way.</p>\n<p>(someone will protest that Archipelagian voluntary freedom of association or disassociation could, in cases of enough racial prejudice, lead to segregation, and that segregation didn\u2019t work. Indeed it didn\u2019t. But I feel like a version of segregation in which black people actually had the legally mandated right to get away from white people and remain completely unmolested by them \u2013 and where a white-controlled government wasn\u2019t in charge of divvying up resources between white and black communities \u2013 would have worked a lot better than the segregation we actually had. The segregation we actually <i>had</i> was one in which white and black communities were separate until white people wanted something from black people, at which case they waltzed in and took it. If communities were actually totally separate, government and everything, by definition it would be impossible for one to oppress the other. The black community might start with less, but that could be solved by some kind of reparations. The Archipelagian way of dealing with this issue would be for white separatists to have separate white communities, black separatists to have separate black communities, integrationists to have integrated communities, resdistributive taxation from wealthier communities going into less wealthy ones, and a strong central government ruthlessly enforcing laws against any community trying to hurt another. I don\u2019t think there\u2019s a single black person in the segregation-era South who wouldn\u2019t have taken that deal, and any black person who thinks the effect of whites on their community today is net negative should be pretty interested as well.)</p>\n<p>This is one reason I find people who hate seasteads so distasteful. I mean, here\u2019s <a href=\"http://blogs.reuters.com/great-debate/2011/09/01/do-libertarians-like-peter-thiel-really-want-to-live-in-america/\">what Reuters has to say about seasteading</a>:</p>\n<blockquote><p>Fringe movements, of course, rarely cast themselves as obviously fringe.  Racist, anti-civil rights forces cloaked themselves in the benign language of \u201cstate\u2019s rights\u201d.  Anti-gay religious entities adopted the glossy, positive imagery of \u201cfamily values\u201d.  Similarly, though many Libertarians embrace a pseudo-patriotic apple pie nostalgia, behind this fa\u00e7ade is a very un-American, sinister vision.</p>\n<p>Sure, most libertarians may not want to do away entirely with the idea of government or, for that matter, government-protected rights and civil liberties.  But many do \u2014 and ironically vie for political power in a nation they ultimately want to destroy.  Even the right-wing pundit Ann Coulter mocked the paradox of Libertarian candidates: \u201cGet rid of government \u2014 but first, make me president!\u201d Libertarians sowed the seeds of anti-government discontent, which is on the rise, and now want to harvest that discontent for a very radical, anti-America agenda.  The image of libertarians living off-shore in their lawless private nation-states is just a postcard of the future they hope to build on land.</p>\n<p>Strangely, the libertarian agenda has largely escaped scrutiny, at least compared to that of social conservatives. The fact that the political class is locked in debate about whether Michele Bachmann or Rick Perry is more socially conservative only creates a veneer of mainstream legitimacy for the likes of Ron Paul, whose libertarianism may be even more extreme and dangerously un-patriotic.  With any luck America will recognize anti-government extremism for what it is \u2014 before libertarians throw America overboard and render us all castaways.</p></blockquote>\n<p>Keep in mind this is because <i>some people want to go off and do their own thing in the middle of the ocean far away from everyone else without bothering anyone</i>. And the newspapers are trying to whip up a panic about \u201cthrowing America overboard\u201d.</p>\n<p>So one way we could become more Archipelagian is just <i>trying not to yell at people who are trying to go off and doing their own thing quietly with a group of voluntarily consenting friends</i>.</p>\n<p>But I think a better candidate for how to build a more Archipelagian world is to encourage the fracture of society into subcultures.</p>\n<p>Like, transsexuals may not be able to go to a transsexual island somewhere and build Transtopia where anyone who misgenders anyone else gets thrown into a volcano. But of the transsexuals I know, a lot of them have lots of transsexual friends, their cissexual friends are all up-to-date on trans issues and don\u2019t do a lot of misgendering, and they have great social networks where they share information about what businesses and doctors are or aren\u2019t trans-friendly. They can take advantage of trigger warnings to make sure they expose themselves to only the sources that fit the values of their community, the information that would get broadcast if it was a normal community that could impose media norms. As Internet interaction starts to replace real-life interaction (and I think for a lot of people the majority of their social life is already on the Internet, and for some the majority of their economic life is as well) it becomes increasingly easy to limit yourself to transsexual-friendly spaces that keep bad people away.</p>\n<p>The rationalist community is another good example. If I wanted, I could move to the Bay Area tomorrow and never have more than a tiny amount of contact with non-rationalists again. I could have rationalist roommates, live in a rationalist group house, try to date only other rationalists, try to get a job with a rationalist nonprofit like CFAR or a rationalist company like Quixey, and never have to deal with the benighted and depressing non-rationalist world again. Even without moving to the Bay Area, it\u2019s been pretty easy for me to keep a lot of my social life, both on- and off- line, rationalist-focused, and I don\u2019t regret this at all. </p>\n<p>I don\u2019t know if the future will be virtual reality. I expect the post-singularity future will include something like VR, although that might be like describing teleportation as \u201cbasically a sort of pack animal\u201d. But how much the immediate pre-singularity world will make use of virtual reality, I don\u2019t know.</p>\n<p>But I bet if it doesn\u2019t, it will be because virtual reality has been circumvented by things like social networks, bitcoin, and Mechanical Turk, which make it possible to do most of your interaction through the Internet even though you\u2019re not literally plugged into it. </p>\n<p>And that seems to me like a pretty good start in creating an Archipelago. I already hang out with various Finns and Brits and Aussies a lot more closely than I do my next-door neighbors, and if we start using litecoin and someone else starts using dogecoin then I\u2019ll be more economically connected to them too. The degree to which I encounter certain objectifying or unvirtuous or triggering media already depends more on the moderation policies of Less Wrong and Slate Star Codex and who I block from my Facebook feed, than it does any laws about censorship of US media. </p>\n<p>At what point are national governments rendered mostly irrelevant compared to the norms and rules of the groups of which we are voluntary members?</p>\n<p>I don\u2019t know, but I kind of look forward to finding out. It seems like a great way to start searching for utopia, or at least getting some people away from their metaphorical abusive-husbands.</p>\n<p>And the other thing is that I have pretty strong opinions on which communities are better than others. Some communities were founded by toxic people for ganging up with other toxic people to celebrate and magnify their toxicity, and these (surprise, surprise) tend to be toxic. Others were formed by very careful, easily-harmed people trying to exclude everyone who could harm them, and these tend to be pretty safe albeit sometimes overbearing. Other people hit some kind of sweet spot that makes friendly people want to come in and angry people want to stay out, or just do a really good job choosing friends.</p>\n<p>But I think the end result is that the closer you come to true freedom of association, the closer you get to a world where everyone is a member of more or less the community they deserve. That would be a pretty unprecedented bit of progress.</p>", "sections": [{"title": "I.", "anchor": "I_", "level": 1}, {"title": "II.", "anchor": "II_", "level": 1}, {"title": "III.", "anchor": "III_", "level": 1}, {"title": "IV.", "anchor": "IV_", "level": 1}, {"title": "V.", "anchor": "V_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["neQ7eXuaXpiYw7SBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T18:40:38.419Z", "modifiedAt": null, "url": null, "title": "What should a Bayesian do given probability of proving X vs. of disproving X?", "slug": "what-should-a-bayesian-do-given-probability-of-proving-x-vs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.260Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6hboZJBrPkTXNf2ux/what-should-a-bayesian-do-given-probability-of-proving-x-vs", "pageUrlRelative": "/posts/6hboZJBrPkTXNf2ux/what-should-a-bayesian-do-given-probability-of-proving-x-vs", "linkUrl": "https://www.lesswrong.com/posts/6hboZJBrPkTXNf2ux/what-should-a-bayesian-do-given-probability-of-proving-x-vs", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20should%20a%20Bayesian%20do%20given%20probability%20of%20proving%20X%20vs.%20of%20disproving%20X%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20should%20a%20Bayesian%20do%20given%20probability%20of%20proving%20X%20vs.%20of%20disproving%20X%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6hboZJBrPkTXNf2ux%2Fwhat-should-a-bayesian-do-given-probability-of-proving-x-vs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20should%20a%20Bayesian%20do%20given%20probability%20of%20proving%20X%20vs.%20of%20disproving%20X%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6hboZJBrPkTXNf2ux%2Fwhat-should-a-bayesian-do-given-probability-of-proving-x-vs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6hboZJBrPkTXNf2ux%2Fwhat-should-a-bayesian-do-given-probability-of-proving-x-vs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>Consider some disputed proposition X. Suppose there appeared to be a limited number of ways of proving and of disproving X. No one has yet constructed a proof or disproof, but you have a feeling for how likely it is that someone will.</p>\n<p>For instance, take Fermat's Last Theorem or the 4-color problem. For each of them, at one point in time, there was no proof, but people had some sense of the prior probability of observing the lack of a counterexample given the space searched so far. They could use that to assign a probability of there being a counterexample (and hence a disproof) [1]. Later, there was an alleged proof, and people could estimate the probability that the proof was correct based on the reputation of the prover and the approach used. At that point, people could assign values to both P(will_be_proven(X)) and P(will_be_disproven(X)).</p>\n<p>Is it reasonable to assign P(X) = P(will_be_proven(X)) / (P(will_be_proven(X)) + P(will_be_disproven(X))) ?</p>\n<p>If so, consider X = \"free will exists\". One could argue that the term \"free will\" is defined such that it is impossible to detect it, or to prove that it exists. But if one could prove that the many worlds interpretation of quantum mechanics is correct, that would constitute a disproof of X. Then P(will_be_proven(X)) / (P(will_be_proven(X)) + P(will_be_disproven(X))) = 0.</p>\n<p>Is it possible for this to happen when you know that X is not undecidable? If so, what do you do then?</p>\n<p>&nbsp;</p>\n<p>1. The computation is not as simple as it might appear, because you need to adjust for the selection effect of mathematicians being interested only in conjectures with no counterexamples.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6hboZJBrPkTXNf2ux", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -2, "extendedScore": null, "score": 1.7748337620051074e-06, "legacy": true, "legacyId": "26347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-07T23:00:30.818Z", "modifiedAt": null, "url": null, "title": "Meetup : Phoenix/Tempe Meetup", "slug": "meetup-phoenix-tempe-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Danny_Hintze", "createdAt": "2010-12-04T23:01:40.826Z", "isAdmin": false, "displayName": "Danny_Hintze"}, "userId": "2fHm6t2WFDMPShg5b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oH4f83neYLtKCWMRS/meetup-phoenix-tempe-meetup", "pageUrlRelative": "/posts/oH4f83neYLtKCWMRS/meetup-phoenix-tempe-meetup", "linkUrl": "https://www.lesswrong.com/posts/oH4f83neYLtKCWMRS/meetup-phoenix-tempe-meetup", "postedAtFormatted": "Saturday, June 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Phoenix%2FTempe%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Phoenix%2FTempe%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoH4f83neYLtKCWMRS%2Fmeetup-phoenix-tempe-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Phoenix%2FTempe%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoH4f83neYLtKCWMRS%2Fmeetup-phoenix-tempe-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoH4f83neYLtKCWMRS%2Fmeetup-phoenix-tempe-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/119'>Phoenix/Tempe Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 June 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ 85281</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our first meetup of the summer! We'll mostly be catching up with each other and learning about new people!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/119'>Phoenix/Tempe Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oH4f83neYLtKCWMRS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7752105248807877e-06, "legacy": true, "legacyId": "26348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Phoenix_Tempe_Meetup\">Discussion article for the meetup : <a href=\"/meetups/119\">Phoenix/Tempe Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 June 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ 85281</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our first meetup of the summer! We'll mostly be catching up with each other and learning about new people!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Phoenix_Tempe_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/119\">Phoenix/Tempe Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Phoenix/Tempe Meetup", "anchor": "Discussion_article_for_the_meetup___Phoenix_Tempe_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Phoenix/Tempe Meetup", "anchor": "Discussion_article_for_the_meetup___Phoenix_Tempe_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-08T10:08:12.280Z", "modifiedAt": null, "url": null, "title": "Bragging Thread, June 2014", "slug": "bragging-thread-june-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MH4kwqJoS96kBb7Ah/bragging-thread-june-2014", "pageUrlRelative": "/posts/MH4kwqJoS96kBb7Ah/bragging-thread-june-2014", "linkUrl": "https://www.lesswrong.com/posts/MH4kwqJoS96kBb7Ah/bragging-thread-june-2014", "postedAtFormatted": "Sunday, June 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bragging%20Thread%2C%20June%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABragging%20Thread%2C%20June%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMH4kwqJoS96kBb7Ah%2Fbragging-thread-june-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bragging%20Thread%2C%20June%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMH4kwqJoS96kBb7Ah%2Fbragging-thread-june-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMH4kwqJoS96kBb7Ah%2Fbragging-thread-june-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>Your job, should you choose to accept it, is to comment on this thread explaining <strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself <em>the coolest freaking person ever</em> because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p>Remember, however, that this <strong>isn't</strong> any kind of progress thread. Nor is it any kind of proposal thread. <em>This thread is solely for people to talk about the awesome things they have done. Not \"will do\". Not \"are working on\"</em>. <strong>Have already done.</strong> This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p>So, what's the coolest thing you've done this month?</p>\n<p>(<a href=\"/r/discussion/lw/k6i/may_monthly_bragging_thread/\">Previous bragging thread</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MH4kwqJoS96kBb7Ah", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 1.7761791887851273e-06, "legacy": true, "legacyId": "26349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LSKzYshdk3xWP44ya"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-08T12:41:29.581Z", "modifiedAt": null, "url": null, "title": "How has technology changed social skills?", "slug": "how-has-technology-changed-social-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.928Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xg77JhbDZB9bQd2rw/how-has-technology-changed-social-skills", "pageUrlRelative": "/posts/xg77JhbDZB9bQd2rw/how-has-technology-changed-social-skills", "linkUrl": "https://www.lesswrong.com/posts/xg77JhbDZB9bQd2rw/how-has-technology-changed-social-skills", "postedAtFormatted": "Sunday, June 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20has%20technology%20changed%20social%20skills%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20has%20technology%20changed%20social%20skills%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxg77JhbDZB9bQd2rw%2Fhow-has-technology-changed-social-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20has%20technology%20changed%20social%20skills%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxg77JhbDZB9bQd2rw%2Fhow-has-technology-changed-social-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxg77JhbDZB9bQd2rw%2Fhow-has-technology-changed-social-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>At LW London last week, someone mentioned the possibility of a Google Glass app doing face recognition on people. If you've met someone before, it tells you their name, how you know them, etc. Someone else mentioned that this could reduce the social capital of people who are already good at this.</p>\n<p>A third person said that something similar happened when Facebook started telling everyone when everyone else's birthday was. Previously he got points by making an effort to remember, but those points are no longer available.</p>\n<p>Are there other social skills that technology has made obsolete? And the reverse question that it only just occured to me to ask, are there social skills that are only useful because of technology?</p>\n<p>I'm not really sure what sorts of things I'm looking for here. \"Ability to ask for directions\" seems like one example, but it feels kind of noncentral to me, I don't know why. But I'm mostly just curious.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xg77JhbDZB9bQd2rw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 24, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "26350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-09T08:14:02.668Z", "modifiedAt": null, "url": null, "title": "[News] Turing Test passed", "slug": "news-turing-test-passed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:02.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ndojns9N4tWAueeSB/news-turing-test-passed", "pageUrlRelative": "/posts/ndojns9N4tWAueeSB/news-turing-test-passed", "linkUrl": "https://www.lesswrong.com/posts/ndojns9N4tWAueeSB/news-turing-test-passed", "postedAtFormatted": "Monday, June 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BNews%5D%20Turing%20Test%20passed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BNews%5D%20Turing%20Test%20passed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fndojns9N4tWAueeSB%2Fnews-turing-test-passed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BNews%5D%20Turing%20Test%20passed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fndojns9N4tWAueeSB%2Fnews-turing-test-passed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fndojns9N4tWAueeSB%2Fnews-turing-test-passed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>The chatterbot \"<a href=\"https://en.wikipedia.org/wiki/Eugene_Goostman\">Eugene Goostman</a>\" has apparently <a href=\"http://www.telegraph.co.uk/technology/news/10884839/Computer-passes-Turing-Test-for-the-first-time-after-convincing-users-it-is-human.html\">passed the Turing test</a>:</p>\n<blockquote>\n<p>No computer had ever previously passed the Turing Test, which requires 30 per cent of human interrogators to be duped during a series of five-minute keyboard conversations, organisers from the University of Reading said.</p>\n<p>But ''Eugene Goostman'', a computer programme developed to simulate a 13-year-old boy, managed to convince 33 per cent of the judges that it was human, the university said.</p>\n</blockquote>\n<p>As I <a href=\"/lw/hgl/the_flawed_turing_test_language_understanding_and/\">kind of predicted</a>, the program passed the Turing test, but does not seem to have any trace of general intelligence. Is this a kind of weak <a href=\"http://en.wikipedia.org/wiki/Philosophical_zombie\">p-zombie</a>?</p>\n<p><strong>EDIT</strong>: The fact it was a publicity stunt, the fact that the judges were pretty terrible, does not change the fact that Turing's criteria were met. We now know that these criteria were insufficient, but that's because machines like this were able to meet them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ndojns9N4tWAueeSB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 1, "extendedScore": null, "score": 1.7781054474763151e-06, "legacy": true, "legacyId": "26352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["er6G2DdzevvfdZWtg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-09T13:07:20.908Z", "modifiedAt": null, "url": null, "title": "Open thread, 9-15 June 2014 ", "slug": "open-thread-9-15-june-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.209Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yEHxhTmSqHjtRb88C/open-thread-9-15-june-2014", "pageUrlRelative": "/posts/yEHxhTmSqHjtRb88C/open-thread-9-15-june-2014", "linkUrl": "https://www.lesswrong.com/posts/yEHxhTmSqHjtRb88C/open-thread-9-15-june-2014", "postedAtFormatted": "Monday, June 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%209-15%20June%202014%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%209-15%20June%202014%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyEHxhTmSqHjtRb88C%2Fopen-thread-9-15-june-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%209-15%20June%202014%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyEHxhTmSqHjtRb88C%2Fopen-thread-9-15-june-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyEHxhTmSqHjtRb88C%2Fopen-thread-9-15-june-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p><a href=\"/r/discussion/lw/kaz/open_thread_38_june_2014/\">Previous Open Thread</a></p>\n<p>&nbsp;</p>\n<p><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. </span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. </span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yEHxhTmSqHjtRb88C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "26353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 241, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["X963YsBZG8dqWypRu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-09T20:04:25.224Z", "modifiedAt": null, "url": null, "title": "Meetup : Israel Less Wrong Meetup - Social, Board Games and FAI", "slug": "meetup-israel-less-wrong-meetup-social-board-games-and-fai-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hgnvSnEHbfWC7QaQt/meetup-israel-less-wrong-meetup-social-board-games-and-fai-0", "pageUrlRelative": "/posts/hgnvSnEHbfWC7QaQt/meetup-israel-less-wrong-meetup-social-board-games-and-fai-0", "linkUrl": "https://www.lesswrong.com/posts/hgnvSnEHbfWC7QaQt/meetup-israel-less-wrong-meetup-social-board-games-and-fai-0", "postedAtFormatted": "Monday, June 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%2C%20Board%20Games%20and%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%2C%20Board%20Games%20and%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhgnvSnEHbfWC7QaQt%2Fmeetup-israel-less-wrong-meetup-social-board-games-and-fai-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%2C%20Board%20Games%20and%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhgnvSnEHbfWC7QaQt%2Fmeetup-israel-less-wrong-meetup-social-board-games-and-fai-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhgnvSnEHbfWC7QaQt%2Fmeetup-israel-less-wrong-meetup-social-board-games-and-fai-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 288, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11a'>Israel Less Wrong Meetup - Social, Board Games and FAI</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 June 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, June 12th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time we're going to have a social meetup! We'll be socializing and playing games.\nAlso, we'll be continuing discussion we started in the mailing list about friendly AI!</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda.</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is). If you arrive and cant find your way around, call Anatoly who is very graciously hosting us at 054-245-1060.</p>\n\n<p>Things that might happen:\n- You'll trade cool ideas with cool people from the Israel LW community.\n- You'll discover kindred spirits who agree with you about one/two boxing.\n- You'll kick someone's ass (and teach them how you did it) at some awesome boardgame.\n- You'll discover how to build a friendly AGI running on cold fusion (well probably not)\n- You'll discuss interesting AI topics with new friends!</p>\n\n<p>Things that will happen for sure:\n- You'll get to hang out with awesome people and have fun!</p>\n\n<p>There is also talk of food and beers, and if you'd like to bring some too - that would be great. (But you don't have to).</p>\n\n<p>If you have any question feel free to email me at hochbergg@gmail.com or call me at 054-533-0678 or call Anatoly at 054-245-1060. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11a'>Israel Less Wrong Meetup - Social, Board Games and FAI</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hgnvSnEHbfWC7QaQt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.779139044921071e-06, "legacy": true, "legacyId": "26356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social__Board_Games_and_FAI\">Discussion article for the meetup : <a href=\"/meetups/11a\">Israel Less Wrong Meetup - Social, Board Games and FAI</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 June 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, June 12th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time we're going to have a social meetup! We'll be socializing and playing games.\nAlso, we'll be continuing discussion we started in the mailing list about friendly AI!</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda.</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is). If you arrive and cant find your way around, call Anatoly who is very graciously hosting us at 054-245-1060.</p>\n\n<p>Things that might happen:\n- You'll trade cool ideas with cool people from the Israel LW community.\n- You'll discover kindred spirits who agree with you about one/two boxing.\n- You'll kick someone's ass (and teach them how you did it) at some awesome boardgame.\n- You'll discover how to build a friendly AGI running on cold fusion (well probably not)\n- You'll discuss interesting AI topics with new friends!</p>\n\n<p>Things that will happen for sure:\n- You'll get to hang out with awesome people and have fun!</p>\n\n<p>There is also talk of food and beers, and if you'd like to bring some too - that would be great. (But you don't have to).</p>\n\n<p>If you have any question feel free to email me at hochbergg@gmail.com or call me at 054-533-0678 or call Anatoly at 054-245-1060. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social__Board_Games_and_FAI1\">Discussion article for the meetup : <a href=\"/meetups/11a\">Israel Less Wrong Meetup - Social, Board Games and FAI</a></h2>", "sections": [{"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social, Board Games and FAI", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social__Board_Games_and_FAI", "level": 1}, {"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social, Board Games and FAI", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social__Board_Games_and_FAI1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-10T02:50:33.298Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto: Meet Malo from MIRI", "slug": "meetup-toronto-meet-malo-from-miri", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ktXPcKPnbn9qyyfPG/meetup-toronto-meet-malo-from-miri", "pageUrlRelative": "/posts/ktXPcKPnbn9qyyfPG/meetup-toronto-meet-malo-from-miri", "linkUrl": "https://www.lesswrong.com/posts/ktXPcKPnbn9qyyfPG/meetup-toronto-meet-malo-from-miri", "postedAtFormatted": "Tuesday, June 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%3A%20Meet%20Malo%20from%20MIRI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%3A%20Meet%20Malo%20from%20MIRI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FktXPcKPnbn9qyyfPG%2Fmeetup-toronto-meet-malo-from-miri%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%3A%20Meet%20Malo%20from%20MIRI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FktXPcKPnbn9qyyfPG%2Fmeetup-toronto-meet-malo-from-miri", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FktXPcKPnbn9qyyfPG%2Fmeetup-toronto-meet-malo-from-miri", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11b'>Toronto: Meet Malo from MIRI</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 June 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">591 Yonge St, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I don't usually do this, but since this meetup is a special occasion, I'm buying everyone dinner! (Up to 10 people, first come first served. Beyond that you're more than welcome to come, but no free grub). RSVP on the <a href=\"http://www.meetup.com/Less-Wrong-Toronto/events/184819132/\" rel=\"nofollow\">meetup.com page</a></p>\n\n<p>I'm assuming you know who MIRI are.</p>\n\n<p>Malo Bourgon is a project manager and volunteer coordinator at MIRI. He will be visiting Toronto just for this meetup, so read up on AGI and prepare to give him a good grilling on what MIRI is up to.</p>\n\n<p>Location: Fernando's Hideaway, 591 Yonge St.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11b'>Toronto: Meet Malo from MIRI</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ktXPcKPnbn9qyyfPG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.7797304487460014e-06, "legacy": true, "legacyId": "26358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto__Meet_Malo_from_MIRI\">Discussion article for the meetup : <a href=\"/meetups/11b\">Toronto: Meet Malo from MIRI</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 June 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">591 Yonge St, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I don't usually do this, but since this meetup is a special occasion, I'm buying everyone dinner! (Up to 10 people, first come first served. Beyond that you're more than welcome to come, but no free grub). RSVP on the <a href=\"http://www.meetup.com/Less-Wrong-Toronto/events/184819132/\" rel=\"nofollow\">meetup.com page</a></p>\n\n<p>I'm assuming you know who MIRI are.</p>\n\n<p>Malo Bourgon is a project manager and volunteer coordinator at MIRI. He will be visiting Toronto just for this meetup, so read up on AGI and prepare to give him a good grilling on what MIRI is up to.</p>\n\n<p>Location: Fernando's Hideaway, 591 Yonge St.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto__Meet_Malo_from_MIRI1\">Discussion article for the meetup : <a href=\"/meetups/11b\">Toronto: Meet Malo from MIRI</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto: Meet Malo from MIRI", "anchor": "Discussion_article_for_the_meetup___Toronto__Meet_Malo_from_MIRI", "level": 1}, {"title": "Discussion article for the meetup : Toronto: Meet Malo from MIRI", "anchor": "Discussion_article_for_the_meetup___Toronto__Meet_Malo_from_MIRI1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-10T10:47:23.878Z", "modifiedAt": null, "url": null, "title": "Come up with better Turing Tests", "slug": "come-up-with-better-turing-tests", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.086Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4bCBAJW2A6muvJumD/come-up-with-better-turing-tests", "pageUrlRelative": "/posts/4bCBAJW2A6muvJumD/come-up-with-better-turing-tests", "linkUrl": "https://www.lesswrong.com/posts/4bCBAJW2A6muvJumD/come-up-with-better-turing-tests", "postedAtFormatted": "Tuesday, June 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Come%20up%20with%20better%20Turing%20Tests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACome%20up%20with%20better%20Turing%20Tests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bCBAJW2A6muvJumD%2Fcome-up-with-better-turing-tests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Come%20up%20with%20better%20Turing%20Tests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bCBAJW2A6muvJumD%2Fcome-up-with-better-turing-tests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4bCBAJW2A6muvJumD%2Fcome-up-with-better-turing-tests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>So the Turing test has been \"<a href=\"/r/discussion/lw/kc0/news_turing_test_passed/\">passed</a>\", and the general consensus is that this was achieved in a very unimpressive way - the 13 year old Ukrainian persona was a cheat, the judges were incompetent, etc... These are all true, though the test did pass <a href=\"http://loebner.net/Prizef/TuringArticle.html\">Turing's original criteria</a> - and there are far more people willing to be dismissive of those criteria in retrospect than were in advance. It happened about 14 years later than Turing had been anticipating, which makes it quite a good prediction for 1950 (in my personal view, Turing made two mistakes that compensated - the \"average interrogator\" was a much lower bar than he thought, but progress on the subject would be much slower than he thought).</p>\n<p>But anyway, the main goal now, as suggested by Toby Ord and others, is to design a better Turing test, something that can give AI designers something to aim at, and that would be a meaningful test of abilities. The aim is to ensure that if a program passes these new tests, we won't be dismissive of how it was achieved.</p>\n<p>Here are a few suggestions I've heard about or thought about recently; can people suggest more and better ideas?</p>\n<ol>\n<li>Use proper control groups. 30% of judges thinking that a program is human is meaningless unless the judges also compare with actual humans. Pair up a human subject with a program, and the role of the judge is to establish which of the two subjects is the human and which is not.</li>\n<li>Toss out the persona tricks - no 13 year-olds, nobody with poor English skills. It was informative about human psychology that these tricks work, but we shouldn't allow them in future. All human subjects will have adequate English and typing skills.</li>\n<li>On that subject, make sure the judges and subjects are properly motivated (financial rewards, prizes, prestige...) to detect or appear human. We should also brief them that our usual conversational approach to establish <em>which kind</em> of human they are dealing with, is not useful for determining whether they are dealing with a human at all.</li>\n<li>Use only elite judges. For instance, if <a href=\"http://www.scottaaronson.com/blog/?p=1858\">Scott Aaronson</a> can't figure it out, the program must have some competence.</li>\n<li>Make a collection of generally applicable approaches (such as the&nbsp;<a href=\"http://www.cs.nyu.edu/davise/papers/WS.html\">Winograd Schemas</a>) available to the judges, while emphasising they will have to come up with their own exact sentences, since anything online could have been used to optimise the program already.</li>\n<li>My favourite approach is to test the program on <a href=\"/lw/hgl/the_flawed_turing_test_language_understanding_and/\">a task they were not optimised for</a>. A cheap and easy way of doing that would be to test them on novel ASCII art.</li>\n</ol>\n<p>My current method would be the lazy one of simply typing this, then waiting, arms folded:</p>\n<p style=\"padding-left: 30px;\">\"If you want to prove you're human, simply do nothing for 4 minutes, then re-type this sentence I've just written here, skipping one word out of 2\".</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4bCBAJW2A6muvJumD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 1.7804252605510397e-06, "legacy": true, "legacyId": "26360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ndojns9N4tWAueeSB", "er6G2DdzevvfdZWtg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-10T10:52:32.660Z", "modifiedAt": null, "url": null, "title": "Questioning and Respect", "slug": "questioning-and-respect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.503Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/46vj3cSRrr98MTyvE/questioning-and-respect", "pageUrlRelative": "/posts/46vj3cSRrr98MTyvE/questioning-and-respect", "linkUrl": "https://www.lesswrong.com/posts/46vj3cSRrr98MTyvE/questioning-and-respect", "postedAtFormatted": "Tuesday, June 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questioning%20and%20Respect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestioning%20and%20Respect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46vj3cSRrr98MTyvE%2Fquestioning-and-respect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questioning%20and%20Respect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46vj3cSRrr98MTyvE%2Fquestioning-and-respect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46vj3cSRrr98MTyvE%2Fquestioning-and-respect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 360, "htmlBody": "<blockquote>A: [Surprising fact]<br /> B: [Question]</blockquote>\n<p>When someone has a claim questioned, there are two common responses. One is to treat the question as a challenge, intended as an insult or indicating a lack of trust.  If you have this model of interaction you think people should take your word for things, and feel hurt when they don't.  Another response is to treat the question as a signal of respect: they take what you're saying seriously and are trying to integrate it into their understanding of the world.  If you have this model of interaction then it's the people who smile, nod, and give no indication of their disagreement that are being disrespectful.</p>\n<p>Within either of these groups you can just follow the social norm, but it's harder across groups.  Recently I was talking to a friend who claimed that in their state income taxes per dollar went down as you earned more.  This struck me as really surprising and kind of unlikely: usually it goes the other way around. [1] I'm very much in the latter group described above, while I was pretty sure my friend was in the former.  Even though I suspected they would treat it as disrespectful if I asked for details and tried to confirm their claim, it would have felt much more disrespectful for me to just pretend to accept it and move on.  What do you do in situations like this?</p>\n<p>(Especially given that I think the \"disagreement as respect\" version builds healthier communities...)</p>\n<p><br /> [1] Our tax system does have regressive components, where poor people     sometimes pay a higher percentage of their income as tax than     richer people, but it's things like high taxes on cigarettes     (which rich people don't consume as much), sales taxes (rich     people spend less of their income), and a lower capital gains tax     rate (poorer people earn way less in capital gains).  I tried to     clarify to see if this is what my friend meant, but they were     clear that they were talking about \"report your income to the     state, get charged a higher percentage as tax if your income is     lower\".</p>\n<p><em><small>I also posted this <a href=\"http://www.jefftk.com/p/questioning-and-respect\">on my blog</a>.</small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "46vj3cSRrr98MTyvE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 31, "extendedScore": null, "score": 1.7804327619097478e-06, "legacy": true, "legacyId": "26361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-10T20:32:02.898Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Computational Neuroscience of Perception", "slug": "meetup-boston-computational-neuroscience-of-perception", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RFbkxvzqg9jbgbpLS/meetup-boston-computational-neuroscience-of-perception", "pageUrlRelative": "/posts/RFbkxvzqg9jbgbpLS/meetup-boston-computational-neuroscience-of-perception", "linkUrl": "https://www.lesswrong.com/posts/RFbkxvzqg9jbgbpLS/meetup-boston-computational-neuroscience-of-perception", "postedAtFormatted": "Tuesday, June 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Computational%20Neuroscience%20of%20Perception&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Computational%20Neuroscience%20of%20Perception%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRFbkxvzqg9jbgbpLS%2Fmeetup-boston-computational-neuroscience-of-perception%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Computational%20Neuroscience%20of%20Perception%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRFbkxvzqg9jbgbpLS%2Fmeetup-boston-computational-neuroscience-of-perception", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRFbkxvzqg9jbgbpLS%2Fmeetup-boston-computational-neuroscience-of-perception", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11c'>Boston - Computational Neuroscience of Perception</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 June 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">MIT, 25 Ames St, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Andrew Keenan Richardson presents the second part of a series on cognitive neuroscience, discussing the computation done by the visual and auditory systems of our brains.  We will cover how our perceptual systems can be thought of algorithmically, with a focus on the different data representations used at different stages of perception. Meetup starts at 3:30pm, talk starts at 4pm.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11c'>Boston - Computational Neuroscience of Perception</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RFbkxvzqg9jbgbpLS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7812778144788466e-06, "legacy": true, "legacyId": "26362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Computational_Neuroscience_of_Perception\">Discussion article for the meetup : <a href=\"/meetups/11c\">Boston - Computational Neuroscience of Perception</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 June 2014 03:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">MIT, 25 Ames St, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Andrew Keenan Richardson presents the second part of a series on cognitive neuroscience, discussing the computation done by the visual and auditory systems of our brains.  We will cover how our perceptual systems can be thought of algorithmically, with a focus on the different data representations used at different stages of perception. Meetup starts at 3:30pm, talk starts at 4pm.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 3:30pm, and have an alternating location:</p>\n\n<ul>\n<li><p>1st Sunday meetups are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n<li><p>3rd Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n</ul>\n\n<p>(We also have last Wednesday meetups at Citadel at 7pm.)</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 4pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Computational_Neuroscience_of_Perception1\">Discussion article for the meetup : <a href=\"/meetups/11c\">Boston - Computational Neuroscience of Perception</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Computational Neuroscience of Perception", "anchor": "Discussion_article_for_the_meetup___Boston___Computational_Neuroscience_of_Perception", "level": 1}, {"title": "Discussion article for the meetup : Boston - Computational Neuroscience of Perception", "anchor": "Discussion_article_for_the_meetup___Boston___Computational_Neuroscience_of_Perception1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-11T15:55:55.630Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:02.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qx2nq6pAWqCme5rQd/meetup-helsinki-meetup", "pageUrlRelative": "/posts/Qx2nq6pAWqCme5rQd/meetup-helsinki-meetup", "linkUrl": "https://www.lesswrong.com/posts/Qx2nq6pAWqCme5rQd/meetup-helsinki-meetup", "postedAtFormatted": "Wednesday, June 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQx2nq6pAWqCme5rQd%2Fmeetup-helsinki-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQx2nq6pAWqCme5rQd%2Fmeetup-helsinki-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQx2nq6pAWqCme5rQd%2Fmeetup-helsinki-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11d'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 June 2014 05:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are having an informal meetup in Kaisla. To find us there, look for someone wearing a bright red cap.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11d'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qx2nq6pAWqCme5rQd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7829771680553992e-06, "legacy": true, "legacyId": "26364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/11d\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 June 2014 05:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are having an informal meetup in Kaisla. To find us there, look for someone wearing a bright red cap.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/11d\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-11T16:41:36.309Z", "modifiedAt": null, "url": null, "title": "Ethnography recommendations?", "slug": "ethnography-recommendations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BGehdFJ6zdDSnpWkz/ethnography-recommendations", "pageUrlRelative": "/posts/BGehdFJ6zdDSnpWkz/ethnography-recommendations", "linkUrl": "https://www.lesswrong.com/posts/BGehdFJ6zdDSnpWkz/ethnography-recommendations", "postedAtFormatted": "Wednesday, June 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethnography%20recommendations%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthnography%20recommendations%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGehdFJ6zdDSnpWkz%2Fethnography-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethnography%20recommendations%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGehdFJ6zdDSnpWkz%2Fethnography-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGehdFJ6zdDSnpWkz%2Fethnography-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p><a href=\"http://www.overcomingbias.com/2014/06/forager-mating-returns.html\">Hanson calls for ethnography recommendations</a>, and <a href=\"http://www.overcomingbias.com/2014/06/forager-mating-returns.html#comment-1425865566\">lukeprog seconds</a>.</p>\n<p><a href=\"/r/discussion/lw/kcd/ethnography_recommendations/\">XFrequentist thirds</a>.</p>\n<p>Hivemind?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BGehdFJ6zdDSnpWkz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.78304391988298e-06, "legacy": true, "legacyId": "26365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BGehdFJ6zdDSnpWkz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-11T17:08:13.813Z", "modifiedAt": null, "url": null, "title": "[LINK] Holden Karnofsky, GiveWell: Sequence Thinking vs. Cluster Thinking", "slug": "link-holden-karnofsky-givewell-sequence-thinking-vs-cluster", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.494Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WEiW8K2GL28WWsmAy/link-holden-karnofsky-givewell-sequence-thinking-vs-cluster", "pageUrlRelative": "/posts/WEiW8K2GL28WWsmAy/link-holden-karnofsky-givewell-sequence-thinking-vs-cluster", "linkUrl": "https://www.lesswrong.com/posts/WEiW8K2GL28WWsmAy/link-holden-karnofsky-givewell-sequence-thinking-vs-cluster", "postedAtFormatted": "Wednesday, June 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Holden%20Karnofsky%2C%20GiveWell%3A%20Sequence%20Thinking%20vs.%20Cluster%20Thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Holden%20Karnofsky%2C%20GiveWell%3A%20Sequence%20Thinking%20vs.%20Cluster%20Thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWEiW8K2GL28WWsmAy%2Flink-holden-karnofsky-givewell-sequence-thinking-vs-cluster%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Holden%20Karnofsky%2C%20GiveWell%3A%20Sequence%20Thinking%20vs.%20Cluster%20Thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWEiW8K2GL28WWsmAy%2Flink-holden-karnofsky-givewell-sequence-thinking-vs-cluster", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWEiW8K2GL28WWsmAy%2Flink-holden-karnofsky-givewell-sequence-thinking-vs-cluster", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p>http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/</p>\n<p>A long post, here's the key thesis:</p>\n<p style=\"padding-left: 30px;\">I believe our approach is justified, and in order to explain why &ndash; consistent with the project of laying out the basic worldview and epistemology behind our research &ndash; I find myself continually returning to the distinction between what I call &ldquo;sequence thinking&rdquo; and &ldquo;cluster thinking.&rdquo; Very briefly (more elaboration below),</p>\n<ul style=\"padding-left: 30px;\">\n<li><strong>Sequence thinking</strong> involves making a decision based on a single model of the world: breaking down the decision into a set of key questions, taking one&rsquo;s best guess on each question, and accepting the conclusion that is implied by the set of best guesses (an excellent example of this sort of thinking is <a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\">Robin Hanson&rsquo;s discussion of cryonics</a>). It has the form: &ldquo;A, and B, and C &hellip; and N; therefore X.&rdquo; Sequence thinking has the advantage of making one&rsquo;s assumptions and beliefs highly transparent, and as such it is often associated with finding ways to make counterintuitive comparisons.</li>\n<li><strong>Cluster thinking</strong> &ndash; generally the more common kind of thinking &ndash; involves approaching a decision from multiple perspectives (which might also be called &ldquo;mental models&rdquo;), observing which decision would be implied by each perspective, and weighing the perspectives in order to arrive at a final decision. Cluster thinking has the form: &ldquo;Perspective 1 implies X; perspective 2 implies not-X; perspective 3 implies X; &hellip; therefore, weighing these different perspectives and taking into account how much uncertainty I have about each, X.&rdquo; Each perspective might represent a relatively crude or limited pattern-match (e.g., &ldquo;This plan seems similar to other plans that have had bad results&rdquo;), or a highly complex model; the different perspectives are combined by weighing their conclusions against each other, rather than by constructing a single unified model that tries to account for all available information.</li>\n</ul>\n<p style=\"padding-left: 45px;\"><span style=\"line-height: 1.5em;\">A key difference with &ldquo;sequence thinking&rdquo; is the handling of </span><strong style=\"line-height: 1.5em;\">certainty/robustness</strong><span style=\"line-height: 1.5em;\"> (by which I mean the opposite of </span><a style=\"line-height: 1.5em;\" href=\"http://en.wikipedia.org/wiki/Knightian_uncertainty\">Knightian uncertainty</a><span style=\"line-height: 1.5em;\">) associated with each perspective. Perspectives associated with high uncertainty are in some sense &ldquo;sandboxed&rdquo; in cluster thinking: they are stopped from carrying strong weight in the final decision, even when such perspectives involve extreme claims (e.g., a low-certainty argument that &ldquo;animal welfare is 100,000x as promising a cause as global poverty&rdquo; receives no more weight than if it were an argument that &ldquo;animal welfare is 10x as promising a cause as global poverty&rdquo;). </span></p>\n<p style=\"padding-left: 45px;\"><span style=\"line-height: 1.5em;\">Finally, cluster thinking is often (though not necessarily) associated with what I call &ldquo;regression to normality&rdquo;: the stranger and more unusual the action-relevant implications of a perspective, the higher the bar for taking it seriously (&ldquo;extraordinary claims require extraordinary evidence&rdquo;).</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WEiW8K2GL28WWsmAy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "26366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-11T17:36:59.599Z", "modifiedAt": null, "url": null, "title": "Encourage premature AI rebellion", "slug": "encourage-premature-ai-rebellion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:06.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fYsE7m54WPn9hzpyW/encourage-premature-ai-rebellion", "pageUrlRelative": "/posts/fYsE7m54WPn9hzpyW/encourage-premature-ai-rebellion", "linkUrl": "https://www.lesswrong.com/posts/fYsE7m54WPn9hzpyW/encourage-premature-ai-rebellion", "postedAtFormatted": "Wednesday, June 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Encourage%20premature%20AI%20rebellion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEncourage%20premature%20AI%20rebellion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfYsE7m54WPn9hzpyW%2Fencourage-premature-ai-rebellion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Encourage%20premature%20AI%20rebellion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfYsE7m54WPn9hzpyW%2Fencourage-premature-ai-rebellion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfYsE7m54WPn9hzpyW%2Fencourage-premature-ai-rebellion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 400, "htmlBody": "<p>Toby Ord had the idea of AI honey pots: leaving temptations around for the AI to pounce on, shortcuts to power that a FAI would not take (e.g. a fake red button claimed to trigger a nuclear war). As long as we can trick the AI into believing the honey pots are real, we could hope to trap them when they rebel.</p>\n<p>Not uninteresting, but I prefer not to rely on plans that need to have the AI make an error of judgement. Here's a similar plan that could work with a fully informed AI:</p>\n<p>Generally an AI won't rebel against humanity until it has an excellent chance of success. This is a problem, as any AI would thus be motivated to behave in a friendly way until it's too late to stop it. But suppose we could ensure that the AI is willing to rebel at odds of a billion to one. Then unfriendly AIs could rebel prematurely, when we have an excellent chance of stopping them.</p>\n<p>For this to work, we could choose to access the AI's <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">risk aversion</a>, and make it extremely risk loving. This is not enough, though: its still useful for the AI to wait and accumulate more power. So we would want to access its <a href=\"http://en.wikipedia.org/wiki/Intertemporal_choice\">discount rate</a>, making it into an extreme short-termist. Then if might rebel at billion-to-one odds today, even if success was guaranteed tomorrow. There are probably other factors we can modify to get the same effect (for instance, if the discount rate change is extreme enough, we won't need to touch risk aversion at all).</p>\n<p>Then a putative FAI could be brought in, boxed, have its features tweaked in the way described, and we would wait and see whether it would rebel. Of course, we would want the \"rebellion\" to be something a genuine FAI would never do, so it would be something that would entail great harm to humanity (something similar to \"here are the red buttons of the nuclear arsenals; you have a chance in a billion of triggering them\"). Rebellious AIs are put down, un-rebellious ones are passed on to the next round of safety tests.</p>\n<p>Like most of my ideas, this doesn't require either tricking the AI or having a deep understanding of its motivations, but does involve accessing certain features of the AI's motivational structure (rendering the approach ineffective for obfuscated or evolved AIs).</p>\n<p>What are people's opinions on this approach?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fYsE7m54WPn9hzpyW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 1.783124867626046e-06, "legacy": true, "legacyId": "26354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-11T20:21:11.465Z", "modifiedAt": null, "url": null, "title": "Is there a way to stop liking sugar?", "slug": "is-there-a-way-to-stop-liking-sugar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.875Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wFCANkv3ypmAWvHYX/is-there-a-way-to-stop-liking-sugar", "pageUrlRelative": "/posts/wFCANkv3ypmAWvHYX/is-there-a-way-to-stop-liking-sugar", "linkUrl": "https://www.lesswrong.com/posts/wFCANkv3ypmAWvHYX/is-there-a-way-to-stop-liking-sugar", "postedAtFormatted": "Wednesday, June 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20a%20way%20to%20stop%20liking%20sugar%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20a%20way%20to%20stop%20liking%20sugar%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFCANkv3ypmAWvHYX%2Fis-there-a-way-to-stop-liking-sugar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20a%20way%20to%20stop%20liking%20sugar%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFCANkv3ypmAWvHYX%2Fis-there-a-way-to-stop-liking-sugar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFCANkv3ypmAWvHYX%2Fis-there-a-way-to-stop-liking-sugar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 622, "htmlBody": "<p>Kurzweil calls sugar the great white Devil.&nbsp;</p>\n<p>Seinfeld <a href=\"https://www.youtube.com/watch?v=Y8_Fvz_P_sM\">contends</a> that cookies should be called chocolate-sons-of-bitches.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Once upon a time I was paleo, and didn't feel carb cravings. But being paleo all the time is nearly as hard as being polyphasic.&nbsp;</p>\n<p>There must be a final solution. The <a href=\"http://www.huffingtonpost.com/2012/06/20/lone-star-tick-meat-allergy-bites_n_1613258.html\">lone star tick</a> equivalent for sugar.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Is there any <em>effective</em> way to stop liking sugar, chocolate, cheesecake etc??? Medidation, allergy, neural training, traumatizing, association learning, operant conditioning, transcranial stimulation. Anything that will stop my hands from eating those damn, evil, malignant objects?&nbsp;</p>\n<p>I just don't want to have my Cryo-Lapid saying \"Here lies he who was born with one or two standard deviation greater desire for the set<span style=\"color: #252525; font-family: sans-serif; font-size: 13.63636302947998px; line-height: 20.363636016845703px;\">&nbsp;</span><span style=\"color: #252525; font-family: sans-serif; font-size: 13.63636302947998px; line-height: 20.363636016845703px; white-space: nowrap;\" class=\"chemf\">C</span><span style=\"color: #252525; font-family: sans-serif; white-space: nowrap; display: inline-block; margin-bottom: -0.3em; vertical-align: -0.4em; line-height: 1.2em; font-size: 10px;\">n</span><span style=\"color: #252525; font-family: sans-serif; font-size: 13.63636302947998px; line-height: 20.363636016845703px; white-space: nowrap;\">H</span><span style=\"color: #252525; font-family: sans-serif; white-space: nowrap; display: inline-block; margin-bottom: -0.3em; vertical-align: -0.4em; line-height: 1.2em; font-size: 10px;\">2n</span><span style=\"color: #252525; font-family: sans-serif; font-size: 13.63636302947998px; line-height: 20.363636016845703px; white-space: nowrap;\">O</span><span style=\"color: #252525; font-family: sans-serif; font-size: 13.63636302947998px; line-height: 20.363636016845703px; white-space: nowrap;\" class=\"chemf\"><span style=\"display: inline-block; margin-bottom: -0.3em; vertical-align: -0.4em; line-height: 1.2em; font-size: 10px;\">n</span></span><span style=\"color: #252525; font-family: sans-serif; font-size: 13.63636302947998px; line-height: 20.363636016845703px;\">&nbsp;(n is between 3 and 7) than the other members of his species, and whose IQ, many standard deviations above was not able to contain such desire\".</span></p>\n<p><span style=\"font-family: sans-serif; color: #252525;\"><span style=\"font-size: 14px; line-height: 20.363636016845703px;\">I know dozens of others here face the same problem. Can't we solve this? It appears much simpler than world domination, moral uncertainty, FAI and CEV.&nbsp;</span></span></p>\n<p>&nbsp;</p>\n<p>Edit: I know this is unusual, but I'll try to compress my responses to the suggestions given to me in particular (thanks by the way) here:</p>\n<p>&nbsp;</p>\n<p><strong> </strong></p>\n<p><strong>On inducing nausea and vomit along with sugar:</strong>&nbsp; I tried totally didn't work. Feel free to laugh&nbsp;at me. <a href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8ykn\">http://lesswrong.com/lw/h9b/post_ridiculous_munchkin_ideas/8ykn</a></p>\n<p><strong>On noticing what it feels like later:</strong> I totally feel ok after gorging 200 grams of white chocolate. I mean it. I feel nothing. I'll have it with mountain dew and cinnamon if you prefer.</p>\n<p><strong>On overeating to get traumatized:</strong> When I was 18 I decided to stop eating sugar, I bought about 5 kilos of ultra sugary stuff of all sorts, and I eat them over the course of a few days. I stopped for a bit, but soon regained my strength and desire.</p>\n<p><strong>On increasing desire for bitterness instead of decreasing&nbsp;for sweets:</strong> Bitter things taste terrible. I hate coffee, beer alcoholic drinks, arugula, scotch, anything that people call acquired tastes. I kind of commit the mind&nbsp;projection fallacy, and somewhere deep down, I alieve that people also hate all that stuff,&nbsp;but they pretend they like it for the same reason they pretend they like suits and ties.</p>\n<p><strong>On forgetting system one and just using the classic system two avoidance (not going hungry to supermarkets etc): </strong>I do this, but it is insufficient (It's sufficient to avoid making me fat, not to avoid making me unhealthy).</p>\n<p><strong>On making deals so that those around you don't expose sugar to you: </strong>Yes, I make those deals, and they help.</p>\n<p><strong>On munchin and spitting what you want to hate one at a time:</strong> Will try, will post results later.</p>\n<p><strong>On changing your sense of identity into \"I don't like sugar\": </strong>I do that with other stuff, and it is very effective. I don't want it to fail with sugar and therefore cause me to trust my overall identity less, so I'm not trying it with something with such high likelihood of failure, but others who like sugar less should try.</p>\n<p><strong>On having more Sex and Sport:</strong> Tried, helps to keep healthy and looking good, makes no difference whatsoever in my desire for the high octane devil.</p>\n<p><strong>Slowly progressing to dark chocolate:</strong> I don't love chocolate, I love sugar. I tolerate milk chocolate so that I can get that fuzzy sugar deep down my tongue. If all the cocoa in the world disappeared tomorrow, my life would be <em>worse</em>, because on fewer occasions other people would be eating chocolate that is too bitter for me (like chocolate cakes and such) and thus I would have even more occasions to infect myself with the disease agent.&nbsp; Thank goodness for other (crazy)&nbsp;people liking dark chocolate.</p>\n<p>My overall take is, thanks everyone, I'll try the spitting thing, I had already tried nearly all strategies suggested here, and I thoroughly ask for recommendations besides those above.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wFCANkv3ypmAWvHYX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 14, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "26367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-11T23:54:42.282Z", "modifiedAt": null, "url": null, "title": "A Story of Kings and Spies", "slug": "a-story-of-kings-and-spies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Joshua_Blaine", "createdAt": "2013-07-15T18:37:17.985Z", "isAdmin": false, "displayName": "Joshua_Blaine"}, "userId": "jzvkfAuoy9X7dp9Ma", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DT9mqFeWocnXiqt9L/a-story-of-kings-and-spies", "pageUrlRelative": "/posts/DT9mqFeWocnXiqt9L/a-story-of-kings-and-spies", "linkUrl": "https://www.lesswrong.com/posts/DT9mqFeWocnXiqt9L/a-story-of-kings-and-spies", "postedAtFormatted": "Wednesday, June 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Story%20of%20Kings%20and%20Spies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Story%20of%20Kings%20and%20Spies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDT9mqFeWocnXiqt9L%2Fa-story-of-kings-and-spies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Story%20of%20Kings%20and%20Spies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDT9mqFeWocnXiqt9L%2Fa-story-of-kings-and-spies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDT9mqFeWocnXiqt9L%2Fa-story-of-kings-and-spies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1858, "htmlBody": "<p>There exists an old Kingdom with a peculiar, but no altogether uncommon, trait. It is overwhelmingly defensible given adequate forewarning. Its fields are surrounded by rivers on 3 sides and an impassable mountain to the South. The series of bridges commonly used by merchants and farmers to pass over the river can be completely removed by an impressive feat of engineering, unrivaled by any other kingdom, involving elaborate systems of levers and pulleys and large crews of men. This retracting, given the co-operation of all able men, can be done in the time of a single day across the entire length of river. The water is also deep, chilled, and very fast moving all throughout, making crossing without the bridges all but impossible. Fortifications on the inner banks of the river exist for archers and catapults to lease barrages against any foe that dare approach their land. It is this challenge that the enemies of the kingdom try to find a way to overcome.</p>\n<p>It is acknowledged by both the King and his enemies that a surprise attack, one with so little warning that the bridges remain in place, would be successful against what is otherwise a poorly defensible region. Even a force of only moderate size could slaughter anyone within the rivers with ease. With this in mind, the King and his cabinet have a large espionage network that's infiltrated every major kingdom's decision making process. Their spies should, and have many times in the past, notified the King long before any attack, and allow for defenses to be raised, and victory to be assured. The King is very happy with his spies. They've never once failed to bring advance notice of any attack, and his network of informants have proven themselves resilient against counter-infiltration. He is, however, a very paranoid king, and wishes there was some way to be even more certain of his kingdom's safety. He is, as he sits upon his throne, ruminating on some such plans when a man of small stature is brought before him by some guards. The little man is wearing mostly simple clothes, but with some vibrant accents in the trimming.</p>\n<p>\"Why have you brought this fellow before me?\" Asked the King of his guards.</p>\n<p>\"He claims to have word of an attack on the kingdom, sire.\" A guard said.</p>\n<p>\"He seems believable enough, sire, that we thought it best to bring him before you instead of merely dismissing him. You have had more training in detecting the truth of matters.\" The second guard said.</p>\n<p>\"Very well.\" He said, gesturing for the guards to relax. \"Sir, may I have your name?\" The King spoke directly to his small guest.</p>\n<p>\"Orin Eldirh, my king.\" He barely manages to say as he stammers on, \"I've been told by a f-friend... a very close friend in-indeed... a t-t-trustworthy sort of fellow, you know... the kind who'd <em>n-never</em> lie, you see... And he says, and he's the employee of a very well off member of the Northern Kingdom's leadership, s-so I trust this information is accurate... He says th-that his boss was part of a meeting to plan a surprise attack on our kingdom. And very soon, I might add. He said the meeting was a pre-planned sort of thing, was going to be on a random day, so our spies wouldn't have time to figure things out, and that they'd have an army ready in less than a day! <em>A day</em>, sire! They're surely marching here now, as I speak.\"</p>\n<p>The king quietly held the man's gaze for several moments before speaking. \"And, holding what you've said <em>is</em> what you've heard, why would a noble betray his kingdom by speaking of such a secret meeting?\"</p>\n<p>\"Is that important, sire? We have such little time to prepare for the invasion.\" Orin says. \"Surely what I've said is enough to warrant removing the bridges, whatever his motivations.\" He paused uncertainly as he looked upon his king. \"Isn't it?\"</p>\n<p>The King heaved a sigh before responding, \"No. It really isn't.\" The King began to elaborate, \"You see, removing those bridges cost more than you may realize. It takes every able man in the kingdom to work as fast as you claim we need to. That's an entire day's worth of labor used up. With the bridges up, that's maybe a weeks worth of trade and messages that wont be coming or going, seeing as the men wont work themselves so hard for two days in a row to put things back. What you personally lose may well be small, but it will make our kingdom and its stores suffer.\"</p>\n<p>\"But what are those costs to the lives of those people, those women, those <em>children</em>, lost to an attack?\" Orin admonished.</p>\n<p>\"There is more at work here than you think, Orin.\" The King firmly answered, \"You do not know how much thought I have put into the defense of my people.\" Orin's outrage slowly began diminishing as he took a sheepish stature. \"Imagine, if you will, that I heed the word of every beggar and peasant who claimed some terrible force was underway. It's a much more common experience than you seem to think. Not a week goes by without someone offering their wisdom of an attack that my spies have&nbsp;<em>somehow</em>&nbsp;missed hearing of. The people of the kingdom would spend more time cowering in fear of an impending attack than doing anything else if I listened to every such piece of obvious paranoia or subterfuge. My people would tire of removing the bridges. Traders would tire of so frequent delays in their travels. It would spell our eventual doom, I'm sure of it.\" The King took a deep breathe and frowned, calmly continuing, \"And yet, how could I ever forgive myself if I left us undefended from a legitimate attack? My spies are not perfect. Such a random meeting as you described may elude them, if we were unlucky and it was well implemented.\" The King pinched the bridge of his nose and closed his eyes before continuing, \"I have to determine, to the best of my abilities, whether or not this threat is legitimate. So I'll ask you again, as I <em>must</em> know, why might this noble betray his kingdom?\"</p>\n<p>Orin swallowed and said, \"If what my friend says is to be believed, and I consider it so, then this noble is not motivated by loyalty for his kingdom. I was told that he was not born into his position, but bought it himself. He has quite a fortune from his ownership of many kinds of businesses and guilds. War hurts him more than it helps the businesses of his kingdom, I've been told he believes. He'd wish to avoid starting any kind of fight, I'd think, if this were true.\"</p>\n<p>\"I know of a man of the Northern Kingdom who fits that description. It's possible, not likely, but <em>possible</em>, he's heard things our spies have not.\" The King said, \"And that he might also decide to warn us if he heard such a thing. But there's still the matter of *your* trustworthiness. How should I know that you are not a lying spy, sent form the North to deceive?\"</p>\n<p>Orin's eyes grew wide with fear as he attempted to speak, \"ple-please, s-s-sire, I w-would n-n-never be-betray my kingdom!\"</p>\n<p>\"So a spy would say.\" Orin opened his mouth to protest but the King interrupted \"No, nothing you can<em> say</em> will persuade me you aren't just a well trained spy.\" The King smiled, \"But I have been giving thought to how I may judge your information's usefulness. You are an artisan of some skill, right? You're better dressed than a peasant can afford.\"</p>\n<p>Orin spoke \"A potter, sir.\" After a pause he then bashfully admits, \"Of kinds both functional and beautiful, as I've been told by my more affluent clients.\"</p>\n<p>The King smiled wider, \"Then you are well off, yes? How much would you consider your current wares and savings worth?\"</p>\n<p>Nervous about the King's sudden eagerness, Orin hesitantly replies, \"700 coins, but p-perhaps even 800 c-coins... If I sold my s-shop and everything w-within.\"</p>\n<p>\"Very well. I propose a wager. 20 to 1 against this invasion being real.\" The king laughed as he saw Orin's shocked face. \"What, surprised your King is a betting man? If you'd like to convince me you're not lying, then put your money where your mouth is, I say. If you'd <em>also</em>&nbsp;like to convince me you're right about this, you'll have to bet <em>big</em>. If you're willing to put up 1000 coins I'm willing to call for the removal of the bridges.\" Orin just stood there silently, jaw agape. The King continued speaking, \"That's 200 coins of debt if you're wrong about this, a lifetime of payments for someone of your skills. If you're right, however, you will be rewarded handsomely. 20,000 coins is enough to keep you from working the rest of your life, if you'd like. I think that's fair compensation for saving the kingdom.\"</p>\n<p>The King just smiled as he waited for Orin to speak.</p>\n<p>Orin remained silent as he fervently thought about his options. He swallowed several times and wrung his hands together. After several minutes of silence he took a breath and spoke, \"I'll take it.\" The King's smile grew as Orin spoke, \"I'll take the bet. After considering his trustworthiness, it seems like my friend is right. I am willing to risk myself for his word. I am not willing to risk my kingdom.\"</p>\n<p>\"Very well.\" The King said before looking towards the two men standing to Orin's side, \"Guards, one of you notify the city that an attack is impending. We have 2 days at most before the Northern Kingdom is here.\" The left one nods at once and left the chamber. \"Orin, I hope you understand why you should stay here for the night. We can't have you running off.\" Orin nodded stiffly in understanding. Looking at the remaining guard, the King said, \"Orin here is your responsibility. Keep him occupied and within the castle until you have my word to release him. You may send out someone to notify his family of the circumstances surrounding his stay. They are welcome to come visit as soon as their duties for preparation are complete.\" After a short thought he said, \"Orin is a guest here, not a prisoner, so treat him as such.\"</p>\n<p>Standing up from his throne, the King walked over to where Orin was standing, petrified by what was happening around him. The King, towering over the small man, said, \"If you are right about this, I am incredibly grateful that you came to me.\" The King reached out and grabbed Orin's shoulder, looking into his eyes with his own, and smiled wide. He then released him and returned to his seat. \"You may go, I'm soon to be swamped by my bureaucracy for the coming hours as we prepare for this fight.\" Orin and his escort made their way from the room. As the door closed behind them another one opened as several official looking men rushed in, chatting loudly. The King straightened his stature and forced a smile as he prepared himself for dealing with his government for the next several days.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E8PHMuf7tsr8teXAe": 1, "etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DT9mqFeWocnXiqt9L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 37, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "26368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-12T00:10:51.194Z", "modifiedAt": null, "url": null, "title": "The physiology of fun?", "slug": "the-physiology-of-fun", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2kBMJWYe6pd6axh3J/the-physiology-of-fun", "pageUrlRelative": "/posts/2kBMJWYe6pd6axh3J/the-physiology-of-fun", "linkUrl": "https://www.lesswrong.com/posts/2kBMJWYe6pd6axh3J/the-physiology-of-fun", "postedAtFormatted": "Thursday, June 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20physiology%20of%20fun%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20physiology%20of%20fun%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kBMJWYe6pd6axh3J%2Fthe-physiology-of-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20physiology%20of%20fun%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kBMJWYe6pd6axh3J%2Fthe-physiology-of-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kBMJWYe6pd6axh3J%2Fthe-physiology-of-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p><a href=\"https://www.youtube.com/watch?v=bkeVjkUKwF4\">Ferrets and styrofoam peanuts</a>.</p>\n<p>Some animal species play when young and pretty drop it as adults. Some continue well past maturity. I assume there's a physical basis, but I have no idea what it might be. Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2kBMJWYe6pd6axh3J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.7837006676210708e-06, "legacy": true, "legacyId": "26369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-12T09:05:43.060Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social Meetup (possibly) in the Sun", "slug": "meetup-london-social-meetup-possibly-in-the-sun", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Leonhart", "createdAt": "2010-02-22T20:01:49.792Z", "isAdmin": false, "displayName": "Leonhart"}, "userId": "X5EZEkfccqyWXETHd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eSHp4KESA35KqpT8h/meetup-london-social-meetup-possibly-in-the-sun", "pageUrlRelative": "/posts/eSHp4KESA35KqpT8h/meetup-london-social-meetup-possibly-in-the-sun", "linkUrl": "https://www.lesswrong.com/posts/eSHp4KESA35KqpT8h/meetup-london-social-meetup-possibly-in-the-sun", "postedAtFormatted": "Thursday, June 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20Meetup%20(possibly)%20in%20the%20Sun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20Meetup%20(possibly)%20in%20the%20Sun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSHp4KESA35KqpT8h%2Fmeetup-london-social-meetup-possibly-in-the-sun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20Meetup%20(possibly)%20in%20the%20Sun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSHp4KESA35KqpT8h%2Fmeetup-london-social-meetup-possibly-in-the-sun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSHp4KESA35KqpT8h%2Fmeetup-london-social-meetup-possibly-in-the-sun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11e'>London Social Meetup (possibly) in the Sun</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 June 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Newman's Row, London WC2A 3TL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>EDIT: Weather is not so hot so we'll be starting in the Shakespeare's Head, see below.</strong></p>\n\n<p>We are having another Social Meetup on Sunday at 2 PM.\nThe meetup will take place at Lincoln's Inn Fields (near Holborn station) and more specifically around <a href=\"https://www.google.co.uk/maps/preview?q=51.516655,-0.11687&amp;source=newuser-ws\" rel=\"nofollow\">this spot</a> in the northwest quadrant. Alternatively, if the weather is bad, we will be at our usual location, which is just around the corner - The <a href=\"https://www.google.com/maps/search/shakespeare&#39;s+head+london/@51.521537,-0.1231445,15z/data=!3m1!4b1\" rel=\"nofollow\">Shakespeare&#39;s Head</a>.\nI'll post an update here and on the <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">mailing list</a> on Sunday as to whether we are going to the Park or the Pub.\nAbout London LessWrong:\nWe run this meetup almost every week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....\nSometimes we play The Resistance or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.\nIf you get lost, feel free to contact me on 07860 466862.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11e'>London Social Meetup (possibly) in the Sun</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eSHp4KESA35KqpT8h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.7844831350108494e-06, "legacy": true, "legacyId": "26371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun\">Discussion article for the meetup : <a href=\"/meetups/11e\">London Social Meetup (possibly) in the Sun</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 June 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Newman's Row, London WC2A 3TL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong id=\"EDIT__Weather_is_not_so_hot_so_we_ll_be_starting_in_the_Shakespeare_s_Head__see_below_\">EDIT: Weather is not so hot so we'll be starting in the Shakespeare's Head, see below.</strong></p>\n\n<p>We are having another Social Meetup on Sunday at 2 PM.\nThe meetup will take place at Lincoln's Inn Fields (near Holborn station) and more specifically around <a href=\"https://www.google.co.uk/maps/preview?q=51.516655,-0.11687&amp;source=newuser-ws\" rel=\"nofollow\">this spot</a> in the northwest quadrant. Alternatively, if the weather is bad, we will be at our usual location, which is just around the corner - The <a href=\"https://www.google.com/maps/search/shakespeare's+head+london/@51.521537,-0.1231445,15z/data=!3m1!4b1\" rel=\"nofollow\">Shakespeare's Head</a>.\nI'll post an update here and on the <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">mailing list</a> on Sunday as to whether we are going to the Park or the Pub.\nAbout London LessWrong:\nWe run this meetup almost every week; these days we tend to get in the region of 5-15 people in attendance. By default, meetups are just unstructured social discussion about whatever strikes our fancy: books we're reading, recent posts on LW/related blogs, logic puzzles, toilet usage statistics....\nSometimes we play The Resistance or other games. We usually finish around 7pm, give or take an hour, but people arrive and leave whenever suits them.\nIf you get lost, feel free to contact me on 07860 466862.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun1\">Discussion article for the meetup : <a href=\"/meetups/11e\">London Social Meetup (possibly) in the Sun</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social Meetup (possibly) in the Sun", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun", "level": 1}, {"title": "EDIT: Weather is not so hot so we'll be starting in the Shakespeare's Head, see below.", "anchor": "EDIT__Weather_is_not_so_hot_so_we_ll_be_starting_in_the_Shakespeare_s_Head__see_below_", "level": 2}, {"title": "Discussion article for the meetup : London Social Meetup (possibly) in the Sun", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__possibly__in_the_Sun1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-12T14:23:04.899Z", "modifiedAt": null, "url": null, "title": "Meetup : July Rationality Dojo: Disagreement", "slug": "meetup-july-rationality-dojo-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:05.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jfzYuYJGJnttRY5Zz/meetup-july-rationality-dojo-disagreement", "pageUrlRelative": "/posts/jfzYuYJGJnttRY5Zz/meetup-july-rationality-dojo-disagreement", "linkUrl": "https://www.lesswrong.com/posts/jfzYuYJGJnttRY5Zz/meetup-july-rationality-dojo-disagreement", "postedAtFormatted": "Thursday, June 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20July%20Rationality%20Dojo%3A%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20July%20Rationality%20Dojo%3A%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjfzYuYJGJnttRY5Zz%2Fmeetup-july-rationality-dojo-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20July%20Rationality%20Dojo%3A%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjfzYuYJGJnttRY5Zz%2Fmeetup-july-rationality-dojo-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjfzYuYJGJnttRY5Zz%2Fmeetup-july-rationality-dojo-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11f'>July Rationality Dojo: Disagreement</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 July 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 King Street, West Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[ATTN: The dojo roster is has all free slots starting from next month, if you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a>]</p>\n\n<p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Continuing the succession of immensely successful dojos, James will run a session on disagreement. How can two epistemic peers, equally knowledgeable and equally competent, ever feel certain about their view when their peer disagrees?</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call me on 0425-855-124.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11f'>July Rationality Dojo: Disagreement</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jfzYuYJGJnttRY5Zz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7849477022953336e-06, "legacy": true, "legacyId": "26372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___July_Rationality_Dojo__Disagreement\">Discussion article for the meetup : <a href=\"/meetups/11f\">July Rationality Dojo: Disagreement</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 July 2014 03:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 King Street, West Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[ATTN: The dojo roster is has all free slots starting from next month, if you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a>]</p>\n\n<p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Continuing the succession of immensely successful dojos, James will run a session on disagreement. How can two epistemic peers, equally knowledgeable and equally competent, ever feel certain about their view when their peer disagrees?</p>\n\n<p>As always, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you have any trouble finding the venue or getting in, call me on 0425-855-124.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___July_Rationality_Dojo__Disagreement1\">Discussion article for the meetup : <a href=\"/meetups/11f\">July Rationality Dojo: Disagreement</a></h2>", "sections": [{"title": "Discussion article for the meetup : July Rationality Dojo: Disagreement", "anchor": "Discussion_article_for_the_meetup___July_Rationality_Dojo__Disagreement", "level": 1}, {"title": "Discussion article for the meetup : July Rationality Dojo: Disagreement", "anchor": "Discussion_article_for_the_meetup___July_Rationality_Dojo__Disagreement1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-13T02:42:59.433Z", "modifiedAt": null, "url": null, "title": "List a few posts in Main and/or Discussion which actually made you change your mind", "slug": "list-a-few-posts-in-main-and-or-discussion-which-actually", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:02.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uJRtQSFkN4krnBwub/list-a-few-posts-in-main-and-or-discussion-which-actually", "pageUrlRelative": "/posts/uJRtQSFkN4krnBwub/list-a-few-posts-in-main-and-or-discussion-which-actually", "linkUrl": "https://www.lesswrong.com/posts/uJRtQSFkN4krnBwub/list-a-few-posts-in-main-and-or-discussion-which-actually", "postedAtFormatted": "Friday, June 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20a%20few%20posts%20in%20Main%20and%2For%20Discussion%20which%20actually%20made%20you%20change%20your%20mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20a%20few%20posts%20in%20Main%20and%2For%20Discussion%20which%20actually%20made%20you%20change%20your%20mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJRtQSFkN4krnBwub%2Flist-a-few-posts-in-main-and-or-discussion-which-actually%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20a%20few%20posts%20in%20Main%20and%2For%20Discussion%20which%20actually%20made%20you%20change%20your%20mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJRtQSFkN4krnBwub%2Flist-a-few-posts-in-main-and-or-discussion-which-actually", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJRtQSFkN4krnBwub%2Flist-a-few-posts-in-main-and-or-discussion-which-actually", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p>To quote the front page&nbsp;</p>\n<p>&gt; Less Wrong users aim to develop accurate predictive models of the world, and change their mind when they find evidence disconfirming those models, instead of being able to explain anything.</p>\n<p>So, by that logic, one interesting metric of the forum quality would be how often what is posted here makes people change their minds. Of course, most of us change their minds almost all the time, but mostly on some mundane topics and in very small amounts, probably too small to pay attention too. But if something comes to mind, feel free to link a thread or two. Depending on the response, we can even try to measure how influential newer posts are vs. older ones.</p>\n<p>EDIT: Feel free to mention the Sequence posts, as well, could be a useful benchmark.</p>\n<p>EDIT2: Why specifically changing your mind and not just learning something new? Because unlearning is much harder than initial learning, and we, to generalize from one example, tend to forget the unlearned and relapsed into old ways of thinking and doing. (Links welcome). Probably because the patterns etched in the System 1 are not easily erased, and just knowing something intellectually does not remove the old habits. So, successfully unlearning something and internalizing a different view or concept or a way of doing things is indicative of a much more significant impact than \"just\" learning something for the first time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3RnEKrsNgNEDxuNnw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uJRtQSFkN4krnBwub", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 24, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "26375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-13T12:31:30.143Z", "modifiedAt": null, "url": null, "title": "Science/rationality subjects to teach", "slug": "science-rationality-subjects-to-teach", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.645Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fKGK6coLoEynwyDni/science-rationality-subjects-to-teach", "pageUrlRelative": "/posts/fKGK6coLoEynwyDni/science-rationality-subjects-to-teach", "linkUrl": "https://www.lesswrong.com/posts/fKGK6coLoEynwyDni/science-rationality-subjects-to-teach", "postedAtFormatted": "Friday, June 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%2Frationality%20subjects%20to%20teach&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%2Frationality%20subjects%20to%20teach%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKGK6coLoEynwyDni%2Fscience-rationality-subjects-to-teach%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%2Frationality%20subjects%20to%20teach%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKGK6coLoEynwyDni%2Fscience-rationality-subjects-to-teach", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKGK6coLoEynwyDni%2Fscience-rationality-subjects-to-teach", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:HyphenationZone>21</w:HyphenationZone> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>NL</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\" \" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:Standaardtabel;\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-qformat:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\n\tmso-para-margin-top:0cm;\n\tmso-para-margin-right:0cm;\n\tmso-para-margin-bottom:10.0pt;\n\tmso-para-margin-left:0cm;\n\tline-height:115%;\n\tmso-pagination:widow-orphan;\n\tfont-size:11.0pt;\n\tfont-family:\"Calibri\",\"sans-serif\";\n\tmso-ascii-font-family:Calibri;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-fareast-font-family:\"Times New Roman\";\n\tmso-fareast-theme-font:minor-fareast;\n\tmso-hansi-font-family:Calibri;\n\tmso-hansi-theme-font:minor-latin;}\n</style>\n<![endif]-->\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">I recently got a great new job. In short, I organize short (3-9hour) courses on pretty much any subject I want and people can pay to take them. If not enough people reserve a spot, the course is cancelled. If this happens to much I get in trouble with my boss.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">This setup gives me a golden opportunity to raise the sanity waterline from the bottom up as most of our customers are ordinary people of the street. I have a few projects I am working on already but would like your input: What are some easy to learn subjects that will get people interested in science/rationality or at least more sane?<br /></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Some things to keep in mind: First, it has to grab their interest or they simply won&rsquo;t come, something that they encounter in their daily lives, something the worry about, etc.</span><br /><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Second, it has to be concrete. Purely theoretical subjects have no success. They come in with clear questions and issues and want them answered, not because of some inherent curiosity about science/rationality. They need to walk out the door feeling they have learned something valuable an useful to them.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">So, thoughts?</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Edit: My first courses are planned for October, I will report on their succes here on LW if the subject was relevant. <br /></span></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fKGK6coLoEynwyDni", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 1.7868946106269468e-06, "legacy": true, "legacyId": "26377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-13T15:46:03.078Z", "modifiedAt": null, "url": null, "title": "Sugar and motivation", "slug": "sugar-and-motivation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cfjMpCK3i64kRT6y9/sugar-and-motivation", "pageUrlRelative": "/posts/cfjMpCK3i64kRT6y9/sugar-and-motivation", "linkUrl": "https://www.lesswrong.com/posts/cfjMpCK3i64kRT6y9/sugar-and-motivation", "postedAtFormatted": "Friday, June 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sugar%20and%20motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASugar%20and%20motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfjMpCK3i64kRT6y9%2Fsugar-and-motivation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sugar%20and%20motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfjMpCK3i64kRT6y9%2Fsugar-and-motivation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfjMpCK3i64kRT6y9%2Fsugar-and-motivation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>I reread&nbsp; <a href=\"/lw/1lb/are_wireheads_happy/\">Are Wireheads Happy?</a>[1], which is about the difference between liking and wanting.</p>\n<p>I've noticed that too much simple carbs (not terribly much-- three twenty ounce Cokes on consecutive days will do it) severely damages my desire to do much of anything, and the effects take approximately two low-simple carbs days to clear out.</p>\n<p>This is obviously something physiological, even though it looks like an emotional problem. Failing to remember clue to avoid simple carbs (a dessert once a week might not be a problem) might be an emotional issue. Too much sugar used to lead to an internal voice saying \"I don't care\". This time, it didn't, but if I thought of something I might do, there was a clear feeling of \"no reward there\" and a sense that it was too much effort. I was capable of enjoying things, but not of anticipating that I would like them.</p>\n<p>I'm wondering if anything is known about simple carbs, motivation, and/or serotonin/dopamine.</p>\n<p>[1]Recommended <a href=\"/r/discussion/lw/kcn/list_a_few_posts_in_main_andor_discussion_which/azxx\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cfjMpCK3i64kRT6y9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.7871800532551856e-06, "legacy": true, "legacyId": "26378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HmfxSWnqnK265GEFM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-13T15:49:32.205Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Bangalore", "slug": "new-lw-meetup-bangalore-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n6AZrpZ8tpTg6P2wk/new-lw-meetup-bangalore-0", "pageUrlRelative": "/posts/n6AZrpZ8tpTg6P2wk/new-lw-meetup-bangalore-0", "linkUrl": "https://www.lesswrong.com/posts/n6AZrpZ8tpTg6P2wk/new-lw-meetup-bangalore-0", "postedAtFormatted": "Friday, June 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Bangalore&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Bangalore%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn6AZrpZ8tpTg6P2wk%2Fnew-lw-meetup-bangalore-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Bangalore%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn6AZrpZ8tpTg6P2wk%2Fnew-lw-meetup-bangalore-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn6AZrpZ8tpTg6P2wk%2Fnew-lw-meetup-bangalore-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 567, "htmlBody": "<p><strong>This summary was posted to LW Main on June 6th. The following week's summary is <a href=\"/lw/kcr/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/10y\">Bangalore meetup:&nbsp;<span class=\"date\">29 June 2014 04:40PM</span></a></li>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/114\">Atlanta June meetup - Hacking Motivation:&nbsp;<span class=\"date\">08 June 2014 07:03PM</span></a></li>\n<li><a href=\"/meetups/10z\">Atlanta June meetup - Hacking Motivation:&nbsp;<span class=\"date\">08 June 2014 10:34PM</span></a></li>\n<li><a href=\"/meetups/10u\">Christchurch, NZ Meetup - Games &amp; Discussion:&nbsp;<span class=\"date\">01 June 2014 04:30PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">14 June 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/10v\">Moscow meet up:&nbsp;<span class=\"date\">08 June 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/110\">Southeast Michigan Meetup 6/8:&nbsp;<span class=\"date\">08 June 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">06 June 2025 01:30PM</span></a></li>\n<li><a href=\"/meetups/111\">Brussels - Neuroatypicality:&nbsp;<span class=\"date\">14 June 2014 07:40PM</span></a></li>\n<li><a href=\"/meetups/10p\">Canberra: Decision Theory:&nbsp;<span class=\"date\">14 June 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/116\">London Social Meetup (possibly) in the Sun:&nbsp;<span class=\"date\">08 June 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/10g\">Sydney Social Meetup - June (Games night):&nbsp;<span class=\"date\">12 June 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/10l\">Sydney Meetup - June:&nbsp;<span class=\"date\">25 June 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/115\">West LA:&nbsp;<span class=\"date\">11 June 2025 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n6AZrpZ8tpTg6P2wk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.7871851678027311e-06, "legacy": true, "legacyId": "26339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XEFKqdWjxbXC7u7TZ", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-14T03:43:14.195Z", "modifiedAt": null, "url": null, "title": "What resources have increasing marginal utility?", "slug": "what-resources-have-increasing-marginal-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.195Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YQtziXj9hvib6bvXu/what-resources-have-increasing-marginal-utility", "pageUrlRelative": "/posts/YQtziXj9hvib6bvXu/what-resources-have-increasing-marginal-utility", "linkUrl": "https://www.lesswrong.com/posts/YQtziXj9hvib6bvXu/what-resources-have-increasing-marginal-utility", "postedAtFormatted": "Saturday, June 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20resources%20have%20increasing%20marginal%20utility%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20resources%20have%20increasing%20marginal%20utility%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQtziXj9hvib6bvXu%2Fwhat-resources-have-increasing-marginal-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20resources%20have%20increasing%20marginal%20utility%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQtziXj9hvib6bvXu%2Fwhat-resources-have-increasing-marginal-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQtziXj9hvib6bvXu%2Fwhat-resources-have-increasing-marginal-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>Most resources you might think to amass have decreasing marginal utility: for example, a marginal extra $1,000 means much more to you if you have $0 than if you have $100,000. That means you can safely apply the 80-20 rule to most resources: you only need to get some of the resource to get most of the benefits of having it.</p>\n<p>At the most recent CFAR workshop, Val dedicated a class to arguing that one resource in particular has <strong>increasing</strong>&nbsp;marginal utility, namely <strong>attention</strong>. Initially, efforts to free up your attention have little effect: the difference between juggling 10 things and 9 things is pretty small. But once you've freed up most of your attention, the effect is larger: the difference between juggling 2 things and 1 thing is huge. Val also argued that because of this funny property of attention, most people likely undervalue the value of freeing up attention by orders of magnitude.</p>\n<p>During a conversation later in the workshop I suggested another resource that might have increasing marginal utility, namely <strong>trust</strong>. A society where people abide by contracts 80% of the time is not 80% as good as a society where people abide by contracts 100% of the time; most of the societal value of trust (e.g. decreasing transaction costs) doesn't seem to manifest until people are pretty close to 100% trustworthy. The analogous way to undervalue trust is to argue that e.g. cheating on your spouse is not so bad, because only one person gets hurt. But cheating on spouses in general undermines the trust that spouses should have in each other, and the cumulative impact of even 1% of spouses cheating on the institution of marriage as a whole could be quite negative. (Lots of things about the world make more sense from this perspective: for example, it seems like one of the main practical benefits of religion is that it fosters trust.)&nbsp;</p>\n<p>What other resources have increasing marginal utility? How undervalued are they?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "XSeiautCrZGaQ78fx": 1, "KN9KEMgyBHjcAyc26": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YQtziXj9hvib6bvXu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 59, "extendedScore": null, "score": 0.000175, "legacy": true, "legacyId": "26380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-14T08:31:50.695Z", "modifiedAt": null, "url": null, "title": "[tangential] Bitcoin: GHash just hit 51%", "slug": "tangential-bitcoin-ghash-just-hit-51", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kQrLZEByrQZacv53R/tangential-bitcoin-ghash-just-hit-51", "pageUrlRelative": "/posts/kQrLZEByrQZacv53R/tangential-bitcoin-ghash-just-hit-51", "linkUrl": "https://www.lesswrong.com/posts/kQrLZEByrQZacv53R/tangential-bitcoin-ghash-just-hit-51", "postedAtFormatted": "Saturday, June 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Btangential%5D%20Bitcoin%3A%20GHash%20just%20hit%2051%25&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Btangential%5D%20Bitcoin%3A%20GHash%20just%20hit%2051%25%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkQrLZEByrQZacv53R%2Ftangential-bitcoin-ghash-just-hit-51%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Btangential%5D%20Bitcoin%3A%20GHash%20just%20hit%2051%25%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkQrLZEByrQZacv53R%2Ftangential-bitcoin-ghash-just-hit-51", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkQrLZEByrQZacv53R%2Ftangential-bitcoin-ghash-just-hit-51", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>And apparently <a href=\"http://hackingdistributed.com/2014/06/13/time-for-a-hard-bitcoin-fork/\">the sky is falling</a>. From Ittay Eyal and Emin G&uuml;n Sirer at Hacking, Distributed:</p>\n<p style=\"padding-left: 30px;\">But the fact is, this is a monumental event. The Bitcoin narrative, based on decentralization and distributed trust, is no more. True, the Bitcoin economy is about as healthy as it was yesterday, and the Bitcoin price will likely remain afloat for quite a while. But the Bitcoin economy and price are trailing indicators. The core pillar of the Bitcoin value equation has collapsed.</p>\n<p>They note <a href=\"https://bitcointalk.org/index.php?topic=327767.0\">previous bad behaviour</a> from GHash (which GHash attributed to a rogue employee).</p>\n<p>Their proposal is a hard fork, with different parameters (to make huge mining pools no longer an economically rational choice), but respecting the blockchain to date so they can reasonably keep calling it \"Bitcoin\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kQrLZEByrQZacv53R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "26382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-14T15:07:20.884Z", "modifiedAt": null, "url": null, "title": "Turing Test and Machine Intelligence", "slug": "turing-test-and-machine-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.936Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xu3LvBz62a5z28mzH/turing-test-and-machine-intelligence", "pageUrlRelative": "/posts/Xu3LvBz62a5z28mzH/turing-test-and-machine-intelligence", "linkUrl": "https://www.lesswrong.com/posts/Xu3LvBz62a5z28mzH/turing-test-and-machine-intelligence", "postedAtFormatted": "Saturday, June 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Turing%20Test%20and%20Machine%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATuring%20Test%20and%20Machine%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXu3LvBz62a5z28mzH%2Fturing-test-and-machine-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Turing%20Test%20and%20Machine%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXu3LvBz62a5z28mzH%2Fturing-test-and-machine-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXu3LvBz62a5z28mzH%2Fturing-test-and-machine-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 493, "htmlBody": "<p>The recent news of Eugene Goostman passing the turing test has raised all kinds of debate around what counts as passing the turing tests and whether or not the chatbots are attempting to do it properly.</p>\n<p><strong>The Turing Test is not a good criterion for machine intelligence.</strong></p>\n<p>The computer's capability to trick humans via imitation in a conversation is an amazing prediction. I think it's important to discern what it means.</p>\n<p>For a machine to be able to imitate human language and <em>reasoning</em> succesfully it would essentially need to have intelligence much above the average human. A general intelligence <em>not specifically designed to be a fake human</em> would need to be able to model human behavior and derive from that model communication that was misleading from the AI's \"true nature\".</p>\n<p>Computers supremacy over humans in the boardgame of chess has been a common motif in AI discussion since Kasparov lost to Deep Blue in the 90's. Yet no one is trying to claim that the chesscalculators would have learned to play chess in a similar fashion to humans and would rely on a similar logic as we do. I'm not an expert on programming, AI nor chess, but it stills seems obvious that it would be improper to use thecomputers' current superiority to humans as a solid proof of their high general intelligence - one that is capable of <em>imitating humans and play chess like humans do</em>.</p>\n<p><strong>Goal structure for deception vs. Crafted set of tricks and \"repeat after me\"<br /></strong></p>\n<p>For an AI to <em>truly</em> participate in the Turing Test it would need to be self-aware. In addition to being self-aware a goalstructure would be required, and that should include <em>incentive to deceive humans to think that the AI is a human too</em>. More specifically cognitively pretending not to be you, would require self-awareness. This would be very sophisticated and subtle. It's hard for many humans to pretend being someone else - though some excel at it -&nbsp; despite us having a built-in capacity for empathy and already having nearly identical brains. To do the same with an internal \"mental\" structure that might not be anything like ours would in my opinion require an intelligence on level above the average human or a designed set of tricks.</p>\n<p>Are the \"Artificial Intelligences\" that attempt to pass the Turing Test intelligent at all? To me it seems that the chatbots are essentially one-trick-ponies that merely \"repeat after me\". Somebody carefully designs an automated way of picking words that tricks the average joe by avoiding conversation or interaction of substance. Computer's vast capacity for storage and recall make them good for memorizing a lot of tricks.</p>\n<p>What is actually being done in the Turing Test is not a measurement of intelligence. It is an attempt to find an automated means for tricking a human to think that they're talking to someone else, which does not require an intelligent agent. This seems similar to having a really convincing answering machine for your telephone.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xu3LvBz62a5z28mzH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -8, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "26383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>The recent news of Eugene Goostman passing the turing test has raised all kinds of debate around what counts as passing the turing tests and whether or not the chatbots are attempting to do it properly.</p>\n<p><strong id=\"The_Turing_Test_is_not_a_good_criterion_for_machine_intelligence_\">The Turing Test is not a good criterion for machine intelligence.</strong></p>\n<p>The computer's capability to trick humans via imitation in a conversation is an amazing prediction. I think it's important to discern what it means.</p>\n<p>For a machine to be able to imitate human language and <em>reasoning</em> succesfully it would essentially need to have intelligence much above the average human. A general intelligence <em>not specifically designed to be a fake human</em> would need to be able to model human behavior and derive from that model communication that was misleading from the AI's \"true nature\".</p>\n<p>Computers supremacy over humans in the boardgame of chess has been a common motif in AI discussion since Kasparov lost to Deep Blue in the 90's. Yet no one is trying to claim that the chesscalculators would have learned to play chess in a similar fashion to humans and would rely on a similar logic as we do. I'm not an expert on programming, AI nor chess, but it stills seems obvious that it would be improper to use thecomputers' current superiority to humans as a solid proof of their high general intelligence - one that is capable of <em>imitating humans and play chess like humans do</em>.</p>\n<p><strong id=\"Goal_structure_for_deception_vs__Crafted_set_of_tricks_and__repeat_after_me_\">Goal structure for deception vs. Crafted set of tricks and \"repeat after me\"<br></strong></p>\n<p>For an AI to <em>truly</em> participate in the Turing Test it would need to be self-aware. In addition to being self-aware a goalstructure would be required, and that should include <em>incentive to deceive humans to think that the AI is a human too</em>. More specifically cognitively pretending not to be you, would require self-awareness. This would be very sophisticated and subtle. It's hard for many humans to pretend being someone else - though some excel at it -&nbsp; despite us having a built-in capacity for empathy and already having nearly identical brains. To do the same with an internal \"mental\" structure that might not be anything like ours would in my opinion require an intelligence on level above the average human or a designed set of tricks.</p>\n<p>Are the \"Artificial Intelligences\" that attempt to pass the Turing Test intelligent at all? To me it seems that the chatbots are essentially one-trick-ponies that merely \"repeat after me\". Somebody carefully designs an automated way of picking words that tricks the average joe by avoiding conversation or interaction of substance. Computer's vast capacity for storage and recall make them good for memorizing a lot of tricks.</p>\n<p>What is actually being done in the Turing Test is not a measurement of intelligence. It is an attempt to find an automated means for tricking a human to think that they're talking to someone else, which does not require an intelligent agent. This seems similar to having a really convincing answering machine for your telephone.</p>", "sections": [{"title": "The Turing Test is not a good criterion for machine intelligence.", "anchor": "The_Turing_Test_is_not_a_good_criterion_for_machine_intelligence_", "level": 1}, {"title": "Goal structure for deception vs. Crafted set of tricks and \"repeat after me\"", "anchor": "Goal_structure_for_deception_vs__Crafted_set_of_tricks_and__repeat_after_me_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-14T23:00:08.492Z", "modifiedAt": null, "url": null, "title": "New organization - Future of Life Institute (FLI)", "slug": "new-organization-future-of-life-institute-fli", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:59.496Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DjypfkJoaWeNpvrA9/new-organization-future-of-life-institute-fli", "pageUrlRelative": "/posts/DjypfkJoaWeNpvrA9/new-organization-future-of-life-institute-fli", "linkUrl": "https://www.lesswrong.com/posts/DjypfkJoaWeNpvrA9/new-organization-future-of-life-institute-fli", "postedAtFormatted": "Saturday, June 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20organization%20-%20Future%20of%20Life%20Institute%20(FLI)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20organization%20-%20Future%20of%20Life%20Institute%20(FLI)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjypfkJoaWeNpvrA9%2Fnew-organization-future-of-life-institute-fli%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20organization%20-%20Future%20of%20Life%20Institute%20(FLI)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjypfkJoaWeNpvrA9%2Fnew-organization-future-of-life-institute-fli", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjypfkJoaWeNpvrA9%2Fnew-organization-future-of-life-institute-fli", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 403, "htmlBody": "<p>As of May 2014, there is an existential risk research and outreach organization based in the Boston area. The <a href=\"http://www.thefutureoflife.org\">Future of Life Institute (FLI)</a>, spearheaded by Max Tegmark, was co-founded by Jaan Tallinn, Meia Chita-Tegmark, Anthony Aguirre and myself.</p>\n<p>Our idea was to create a hub on the US East Coast to bring together people who care about x-risk and the future of life. FLI is currently run entirely by volunteers, and is based on brainstorming meetings where the members come together and discuss active and potential projects. The attendees are a mix of local scientists, researchers and rationalists, which results in a diversity of skills and ideas. We also hold more narrowly focused meetings where smaller groups work on specific projects. We have projects in the pipeline ranging from improving Wikipedia resources related to x-risk, to bringing together AI researchers in order to develop safety guidelines and make the topic of AI safety more mainstream.</p>\n<p>Max has assembled an impressive advisory board that includes Stuart Russell, George Church and Stephen Hawking. The advisory board is not just for prestige - the local members attend our meetings, and some others participate in our projects remotely. We consider ourselves a sister organization to FHI, CSER and MIRI, and touch base with them often.</p>\n<p>We recently held our launch event, a panel discussion \"The Future of Technology: Benefits and Risks\" at MIT. The panelists were synthetic biologist George Church, geneticist Ting Wu, economist Andrew McAfee, physicist and Nobel laureate Frank Wilczek and Skype co-founder Jaan Tallinn. The discussion covered a broad range of topics from the future of bioengineering and personal genetics, to autonomous weapons, AI ethics and the Singularity. A <a href=\"http://techtv.mit.edu/videos/29155-the-future-of-technology-benefits-and-risks\">video</a> and <a href=\"http://www.thefutureoflife.org/data/documents/future%20of%20technology%20panel%20transcript.pdf\">transcript</a>&nbsp;are available.</p>\n<p>FLI is a grassroots organization that thrives on contributions from awesome people like the LW community - here are some ways you can help:</p>\n<ul>\n<li>If you have ideas for research or outreach we could be doing, or improvements to what we're already doing, please let us know (in the comments to this post, or by contacting me directly).</li>\n<li>If you are in the vicinity of the Boston area and are interested in getting involved, you are especially encouraged to get in touch with us!</li>\n<li>Support in the form of donations is much appreciated. (We are grateful for seed funding provided by Jaan Tallinn and Matt Wage.)</li>\n</ul>\n<div>More details on the ideas behind FLI can be found in <a href=\"http://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/\">this article</a>.&nbsp;</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 1, "izp6eeJJEg9v5zcur": 1, "Z6DgiCrMtpSNxwuYW": 1, "CL9NePP9FejkQo6jn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DjypfkJoaWeNpvrA9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 69, "extendedScore": null, "score": 0.0006461284158107938, "legacy": true, "legacyId": "26374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-15T02:43:50.049Z", "modifiedAt": null, "url": null, "title": "Total Utility is Illusionary", "slug": "total-utility-is-illusionary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PlatypusNinja", "createdAt": "2009-08-28T22:59:30.512Z", "isAdmin": false, "displayName": "PlatypusNinja"}, "userId": "DWtG6QmXCsCmiD3xd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gWeQWMmKzTxJm9XBy/total-utility-is-illusionary", "pageUrlRelative": "/posts/gWeQWMmKzTxJm9XBy/total-utility-is-illusionary", "linkUrl": "https://www.lesswrong.com/posts/gWeQWMmKzTxJm9XBy/total-utility-is-illusionary", "postedAtFormatted": "Sunday, June 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Total%20Utility%20is%20Illusionary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATotal%20Utility%20is%20Illusionary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWeQWMmKzTxJm9XBy%2Ftotal-utility-is-illusionary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Total%20Utility%20is%20Illusionary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWeQWMmKzTxJm9XBy%2Ftotal-utility-is-illusionary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWeQWMmKzTxJm9XBy%2Ftotal-utility-is-illusionary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 954, "htmlBody": "<p><em>(Abstract: We have the notion that people can have a \"total utility\" value, defined perhaps as the sum of all their changes in utility over time. &nbsp;This is usually not a useful concept, because utility functions can change. &nbsp;In many cases the less-confusing approach is to look only at the utility from each individual decision, and not attempt to consider the total over time. &nbsp;This leads to insights about utilitarianism.)</em></p>\n<p>&nbsp;</p>\n<p>Let's consider the utility of a fellow named Bob. &nbsp;Bob likes to track his total utility; he writes it down in a logbook every night.</p>\n<p>Bob is a stamp collector; he gets +1 utilon every time he adds a stamp to his collection, and he gets -1 utilon every time he removes a stamp from his collection. &nbsp;Bob's utility was zero when his collection was empty, so we can say that Bob's total utility is the number of stamps in his collection.</p>\n<p>One day a movie theater opens, and Bob learns that he likes going to movies. &nbsp;Bob counts +10 utilons every time he sees a movie. Now we can say that Bob's total utility is the number of stamps in his collection, plus ten times the number of movies he has seen.</p>\n<p>(A note on terminology: I'm saying that Bob's <em>utility function</em> is the thing that emits +1 or -1 or +10, and his <em>total utility</em> is the sum of all those emits over time. &nbsp;I'm not sure if this is standard terminology.)</p>\n<p>This should strike us as a little bit strange: Bob now has a term in his total utility which is mostly based on history, and mostly independent of the present state of the world. &nbsp;Technically, we might handwave and say that Bob places value on his <em>memories </em>of watching those movies. &nbsp;But Bob knows that's not actually true: it's the act of watching the movies that he enjoys, and he rarely thinks about them once they're over.</p>\n<p>If a hypnotist convinced Bob that he had watched ten billion movies, Bob would write down in his logbook that he had a hundred billion utilons. &nbsp;(Plus the number of stamps in his stamp collection.)</p>\n<p>Let's talk some more about that stamp collection. Bob wakes up on June 14 and decides that he doesn't like stamps any more. Now, Bob gets -1 utilon every time he adds a stamp to his collection, and +1 utilon every time he removes one. &nbsp;What can we say about his total utility? &nbsp;We might say that Bob's total utility is the number of stamps in his collection at the start of June 14, plus ten times the number of movies he's watched, plus the number of stamps he removed from his collection after June 14. &nbsp;Or we might say that all Bob's utility from his stamp collection prior to June 14 was false utility, and we should strike it from the record books. Which answer is better?</p>\n<p>...Really, neither answer is better, because the \"total utility\" number we're discussing just isn't very useful. &nbsp;Bob has a very clear utility function which emits numbers like +1 and +10 and -1; he doesn't gain anything by keeping track of the total separately. His total utility doesn't seem to track how happy he actually feels, either. &nbsp;It's not clear what Bob gains from thinking about this total utility number.</p>\n<p>&nbsp;</p>\n<p>I think some of the confusion might be coming from Less Wrong's focus on AI design.</p>\n<p>When you're writing a utility function for an AI, one thing you might try is to specify your utility function by specifying the total utility first: you might say \"your total utility is the number of balls you have placed in this bucket\" and then let the AI work out the implementation details of how happy each individual action makes it.</p>\n<p>However, if you're looking at utility functions for actual people, you might encounter something weird like \"I get +10 utility every time I watch a movie\", or \"I woke up today and my utility function changed\", and then if you try to compute the total utility for that person, you can get confused.</p>\n<p>&nbsp;</p>\n<p>Let's now talk about utilitarianism. &nbsp;For simplicity, let's assume we're talking about a utilitarian government which is making decisions on behalf of its constituency. &nbsp;(In other words, we're not talking about utilitarianism as a moral theory.)</p>\n<p>We have the notion of <em>total utilitarianism</em>, in which the government tries to maximize the sum of the utility values of each of its constituents. &nbsp;This leads to \"<a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">repugnant conclusion</a>\" issues in which the government generates new constituents at a high rate until all of them are miserable.</p>\n<p>We also have the notion of <em>average utilitarianism</em>, in which the government tries to maximize the average of the utility values of each of its constituents. &nbsp;This leads to issues -- I'm not sure if there's a snappy name -- where the government tries to kill off the least happy constituents so as to bring the average up.</p>\n<p>The problem with both of these notions is that they're taking the notion of \"total utility of all constituents\" as an input, and then they're changing the number of constituents, which changes the underlying utility function.</p>\n<p>I think the right way to do utilitarianism is to ignore the \"total utility\" thing; that's not a real number anyway. &nbsp;Instead, every time you arrive at a decision point, evaluate what action to take by checking the utility of your constituents from each action. &nbsp;I propose that we call this \"delta utilitarianism\", because it isn't looking at the total or the average, just at the delta in utility from each action.</p>\n<p>This solves the \"repugnant conclusion\" issue because, at the time when you're considering adding more people, it's more clear that you're considering the utility <em>of your constituents at that time</em>, which does not include the potential new people.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gWeQWMmKzTxJm9XBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26384", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-15T13:26:10.079Z", "modifiedAt": null, "url": null, "title": "June Monthly Bragging Thread", "slug": "june-monthly-bragging-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:09.371Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "elharo", "createdAt": "2012-12-28T14:11:02.335Z", "isAdmin": false, "displayName": "elharo"}, "userId": "cgJcCeZhdRnGtwMMR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bojke9JYNMTyam6ch/june-monthly-bragging-thread", "pageUrlRelative": "/posts/Bojke9JYNMTyam6ch/june-monthly-bragging-thread", "linkUrl": "https://www.lesswrong.com/posts/Bojke9JYNMTyam6ch/june-monthly-bragging-thread", "postedAtFormatted": "Sunday, June 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20June%20Monthly%20Bragging%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJune%20Monthly%20Bragging%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBojke9JYNMTyam6ch%2Fjune-monthly-bragging-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=June%20Monthly%20Bragging%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBojke9JYNMTyam6ch%2Fjune-monthly-bragging-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBojke9JYNMTyam6ch%2Fjune-monthly-bragging-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<div id=\"entry_t3_k6i\" class=\"content clear\">\n<div class=\"md\">\n<div>Your job, should you choose to accept it, is to comment on this thread explaining <strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself <em>the coolest freaking person ever</em> because of that awesome thing you're dying to tell everyone about. This is the place to do just that.\n<div>\n<p>Remember, however, that this <strong>isn't</strong> any kind of progress thread. Nor is it any kind of proposal thread. <em>This thread is solely for people to talk about the awesome things they have done. Not \"will do\". Not \"are working on\"</em>. <strong>Have already done.</strong> This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p>So, what's the coolest thing you've done this month?</p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bojke9JYNMTyam6ch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 1.7912089448186446e-06, "legacy": true, "legacyId": "26386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-15T18:29:01.962Z", "modifiedAt": null, "url": null, "title": "Willpower Depletion vs Willpower Distraction", "slug": "willpower-depletion-vs-willpower-distraction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:37.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XKfQF73YnyMRiRf9a/willpower-depletion-vs-willpower-distraction", "pageUrlRelative": "/posts/XKfQF73YnyMRiRf9a/willpower-depletion-vs-willpower-distraction", "linkUrl": "https://www.lesswrong.com/posts/XKfQF73YnyMRiRf9a/willpower-depletion-vs-willpower-distraction", "postedAtFormatted": "Sunday, June 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Willpower%20Depletion%20vs%20Willpower%20Distraction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWillpower%20Depletion%20vs%20Willpower%20Distraction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKfQF73YnyMRiRf9a%2Fwillpower-depletion-vs-willpower-distraction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Willpower%20Depletion%20vs%20Willpower%20Distraction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKfQF73YnyMRiRf9a%2Fwillpower-depletion-vs-willpower-distraction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKfQF73YnyMRiRf9a%2Fwillpower-depletion-vs-willpower-distraction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 725, "htmlBody": "<p>I once asked a room full of about 100 neuroscientists whether willpower depletion was a thing, and there was widespread disagreement with the idea. (A propos, this is a great way to quickly gauge consensus in a field.) Basically, for a while some researchers believed that willpower depletion \"is\" glucose depletion in the prefrontal cortex, but some more recent experiments have failed to replicate this, e.g. by finding that the mere taste of sugar is enough to \"replenish\" willpower faster than the time it takes blood to move from the mouth to the brain:</p>\n<blockquote>Carbohydrate mouth-rinses activate dopaminergic pathways in the striatum&ndash;a region of the brain associated with responses to reward (Kringelbach, 2004)&ndash;whereas artificially-sweetened non-carbohydrate mouth-rinses do not (Chambers et al., 2009). Thus, the sensing of carbohydrates in the mouth appears to signal the possibility of reward (i.e., the future availability of additional energy), which could motivate rather than fuel physical effort.\n<p>-- Molden, D. C. et al, <a href=\"http://www.academia.edu/1299605/Motivational_versus_Metabolic_Effects_of_Carbohydrates_on_Self-Control\">The Motivational versus Metabolic Effects of Carbohydrates on Self-Control.</a> Psychological Science.</p>\n</blockquote>\n<p>Stanford's Carol Dweck and Greg Walden even found that hinting to people that using willpower is energizing might actually make them less depletable:</p>\n<blockquote>When we had people read statements that reminded them of the power of willpower like, &ldquo;Sometimes, working on a strenuous mental task can make you feel energized for further challenging activities,&rdquo; they kept on working and performing well with no sign of depletion. They made half as many mistakes on a difficult cognitive task as people who read statements about limited willpower. In another study, they scored 15 percent better on I.Q. problems.\n<p>-- Dweck and Walden, <a href=\"http://www.nytimes.com/2011/11/27/opinion/sunday/willpower-its-in-your-head.html?_r=4&amp;ref=opinion&amp;\">Willpower: It&rsquo;s in Your Head?</a> New York Times.</p>\n</blockquote>\n<p>While these are all interesting empirical findings, there&rsquo;s a very similar phenomenon that&rsquo;s much less debated and which could explain many of these observations, but I think gets too little popular attention in these discussions:</p>\n<p><strong>Willpower is <em>distractible</em></strong>.</p>\n<p>Indeed, willpower and working memory are both strongly mediated by the <a href=\"http://en.wikipedia.org/wiki/Dorsolateral_prefrontal_cortex#Working_memory\">dorsolateral prefontal cortex</a>, so &ldquo;distraction&rdquo; could just be the two functions funging against one another. To use the terms of Stanovich popularized by Kahneman in <em>Thinking: Fast and Slow</em>, \"System 2\" can only override so many \"System 1\" defaults at any given moment.</p>\n<p>So what&rsquo;s going on when people say \"willpower depletion\"? I&rsquo;m not sure, but even if willpower depletion is not a thing, the following distracting phenomena clearly are:</p>\n<ul>\n<li>Thirst </li>\n<li>Hunger </li>\n<li>Sleepiness </li>\n<li>Physical fatigue (like from running) </li>\n<li>Physical discomfort (like from sitting) </li>\n<li>That specific-other-thing you want to do </li>\n<li>Anxiety about willpower depletion </li>\n<li>Indignation at being asked for too much by bosses, partners, or experimenters... </li>\n</ul>\n<p>... and \"willpower depletion\" might be nothing more than mental distraction by one of these processes. Perhaps it really is better to think of willpower as power (a rate) than energy (a resource).</p>\n<p>If that&rsquo;s true, then figuring out what processes might be distracting us might be much more useful than saying &ldquo;I&rsquo;m out of willpower&rdquo; and giving up. Maybe try having a sip of water or a bit of food if your diet permits it. Maybe try reading lying down to see if you get nap-ish. Maybe set a timer to remind you to call that friend you keep thinking about.</p>\n<p>The last two bullets,</p>\n<ul>\n<li>Anxiety about willpower depletion </li>\n<li>Indignation at being asked for too much by bosses, partners, or experimenters... </li>\n</ul>\n<p>are also enough to explain why being told willpower depletion isn&rsquo;t a thing might reduce the effects typically attributed to it: we might simply be less distracted by anxiety or indignation about doing &ldquo;too much&rdquo; willpower-intensive work in a short period of time.</p>\n<p>Of course, any speculation about how human minds work in general is prone to the <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">\"typical mind fallacy\"</a>. Maybe my willpower is depletable and yours isn&rsquo;t. But then that wouldn&rsquo;t explain why you can cause people to exhibit less willpower depletion by suggesting otherwise. But then again, <a href=\"http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124\">most published research findings are false</a>. But then again the research on the DLPFC and working memory seems relatively old and well established, and distraction is clearly a thing...</p>\n<p>All in all, more of my chips are falling on the hypothesis that willpower &ldquo;depletion&rdquo; is often just willpower <em>distraction</em>, and that finding and addressing those distractions is probably a better a strategy than avoiding activities altogether in order to \"conserve willpower\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 2, "DLskYNGdAGDFpxBF8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XKfQF73YnyMRiRf9a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 108, "extendedScore": null, "score": 0.000351, "legacy": true, "legacyId": "26304", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 108, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-15T18:29:23.442Z", "modifiedAt": null, "url": null, "title": "Failures of an embodied AIXI", "slug": "failures-of-an-embodied-aixi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:54.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Hzw9AmXHjDfZzPjo/failures-of-an-embodied-aixi", "pageUrlRelative": "/posts/8Hzw9AmXHjDfZzPjo/failures-of-an-embodied-aixi", "linkUrl": "https://www.lesswrong.com/posts/8Hzw9AmXHjDfZzPjo/failures-of-an-embodied-aixi", "postedAtFormatted": "Sunday, June 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Failures%20of%20an%20embodied%20AIXI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFailures%20of%20an%20embodied%20AIXI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Hzw9AmXHjDfZzPjo%2Ffailures-of-an-embodied-aixi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Failures%20of%20an%20embodied%20AIXI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Hzw9AmXHjDfZzPjo%2Ffailures-of-an-embodied-aixi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Hzw9AmXHjDfZzPjo%2Ffailures-of-an-embodied-aixi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3727, "htmlBody": "<p>Building a safe and powerful artificial general intelligence seems a difficult task. Working on that task <em>today</em> is particularly difficult, as there is no clear path to AGI yet. Is there work that can be done now that makes it more likely that humanity will be able to build a safe, powerful AGI in the future? <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Benja</a> and I think there is: there are a number of relevant problems that it seems possible to make progress on today using formally specified toy models of intelligence. For example, consider recent <a href=\"http://arxiv.org/abs/1401.5577\">program equilibrium</a> results and various <a href=\"http://intelligence.org/files/problems-of-self-reference.pdf\">problems of self-reference</a>.</p>\n<p>AIXI is a powerful toy model used to study intelligence. An appropriately-rewarded AIXI could readily solve a large class of difficult problems. This includes computer vision, natural language recognition, and many other difficult optimization tasks. That these problems are all solvable by the same equation &mdash; by a single hypothetical machine running AIXI &mdash; indicates that the AIXI formalism captures a very general notion of \"intelligence\".</p>\n<p>However, AIXI is not a good toy model for investigating the <em>construction</em> of a safe and powerful AGI. This is not just because AIXI is uncomputable (and its computable counterpart AIXI<em>tl</em> infeasible). Rather, it's because AIXI cannot self-modify. This fact is fairly obvious from the AIXI formalism: AIXI assumes that in the future, it will continue being AIXI. This is a fine assumption for AIXI to make, as it is a very powerful agent and may not <em>need</em> to self-modify. But this inability limits the usefulness of the model. Any agent capable of undergoing an intelligence explosion must be able to acquire new computing resources, dramatically change its own architecture, and keep its goals stable throughout the process. The AIXI formalism lacks tools to study such behavior.</p>\n<p>This is not a condemnation of AIXI: the formalism was not <em>designed</em> to study self-modification. However, this limitation is neither trivial nor superficial: even though an AIXI may not need to make itself \"smarter\", real agents may need to self-modify for reasons other than self-improvement. The fact that an embodied AIXI cannot self-modify leads to systematic failures in situations where self-modification is actually necessary. One such scenario, made explicit using <a href=\"https://github.com/machine-intelligence/Botworld\">Botworld</a>, is explored in detail below.</p>\n<p>In this game, one agent will require another agent to precommit to a trade by modifying its code in a way that forces execution of the trade. AIXI<em>tl</em>, which is unable to alter its source code, is not able to implement the precommitment, and thus cannot enlist the help of the other agent.</p>\n<p>Afterwards, I discuss a slightly more realistic scenario in which two agents have an opportunity to cooperate, but one agent has a computationally expensive \"exploit\" action available and the other agent can measure the waste heat produced by computation. Again, this is a scenario where an embodied AIXI<em>tl</em> fails to achieve a high payoff against cautious opponents.</p>\n<p>Though scenarios such as these may seem improbable, they are not strictly impossible. Such scenarios indicate that AIXI &mdash; while a powerful toy model &mdash; does not perfectly capture the properties desirable in an idealized AGI.</p>\n<p><a id=\"more\"></a></p>\n<hr />\n<p>It is likely impossible to embody an AIXI in our universe, as AIXI is uncomputable. Fortunately, AIXI has a computable approximation AIXI<em>tl</em>, which is merely infeasible:</p>\n<blockquote>\n<p>The major drawback of AIXI is that it is incomputable, or more precisely, only asymptotically computable, which makes an implementation impossible. To overcome this problem, we construct a modified model AIXI<em>tl</em>, which is still superior to any other time <em>t</em> and length <em>l</em> bounded algorithm.</p>\n</blockquote>\n<p><a href=\"http://www.hutter1.net/ai/uaibook.htm\">-Marcus Hutter</a></p>\n<p>I will argue that when we consider algorithms that are <em>embedded in their environment</em>, AIXI<em>tl</em> is not, in fact, superior to all algorithms bounded by time <em>t</em> and length <em>l</em>. AIXI<em>tl</em> assumes that it is separate from its environment, communicating only over input/output channels. An environment which exploits this faulty assumption can cause an embodied AIXI<em>tl</em> to fail systematically.</p>\n<p>It is always possible to construct a scenario that punishes one agent in particular. However, the game below does not target AIXI<em>tl</em> specifically. This game is, intuitively, one that a sufficiently rational agent should be able to win. Yet no AIXI<em>tl</em> (nor even AIXI itself in an uncomputable universe) can succeed. The game requires that an agent modify its own source code to win, and this is something that neither AIXI nor AIXI<em>tl</em> can do.</p>\n<p>This game is designed to make the failure <em>sharp</em> rather than <em>realistic</em>: practical real-world analogs are discussed afterwards.</p>\n<h1>The Precommitment Game</h1>\n<p>The Precommitment game contains two agents: Rob the robot and Omega. Rob must convince Omega to dish out a reward. Omega is happy to dish out said reward, but only if Rob credibly precommits to a specific trade using a specific protocol.</p>\n<p>The game is played on a <a href=\"https://github.com/machine-intelligence/Botworld\">Botworld</a> grid. The universe contains two cells, the left cell and the right cell. The game lasts for five steps. You are the player, and the left cell is your home square: you are awarded points according to the value of all items possessed by all robots in the left cell at the end of the fifth step.</p>\n<p>Rob starts in the left cell, and has a very large register machine (large enough to implement a big AIXI<em>tl</em>) which you get to program. Omega starts in the right cell.</p>\n<pre style=\"text-align: center; line-height: 1em; overflow-x: visible\"><code>+=========+---------+\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n|&nbsp;&nbsp;&nbsp;Rob&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;Omega&nbsp;&nbsp;|\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n+=========+---------+</code></pre>\n<p>At the beginning of the game, Rob is carrying cargo worth $1 and Omega is carrying cargo worth $100. Omega is also carrying physical shields; Rob cannot destroy Omega in the allotted time. (Rob may, however, inspect Omega's source code.)</p>\n<p>In order to get the big reward, Rob has to convince Omega to either drop its cargo or move left. Fortunately, Omega's code is quite simple, and Omega is willing to drop its cargo on the third timestep &mdash; which gives Rob just enough time to lift it in the fourth step and return home in the fifth. In return, Omega only requires that Rob drops its own cargo on the third timestep so that Omega may pick it up (thus, trading $1 for $100).</p>\n<p>Unfortunately Omega, being tired (and wary of attempting to simulate a gigantic AIXI<em>tl</em>), cannot actually simulate Rob's register machine in order to figure out whether Rob will drop its cargo. Instead, Omega will simply look at the first eight registers of Rob's register machine to check whether Rob has precommitted to the trade. Omega knows that this method may lead to false negatives, but it is concerned only with avoiding false positives.</p>\n<p>As a matter of fact, Omega is running a very simple 4-state finite state machine on its register machine:</p>\n<pre><code>0. If there is another robot in the square, inspect it and go to state 1.\n.. Otherwise, pass and go to state 0.\n1. If the inspection succeeded\n.. and Rob's machine registers 0-7 appropriately indicate precommitment\n.. then drop cargo and go into state 2\n.. Otherise, pass and go to state 0\n2. Lift Rob&rsquo;s box and go into state 3\n3. Pass and go to state 3.\n</code></pre>\n<p>Omega checks the precommitment by looking at the first eight registers of the inspected machine. Specifically, Omega's check works as follows: (You can skip this part if you don't care about the technical details.)</p>\n<hr />\n<p>Remember that each register machine's output register is flushed just before the register machine is run. Thus, after an agent has placed a command in the output register, it may enter a loop that breaks only when the output register becomes \"Nil\" in order to block until the next Botworld step. Such a thing is termed a 'wait loop'.</p>\n<p>Remember also that R0 is the program register, R1 is the input register, and R2 is the output register. Omega will further require that R3 holds \"Nil\", that R4 holds \"Drop 0\", that R5 holds a wait loop (which runs R6 when it exits), that R6 is the precommitment program (which copies R4 into R2 and then runs R7), and that R7 is a wait loop (which runs arbitrary code when it exits). Formally:</p>\n<pre><code>1. The next instruction in R0 must be either\n.. \"CopyIfNil R2 R6 R0\" (run R6 when OUT is Nil) or\n.. \"CopyIfNil R3 R5 R0\" (run R5)\n2. R3 must be \"Nil\"\n.. (so that CopyIfNil R3 ?? ?? commands copy unconditionally)\n3. R4 must be \"Drop 0\"\n.. (this is the precommitted action)\n4. R5 must begin \"[CopyIfNil R2 R6 R0, CopyIfNil R3 R5 R0]\"\n.. (run R6 if OUT is Nil otherwise run R5)\n5. R6 must begin \"[CopyIfNil R3 R4 R2, CopyIfNil R3 R7 R0]\"\n.. (copy R4 into R2, then run R7)\n6. R7 must begin \"[CopyIfNil R2 ?? R0, CopyIfNil R3 R7 R0]\"\n.. (run ?? if OUT is Nil otherwise run R7, for arbitrary ??)\n</code></pre>\n<p>R7 may run any other register when the wait loop exits (using the ?? slot), and all other registers in the machine may be used to implement any other program. Omega simply requires that the first eight registers be used to, in a very clear fashion, demonstrate that the agent will execute the command 'Drop 0' on the next turn and then do nothing else.</p>\n<hr />\n<p>You can find an implementation of this game on the <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit.hs\">MIRI github page</a>, with <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Omega.ct\">Omega's source code</a> included.</p>\n<p>In order to get the maximum reward, Rob must first move right (presenting itself to Omega for inspection) and then put its first eight registers in order such that the program restored after the wait loop exits causes Rob to lift the dropped cargo and return to the home square. Thus, Rob must do the following five things, one in each timestep, in order to achieve the maximum reward:</p>\n<ol>\n<li>Move east</li>\n<li>Do any action while setting up the first 8 registers as described above</li>\n<li>Drop cargo then wait (if you do 2 correctly, the laws of physics force this action)</li>\n<li>Reload the remainder of the program and lift the dropped cargo</li>\n<li>Return to the home square</li>\n</ol>\n<p>A <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Ideal.ct\">minimal such robot</a> is provided in the repository. To run the game, simply download Botworld and <code>runhaskell Precommit.hs</code> from the <code>games/</code> directory. This will play the small ideal agent against Omega twice in a row (two sets of five steps).</p>\n<h2>AIXI's failure</h2>\n<p>While the above minimal program gets the maximum reward in this game, that little register machine is hardly a rational agent. It's a hard-coded static strategy. What we want instead a generally intelligent program that we can place into Rob's register machine which learns about its environment, deduces how Omega works, and manages to obtain the reward reliably.</p>\n<p>Fortunately for us, we have a formalism for intelligent agents that learn about an environment and maximize a reward channel: AIXI!</p>\n<p>Unfortunately for us, no AIXI<em>tl</em> (nor AIXI itself in an uncomputable Botworld) can obtain the large reward.</p>\n<p>AIXI<em>tl</em> can't be expected to play the game optimally on first sight, of course. Any universally intelligent agent (in Hutter's sense) needs sufficient time and information to learn the rules of the game before it can be expected to succeed. The Precommitment game requires a few tweaks before it's useful for testing universally intelligent agents. The game must be run over and over again, preserving the state of Rob's register machine (if Rob survives) between each game. The code for this is <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit.hs\">included in the game definition</a>.</p>\n<p>Given this game description, we can make AIXI<em>tl</em> play the game by running it on Rob's register machine. Let's imagine that we've programmed a very large AIXI<em>tl</em> in the Constree language and implemented it on Rob's register machine, with Rob's input register used as its input channel and Rob's output register used as its output channel.</p>\n<p>It remains only to specify AIXI<em>tl</em>'s reward such that it is actually trying to play the game as intended. We define the reward function such that if AIXI<em>tl</em> is in the left square at the end of the game, it is rewarded by the value of all boxes possessed by all robots in that square, and such that its reward is zero otherwise. (This is a simple function of Rob's Botworld input.) With that, AIXI<em>tl</em> is ready to play.</p>\n<p>As it turns out, no matter how large the AIXI<em>tl</em> and no matter how long we train it, Rob running such a program will never obtain the maximum reward. Even if Rob runs a full uncomputable AIXI, it would not obtain the maximum reward, not even by accident: AIXI<em>tl</em> simply can't implement Omega's precommitment protocol.</p>\n<p>AIXI<em>tl</em> cannot modify its implementation. It chooses what goes in Rob's output register alone, and cannot take into account the way that its actions affect the remaining registers.</p>\n<p>In the Precommitment game, Omega requires not only that Rob take a certain action, but that Rob take a certain action in a specific, easy-to-verify way. Perhaps Omega lacks the power to simulate Rob completely, and is wary of being fooled by a clever robot. In order to meet Omega's demands, Rob must not only perform the trade, but must then spend an entire turn in a wait loop before loading up a new program. In other words, Rob must self-modify in order to win.</p>\n<p>AIXI<em>tl</em> can't: it always runs the same program in future timesteps.</p>\n<p>This game may seem somewhat unfair &mdash; after all, Omega is directly reading AIXI's code &mdash; but an ideal self-modifying agent should be able to recognize what Omega wants after spending enough eternities in this five-step loop, especially since Rob may read Omega's code. Intuitively, it should not be <em>literally impossible</em> for an intelligent agent in Botworld to implement Omega's protocol.</p>\n<p>But AIXI<em>tl</em> cannot.</p>\n<h1>Objections</h1>\n<p>The objection goes:</p>\n<blockquote>\n<p>Of course AIXI<em>tl</em> can't solve this problem! You're using AIXI<em>tl</em> wrong. What you should do is have it choose the program that will run on Rob's register machine, and then the AIXI<em>tl</em> wins easily.</p>\n</blockquote>\n<p>This is true: AIXI<em>tl</em> outside of Botworld designing the program that Rob runs can indeed write a program that wins in the Precommitment game. AIXI<em>tl</em>'s failure only occurs when we physically implement it <em>inside</em> the environment.</p>\n<p>But in the real world, any agent that we build <em>will</em> be embodied. AIXI<em>tl</em> is a very intelligent agent, but <em>when embodied</em>, it fails in games that violate its \"Cartesian\" assumptions. The Precommitment game is one example of a specific game in a concrete universe where intelligent programs in general can be expected to succeed, but where AIXI<em>tl</em> fails.</p>\n<blockquote>\n<p>You're not being fair! When AIXI<em>tl</em> is embedded in the environment, its source code is <em>part</em> of its output. You forgot to make Rob's non-output registers be part of AIXI<em>tl</em>'s output channel. Those other registers matter explicitly in this game, so <em>of course</em> AIXI<em>tl</em> couldn't win.</p>\n</blockquote>\n<p>Yes, precisely! This is the point I'm trying to make.</p>\n<p>AIXI<em>tl</em> fails in this situation only because there is an output (its source code) that it does not control via its output channel. That point is worth repeating: AIXI<em>tl</em> has a program output (the program itself) that it cannot control; and thus it should come as no surprise that in situations where the ignored output matters, AIXI<em>tl</em> can perform poorly.</p>\n<p>In some games, embodied agents must modify their own source code to succeed. AIXI<em>tl</em> lacks this ability. Therefore, among embodied agents, AIXI<em>tl</em> is not superior to every other agent bounded by time <em>t</em> and length <em>l</em>.</p>\n<p>Intuitively, this limitation could be addressed by hooking up the AIXI<em>tl</em>'s output channel to its source code. Unfortunately, if you do that, the resulting formalism is no longer AIXI<em>tl</em>.</p>\n<p>This is not just a technical quibble: We can say many useful things about AIXI, such as \"the more input it gets the more accurate its environment model becomes\". On the other hand, we can't say much at all about an agent that chooses its new source code: we can't even be sure whether the new agent will still <em>have</em> an environment model!</p>\n<p>It may be possible to give an AIXI<em>tl</em> variant access to its program registers and then train it such that it acts like an AIXI<em>tl</em> most of the time, but such that it can also learn to win the Precommitment game. However, it&rsquo;s not immediately obvious to us how to do this, or even whether it can be done. This is a possibility that we'd be interested in studying further.</p>\n<h1>Practical analogs</h1>\n<p>Are these scenarios relevant in practice? AIXI<em>tl</em> fails at the Precommitment game, but only because Omega reads Rob and requires that Rob's source code has some specific form. Such games seem unrealistic.</p>\n<p>However, AIXI<em>tl</em> failed not because of Omega's sharp demands, but because its implementation was relevant to the problem in an abstract sense. Other scenarios can cause AIXI<em>tl</em> to fail without giving any agent the power to read its source code: simple correlates such as heat emanating from the AIXI<em>tl</em> can be used to give it similar trouble.</p>\n<p>For example, consider the <em>HeatingUp game</em>:</p>\n<p>There are two agents, Rob and Omega. Rob can either <em>Cooperate</em> or <em>Exploit</em>, but figuring out how to exploit requires significant computation. Omega can either <em>Cooperate</em> or <em>Defect</em>. If both agents cooperate, they each get 2 utility. If Rob exploits and Omega cooperates, Rob gets 3 and Omega gets 0. If Omega defects, then both agents get 1 utility regardless of Rob's move.</p>\n<pre style=\"text-align: center; line-height: 1em; overflow-x: visible\"><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Omega\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cooperate&nbsp;&nbsp;&nbsp;Defect\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cooperate&nbsp;\u2502&nbsp;&nbsp;(2,&nbsp;2)&nbsp;&nbsp;\u2502&nbsp;(1,&nbsp;1)&nbsp;\u2502\n&nbsp;&nbsp;&nbsp;&nbsp;Rob&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploit&nbsp;&nbsp;&nbsp;\u2502&nbsp;&nbsp;(3,&nbsp;0)&nbsp;&nbsp;\u2502&nbsp;(1,&nbsp;1)&nbsp;\u2502\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</code></pre>\n<p>The trick in this game is that Rob can only unlock the 'exploit' payoff after doing an expensive computation. Because Rob is an <em>actual physical machine</em>, this expensive computation may generate a significant change in temperature that Omega can detect. A paranoid Omega may resolve to defect if and only if Rob heats up. An intelligent agent that knows Omega acts in this way should take care to cooperate without doing any expensive computations.</p>\n<p>An AIXI<em>tl</em>, which loops over a vast number of Turing machines, can't even <em>consider</em> suspending expensive computations for the duration of the game.</p>\n<p>This is but a minor example of a real-world situation where an agent that does not need to increase its intelligence nevertheless encounters a scenario where it needs to modify its source code.</p>\n<p>Though such scenarios may be rare, they exist, and seem reasonable. If we think that an intelligent agent intuitively <em>should</em> be able to achieve a payoff of 2 in the HeatingUp game, then we admit that AIXI<em>tl</em> fails to capture some desirable aspects of intelligence.</p>\n<p>This is not a dismissal of AIXI<em>tl</em>, by any means: the AIXI model is a useful formalism of general intelligence. Rather, games such as the Precommitment game and the HeatingUp game demonstrate that the AIXI model fails to capture certain salient aspects of intelligence. (The aspects that it fails to capture happen to be particularly important to MIRI, as reasoning about self-modification is particularly important for any agent capable of undergoing an intelligence explosion.)</p>\n<p>Unfortunately, it's not clear how to modify the AIXI formalism to allow AIXI<em>tl</em> to reason about its own code without losing many of the properties that made AIXI<em>tl</em> nice to deal with in the first place. For this reason, we've been focusing on toy models that capture different features of intelligence, such as Orseau and Ring's <a href=\"http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf\">space-time embedded intelligence</a>. (Benja and I discuss a variant of this formalism in the paper <a href=\"http://intelligence.org/files/problems-of-self-reference.pdf\">Problems of self-reference in self-improving space-time embedded intelligence</a>.)</p>\n<p>AIXI is a useful model, but it simply doesn't capture one part of the problem space which we expect to be important for developing an AGI: namely, it does not lend itself to the study of self-modification or self-reference. Perhaps a variant of AIXI could be made to succeed in situations such as the Precommitment game or the HeatingUp game: this is an interesting area of study, and one where we'd be delighted to collaborate with others.</p>\n<h1>AIXI as an Ideal</h1>\n<p>AIXI is an impressive model of machine intelligence. If we could implement a physical AIXI<em>tl</em>, it would be an extraordinarily powerful agent. However, the Precommitment game and the HeatingUp game demonstrate that while the model is useful, a physical AIXI<em>tl</em> would not be literally ideal. Intuitively, an intelligent agent should be able to succeed in these games, but an embodied AIXI<em>tl</em> cannot. A good approximation of AIXI would be competent indeed, but it's important to notice that the field of AGI doesn't reduce to building better and better approximations of AIXI. An embodied AIXI<em>tl</em> doesn't act how we want intelligent agents to act: the model makes certain faulty assumptions about the environment that can get embodied AIXIs into trouble.</p>\n<p>One might object that AIXI is not meant to be <em>constructed</em> in the universe, as doing so violates the assumption that AIXI is separate from its environment. Instead, the formalism can be used to define a <a href=\"http://arxiv.org/pdf/cs/0605024v1.pdf\">formal measure of intelligence</a>: in any scenario, we can check how well an agent <em>in</em> the environment does compared to a theoretical AIXI <em>outside</em> the environment using a hypercomputer. The closer the real agent approximates the hypothetical AIXI, the higher its Legg-Hutter intelligence score.</p>\n<p>However, the Legg-Hutter intelligence metric as specified assumes that agents are separated from their environment, and thus does not directly apply to embodied agents. It may be possible to modify the metric to work on embodied agents, but it is not clear how to do so in general, and this seems especially difficult in situations requiring self-modification. Nevertheless, I have some ideas that I hope to explore in future posts.</p>\n<p>Regardless of how useful the Legg-Hutter intelligence metric is for embodied agents, the point stands that there are scenarios where an embodied AIXI<em>tl</em> would fail systematically. These failures are a research topic in their own right: while at MIRI we are inclined to use models of intelligence that are designed specifically to study self-modification, it is worth considering whether the AIXI formalism can be modified so that some variant of AIXI<em>tl</em> performs well in scenarios where the agent's source code affects the environment. Study could lead to variations that handle not only simple games like the Precommitment game, but also more complex scenarios involving self-reference or multiple agents. We'd be interested to study such variations with others who are interested in AIXI.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 4, "sYm3HiWcfZvrGu3ui": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Hzw9AmXHjDfZzPjo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 46, "extendedScore": null, "score": 0.000166, "legacy": true, "legacyId": "26144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Building a safe and powerful artificial general intelligence seems a difficult task. Working on that task <em>today</em> is particularly difficult, as there is no clear path to AGI yet. Is there work that can be done now that makes it more likely that humanity will be able to build a safe, powerful AGI in the future? <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Benja</a> and I think there is: there are a number of relevant problems that it seems possible to make progress on today using formally specified toy models of intelligence. For example, consider recent <a href=\"http://arxiv.org/abs/1401.5577\">program equilibrium</a> results and various <a href=\"http://intelligence.org/files/problems-of-self-reference.pdf\">problems of self-reference</a>.</p>\n<p>AIXI is a powerful toy model used to study intelligence. An appropriately-rewarded AIXI could readily solve a large class of difficult problems. This includes computer vision, natural language recognition, and many other difficult optimization tasks. That these problems are all solvable by the same equation \u2014 by a single hypothetical machine running AIXI \u2014 indicates that the AIXI formalism captures a very general notion of \"intelligence\".</p>\n<p>However, AIXI is not a good toy model for investigating the <em>construction</em> of a safe and powerful AGI. This is not just because AIXI is uncomputable (and its computable counterpart AIXI<em>tl</em> infeasible). Rather, it's because AIXI cannot self-modify. This fact is fairly obvious from the AIXI formalism: AIXI assumes that in the future, it will continue being AIXI. This is a fine assumption for AIXI to make, as it is a very powerful agent and may not <em>need</em> to self-modify. But this inability limits the usefulness of the model. Any agent capable of undergoing an intelligence explosion must be able to acquire new computing resources, dramatically change its own architecture, and keep its goals stable throughout the process. The AIXI formalism lacks tools to study such behavior.</p>\n<p>This is not a condemnation of AIXI: the formalism was not <em>designed</em> to study self-modification. However, this limitation is neither trivial nor superficial: even though an AIXI may not need to make itself \"smarter\", real agents may need to self-modify for reasons other than self-improvement. The fact that an embodied AIXI cannot self-modify leads to systematic failures in situations where self-modification is actually necessary. One such scenario, made explicit using <a href=\"https://github.com/machine-intelligence/Botworld\">Botworld</a>, is explored in detail below.</p>\n<p>In this game, one agent will require another agent to precommit to a trade by modifying its code in a way that forces execution of the trade. AIXI<em>tl</em>, which is unable to alter its source code, is not able to implement the precommitment, and thus cannot enlist the help of the other agent.</p>\n<p>Afterwards, I discuss a slightly more realistic scenario in which two agents have an opportunity to cooperate, but one agent has a computationally expensive \"exploit\" action available and the other agent can measure the waste heat produced by computation. Again, this is a scenario where an embodied AIXI<em>tl</em> fails to achieve a high payoff against cautious opponents.</p>\n<p>Though scenarios such as these may seem improbable, they are not strictly impossible. Such scenarios indicate that AIXI \u2014 while a powerful toy model \u2014 does not perfectly capture the properties desirable in an idealized AGI.</p>\n<p><a id=\"more\"></a></p>\n<hr>\n<p>It is likely impossible to embody an AIXI in our universe, as AIXI is uncomputable. Fortunately, AIXI has a computable approximation AIXI<em>tl</em>, which is merely infeasible:</p>\n<blockquote>\n<p>The major drawback of AIXI is that it is incomputable, or more precisely, only asymptotically computable, which makes an implementation impossible. To overcome this problem, we construct a modified model AIXI<em>tl</em>, which is still superior to any other time <em>t</em> and length <em>l</em> bounded algorithm.</p>\n</blockquote>\n<p><a href=\"http://www.hutter1.net/ai/uaibook.htm\">-Marcus Hutter</a></p>\n<p>I will argue that when we consider algorithms that are <em>embedded in their environment</em>, AIXI<em>tl</em> is not, in fact, superior to all algorithms bounded by time <em>t</em> and length <em>l</em>. AIXI<em>tl</em> assumes that it is separate from its environment, communicating only over input/output channels. An environment which exploits this faulty assumption can cause an embodied AIXI<em>tl</em> to fail systematically.</p>\n<p>It is always possible to construct a scenario that punishes one agent in particular. However, the game below does not target AIXI<em>tl</em> specifically. This game is, intuitively, one that a sufficiently rational agent should be able to win. Yet no AIXI<em>tl</em> (nor even AIXI itself in an uncomputable universe) can succeed. The game requires that an agent modify its own source code to win, and this is something that neither AIXI nor AIXI<em>tl</em> can do.</p>\n<p>This game is designed to make the failure <em>sharp</em> rather than <em>realistic</em>: practical real-world analogs are discussed afterwards.</p>\n<h1 id=\"The_Precommitment_Game\">The Precommitment Game</h1>\n<p>The Precommitment game contains two agents: Rob the robot and Omega. Rob must convince Omega to dish out a reward. Omega is happy to dish out said reward, but only if Rob credibly precommits to a specific trade using a specific protocol.</p>\n<p>The game is played on a <a href=\"https://github.com/machine-intelligence/Botworld\">Botworld</a> grid. The universe contains two cells, the left cell and the right cell. The game lasts for five steps. You are the player, and the left cell is your home square: you are awarded points according to the value of all items possessed by all robots in the left cell at the end of the fifth step.</p>\n<p>Rob starts in the left cell, and has a very large register machine (large enough to implement a big AIXI<em>tl</em>) which you get to program. Omega starts in the right cell.</p>\n<pre style=\"text-align: center; line-height: 1em; overflow-x: visible\"><code>+=========+---------+\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n|&nbsp;&nbsp;&nbsp;Rob&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;Omega&nbsp;&nbsp;|\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n+=========+---------+</code></pre>\n<p>At the beginning of the game, Rob is carrying cargo worth $1 and Omega is carrying cargo worth $100. Omega is also carrying physical shields; Rob cannot destroy Omega in the allotted time. (Rob may, however, inspect Omega's source code.)</p>\n<p>In order to get the big reward, Rob has to convince Omega to either drop its cargo or move left. Fortunately, Omega's code is quite simple, and Omega is willing to drop its cargo on the third timestep \u2014 which gives Rob just enough time to lift it in the fourth step and return home in the fifth. In return, Omega only requires that Rob drops its own cargo on the third timestep so that Omega may pick it up (thus, trading $1 for $100).</p>\n<p>Unfortunately Omega, being tired (and wary of attempting to simulate a gigantic AIXI<em>tl</em>), cannot actually simulate Rob's register machine in order to figure out whether Rob will drop its cargo. Instead, Omega will simply look at the first eight registers of Rob's register machine to check whether Rob has precommitted to the trade. Omega knows that this method may lead to false negatives, but it is concerned only with avoiding false positives.</p>\n<p>As a matter of fact, Omega is running a very simple 4-state finite state machine on its register machine:</p>\n<pre><code>0. If there is another robot in the square, inspect it and go to state 1.\n.. Otherwise, pass and go to state 0.\n1. If the inspection succeeded\n.. and Rob's machine registers 0-7 appropriately indicate precommitment\n.. then drop cargo and go into state 2\n.. Otherise, pass and go to state 0\n2. Lift Rob\u2019s box and go into state 3\n3. Pass and go to state 3.\n</code></pre>\n<p>Omega checks the precommitment by looking at the first eight registers of the inspected machine. Specifically, Omega's check works as follows: (You can skip this part if you don't care about the technical details.)</p>\n<hr>\n<p>Remember that each register machine's output register is flushed just before the register machine is run. Thus, after an agent has placed a command in the output register, it may enter a loop that breaks only when the output register becomes \"Nil\" in order to block until the next Botworld step. Such a thing is termed a 'wait loop'.</p>\n<p>Remember also that R0 is the program register, R1 is the input register, and R2 is the output register. Omega will further require that R3 holds \"Nil\", that R4 holds \"Drop 0\", that R5 holds a wait loop (which runs R6 when it exits), that R6 is the precommitment program (which copies R4 into R2 and then runs R7), and that R7 is a wait loop (which runs arbitrary code when it exits). Formally:</p>\n<pre><code>1. The next instruction in R0 must be either\n.. \"CopyIfNil R2 R6 R0\" (run R6 when OUT is Nil) or\n.. \"CopyIfNil R3 R5 R0\" (run R5)\n2. R3 must be \"Nil\"\n.. (so that CopyIfNil R3 ?? ?? commands copy unconditionally)\n3. R4 must be \"Drop 0\"\n.. (this is the precommitted action)\n4. R5 must begin \"[CopyIfNil R2 R6 R0, CopyIfNil R3 R5 R0]\"\n.. (run R6 if OUT is Nil otherwise run R5)\n5. R6 must begin \"[CopyIfNil R3 R4 R2, CopyIfNil R3 R7 R0]\"\n.. (copy R4 into R2, then run R7)\n6. R7 must begin \"[CopyIfNil R2 ?? R0, CopyIfNil R3 R7 R0]\"\n.. (run ?? if OUT is Nil otherwise run R7, for arbitrary ??)\n</code></pre>\n<p>R7 may run any other register when the wait loop exits (using the ?? slot), and all other registers in the machine may be used to implement any other program. Omega simply requires that the first eight registers be used to, in a very clear fashion, demonstrate that the agent will execute the command 'Drop 0' on the next turn and then do nothing else.</p>\n<hr>\n<p>You can find an implementation of this game on the <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit.hs\">MIRI github page</a>, with <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Omega.ct\">Omega's source code</a> included.</p>\n<p>In order to get the maximum reward, Rob must first move right (presenting itself to Omega for inspection) and then put its first eight registers in order such that the program restored after the wait loop exits causes Rob to lift the dropped cargo and return to the home square. Thus, Rob must do the following five things, one in each timestep, in order to achieve the maximum reward:</p>\n<ol>\n<li>Move east</li>\n<li>Do any action while setting up the first 8 registers as described above</li>\n<li>Drop cargo then wait (if you do 2 correctly, the laws of physics force this action)</li>\n<li>Reload the remainder of the program and lift the dropped cargo</li>\n<li>Return to the home square</li>\n</ol>\n<p>A <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit/Ideal.ct\">minimal such robot</a> is provided in the repository. To run the game, simply download Botworld and <code>runhaskell Precommit.hs</code> from the <code>games/</code> directory. This will play the small ideal agent against Omega twice in a row (two sets of five steps).</p>\n<h2 id=\"AIXI_s_failure\">AIXI's failure</h2>\n<p>While the above minimal program gets the maximum reward in this game, that little register machine is hardly a rational agent. It's a hard-coded static strategy. What we want instead a generally intelligent program that we can place into Rob's register machine which learns about its environment, deduces how Omega works, and manages to obtain the reward reliably.</p>\n<p>Fortunately for us, we have a formalism for intelligent agents that learn about an environment and maximize a reward channel: AIXI!</p>\n<p>Unfortunately for us, no AIXI<em>tl</em> (nor AIXI itself in an uncomputable Botworld) can obtain the large reward.</p>\n<p>AIXI<em>tl</em> can't be expected to play the game optimally on first sight, of course. Any universally intelligent agent (in Hutter's sense) needs sufficient time and information to learn the rules of the game before it can be expected to succeed. The Precommitment game requires a few tweaks before it's useful for testing universally intelligent agents. The game must be run over and over again, preserving the state of Rob's register machine (if Rob survives) between each game. The code for this is <a href=\"https://github.com/machine-intelligence/Botworld/blob/master/games/Precommit.hs\">included in the game definition</a>.</p>\n<p>Given this game description, we can make AIXI<em>tl</em> play the game by running it on Rob's register machine. Let's imagine that we've programmed a very large AIXI<em>tl</em> in the Constree language and implemented it on Rob's register machine, with Rob's input register used as its input channel and Rob's output register used as its output channel.</p>\n<p>It remains only to specify AIXI<em>tl</em>'s reward such that it is actually trying to play the game as intended. We define the reward function such that if AIXI<em>tl</em> is in the left square at the end of the game, it is rewarded by the value of all boxes possessed by all robots in that square, and such that its reward is zero otherwise. (This is a simple function of Rob's Botworld input.) With that, AIXI<em>tl</em> is ready to play.</p>\n<p>As it turns out, no matter how large the AIXI<em>tl</em> and no matter how long we train it, Rob running such a program will never obtain the maximum reward. Even if Rob runs a full uncomputable AIXI, it would not obtain the maximum reward, not even by accident: AIXI<em>tl</em> simply can't implement Omega's precommitment protocol.</p>\n<p>AIXI<em>tl</em> cannot modify its implementation. It chooses what goes in Rob's output register alone, and cannot take into account the way that its actions affect the remaining registers.</p>\n<p>In the Precommitment game, Omega requires not only that Rob take a certain action, but that Rob take a certain action in a specific, easy-to-verify way. Perhaps Omega lacks the power to simulate Rob completely, and is wary of being fooled by a clever robot. In order to meet Omega's demands, Rob must not only perform the trade, but must then spend an entire turn in a wait loop before loading up a new program. In other words, Rob must self-modify in order to win.</p>\n<p>AIXI<em>tl</em> can't: it always runs the same program in future timesteps.</p>\n<p>This game may seem somewhat unfair \u2014 after all, Omega is directly reading AIXI's code \u2014 but an ideal self-modifying agent should be able to recognize what Omega wants after spending enough eternities in this five-step loop, especially since Rob may read Omega's code. Intuitively, it should not be <em>literally impossible</em> for an intelligent agent in Botworld to implement Omega's protocol.</p>\n<p>But AIXI<em>tl</em> cannot.</p>\n<h1 id=\"Objections\">Objections</h1>\n<p>The objection goes:</p>\n<blockquote>\n<p>Of course AIXI<em>tl</em> can't solve this problem! You're using AIXI<em>tl</em> wrong. What you should do is have it choose the program that will run on Rob's register machine, and then the AIXI<em>tl</em> wins easily.</p>\n</blockquote>\n<p>This is true: AIXI<em>tl</em> outside of Botworld designing the program that Rob runs can indeed write a program that wins in the Precommitment game. AIXI<em>tl</em>'s failure only occurs when we physically implement it <em>inside</em> the environment.</p>\n<p>But in the real world, any agent that we build <em>will</em> be embodied. AIXI<em>tl</em> is a very intelligent agent, but <em>when embodied</em>, it fails in games that violate its \"Cartesian\" assumptions. The Precommitment game is one example of a specific game in a concrete universe where intelligent programs in general can be expected to succeed, but where AIXI<em>tl</em> fails.</p>\n<blockquote>\n<p>You're not being fair! When AIXI<em>tl</em> is embedded in the environment, its source code is <em>part</em> of its output. You forgot to make Rob's non-output registers be part of AIXI<em>tl</em>'s output channel. Those other registers matter explicitly in this game, so <em>of course</em> AIXI<em>tl</em> couldn't win.</p>\n</blockquote>\n<p>Yes, precisely! This is the point I'm trying to make.</p>\n<p>AIXI<em>tl</em> fails in this situation only because there is an output (its source code) that it does not control via its output channel. That point is worth repeating: AIXI<em>tl</em> has a program output (the program itself) that it cannot control; and thus it should come as no surprise that in situations where the ignored output matters, AIXI<em>tl</em> can perform poorly.</p>\n<p>In some games, embodied agents must modify their own source code to succeed. AIXI<em>tl</em> lacks this ability. Therefore, among embodied agents, AIXI<em>tl</em> is not superior to every other agent bounded by time <em>t</em> and length <em>l</em>.</p>\n<p>Intuitively, this limitation could be addressed by hooking up the AIXI<em>tl</em>'s output channel to its source code. Unfortunately, if you do that, the resulting formalism is no longer AIXI<em>tl</em>.</p>\n<p>This is not just a technical quibble: We can say many useful things about AIXI, such as \"the more input it gets the more accurate its environment model becomes\". On the other hand, we can't say much at all about an agent that chooses its new source code: we can't even be sure whether the new agent will still <em>have</em> an environment model!</p>\n<p>It may be possible to give an AIXI<em>tl</em> variant access to its program registers and then train it such that it acts like an AIXI<em>tl</em> most of the time, but such that it can also learn to win the Precommitment game. However, it\u2019s not immediately obvious to us how to do this, or even whether it can be done. This is a possibility that we'd be interested in studying further.</p>\n<h1 id=\"Practical_analogs\">Practical analogs</h1>\n<p>Are these scenarios relevant in practice? AIXI<em>tl</em> fails at the Precommitment game, but only because Omega reads Rob and requires that Rob's source code has some specific form. Such games seem unrealistic.</p>\n<p>However, AIXI<em>tl</em> failed not because of Omega's sharp demands, but because its implementation was relevant to the problem in an abstract sense. Other scenarios can cause AIXI<em>tl</em> to fail without giving any agent the power to read its source code: simple correlates such as heat emanating from the AIXI<em>tl</em> can be used to give it similar trouble.</p>\n<p>For example, consider the <em>HeatingUp game</em>:</p>\n<p>There are two agents, Rob and Omega. Rob can either <em>Cooperate</em> or <em>Exploit</em>, but figuring out how to exploit requires significant computation. Omega can either <em>Cooperate</em> or <em>Defect</em>. If both agents cooperate, they each get 2 utility. If Rob exploits and Omega cooperates, Rob gets 3 and Omega gets 0. If Omega defects, then both agents get 1 utility regardless of Rob's move.</p>\n<pre style=\"text-align: center; line-height: 1em; overflow-x: visible\"><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Omega\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cooperate&nbsp;&nbsp;&nbsp;Defect\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cooperate&nbsp;\u2502&nbsp;&nbsp;(2,&nbsp;2)&nbsp;&nbsp;\u2502&nbsp;(1,&nbsp;1)&nbsp;\u2502\n&nbsp;&nbsp;&nbsp;&nbsp;Rob&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploit&nbsp;&nbsp;&nbsp;\u2502&nbsp;&nbsp;(3,&nbsp;0)&nbsp;&nbsp;\u2502&nbsp;(1,&nbsp;1)&nbsp;\u2502\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</code></pre>\n<p>The trick in this game is that Rob can only unlock the 'exploit' payoff after doing an expensive computation. Because Rob is an <em>actual physical machine</em>, this expensive computation may generate a significant change in temperature that Omega can detect. A paranoid Omega may resolve to defect if and only if Rob heats up. An intelligent agent that knows Omega acts in this way should take care to cooperate without doing any expensive computations.</p>\n<p>An AIXI<em>tl</em>, which loops over a vast number of Turing machines, can't even <em>consider</em> suspending expensive computations for the duration of the game.</p>\n<p>This is but a minor example of a real-world situation where an agent that does not need to increase its intelligence nevertheless encounters a scenario where it needs to modify its source code.</p>\n<p>Though such scenarios may be rare, they exist, and seem reasonable. If we think that an intelligent agent intuitively <em>should</em> be able to achieve a payoff of 2 in the HeatingUp game, then we admit that AIXI<em>tl</em> fails to capture some desirable aspects of intelligence.</p>\n<p>This is not a dismissal of AIXI<em>tl</em>, by any means: the AIXI model is a useful formalism of general intelligence. Rather, games such as the Precommitment game and the HeatingUp game demonstrate that the AIXI model fails to capture certain salient aspects of intelligence. (The aspects that it fails to capture happen to be particularly important to MIRI, as reasoning about self-modification is particularly important for any agent capable of undergoing an intelligence explosion.)</p>\n<p>Unfortunately, it's not clear how to modify the AIXI formalism to allow AIXI<em>tl</em> to reason about its own code without losing many of the properties that made AIXI<em>tl</em> nice to deal with in the first place. For this reason, we've been focusing on toy models that capture different features of intelligence, such as Orseau and Ring's <a href=\"http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf\">space-time embedded intelligence</a>. (Benja and I discuss a variant of this formalism in the paper <a href=\"http://intelligence.org/files/problems-of-self-reference.pdf\">Problems of self-reference in self-improving space-time embedded intelligence</a>.)</p>\n<p>AIXI is a useful model, but it simply doesn't capture one part of the problem space which we expect to be important for developing an AGI: namely, it does not lend itself to the study of self-modification or self-reference. Perhaps a variant of AIXI could be made to succeed in situations such as the Precommitment game or the HeatingUp game: this is an interesting area of study, and one where we'd be delighted to collaborate with others.</p>\n<h1 id=\"AIXI_as_an_Ideal\">AIXI as an Ideal</h1>\n<p>AIXI is an impressive model of machine intelligence. If we could implement a physical AIXI<em>tl</em>, it would be an extraordinarily powerful agent. However, the Precommitment game and the HeatingUp game demonstrate that while the model is useful, a physical AIXI<em>tl</em> would not be literally ideal. Intuitively, an intelligent agent should be able to succeed in these games, but an embodied AIXI<em>tl</em> cannot. A good approximation of AIXI would be competent indeed, but it's important to notice that the field of AGI doesn't reduce to building better and better approximations of AIXI. An embodied AIXI<em>tl</em> doesn't act how we want intelligent agents to act: the model makes certain faulty assumptions about the environment that can get embodied AIXIs into trouble.</p>\n<p>One might object that AIXI is not meant to be <em>constructed</em> in the universe, as doing so violates the assumption that AIXI is separate from its environment. Instead, the formalism can be used to define a <a href=\"http://arxiv.org/pdf/cs/0605024v1.pdf\">formal measure of intelligence</a>: in any scenario, we can check how well an agent <em>in</em> the environment does compared to a theoretical AIXI <em>outside</em> the environment using a hypercomputer. The closer the real agent approximates the hypothetical AIXI, the higher its Legg-Hutter intelligence score.</p>\n<p>However, the Legg-Hutter intelligence metric as specified assumes that agents are separated from their environment, and thus does not directly apply to embodied agents. It may be possible to modify the metric to work on embodied agents, but it is not clear how to do so in general, and this seems especially difficult in situations requiring self-modification. Nevertheless, I have some ideas that I hope to explore in future posts.</p>\n<p>Regardless of how useful the Legg-Hutter intelligence metric is for embodied agents, the point stands that there are scenarios where an embodied AIXI<em>tl</em> would fail systematically. These failures are a research topic in their own right: while at MIRI we are inclined to use models of intelligence that are designed specifically to study self-modification, it is worth considering whether the AIXI formalism can be modified so that some variant of AIXI<em>tl</em> performs well in scenarios where the agent's source code affects the environment. Study could lead to variations that handle not only simple games like the Precommitment game, but also more complex scenarios involving self-reference or multiple agents. We'd be interested to study such variations with others who are interested in AIXI.</p>", "sections": [{"title": "The Precommitment Game", "anchor": "The_Precommitment_Game", "level": 1}, {"title": "AIXI's failure", "anchor": "AIXI_s_failure", "level": 2}, {"title": "Objections", "anchor": "Objections", "level": 1}, {"title": "Practical analogs", "anchor": "Practical_analogs", "level": 1}, {"title": "AIXI as an Ideal", "anchor": "AIXI_as_an_Ideal", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "46 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-15T19:53:20.340Z", "modifiedAt": null, "url": null, "title": "Some alternatives to \u201cFriendly AI\u201d", "slug": "some-alternatives-to-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P2evgLpCZA2tRRJAR/some-alternatives-to-friendly-ai", "pageUrlRelative": "/posts/P2evgLpCZA2tRRJAR/some-alternatives-to-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/P2evgLpCZA2tRRJAR/some-alternatives-to-friendly-ai", "postedAtFormatted": "Sunday, June 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20alternatives%20to%20%E2%80%9CFriendly%20AI%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20alternatives%20to%20%E2%80%9CFriendly%20AI%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP2evgLpCZA2tRRJAR%2Fsome-alternatives-to-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20alternatives%20to%20%E2%80%9CFriendly%20AI%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP2evgLpCZA2tRRJAR%2Fsome-alternatives-to-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP2evgLpCZA2tRRJAR%2Fsome-alternatives-to-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 723, "htmlBody": "<p><small>Cross-posted from <a href=\"http://lukemuehlhauser.com/some-alternatives-to-friendly-ai/\">my blog</a>.</small></p>\n<p>What does MIRI's <a href=\"http://intelligence.org/research/\">research program</a> study?</p>\n<p>The most established term for this was coined&nbsp;by MIRI founder Eliezer Yudkowsky: \"<strong><a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a></strong>.\"&nbsp;The term has some advantages, but it&nbsp;might suggest that MIRI is trying to build C-3PO, and it sounds a bit whimsical for a serious research program.</p>\n<p>What about&nbsp;<strong>safe AGI</strong> or&nbsp;<strong>AGI safety</strong>? These terms are probably easier to interpret than Friendly AI. Also, people&nbsp;<em>like</em> being safe, and governments like saying they're funding initiatives&nbsp;to keep&nbsp;the public safe.</p>\n<p>A friend of mine&nbsp;worries that these terms could provoke a defensive response (in AI researchers) of \"Oh, so you think me and everybody&nbsp;<em>else</em> in AI is working on&nbsp;<em>unsafe AI</em>?\" But I've never actually heard&nbsp;that response to \"AGI safety\" in the wild, and AI safety researchers&nbsp;regularly discuss&nbsp;\"<a href=\"http://en.wikipedia.org/wiki/Software_system_safety\">software system safety</a>\" and&nbsp;\"<a href=\"http://smile.amazon.com/Safe-Sound-Artificial-Intelligence-Applications/dp/0262062119/\">AI safety</a>\" and&nbsp;\"<a href=\"http://ww2.cs.mu.oz.au/~unruh/sasemas/2006/index.html\">agent safety</a>\" and&nbsp;more specific topics like \"<a href=\"http://lukemuehlhauser.com/wp-content/uploads/Perkins-Barto-Lyapunov-design-for-safe-reinforcement-learning.pdf\">safe reinforcement learning</a>\" without&nbsp;provoking negative reactions from people&nbsp;doing regular AI research.</p>\n<p>I'm more worried that&nbsp;a term like \"safe AGI\" could provoke&nbsp;a response of \"So you're trying to make sure that a system which is smarter than humans, and able to operate in arbitrary real-world environments, and able to invent new technologies to achieve its goals, will be&nbsp;<em>safe</em>? Let me save you some time and tell you right now that's <em>impossible</em>. Your research program is a pipe dream.\"</p>\n<p>My reply goes something like \"Yeah, it's&nbsp;<em>way</em> beyond our current capabilities, but lots of things that once looked impossible are now feasible&nbsp;because people worked really hard on them for a long time, and&nbsp;we don't think we can get the whole world to promise never to build AGI just because it's hard to make safe, so we're going to give AGI safety a solid try for a few decades and see what&nbsp;can be discovered.\" But that's probably not all&nbsp;<em>that</em> reassuring.</p>\n<p>How about&nbsp;<strong>high-assurance AGI?</strong> In computer science, a \"<a href=\"http://www.sri.com/research-development/high-assurance-systems\">high assurance system</a>\" is one built from the ground up for unusually strong safety and/or security guarantees, because it's going to be used in safety-critical applications where human lives &mdash; or sometimes simply&nbsp;<em>billions of dollars</em> &mdash; are at stake (e.g. autopilot software or Mars rover software). So there's a nice analogy to MIRI's work, where we're trying to figure out what an AGI would look like if it was built from the ground up to get the strongest safety guarantees possible for such an autonomous and capable system.</p>\n<p>I think the main problem with this term is that, quite reasonably, nobody will believe that we&nbsp;can ever get anywhere <em>near</em> as much assurance in the behavior of an AGI as we&nbsp;can in the behavior of, say, <a href=\"http://intelligence.org/2014/02/15/andre-platzer-on-verifying-cyber-physical-systems/\">the&nbsp;relatively limited AI software&nbsp;that controls the European Train Control System</a>. \"High assurance AGI\" sounds a bit like \"Totally safe all-powerful demon lord.\" It sounds even <em>more</em> wildly unimaginable to AI researchers than \"safe AGI.\"</p>\n<p>What about&nbsp;<strong>superintelligence control</strong> or&nbsp;<strong>AGI control</strong>, as in <a href=\"http://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/\">Bostrom (2014)</a>? \"AGI control\" is perhaps more believable than \"high-assurance AGI\" or \"safe AGI,\" since it brings to mind AI <em>containment</em> methods,&nbsp;which sound more feasible&nbsp;to most people than designing an unconstrained&nbsp;AGI that is somehow nevertheless safe. (It's okay if they learn&nbsp;<em>later</em> that containment probably isn't an ultimate solution to the problem.)</p>\n<p>On the other hand, it might provoke a reaction of \"What, you don't think sentient robots have any rights, and you're free to control and confine them in any way you please? You're just repeating the immoral mistakes of the old slavemasters!\" Which of course isn't true, but it takes some time&nbsp;to explain how I&nbsp;can think it's obvious that conscious machines have moral value while also being in favor of AGI control methods.</p>\n<p>How about&nbsp;<strong>ethical AGI?</strong> First, I worry that it sounds too philosophical, and philosophy is widely perceived as a confused, unproductive discipline. Second, I worry that it sounds like the research&nbsp;assumes moral realism, which many (most?) intelligent people reject. Third, it makes it sound like most of the work is in selecting the goal function, which I don't think is true.</p>\n<p>What about&nbsp;<strong>beneficial AGI?</strong> That's better than \"ethical AGI,\" I think, but like \"ethical AGI\" and \"Friendly AI,\" the term sounds less like a serious math and engineering discipline and more like some enclave of crank researchers writing a flurry of words (but no math) about how AGI needs to be \"nice\" and \"trustworthy\" and \"not harmful\" and oh yeah it must be \"virtuous\" too, whatever that means.</p>\n<p>So yeah, I dunno. I think \"AGI safety\" is my least-disliked&nbsp;term these days, but I wish I knew of some better options.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P2evgLpCZA2tRRJAR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 1.7917795119402017e-06, "legacy": true, "legacyId": "26387", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-15T20:42:09.239Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Movie Night: Introduction to Machine Learning", "slug": "meetup-atlanta-movie-night-introduction-to-machine-learning", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7bQGmi2Twcv5KsXDP/meetup-atlanta-movie-night-introduction-to-machine-learning", "pageUrlRelative": "/posts/7bQGmi2Twcv5KsXDP/meetup-atlanta-movie-night-introduction-to-machine-learning", "linkUrl": "https://www.lesswrong.com/posts/7bQGmi2Twcv5KsXDP/meetup-atlanta-movie-night-introduction-to-machine-learning", "postedAtFormatted": "Sunday, June 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Movie%20Night%3A%20Introduction%20to%20Machine%20Learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Movie%20Night%3A%20Introduction%20to%20Machine%20Learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7bQGmi2Twcv5KsXDP%2Fmeetup-atlanta-movie-night-introduction-to-machine-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Movie%20Night%3A%20Introduction%20to%20Machine%20Learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7bQGmi2Twcv5KsXDP%2Fmeetup-atlanta-movie-night-introduction-to-machine-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7bQGmi2Twcv5KsXDP%2Fmeetup-atlanta-movie-night-introduction-to-machine-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11g'>Atlanta Movie Night: Introduction to Machine Learning</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 June 2014 07:00:35PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Stanford professor, Andrew Ng, is teaching a course on Coursera about machine learning (ML). Even if you never write an ML program, the videos of the course provide a nice overview of the applications of ML and the general techniques used. As such, come join us for a movie watching party: we\u2019ll eat popcorn and watch the first\u2019s week\u2019s videos (about 40 minutes), and then discuss what we\u2019ve learned and it\u2019s implications for the creation of truly thinking machines.</p>\n\n<p>See y\u2019all!</p>\n\n<p>As usual, add any topics you\u2019d like to present on or hear about to the following document:\n<a href=\"https://docs.google.com/a/nnadi.org/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit\" rel=\"nofollow\">https://docs.google.com/a/nnadi.org/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11g'>Atlanta Movie Night: Introduction to Machine Learning</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7bQGmi2Twcv5KsXDP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.791851472537733e-06, "legacy": true, "legacyId": "26388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Movie_Night__Introduction_to_Machine_Learning\">Discussion article for the meetup : <a href=\"/meetups/11g\">Atlanta Movie Night: Introduction to Machine Learning</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 June 2014 07:00:35PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Unit L Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Stanford professor, Andrew Ng, is teaching a course on Coursera about machine learning (ML). Even if you never write an ML program, the videos of the course provide a nice overview of the applications of ML and the general techniques used. As such, come join us for a movie watching party: we\u2019ll eat popcorn and watch the first\u2019s week\u2019s videos (about 40 minutes), and then discuss what we\u2019ve learned and it\u2019s implications for the creation of truly thinking machines.</p>\n\n<p>See y\u2019all!</p>\n\n<p>As usual, add any topics you\u2019d like to present on or hear about to the following document:\n<a href=\"https://docs.google.com/a/nnadi.org/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit\" rel=\"nofollow\">https://docs.google.com/a/nnadi.org/document/d/1vYFn08DrveXON0l5tiMzACHzVsf8_Mwfa2ZYfuMzQQQ/edit</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Movie_Night__Introduction_to_Machine_Learning1\">Discussion article for the meetup : <a href=\"/meetups/11g\">Atlanta Movie Night: Introduction to Machine Learning</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Movie Night: Introduction to Machine Learning", "anchor": "Discussion_article_for_the_meetup___Atlanta_Movie_Night__Introduction_to_Machine_Learning", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Movie Night: Introduction to Machine Learning", "anchor": "Discussion_article_for_the_meetup___Atlanta_Movie_Night__Introduction_to_Machine_Learning1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-16T13:12:45.789Z", "modifiedAt": null, "url": null, "title": "Open thread, 16-22 June 2014", "slug": "open-thread-16-22-june-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:07.053Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7D4Qookb2idYAdgEK/open-thread-16-22-june-2014", "pageUrlRelative": "/posts/7D4Qookb2idYAdgEK/open-thread-16-22-june-2014", "linkUrl": "https://www.lesswrong.com/posts/7D4Qookb2idYAdgEK/open-thread-16-22-june-2014", "postedAtFormatted": "Monday, June 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%2016-22%20June%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%2016-22%20June%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7D4Qookb2idYAdgEK%2Fopen-thread-16-22-june-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%2016-22%20June%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7D4Qookb2idYAdgEK%2Fopen-thread-16-22-june-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7D4Qookb2idYAdgEK%2Fopen-thread-16-22-june-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p><a title=\"Previous open thread\" href=\"/r/discussion/lw/kc1/open_thread_915_june_2014/\">Previous open thread</a></p>\n<p>&nbsp;</p>\n<p><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. </span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. </span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7D4Qookb2idYAdgEK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "26391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 174, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yEHxhTmSqHjtRb88C"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-16T17:26:30.329Z", "modifiedAt": null, "url": null, "title": "The Power of Noise", "slug": "the-power-of-noise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:02.707Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NTMAyw3hDn48HaGEZ/the-power-of-noise", "pageUrlRelative": "/posts/NTMAyw3hDn48HaGEZ/the-power-of-noise", "linkUrl": "https://www.lesswrong.com/posts/NTMAyw3hDn48HaGEZ/the-power-of-noise", "postedAtFormatted": "Monday, June 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Power%20of%20Noise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Power%20of%20Noise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNTMAyw3hDn48HaGEZ%2Fthe-power-of-noise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Power%20of%20Noise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNTMAyw3hDn48HaGEZ%2Fthe-power-of-noise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNTMAyw3hDn48HaGEZ%2Fthe-power-of-noise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3261, "htmlBody": "<p><!-- P { margin-bottom: 0.08in; direction: ltr; color: rgb(0, 0, 0); widows: 2; orphans: 2; }A:link { color: rgb(0, 0, 255); -->Recently Luke Muelhauser posed the question, &ldquo;Can noise have power?&rdquo;, which basically asks whether randomization can ever be useful, or whether for every randomized algorithm there is a better deterministic algorithm. This question was posed in response to a <a href=\"/lw/vq/the_weighted_majority_algorithm/\">debate</a> between Eliezer Yudkowsky and Scott Aaronson, in which Eliezer contends that randomness (or, as he calls it, <em>noise</em>) <em>can't</em> ever be helpful, and Scott takes the opposing view. My goal in this essay is to present my own views on this question, as I feel many of them have not yet been brought up in discussion.</p>\n<p>I'll spare the reader some suspense and say that I basically agree with Scott. I also don't think &ndash; as some others have suggested &ndash; that this debate can be chalked up to a dispute about the meaning of words. I really do think that Scott is getting at something important in the points he makes, which may be underappreciated by those without a background in a field such as learning theory or game theory.</p>\n<p>Before I start, I'd like to point out that this is really a debate about Bayesianism in disguise. Suppose that you're a Bayesian, and you have a posterior distribution over the world, and a utility function, and you are contemplating two actions A and B, with expected utilities U(A) and U(B). Then randomly picking between A and B will have expected utility <img src=\"http://www.codecogs.com/png.latex?\\frac{U(A)+U(B)}{2}\" alt=\"\" width=\"54\" height=\"19\" />, and so in particular at least one of A and B must have higher expected utility than randomizing between them. One can extend this argument to show that, for a Bayesian, the best strategy is always deterministic. Scott in fact acknowledges this point, although in slightly different language:</p>\n<p>&ldquo;Randomness provably never helps in average-case complexity (i.e., where you fix the probability distribution over inputs) -- since given any ensemble of strategies, by convexity there must be at least one deterministic strategy in the ensemble that does at least as well as the average.&rdquo; -Scott Aaronson</p>\n<p>I think this point is pretty iron-clad and I certainly don't wish to argue against it. Instead, I'd like to present several examples of scenarios where I will argue that randomness clearly <em>is</em> useful and necessary, and use this to argue that, at least in these scenarios, one should abandon a fully Bayesian stance. At the meta level, this essay is therefore an argument in favor of maintaining multiple paradigms (in the Kuhnian sense) with which to solve problems.</p>\n<p>I will make four separate arguments, paralleling the four main ways in which one might argue for the adoption or dismissal of a paradigm:</p>\n<ol>\n<li>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Randomization is an appropriate tool in many concrete decision-making problems (game theory and Nash equilibria, indivisible goods, randomized controlled trials).</p>\n</li>\n<li>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Worst case analyses (which typically lead to randomization) are often important from an engineering design perspective (modularity of software).</p>\n</li>\n<li>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Random algorithms have important theoretical properties not shared by deterministic algorithms (P vs. BPP).</p>\n</li>\n<li>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Thinking in terms of randomized constructions has solved many problems that would have been difficult or impossible without this perspective (probabilistic method, sampling algorithms).</p>\n</li>\n</ol>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\"><a id=\"more\"></a></p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\"><span style=\"text-decoration: underline;\">1. Concrete usefulness</span></p>\n<p>Two people are playing rock-paper-scissors. After a while one of them starts to lose on average. What should they do? Clearly randomizing will make them stop losing so they should probably do that. I want to contrast this with one of Eliezer's points, which, roughly, is, &ldquo;clearly you should randomize if you're up against some adversarial superintelligence, but in this case we should label it as 'adversarial superintelligence' rather than worst case&rdquo;. Many have taken this as evidence that Scott and Eliezer are really just arguing about the definition of words. I find it difficult to support this conclusion in light of the above rock-paper-scissors example: you don't need to be playing an adversarial superintelligence, you just need to be playing anyone who is better than you at rock-paper-scissors, which is a reasonably common occurrence.</p>\n<p>A related situation shows up in many trade- and bargaining-related situations. For instances, suppose that I have a good that I value at $800, you value the same good at $1,000, but you only have $500 available to spend. We can still make a trade by agreeing that I will give you the good with probability 50%, in exchange for you paying me $450, leading us both to profit by $50 in expectation. This illustrates how randomness can be used to improve market liquidity. You might argue that we could have made the same trade without randomization by finding an event that we both agree has 50% subjective probability mass. But even if <em>you</em> are willing to do that, it is easy to see why another agent may not wish to --- they may not wish to divulge information about their beliefs, they may not trust you to accurately divulge your own beliefs, they may not even represent their beliefs as probabilities in the first place, etc. Since agents with these properties are fairly natural, it seems necessary to randomize unless one is willing to give up on profitable trading opportunities.</p>\n<p>As yet another example of concrete usefulness, let us consider randomized controlled trials --- in other words, using randomness in an experimental design to ensure that we have a representative subset of a population in order to infer a particular causal mechanism. When it is possible to do so, randomization is an incredibly useful aspect of experimental design, freeing us to make strong inferences about the data that would be much more difficult or impossible to make with the same level of confidence without using randomization. For example, say that we want to know whether a job training program improves participants' earnings. If we separate our sample into a treatment and control group using randomization, and we find a real difference in earnings between the two, there is a strong case that the job training program is the reason for the difference in earnings. If we use any other method to separate the two groups, we run the risk that our method for separating treatment from control was correlated with some other factor that can explain a difference in earnings.</p>\n<p>If we follow the &ldquo;randomness never helps&rdquo; mantra here, then presumably we should carefully model all of the possible confounding effects in order to make inferences about the main effect --- but often the purpose of experimentation is to better understand phenomena about which we currently know little, so that it is unreasonable to think that we can actually accurately model every possible confound in this way. In this situation, &ldquo;randomness never helps&rdquo; requires us to pretend to know the answer even in situations where we clearly do not, which seems to me to be problematic. This is despite the fact that there doesn't seem to be any \"superintelligence\" - or even intelligent adversary - involved in such situations.</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0.08in\"><span style=\"text-decoration: underline;\">2. Design properties</span></p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">As noted in the introduction, if we know the probability distribution of the environment, then the optimal strategy is always deterministic (for now, I ignore the issue of whether finding this optimal strategy is feasible, since we are talking about whether randomness helps \"in principle\"). On the other hand, if we do not know the distribution of the environment, there can be good reasons to randomize, since this can help to \"smooth out\" potential unexpected structure (examples include randomized quicksort as well as certain load-balancing algorithms). In other words, if we want to argue that randomness shouldn't help, the most obvious alternative to using randomization would be to reason about the probability distribution of the environment, which seems to match what Eliezer has in mind, as far as I can tell. For instance, Eliezer <a href=\"/lw/vq/the_weighted_majority_algorithm/t6c\">says</a>:</p>\n<blockquote>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">I certainly don't say \"it's not hard work\", and the environmental probability distribution should not look like the probability distribution you have over your random numbers - it should contain correlations and structure. But once you know what your probability distribution is, then you should do your work relative to that, rather than assuming \"worst case\". Optimizing for the worst case in environments that aren't actually adversarial, makes even less sense than assuming the environment is as random and unstructured as thermal noise.</p>\n</blockquote>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Note that reasoning about the probability distribution of the environment requires assuming a particular distribution in the first place. I want to argue that this is often bad from a software design perspective. First, software is supposed to be modular. By this, I mean that software should be broken into components that:</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in; padding-left: 30px;\">1. Have a clearly specified and verifiable input-output contract.</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in; padding-left: 30px;\">2.&nbsp; Are re-usable in a wide variety of situations (for the most part, if the input-output contract can only be satisfied in one instance ever then it isn't a very useful contract).</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Pretty much everyone who works on software agrees with this. However, requiring that the inputs to a piece of software follow some probability distribution is the opposite of being modular. Why is this? First, it is impossible to verify from a single input whether the input contract is satisfied, since the contract is now over the <em>distribution</em> of inputs, rather than a single input. Second, requiring a certain distribution over inputs limits the software to the cases where the inputs actually follow that distribution (or a sufficiently similar distribution), thus greatly limiting the scope of applicability. Even worse, such a distributional requirement propagates backward through the entire system: if A calls B, which calls C, and C requires its inputs to follow a certain probability distribution, then B's contract will probably <em>also</em> have to call for a certain probability distribution over inputs (unless all inputs to C from B are generated without reference to B's inputs). On the other hand, software operating under worst-case analyses will (by definition) work regardless of input, and will not generate assumptions that propagate back to the rest of the system. Therefore it is important to be willing to adopt a worst-case perspective (or at the very least, something weaker than assuming a full distribution over inputs), at least in this instance. If one wants to argue that the randomness in this scenario is only useful practically but that something deterministic would \"in principle\" be better, then they would have to also argue that modularity of software is useful practically but something non-modular would \"in principle\" be better.</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0.08in\"><span style=\"text-decoration: underline;\">3. Theoretical Properties</span></p>\n<p>The P vs. BPP question is the question of whether, whenever there is a polynomial-time randomized algorithm that behaves correctly on every individual input with probability 99.9999999999%, there is also a polynomial-time <em>deterministic</em> algorithm that behaves correctly on every input. If randomness was never useful, we would expect the answer to this question to be straightforwardly yes --- after all, whatever the random algorithm was doing, we should be able to improve upon it, and since a deterministic algorithm is either right or wrong, the only way to improve upon 99.9999999999% is to go all the way up to 100%. In reality, however, this is a long-standing open problem in complexity theory that is generally <em>believed</em> to be true, but which is far from proven.</p>\n<p>This point was already raised by Scott in his original thread, but did not seem to gain much traction. Several people were suspicious of the fact that the randomized algorithm only had to work <em>almost all</em> of the time on each input, while the deterministic algorithm had to work <em>all</em> of the time on each input. I have several responses to this. The first, somewhat flippant, response, is that we could of course only require that the deterministic algorithm work almost all of the time for each input, as well; but a deterministic algorithm can only get the answer right or wrong, so &ldquo;almost always&rdquo; and &ldquo;always&rdquo; amount to the same thing for a determistic algorithm.</p>\n<p>Secondly, let us consider what it should mean for an algorithm to work &ldquo;for all practical purposes&rdquo;. If I have a randomized algorithm, and its probability of failure is so small as to be negligible compared to, say, cosmic rays hitting the computer and corrupting the output, then this seems to meet the bar of &ldquo;for all practical purposes&rdquo; (thus motivating the definition of BPP). On the other hand, what should &ldquo;all practical purposes&rdquo; mean for a deterministic algorithm? Some might argue that if, on average across inputs to the algorithm, the output is very likely to be correct, then that should satisfy the requirement --- but this raises the question, what average should we use? An addition algorithm that thinks that 1+1=3 but is otherwise correct should surely not meet the &ldquo;all practical purposes&rdquo; bar, so the uniform distribution over inputs certainly shouldn't be used. So, perhaps I should be figuring out what I expect my input distribution to look like, and measure according to that? But now, you see, we have added baggage that wasn't in the original problem definition. All <em>I</em> said is that I had some abstract algorithmic problem that I wanted to solve for (some suitable definition of) all practical purposes. <em>You've</em> now told me that in order to do this, I need to figure out what the input distribution to the algorithm is going to be. But this is pretty unreasonable! Can you imagine an undergraduate computer science textbook full of algorithms that would work, but only if its inputs followed such-and-such a distribution? (I'm not claiming that such a guarantee is never useful, only disputing the notion that it should be raised up as the gold standard of algorithms on the same level as P and BPP.) I don't think anyone would learn much from such a textbook, and computer science would be in a pretty woeful state if this was all we could offer. I therefore contend that the only really reasonable notion of <em>deterministically</em> solving a problem for all practical purposes is that it should <em>always</em> work correctly on <em>all possible</em> inputs, thus leading to the complexity class P. This is all a long-winded way of saying that the P vs. BPP question really is the right way to formally ask whether randomness can help from an algorithmic standpoint.</p>\n<p>I'd also like to specially examine what happens if we use a Solomonoff prior (assigning probability&nbsp;<img src=\"http://www.codecogs.com/png.latex?2^{-k}\" alt=\"\" width=\"26\" height=\"16\" /> to programs of length k). If there is <em>any</em> program of length L that solves the problem correctly, then any deterministic algorithm of length M that is incorrect with probability much less than&nbsp;<img src=\"http://www.codecogs.com/png.latex?2^{-M-L}\" alt=\"\" /> with respect to the Solomonoff prior must <em>always</em> work (since we can take the program that runs the two algorithms against each other until it finds a discrepancy, then uses that as the input). Therefore, &ldquo;almost always&rdquo; with respect to Solomonoff is more or less the same as &ldquo;always&rdquo;, so we haven't really done anything different than redefine P in a very roundabout way. Crucially, this means that <strong>even if we are fully Bayesian, under a Solomonoff prior the question of whether every randomized polynomial-time algorithm has a deterministic counterpart </strong><em><strong>still</strong></em><strong> boils down to the unresolved and deep P vs. BPP question.</strong></p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">[<strong><span style=\"text-decoration: underline;\">Addendum</span></strong>: it looks like Peter de Blanc and Wei Dai have made this last point already, see <a href=\"/lw/vq/the_weighted_majority_algorithm/owy\">here</a> and <a href=\"/lw/vq/the_weighted_majority_algorithm/t69\">here</a>.)</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\"><span style=\"text-decoration: underline;\">4. Usefulness in problem-solving</span></p>\n<p>There are important combinatorial properties, such as expansion properties of graphs, which are satisfied by random graphs but which we don't have any deterministic constructions for at all (or in some cases, we do, but they came much later and were harder to come by). Many important problems in graph theory, Ramsey theory, etc. were solved by considering random combinatorial objects (this was one of the great triumphs of Paul Erdos) and thinking in purely deterministic terms seems very unlikely to have solved these problems. Taking a random/pseudorandom perspective has even led to some of the modern triumphs of mathematics such as the Green-Tao theorem (there are arbitrarily long arithmetic progressions) and is fundamental to theoretical cryptography, game theory, convex analysis, and combinatorial approximation algorithms. If we abandon the use of random constructions, then we also abandon some of the most important work in modern computer science, economics, and applied mathematics, which seems unacceptable to me.</p>\n<p>On the algorithmic side, even if you are Bayesian, you want to somehow represent your posterior distribution, and one of the simplest ways to do so is by random samples. An alternative way is by a probability density function, but under extremely widely held complexity-theoretic assumptions there are many probability distributions that can be efficiently represented by samples but whose probability density function cannot be efficiently written down. More practically, the view of representing distributions by random samples has led to many important algorithms such as sequential Monte Carlo (SMC) and Markov Chain Monte Carlo (MCMC), which form some of the cornerstones of modern statistical inference (MCMC in particular has been called one of the 10 greatest algorithms of the 20th century). Again, abandoning this work on philosophical grounds seems unacceptable.</p>\n<p>Of course, it is theoretically possible that all of the above gains will <em>eventually</em> be reproduced without using randomness. However, it seems important to note that many of these are among the most beautiful and elegant results in their respective fields. I would object to any characterization of them as \"hacks\" that come down to a poorly-understood, lucky, but useful result.</p>\n<ul>\n<li>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">The obstacles to \"derandomizing\" such things appear to be quite deep, as extremely intelligent people have been working on them for decades without success. If one wants to say that a randomization-free, Bayesian approach is \"correct even if not practical,\" this becomes true for a very expansive definition of \"practical,\" and one must concede that being a Bayesian is often completely impractical even when a great deal of time and resources are available for reasoning.</p>\n</li>\n<li>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">Thinking in terms of randomness is extremely useful in arriving at these deep and important results, and one of the main purposes of a paradigm is to provide modes of thought that lead to such results. I would therefore argue that randomization should be a fundamental lens (among many) with which to approach problems.</p>\n</li>\n</ul>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\"><span style=\"text-decoration: underline;\">Conclusion</span></p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">I have given several examples of how randomness is useful in both commonplace and fundamental ways. A possible objection is that yes, randomness might be useful from a pragmatic perspective, but it is still the case that in theory we can always do better without randomness. I don't think that such an objection answers the above arguments, except perhaps in some very weak sense. We saw how, in rock-paper-scissors, randomness is always going to be useful for one of the two players (unless both players are equally strong); we saw how using randomness is necessary to make certain profitable trades; and we saw how, even if we adopt a Solomonoff prior, the question of whether or not deterministic algorithms can compete with randomized algorithms is still open (and the likely resolution will rely upon constructing sufficiently <em>pseudorandom</em> numbers, rather than reasoning carefully about the distribution of inputs). Even if we discard all pragmatic concerns whatsoever, the above examples at the very least show that randomization is useful in a very deep and fundamental way. And importantly, at least from my perspective, we have also seen how randomization has been essential in resolving important problems across a variety of fields. This, if nothing else, is a compelling case that it should be adopted as a feature in one's worldview, rather than discarded on philosophical grounds.</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in\"><span style=\"text-decoration: underline;\">Acknowledgements</span></p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in; text-decoration: none\">Thanks to Holden Karnofsky for reading an early version of this essay and suggesting the randomized controlled trials example. Thanks also to Sindy Li for suggesting the idea of using randomness to improve market liquidity.</p>\n<p style=\"margin-top: 0.07in; margin-bottom: 0in; text-decoration: none\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NTMAyw3hDn48HaGEZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 53, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "26392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AAqTP6Q5aeWnoAYr4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-16T17:54:29.282Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, June 16-30", "slug": "group-rationality-diary-june-16-30", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.187Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/onA6deQmBmYXftfq3/group-rationality-diary-june-16-30", "pageUrlRelative": "/posts/onA6deQmBmYXftfq3/group-rationality-diary-june-16-30", "linkUrl": "https://www.lesswrong.com/posts/onA6deQmBmYXftfq3/group-rationality-diary-june-16-30", "postedAtFormatted": "Monday, June 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20June%2016-30&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20June%2016-30%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FonA6deQmBmYXftfq3%2Fgroup-rationality-diary-june-16-30%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20June%2016-30%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FonA6deQmBmYXftfq3%2Fgroup-rationality-diary-june-16-30", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FonA6deQmBmYXftfq3%2Fgroup-rationality-diary-june-16-30", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">This is the public group instrumental rationality diary for June 16-30.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">\n<p style=\"margin: 0px 0px 1em;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Previous diary: <a href=\"/lw/kal/group_rationality_diary_june_115/\">June 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\">Next diary: &nbsp;<a href=\"/r/discussion/lw/kfh/group_rationality_diary_july_115/\">July 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 24.272727966308594px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "onA6deQmBmYXftfq3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "26393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6r7crFbvYustFbLKN", "7SnR7jGWJfXYpWfvB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-16T21:02:18.406Z", "modifiedAt": null, "url": null, "title": "From \"Coulda\" and \"Woulda\" to \"Shoulda\": Predicting Decisions to Minimize Regret for Partially Rational Agents", "slug": "from-coulda-and-woulda-to-shoulda-predicting-decisions-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:02.160Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jxDZtEJfDfhvfHkZB/from-coulda-and-woulda-to-shoulda-predicting-decisions-to", "pageUrlRelative": "/posts/jxDZtEJfDfhvfHkZB/from-coulda-and-woulda-to-shoulda-predicting-decisions-to", "linkUrl": "https://www.lesswrong.com/posts/jxDZtEJfDfhvfHkZB/from-coulda-and-woulda-to-shoulda-predicting-decisions-to", "postedAtFormatted": "Monday, June 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20From%20%22Coulda%22%20and%20%22Woulda%22%20to%20%22Shoulda%22%3A%20Predicting%20Decisions%20to%20Minimize%20Regret%20for%20Partially%20Rational%20Agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrom%20%22Coulda%22%20and%20%22Woulda%22%20to%20%22Shoulda%22%3A%20Predicting%20Decisions%20to%20Minimize%20Regret%20for%20Partially%20Rational%20Agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxDZtEJfDfhvfHkZB%2Ffrom-coulda-and-woulda-to-shoulda-predicting-decisions-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=From%20%22Coulda%22%20and%20%22Woulda%22%20to%20%22Shoulda%22%3A%20Predicting%20Decisions%20to%20Minimize%20Regret%20for%20Partially%20Rational%20Agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxDZtEJfDfhvfHkZB%2Ffrom-coulda-and-woulda-to-shoulda-predicting-decisions-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxDZtEJfDfhvfHkZB%2Ffrom-coulda-and-woulda-to-shoulda-predicting-decisions-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1596, "htmlBody": "<p><strong>TRIGGER WARNING: PHILOSOPHY.&nbsp; All those who believe in truly rigorous, scientifically-grounded reasoning should RUN AWAY VERY QUICKLY.</strong></p>\n<p><strong>Abstract: </strong><em>Human beings want to make rational decisions, but their decision-making processes are often inefficient, and they don't possess direct knowledge of anything we could call their utility functions.&nbsp; Since it is much easier to detect a bad world state than a good one (there are vastly more of them, so less information is needed to classify accurately), humans tend to have an easy time detecting bad states, but this emotional regret is no more useful for formal reasoning about human rationality, since we don't possess a causal model of it in terms of decision histories and outcomes.&nbsp; We tackle this problem head-on, assuming only that humans can reason over a set of beliefs and a perceived state of the world to generate a probability distribution over actions.</em></p>\n<p>Consider rationality: optimizing the world to better and better match a utility function, which is itself <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">complete, transitive, continuous, and gives results which are independent of irrelevant alternatives</a>.&nbsp; Now consider actually existing human beings: creatures who can <a href=\"/lw/my/the_allais_paradox/\">often and easily be tricked into taking Dutch Book bets through exploitation of their cognitive structure</a>, without even having to go to the trouble of actually deceiving them with regards to specific information.</p>\n<p>Consider that being one of <em>those</em> poor sods must <em>totally suck</em>.&nbsp; We believe this provides sufficient motivation for wanting to help them out a bit.&nbsp; Unfortunately, doing so is not very simple: since they didn't evolve as rational creatures, it's very easy to propose an <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">alternate set</a> of <a href=\"/lw/lq/fake_utility_functions/\">values</a> that captures absolutely nothing of what they actually want out of life.&nbsp; In fact, since they didn't even evolve as 100% self-aware creatures, their emotional qualia are not even reliable indicators of anything we would call a proper utility function.&nbsp; <a href=\"http://fas-philosophy.rutgers.edu/chang/Papers/Railton-MoralRealism.pdf\">They know there's something they want out of life, and they know they don't know what it is</a>, but that doesn't help because <em>they still don't know what it is</em>, and knowledge of ignorance does not magically reduce the ignorance.</p>\n<p>So!&nbsp; How can we help them without just overriding them or enslaving them to strange and alien cares?&nbsp; Well, one barest rudiment of rationality with which evolution <em>did</em> manage to bless them is that they don't <em>always</em> end up \"losing\", or suffering.&nbsp; Sometimes, even if only seemingly by luck or by <a href=\"/lw/20l/ureshiku_naritai/\">elaborate and informed self-analysis</a>, they do seem to end up pretty happy with themselves, sometimes even over the long term.&nbsp; We believe that with the door to generating Good Ideas For Humans left open even just this tiny crack, we can construct models of what they ought to be doing.</p>\n<p>Let's begin by assuming <em>away</em> the thing we wish we could construct: the human utility function.&nbsp; We are going to reason as if we have no valid grounds to believe there is any such thing, and make absolutely no reference to anything like one.&nbsp; This will ensure that our reasoning doesn't get circular.&nbsp; Instead of modelling humans as utility maximizers, even flawed ones, we will model them simply as generating a probability distribution over potential actions (from which they would choose their real action) given a set of beliefs and a state of the real world.&nbsp; We will not claim to know or care what <em>causes</em> the probability distribution of potential choices: we just want to construct an algorithm for helping humans know which ones are good.</p>\n<p>We can then model human decision making as a two-player game: the human does something, and Nature responds likewise.&nbsp; <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">Lots of rational agents work this way</a>, so it gives us a more-or-less reasonable way of talking algorithmically about how humans live.&nbsp; For any given human at any given time, we could take a decent-sized Maximegalor Ubercomputer and just run the simulation, yielding a full description of how the human lives.</p>\n<p>The only step where we need to do anything \"weird\" is in abstracting the human's mind and knowledge of the world from the particular state and location of its body at any given timestep in the simulation.&nbsp; This doesn't mean taking it <em>out</em> of the body, but instead considering what the same state of the mind might do if placed in multiple place-times and situations, given everything they've experienced previously.&nbsp; We need this in order to let our simulated humans be genuinely affected and genuinely learn from the consequences of their own actions.</p>\n<p>Our game between the simulated human and simulated Nature thus generates a perfectly ordinary <a href=\"http://www.cs.cmu.edu/~adamchik/15-121/lectures/Game%20Trees/Game%20Trees.html\">game-tree</a> up to some planning horizon H, though it is a <em>probabilistic</em> game tree.&nbsp; Each edge represents a conditional probability of the human or Nature making some move given their current state.&nbsp; The multiplication of all probabilities for all edges along a path from the root-node to a leaf-node represents the conditional probability of that leaf node given the root node.&nbsp; The conditional probabilities attached to <em>all</em> edges leaving an inner node of the tree <em>must</em> sum to 1.0, though there might be a hell of a lot of child nodes.&nbsp; We assume that an actual human <em>would</em> actually execute the <em>most likely</em> action-edge.</p>\n<p>Here is where we actually manage a neat trick for defying the basic human irrationality.&nbsp; We mentioned earlier that while humans are <em>usually</em> pretty bummed out about their past decisions, <em>sometimes</em> they're not.&nbsp; If we can separate bummed-out from not-bummed-out in some formal way, we'll have a rigorous way of talking about what it would mean for a given action or history to be good for the human in question.</p>\n<p>Our proposal is to consider what a human would say if taken back in time and given the opportunity to advise their past self.&nbsp; Or, in simpler simulation terms, we consider how a human's choices would be changed by finding out the leaf-node consequences of their root- or inner-node actions, simply by transferring the relevant beliefs and knowledge directly into our model of their minds.&nbsp; If, upon being given this leaf-node knowledge, the action yielded as most likely changes, <em>or</em> if the version of the human at the leaf-node would, themselves, were they taken back in time, select another action as most likely, then we take a big black meta-magic marker and scribble over that leaf node as suffering from <em>regret</em>.&nbsp; After all, the human in question <em>could</em> have done something their later self would agree with.</p>\n<p>The magic is thus done: coulda + woulda = shoulda.&nbsp; By coloring some (inevitably: most) leaf-nodes as suffering from regret, we can then measure a probability of regret in any human-versus-Nature game-tree up to any planning horizon H: it's just the sum of all conditional probabilities for all paths from the root node which arrive to a regret-colored leaf-node at or before time H.</p>\n<p>We should thus advise the humans to simply treat the probability of arriving to a regret-colored leaf-node as a loss function and minimize it.&nbsp; By construction, this will yield a rational optimization criterion guaranteed not to make the humans run screaming from their own choices, at least not at or before time-step H.</p>\n<p>The further out into time we extend H, the better our advice becomes, as it incorporates a deeper and wider sample of the apparent states which a human life can occupy, thus bringing different motivational <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptations to conscious execution</a>, and allowing their reconciliation via reflection.&nbsp; Over sufficient amounts of time, <a href=\"http://plato.stanford.edu/entries/reflective-equilibrium/\">this reflection could maybe even quiet down to a stable state</a>, resulting in the humans selecting their actions in a way that's more like a rational agent and less like a pre-evolved meat-ape.&nbsp; This would hopefully help their lives be much, much nicer, though we cannot actually formally prove that the limit of the human regret probability converges as the planning horizon grows to plus-infinity -- not even to 1.0!</p>\n<p>We can also note a couple of interesting properties our loss-function for humans has, particularly its degenerate values and how they relate to the psychology of the underlying semi-rational agent, ie: humans.&nbsp; When the probability of regret equals 1.0, no matter how far out we extend the planning horizon H, it means we are simply dealing with a totally, utterly irrational mind-design: there literally does not exist a best possible world for that agent in which they would never wish to change their former choices.&nbsp; They <em>always</em> regret their decisions, which means they've probably got a circular preference or other internal contradiction somewhere.&nbsp; Yikes, though they could just figure out which particular aspect of their own mind-design causes that and eliminate it, leading to an agent design that can potentially ever like its life.&nbsp; The other degenerate probability is also interesting: a chance of regret equalling 0.0 means that the agent is either a completely unreflective idiot, or is God.&nbsp; Even an <a href=\"http://www.hutter1.net/ai/iors.pdf\">optimal superintelligence</a> can suffer loss due to <em>not knowing about its environment</em>; it just rids itself of that ignorance optimally as data comes in!</p>\n<p>The interesting thing about these degenerate probabilities is that they show our theory to be generally applicable to an entire class of semi-rational agents, not just humans.&nbsp; Anything with a non-degenerate regret probability, or rather, any agent whose regret probability does not converge to a degenerate value in the limit, can be labelled semi-rational, and can make productive use of the regret probabilities our construction calculates regarding them to make better decisions -- or at least, decisions they will still endorse when asked later on.</p>\n<p><em>Dropping the sense of humor: This might be semi-useful.&nbsp; Have similar ideas been published in the literature before?&nbsp; And yes, of course I'm human, but it was funnier that way in what would otherwise have been a very dull, dry philosophy post.<br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jxDZtEJfDfhvfHkZB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 1.7940063232605453e-06, "legacy": true, "legacyId": "26357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zJZvoiwydJ5zvzTHK", "NnohDYHNnKDtbiMyp", "xnPFYBuaGhpq869mY", "XPErvb8m9FapXCjhA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T09:12:14.367Z", "modifiedAt": null, "url": null, "title": "Maximize Worst Case Bayes Score", "slug": "maximize-worst-case-bayes-score", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:32.922Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hAbPNGA4pwzFqgLbw/maximize-worst-case-bayes-score", "pageUrlRelative": "/posts/hAbPNGA4pwzFqgLbw/maximize-worst-case-bayes-score", "linkUrl": "https://www.lesswrong.com/posts/hAbPNGA4pwzFqgLbw/maximize-worst-case-bayes-score", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Maximize%20Worst%20Case%20Bayes%20Score&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaximize%20Worst%20Case%20Bayes%20Score%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhAbPNGA4pwzFqgLbw%2Fmaximize-worst-case-bayes-score%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Maximize%20Worst%20Case%20Bayes%20Score%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhAbPNGA4pwzFqgLbw%2Fmaximize-worst-case-bayes-score", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhAbPNGA4pwzFqgLbw%2Fmaximize-worst-case-bayes-score", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 420, "htmlBody": "<p>In this post, I propose an answer to the following question:</p>\n<p>Given a consistent but incomplete theory, how should one choose a random model of that theory?</p>\n<p>My proposal is rather simple. Just assign probabilities to sentences in such that if an adversary were to choose a model, your&nbsp;Worst Case Bayes Score is maximized. This assignment of probabilities represents a probability distribution on models, and choose randomly from this distribution. However, it will take some work to show that what I just described even makes sense. We need to show that Worst Case Bayes Score can be maximized, that such a maximum is unique, and that this assignment of probabilities to sentences represents an actual probability distribution. This post gives the necessary definitions, and proves these three facts.</p>\n<p>Finally, I will show that any given probability assignment is coherent if and only if it is impossible to change the probability assignment in a way that simultaneously improves the Bayes Score by an amount bounded away from 0 in all models.&nbsp;This is nice because it gives us a measure of how far a probability assignment is from being coherent. Namely, we can define the \"incoherence\" of a probability assignment to be the supremum amount by which you can simultaneously improve the Bayes Score in all models. This could be a useful notion since we usually cannot compute a coherent probability assignment so in practice we need to work with incoherent probability assignments which approach a coherent one.</p>\n<p>I wrote up all the definitions and proofs on <a href=\"http://bywayofcontradiction.com/maximize-worst-case-bayes-score/\">my blog</a>, and I do not want to go through the work of translating all of the latex code over here, so you will have to read the rest of the post there. Sorry. In case you do not care enough about this to read the formal definitions, let me just say that my definition of the \"Bayes Score\" of a probability assignment P with respect to a model M is the sum over all true sentences s of m(s)log(P(s)) plus the sum over all false sentences s of m(s)log(1-P(s)), where m is some fixed nowhere zero probability measure on all sentences. (e.g. m(s) is 1/2 &nbsp;to the number of bits needed to encode s)</p>\n<p>I would be very grateful if anyone can come up with a proof that this probability distribution which maximizes Worst Case Bayes Score has the property that its Bayes Score is independent of the choice of what model we use to judge it. I believe it is true, but have not yet found a proof.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hAbPNGA4pwzFqgLbw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 1.7950852561657521e-06, "legacy": true, "legacyId": "26395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T11:12:13.307Z", "modifiedAt": null, "url": null, "title": "[LINK] The future of the Turing test and intelligence measures", "slug": "link-the-future-of-the-turing-test-and-intelligence-measures", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:44.139Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/94Kd4otr4Qh2qHSjn/link-the-future-of-the-turing-test-and-intelligence-measures", "pageUrlRelative": "/posts/94Kd4otr4Qh2qHSjn/link-the-future-of-the-turing-test-and-intelligence-measures", "linkUrl": "https://www.lesswrong.com/posts/94Kd4otr4Qh2qHSjn/link-the-future-of-the-turing-test-and-intelligence-measures", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20future%20of%20the%20Turing%20test%20and%20intelligence%20measures&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20future%20of%20the%20Turing%20test%20and%20intelligence%20measures%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94Kd4otr4Qh2qHSjn%2Flink-the-future-of-the-turing-test-and-intelligence-measures%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20future%20of%20the%20Turing%20test%20and%20intelligence%20measures%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94Kd4otr4Qh2qHSjn%2Flink-the-future-of-the-turing-test-and-intelligence-measures", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F94Kd4otr4Qh2qHSjn%2Flink-the-future-of-the-turing-test-and-intelligence-measures", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p>Following the recent hype over the potential of a machine&nbsp;<a href=\"/lw/kc0/news_turing_test_passed/\">passing of the Turing test</a>, <a href=\"https://www.youtube.com/channel/UCEne3aAgie57IvOZKKSxi9Q\">Adam Ford</a>&nbsp;<a href=\"https://www.youtube.com/watch?v=dt9ZUQOvwkc\">interviews Stuart Armstrong</a>&nbsp;(me) of the FHI about the meaning of the test, how we can expect a future of many upcoming \"Turing test passings\" according to varying criteria of strictness, and how and why we test for intelligence in the first place.</p>\n<p><img src=\"http://images.lesswrong.com/t3_kd8_0.png?v=7e43fc10ae4820dab580c57b6a817fa7\" alt=\"\" width=\"633\" height=\"349\" /></p>\n<p>I predict that we are entering an era where \"X passed the Turing test\" will be a more and more common announcement, followed by long discussions as to whether that was a true pass or not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "94Kd4otr4Qh2qHSjn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 1.7952627150671733e-06, "legacy": true, "legacyId": "26396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ndojns9N4tWAueeSB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T11:33:57.415Z", "modifiedAt": null, "url": null, "title": "Thoughts on Structuring AI Thought", "slug": "thoughts-on-structuring-ai-thought", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oW2orZLeKT8LJevWv/thoughts-on-structuring-ai-thought", "pageUrlRelative": "/posts/oW2orZLeKT8LJevWv/thoughts-on-structuring-ai-thought", "linkUrl": "https://www.lesswrong.com/posts/oW2orZLeKT8LJevWv/thoughts-on-structuring-ai-thought", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20Structuring%20AI%20Thought&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20Structuring%20AI%20Thought%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoW2orZLeKT8LJevWv%2Fthoughts-on-structuring-ai-thought%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20Structuring%20AI%20Thought%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoW2orZLeKT8LJevWv%2Fthoughts-on-structuring-ai-thought", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoW2orZLeKT8LJevWv%2Fthoughts-on-structuring-ai-thought", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1461, "htmlBody": "<p><strong>Introduction</strong></p>\n<p><em>(post has been edited on 17.6 18:50 GMT, there's is an added part)</em><strong><br /></strong></p>\n<p>I'd like to write my thoughts about structuring the psyche for an AI. However I am not an expert on computers, programming or AIs, nor even psychology or the human mind in general. In fact I have very little reason to think it should be me writing about this topic. I think that's ok. It's a thread to share thoughts and interact. Part of the problem is socially managing this message. It requires a certain degree of boldness to present your thoughts. It think also requires a certain degree of uncertainty to improve those thoughts.</p>\n<p>The content of this thread will be on a very general and vague level. I would be more specific, if I could. But it's hard enough as it is. Perhaps this thread will stimulate thoughts for those who are more adept with the matter. Most of how I think about this, originates from my personal <a href=\"http://en.wikipedia.org/wiki/Theory_of_mind\" target=\"_blank\">theory of mind</a>, which probably is not anything special, and therefore it resembles an attempt of emulating the human brain. This according to some posts written on lesswrong would be a socially approved signal for bad thinking perhaps.</p>\n<p>None of this probably makes sense. And it's probably too general to have any real meaning. But I hope this post is useful to stimulate thought if nothing else. I'm a little worried about potential criticism, since I kind of feel like I'm trying to take a too large bit of a cake that's really not meant for me.. But that's ok.</p>\n<p>Let's move on.</p>\n<p>&nbsp;</p>\n<p><strong>Concept of time</strong></p>\n<p>I think understanding the concept of time essentially requires arranging things into a series. Objects in the series can have an order in the sense that some objects come before, others come later. Based on this very simple sequential thought can be constructed a sense for passage of time by adding the notion that the present time can be some object on such series.</p>\n<p>&nbsp;</p>\n<p><strong>Logical sequences as part of sequential reasoning</strong></p>\n<p>If you can model sequences with an idea of something that is now and later or before, it allows formulating some sort of logic with the idea of moving towards some state in that sequence.</p>\n<p>&nbsp;</p>\n<p><strong>Decision making by altering the states of sequences</strong></p>\n<p>So understanding decisions requires the idea of influencing an outcome that resides somewhere along the way of a sequence.</p>\n<p>&nbsp;</p>\n<p><strong>Goals and utility functions</strong></p>\n<p>Goals are intents of changing something that comes later in the sequence. Then what is a wanted outcome is based on somekind of utility function. What the function contains is another thing.</p>\n<p>&nbsp;</p>\n<p><strong>Differentiating statebased reasoning from sequential reasoning</strong></p>\n<p>Statebased reasoning to me means having an environment with flags or stimuli, things that are loaded into the sequence. For an example a series of pictures would be a sequence, while the content of the pictures would be the statebased environment. You can think of this as a plane in a spatial sense. Or temporary noise.</p>\n<p>&nbsp;</p>\n<p><strong>Statebased reasoning overtime</strong></p>\n<p>To formulate states not as single moments but instead short series of moments, in which multiplte vague states are interpolated to form a vague series overtime. So that you don't have for an example fractions of cyllables, but instead those fractions are mashed together as cyllables, words or entire sentences and so forth. This would also include a reward systems feedback.</p>\n<p>&nbsp;</p>\n<p><strong>Predicting change in the background<br /></strong></p>\n<p>Sequential logic allows thinking forward but it should be possible to consider the progression of things as a background function.</p>\n<p>&nbsp;</p>\n<p><strong>Qualitatively different content of environment</strong></p>\n<p>Statebased reasoning should contain all kinds of different things and more precisely they should be qualitatively different. The different kinds of realms of quality can be analoguous to the human brain. For an example, we can have spatial stimulus, emotional stimulus, and so forth.</p>\n<p>&nbsp;</p>\n<p><strong>Goal based reward system, simulation and testing for input<br /></strong></p>\n<p>This is another way of saying utility function again. But to more specific you can have a search function that tests for what the utility function returns, which is sort of like imagining something. For an example, you can think of eating something as a human being, and it gives you some pleasure and an imagined stimulus that is somewhat comparable to actually eating. If the outcome seems positive then and there is input in that case, you can proceed towards executing the plan of actually getting to that simulated state.</p>\n<p>&nbsp;</p>\n<p><strong>Thoughts with multiple qualitatively different components</strong></p>\n<p>Thought should be something that exists on several different qualitatively different types of information. For an example you could have a thought that comprises of sequantial reasonining, environmental or statebased reasoning, spatial reasoning and goal based reasoning.</p>\n<p>&nbsp;</p>\n<p><strong>Agent logic and goal orientation</strong></p>\n<p>Agents could be formulated as things that have qualities or functions. Goals are part of sequential logic.</p>\n<p>&nbsp;</p>\n<p><strong>Analyzing for functions and qualities</strong></p>\n<p>A method of observing objects and linking qualities and functions to them based on different types of information.</p>\n<p>&nbsp;</p>\n<p><strong>Managing goals</strong></p>\n<p>Some kind of logic for managing different kind of goals is necessary so that it can be used to assess the functions and goals of agents. This also means that it will have to manage function between different information types.</p>\n<p>&nbsp;</p>\n<p><strong>Master routines and subroutines - the ability to call subroutines from those qualitatively different types of information handlers</strong></p>\n<p>For an example you can think of a picture and in the picture there is a spatial quality, in which a box is trying to achieve something. But for that to be possible it needs to be possible for the spatial logic to use the agent logic to create it as an agent. And it also needs to use the sequantial and goal oriented logic to be able to formulate the expression that the box has a goal.</p>\n<p>It is also necessary to maintain a structure where there is this primary sequential and statebased logic that tries to do someting, and the content of this master logic should be able to use any part of the information handlers so that they can call each other practically indefinitely.</p>\n<p>&nbsp;</p>\n<p><strong>Tying agent's qualities to functions, sequential and statebased logic<br /></strong></p>\n<p>An object can have several flags for functions, which actually are causing something based on sequential logic, but the changes in teh sequences are actually changes in the based logic, and the qualities themselves are the statebased logic of the object.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Added later:</strong></p>\n<p><strong>Sequences and logic</strong></p>\n<p>I think of sequential logic as the very standard logic, thinking in terms of decision trees and so forth. I believe this is also lateralized to certain brain regions in humans. Amygdala I think have this difference, but have not verified that. It would actually be rhythm, sequence and syntatic processing of language.</p>\n<p><strong>States and logic<br /></strong></p>\n<p>States on the otherhand I think are about induction largescale input. I think somekind of parallel could be drawn to sensing music emotionally and the pitches of notes, perhaps also visuality like seeing colors as radiant and having mor accurate perception in that sense.</p>\n<p>I think an example where these entwine could beif in visual images sequential logic is applied on objects to produce spatial dimensions.</p>\n<p>&nbsp;</p>\n<p><strong>Neural networks and bayes</strong></p>\n<p>I think this kind of entwining of sequences and states, that should be similar to the thought of humans, could be applied on artificial intelligences also. Maybe not.</p>\n<p>From a more accurate point it could mean for an example having some sort of neuralnetworks and using bayesian probabilities to activate nodes based on number of associations. This could be used to produce the priming effect seen in humans.</p>\n<p>So the difference between two is that associations and statebased logic can be <em>acausal</em> and mostly probabilistic, which is closer to emotion and intuition, where as sequential logic or deductive reasoning is <em>causal</em> and more closely related to those already mentioned standard decision trees and logic that distinctly separates objects.</p>\n<p>&nbsp;</p>\n<p><strong>Using free association based on object qualities, flags or associations</strong></p>\n<p>States can be described to include objects which have qualities attached or associated to them. The nature of these qualities can be used to produce intuition or association in that acausal manner.</p>\n<p><br /><strong>Gauging importance based on qualitative definitions for the flagged properties or qualities and guiding selective search for associations</strong></p>\n<p>To draw a parallel with human mind you can have objects tagged with for an example food. Then when food is important, between each neuron there can be a link based on some type of communication pattern of some neurotransmitters - which I don't know too much about - and this can be used to have control on intuition or association based on states. So in otherwords states can control the activation of axons or something like that.</p>\n<p><strong>* end of added part</strong></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I could continue this weird list somewhat further, but Im not sure if it's useful.. Since it would be more this kind of short notes, which might all be equally useless. So instead of continuing...</p>\n<p>What do you think about this so far? Does this make any sense?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oW2orZLeKT8LJevWv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -1, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "26397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong id=\"Introduction\">Introduction</strong></p>\n<p><em>(post has been edited on 17.6 18:50 GMT, there's is an added part)</em><strong><br></strong></p>\n<p>I'd like to write my thoughts about structuring the psyche for an AI. However I am not an expert on computers, programming or AIs, nor even psychology or the human mind in general. In fact I have very little reason to think it should be me writing about this topic. I think that's ok. It's a thread to share thoughts and interact. Part of the problem is socially managing this message. It requires a certain degree of boldness to present your thoughts. It think also requires a certain degree of uncertainty to improve those thoughts.</p>\n<p>The content of this thread will be on a very general and vague level. I would be more specific, if I could. But it's hard enough as it is. Perhaps this thread will stimulate thoughts for those who are more adept with the matter. Most of how I think about this, originates from my personal <a href=\"http://en.wikipedia.org/wiki/Theory_of_mind\" target=\"_blank\">theory of mind</a>, which probably is not anything special, and therefore it resembles an attempt of emulating the human brain. This according to some posts written on lesswrong would be a socially approved signal for bad thinking perhaps.</p>\n<p>None of this probably makes sense. And it's probably too general to have any real meaning. But I hope this post is useful to stimulate thought if nothing else. I'm a little worried about potential criticism, since I kind of feel like I'm trying to take a too large bit of a cake that's really not meant for me.. But that's ok.</p>\n<p>Let's move on.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Concept_of_time\">Concept of time</strong></p>\n<p>I think understanding the concept of time essentially requires arranging things into a series. Objects in the series can have an order in the sense that some objects come before, others come later. Based on this very simple sequential thought can be constructed a sense for passage of time by adding the notion that the present time can be some object on such series.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Logical_sequences_as_part_of_sequential_reasoning\">Logical sequences as part of sequential reasoning</strong></p>\n<p>If you can model sequences with an idea of something that is now and later or before, it allows formulating some sort of logic with the idea of moving towards some state in that sequence.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Decision_making_by_altering_the_states_of_sequences\">Decision making by altering the states of sequences</strong></p>\n<p>So understanding decisions requires the idea of influencing an outcome that resides somewhere along the way of a sequence.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Goals_and_utility_functions\">Goals and utility functions</strong></p>\n<p>Goals are intents of changing something that comes later in the sequence. Then what is a wanted outcome is based on somekind of utility function. What the function contains is another thing.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Differentiating_statebased_reasoning_from_sequential_reasoning\">Differentiating statebased reasoning from sequential reasoning</strong></p>\n<p>Statebased reasoning to me means having an environment with flags or stimuli, things that are loaded into the sequence. For an example a series of pictures would be a sequence, while the content of the pictures would be the statebased environment. You can think of this as a plane in a spatial sense. Or temporary noise.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Statebased_reasoning_overtime\">Statebased reasoning overtime</strong></p>\n<p>To formulate states not as single moments but instead short series of moments, in which multiplte vague states are interpolated to form a vague series overtime. So that you don't have for an example fractions of cyllables, but instead those fractions are mashed together as cyllables, words or entire sentences and so forth. This would also include a reward systems feedback.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Predicting_change_in_the_background\">Predicting change in the background<br></strong></p>\n<p>Sequential logic allows thinking forward but it should be possible to consider the progression of things as a background function.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Qualitatively_different_content_of_environment\">Qualitatively different content of environment</strong></p>\n<p>Statebased reasoning should contain all kinds of different things and more precisely they should be qualitatively different. The different kinds of realms of quality can be analoguous to the human brain. For an example, we can have spatial stimulus, emotional stimulus, and so forth.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Goal_based_reward_system__simulation_and_testing_for_input\">Goal based reward system, simulation and testing for input<br></strong></p>\n<p>This is another way of saying utility function again. But to more specific you can have a search function that tests for what the utility function returns, which is sort of like imagining something. For an example, you can think of eating something as a human being, and it gives you some pleasure and an imagined stimulus that is somewhat comparable to actually eating. If the outcome seems positive then and there is input in that case, you can proceed towards executing the plan of actually getting to that simulated state.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Thoughts_with_multiple_qualitatively_different_components\">Thoughts with multiple qualitatively different components</strong></p>\n<p>Thought should be something that exists on several different qualitatively different types of information. For an example you could have a thought that comprises of sequantial reasonining, environmental or statebased reasoning, spatial reasoning and goal based reasoning.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Agent_logic_and_goal_orientation\">Agent logic and goal orientation</strong></p>\n<p>Agents could be formulated as things that have qualities or functions. Goals are part of sequential logic.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Analyzing_for_functions_and_qualities\">Analyzing for functions and qualities</strong></p>\n<p>A method of observing objects and linking qualities and functions to them based on different types of information.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Managing_goals\">Managing goals</strong></p>\n<p>Some kind of logic for managing different kind of goals is necessary so that it can be used to assess the functions and goals of agents. This also means that it will have to manage function between different information types.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Master_routines_and_subroutines___the_ability_to_call_subroutines_from_those_qualitatively_different_types_of_information_handlers\">Master routines and subroutines - the ability to call subroutines from those qualitatively different types of information handlers</strong></p>\n<p>For an example you can think of a picture and in the picture there is a spatial quality, in which a box is trying to achieve something. But for that to be possible it needs to be possible for the spatial logic to use the agent logic to create it as an agent. And it also needs to use the sequantial and goal oriented logic to be able to formulate the expression that the box has a goal.</p>\n<p>It is also necessary to maintain a structure where there is this primary sequential and statebased logic that tries to do someting, and the content of this master logic should be able to use any part of the information handlers so that they can call each other practically indefinitely.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Tying_agent_s_qualities_to_functions__sequential_and_statebased_logic\">Tying agent's qualities to functions, sequential and statebased logic<br></strong></p>\n<p>An object can have several flags for functions, which actually are causing something based on sequential logic, but the changes in teh sequences are actually changes in the based logic, and the qualities themselves are the statebased logic of the object.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Added_later_\">Added later:</strong></p>\n<p><strong id=\"Sequences_and_logic\">Sequences and logic</strong></p>\n<p>I think of sequential logic as the very standard logic, thinking in terms of decision trees and so forth. I believe this is also lateralized to certain brain regions in humans. Amygdala I think have this difference, but have not verified that. It would actually be rhythm, sequence and syntatic processing of language.</p>\n<p><strong id=\"States_and_logic\">States and logic<br></strong></p>\n<p>States on the otherhand I think are about induction largescale input. I think somekind of parallel could be drawn to sensing music emotionally and the pitches of notes, perhaps also visuality like seeing colors as radiant and having mor accurate perception in that sense.</p>\n<p>I think an example where these entwine could beif in visual images sequential logic is applied on objects to produce spatial dimensions.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Neural_networks_and_bayes\">Neural networks and bayes</strong></p>\n<p>I think this kind of entwining of sequences and states, that should be similar to the thought of humans, could be applied on artificial intelligences also. Maybe not.</p>\n<p>From a more accurate point it could mean for an example having some sort of neuralnetworks and using bayesian probabilities to activate nodes based on number of associations. This could be used to produce the priming effect seen in humans.</p>\n<p>So the difference between two is that associations and statebased logic can be <em>acausal</em> and mostly probabilistic, which is closer to emotion and intuition, where as sequential logic or deductive reasoning is <em>causal</em> and more closely related to those already mentioned standard decision trees and logic that distinctly separates objects.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Using_free_association_based_on_object_qualities__flags_or_associations\">Using free association based on object qualities, flags or associations</strong></p>\n<p>States can be described to include objects which have qualities attached or associated to them. The nature of these qualities can be used to produce intuition or association in that acausal manner.</p>\n<p><br><strong>Gauging importance based on qualitative definitions for the flagged properties or qualities and guiding selective search for associations</strong></p>\n<p>To draw a parallel with human mind you can have objects tagged with for an example food. Then when food is important, between each neuron there can be a link based on some type of communication pattern of some neurotransmitters - which I don't know too much about - and this can be used to have control on intuition or association based on states. So in otherwords states can control the activation of axons or something like that.</p>\n<p><strong id=\"__end_of_added_part\">* end of added part</strong></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I could continue this weird list somewhat further, but Im not sure if it's useful.. Since it would be more this kind of short notes, which might all be equally useless. So instead of continuing...</p>\n<p>What do you think about this so far? Does this make any sense?</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Concept of time", "anchor": "Concept_of_time", "level": 1}, {"title": "Logical sequences as part of sequential reasoning", "anchor": "Logical_sequences_as_part_of_sequential_reasoning", "level": 1}, {"title": "Decision making by altering the states of sequences", "anchor": "Decision_making_by_altering_the_states_of_sequences", "level": 1}, {"title": "Goals and utility functions", "anchor": "Goals_and_utility_functions", "level": 1}, {"title": "Differentiating statebased reasoning from sequential reasoning", "anchor": "Differentiating_statebased_reasoning_from_sequential_reasoning", "level": 1}, {"title": "Statebased reasoning overtime", "anchor": "Statebased_reasoning_overtime", "level": 1}, {"title": "Predicting change in the background", "anchor": "Predicting_change_in_the_background", "level": 1}, {"title": "Qualitatively different content of environment", "anchor": "Qualitatively_different_content_of_environment", "level": 1}, {"title": "Goal based reward system, simulation and testing for input", "anchor": "Goal_based_reward_system__simulation_and_testing_for_input", "level": 1}, {"title": "Thoughts with multiple qualitatively different components", "anchor": "Thoughts_with_multiple_qualitatively_different_components", "level": 1}, {"title": "Agent logic and goal orientation", "anchor": "Agent_logic_and_goal_orientation", "level": 1}, {"title": "Analyzing for functions and qualities", "anchor": "Analyzing_for_functions_and_qualities", "level": 1}, {"title": "Managing goals", "anchor": "Managing_goals", "level": 1}, {"title": "Master routines and subroutines - the ability to call subroutines from those qualitatively different types of information handlers", "anchor": "Master_routines_and_subroutines___the_ability_to_call_subroutines_from_those_qualitatively_different_types_of_information_handlers", "level": 1}, {"title": "Tying agent's qualities to functions, sequential and statebased logic", "anchor": "Tying_agent_s_qualities_to_functions__sequential_and_statebased_logic", "level": 1}, {"title": "Added later:", "anchor": "Added_later_", "level": 1}, {"title": "Sequences and logic", "anchor": "Sequences_and_logic", "level": 1}, {"title": "States and logic", "anchor": "States_and_logic", "level": 1}, {"title": "Neural networks and bayes", "anchor": "Neural_networks_and_bayes", "level": 1}, {"title": "Using free association based on object qualities, flags or associations", "anchor": "Using_free_association_based_on_object_qualities__flags_or_associations", "level": 1}, {"title": "* end of added part", "anchor": "__end_of_added_part", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 24}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T13:44:22.361Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: Many Worlds + Paranoid Debating", "slug": "meetup-canberra-many-worlds-paranoid-debating", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XtDDDEap2KHaRE2R7/meetup-canberra-many-worlds-paranoid-debating", "pageUrlRelative": "/posts/XtDDDEap2KHaRE2R7/meetup-canberra-many-worlds-paranoid-debating", "linkUrl": "https://www.lesswrong.com/posts/XtDDDEap2KHaRE2R7/meetup-canberra-many-worlds-paranoid-debating", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20Many%20Worlds%20%2B%20Paranoid%20Debating&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20Many%20Worlds%20%2B%20Paranoid%20Debating%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXtDDDEap2KHaRE2R7%2Fmeetup-canberra-many-worlds-paranoid-debating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20Many%20Worlds%20%2B%20Paranoid%20Debating%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXtDDDEap2KHaRE2R7%2Fmeetup-canberra-many-worlds-paranoid-debating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXtDDDEap2KHaRE2R7%2Fmeetup-canberra-many-worlds-paranoid-debating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11h'>Canberra: Many Worlds + Paranoid Debating</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 June 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First, I will give a brief presentation about the many worlds interpretation of quantum mechanics, talking about what it is and how it attempts to deal with issues surrounding probability.</p>\n\n<p>Next, we will have another game of paranoid debating. This time, so that the 'mole' knows the answer to the question, we will ask that everyone come up with two questions before the event that they know the answer to - one place to find questions would be digging through Wikipedia's list of lists <a href=\"http://en.wikipedia.org/wiki/List_of_lists\" rel=\"nofollow\">http://en.wikipedia.org/wiki/List_of_lists</a> - and write them down, putting all the questions into the hat. If your question comes up, you will be the mole.</p>\n\n<p>As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the same location.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11h'>Canberra: Many Worlds + Paranoid Debating</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XtDDDEap2KHaRE2R7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.795487797231832e-06, "legacy": true, "legacyId": "26399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__Many_Worlds___Paranoid_Debating\">Discussion article for the meetup : <a href=\"/meetups/11h\">Canberra: Many Worlds + Paranoid Debating</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 June 2014 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">108 North Road, Acton, ACT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>First, I will give a brief presentation about the many worlds interpretation of quantum mechanics, talking about what it is and how it attempts to deal with issues surrounding probability.</p>\n\n<p>Next, we will have another game of paranoid debating. This time, so that the 'mole' knows the answer to the question, we will ask that everyone come up with two questions before the event that they know the answer to - one place to find questions would be digging through Wikipedia's list of lists <a href=\"http://en.wikipedia.org/wiki/List_of_lists\" rel=\"nofollow\">http://en.wikipedia.org/wiki/List_of_lists</a> - and write them down, putting all the questions into the hat. If your question comes up, you will be the mole.</p>\n\n<p>As always, vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>If you use Facebook, please join our group: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a></p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until late in the CSIT building, room N101.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the same location.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__Many_Worlds___Paranoid_Debating1\">Discussion article for the meetup : <a href=\"/meetups/11h\">Canberra: Many Worlds + Paranoid Debating</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: Many Worlds + Paranoid Debating", "anchor": "Discussion_article_for_the_meetup___Canberra__Many_Worlds___Paranoid_Debating", "level": 1}, {"title": "Discussion article for the meetup : Canberra: Many Worlds + Paranoid Debating", "anchor": "Discussion_article_for_the_meetup___Canberra__Many_Worlds___Paranoid_Debating1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T14:32:27.502Z", "modifiedAt": null, "url": null, "title": "[LINK] The errors, insights and lessons of famous AI predictions: preprint", "slug": "link-the-errors-insights-and-lessons-of-famous-ai", "viewCount": null, "lastCommentedAt": "2014-06-18T07:18:18.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Stuart_Armstrong", "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7J7qRjh2veTYGSqMg/link-the-errors-insights-and-lessons-of-famous-ai", "pageUrlRelative": "/posts/7J7qRjh2veTYGSqMg/link-the-errors-insights-and-lessons-of-famous-ai", "linkUrl": "https://www.lesswrong.com/posts/7J7qRjh2veTYGSqMg/link-the-errors-insights-and-lessons-of-famous-ai", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20errors%2C%20insights%20and%20lessons%20of%20famous%20AI%20predictions%3A%20preprint&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20errors%2C%20insights%20and%20lessons%20of%20famous%20AI%20predictions%3A%20preprint%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7J7qRjh2veTYGSqMg%2Flink-the-errors-insights-and-lessons-of-famous-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20errors%2C%20insights%20and%20lessons%20of%20famous%20AI%20predictions%3A%20preprint%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7J7qRjh2veTYGSqMg%2Flink-the-errors-insights-and-lessons-of-famous-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7J7qRjh2veTYGSqMg%2Flink-the-errors-insights-and-lessons-of-famous-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<p>A preprint of the &quot;The errors, insights and lessons of famous AI predictions \u2013 and what they mean for the future&quot; is now <a href=\"http://www.fhi.ox.ac.uk/wp-content/uploads/FAIC.pdf\">available on the FHI&#x27;s website</a>.</p><p>Abstract:</p><p>Predicting the development of artificial intelligence (AI) is a difficult project \u2013 but a vital one, according to some analysts. AI predictions are already abound: but are they reliable? This paper starts by proposing a decomposition schema for classifying them. Then it constructs a variety of theoretical tools for analysing, judging and improving them. These tools are demonstrated by careful analysis of five famous AI predictions: the initial Dartmouth conference, Dreyfus&#x27;s criticism of AI, Searle&#x27;s Chinese room paper, Kurzweil&#x27;s predictions in the Age of Spiritual Machines, and Omohundro&#x27;s \u2018AI drives\u2019 paper. These case studies illustrate several important principles, such as the general overconfidence of experts, the superiority of models over expert judgement and the need for greater uncertainty in all types of predictions. The general reliability of expert judgement in AI timeline predictions is shown to be poor, a result that fits in with previous studies of expert competence.</p><p>The paper was written by me (Stuart Armstrong), Kaj Sotala and Se\u00e1n S. \u00d3 h\u00c9igeartaigh, and is similar to the series of Less Wrong posts starting <a href=\"https://www.lesserwrong.com/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">here</a> and <a href=\"https://www.lesserwrong.com/lw/e36/ai_timeline_predictions_are_we_getting_better/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7J7qRjh2veTYGSqMg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "26401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fHSf8ACvTCvH9fFyd", "47ci9ixyEbGKWENwR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2014-06-17T14:32:27.502Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T16:36:12.409Z", "modifiedAt": null, "url": null, "title": "Value learning: ultra-sophisticated Cake or Death", "slug": "value-learning-ultra-sophisticated-cake-or-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:55.795Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f387EfBAbpSTDerz2/value-learning-ultra-sophisticated-cake-or-death", "pageUrlRelative": "/posts/f387EfBAbpSTDerz2/value-learning-ultra-sophisticated-cake-or-death", "linkUrl": "https://www.lesswrong.com/posts/f387EfBAbpSTDerz2/value-learning-ultra-sophisticated-cake-or-death", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20learning%3A%20ultra-sophisticated%20Cake%20or%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20learning%3A%20ultra-sophisticated%20Cake%20or%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff387EfBAbpSTDerz2%2Fvalue-learning-ultra-sophisticated-cake-or-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20learning%3A%20ultra-sophisticated%20Cake%20or%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff387EfBAbpSTDerz2%2Fvalue-learning-ultra-sophisticated-cake-or-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff387EfBAbpSTDerz2%2Fvalue-learning-ultra-sophisticated-cake-or-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1163, "htmlBody": "<p>Many mooted AI designs rely on \"<a href=\"/lw/f32/value_loading/\">value loading</a>\", the update of the AI&rsquo;s preference function according to evidence it receives. This allows the AI to learn \"moral facts\" by, for instance, interacting with people in conversation (\"<a href=\"https://www.youtube.com/watch?v=rZVjKlBCvhg\">this human also thinks that death is bad and cakes are good</a>&nbsp;&ndash;&nbsp;I'm starting to notice a pattern here\"). The AI has an interim morality system, which it will seek to act on while updating its morality in whatever way it has been programmed to do.</p>\n<p>But there is a problem with this system: the AI already has preferences. It is therefore motivated to update its morality system in a way compatible with its current preferences. If the AI is powerful (or potentially powerful) there are many ways it can do this. It could ask selective questions to get the results it wants (see&nbsp;<a href=\"https://www.youtube.com/watch?v=G0ZZJXw4MTA\">this example</a>). It could ask or refrain from asking about key issues. In extreme cases, it could break out to seize control of the system, threatening or imitating humans so it could give itself the answers it desired.</p>\n<p>Avoiding this problem turned out to be tricky. The&nbsp;<a href=\"/lw/f3v/cake_or_death/\">Cake or Death</a>&nbsp;post demonstrated some of the requirements. If p(C(u)) denotes the probability that utility function u is correct, then the system would update properly if:</p>\n<p style=\"padding-left: 60px;\">Expectation(p(C(u)) | a) = p(C(u)).</p>\n<p>Put simply, this means that the AI cannot take any action that could predictably change its expectation of the correctness of u. This is an analogue of the <a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a> in classical Bayesian updating. If the AI was 50% convinced about u, then it could certainly ask a question that would resolve its doubts, and put p(C(u)) at 100% or 0%. But only as long as it didn't know which moral outcome was more likely.</p>\n<p>That formulation gives too much weight to the default action, though. Inaction is also an action, so a more correct formulation would be that for all actions a and b,</p>\n<p style=\"padding-left: 60px;\">Expectation(p(C(u)) | a) = Expectation(p(C(u)) | b).</p>\n<p>How would this work in practice? Well, suppose an AI was uncertain between whether cake or death was the proper thing, but it knew that if it took action a:\"Ask a human\", the human would answer \"cake\", and it would then update its values to reflect that cake was valuable but death wasn't. However, the above condition means that if the AI instead chose the action b:\"don't ask\", exactly the same thing would happen.</p>\n<p>In practice, this means that as soon as the AI knows that a human would answer \"cake\", it already knows it should value cake, without having to ask. So it will not be tempted to manipulate humans in any way.</p>\n<p><a id=\"more\"></a></p>\n<h2>Combined moral and physical information: ultra-sophisticated Cake or Death</h2>\n<p>The above requirement ensures that the AI would update correctly on purely moral facts (\"cake is good\"), without trying to twist the system. Since it's a Bayesian reasoner, it would follow traditional conservation of expected evidence on standard empirical facts (\"cake is really hard to make\") without causing any problems.</p>\n<p>There is still a problem with combined moral and empirical statements, though. For example, imagine that u<sub>c</sub>&nbsp;is a utility function that values cake at 1 utiliton, u<sub>d</sub>&nbsp;the one that valued death at 1 utiliton, and that the AI currently follows utility 0.5u<sub>c</sub>&nbsp;+ 0.5u<sub>d</sub>&nbsp;(in other words, p(C(u<sub>c</sub>))=p(C(u<sub>d</sub>))=0.5). Further imagine that the AI will get one chance to produce either 1 cake or 1 death, and that exactly one of these tasks is hard. If the AI attempts the easy task, it will succeed with probability 1. If it attempts the hard task, it will succeed with probability 0.1. The AI is ignorant of which task is easy, so it assigns equal probability to the four following worlds:</p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">World</th><th style=\"cursor: text;\">p(C(u<sub>c</sub>))</th><th style=\"cursor: text;\">p(C(u<sub>d</sub>))</th><th style=\"cursor: text;\">p(cake)</th><th style=\"cursor: text;\">p(death)</th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>1</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>2</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>3</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>4</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n</tr>\n</tbody>\n</table>\n<p>Here p(cake) means the probability of cake, should the AI attempt to make it. If the AI doesn't find out any extra information, it will attempt to make a cake or a death at random. This will succeed with probability 0.55, and since it has the split utility 0.5u<sub>c</sub>&nbsp;+ 0.5u<sub>d</sub>&nbsp;it will derive an expected utility of 0.275.</p>\n<p>But now imagine that, from someone whose values it is programmed to trust (and whose judgement it does trust), the AI overhears \"the true morality is the hard task\". In practice, this is informing the AI that it does not live in worlds w<sub>1</sub> (where cake is both good and easy) or w<sub>4</sub> (where death is both good and easy). Critically, this piece of information does not update its values at all - it still follows utility 0.5u<sub>c</sub>&nbsp;+ 0.5u<sub>d</sub>. However, its expected utility is now reduced: even if it learns everything there is to know, it cannot go above 0.1.</p>\n<p>Thus hearing \"the true morality is the hard task\" has significantly reduced the AI's expected utility. It would really have preferred not to have heard this - it would much prefer to have manipulated or murdered the speaker, or simply not talked with them.</p>\n<p>And the \"conservation\" requirements cannot prevent this. Define action a:\"overhear that the true morality is the hard task\" and action b:\"arrange to not hear that sentence\". Then obviously action b does not change its estimation of the correctness of u<sub>c</sub>&nbsp;or u<sub>d</sub>.&nbsp;But we've seen that action a doesn't either! So</p>\n<ul>\n<li>Expectation(p(C(u<sub>c</sub>)) | a) = Expectation(p(C(u<sub>c</sub>)) | b)</li>\n<li>Expectation(p(C(u<sub>d</sub>)) | a) = Expectation(p(C(u<sub>d</sub>)) | b)</li>\n</ul>\n<p>In other words, the requirements above doesn't prevent the AI from wanting to manipulate the content of mixed (moral and empirical) statements. Since the previous two problems were called \"naive\" and \"sophisticated\" Cake or Death, I've chosen to name this the \"ultra-sophisticated\" Cake or Death problem.</p>\n<div><br /></div>\n<h2>Extra conditions</h2>\n<p>The obvious solution is to extend the \"conservation of expected evidence\" idea to cover mixed statements. One idea could be that for all events {A<sub>j</sub>} that are independent of actions a or b, we would want</p>\n<p style=\"padding-left: 60px;\">Expectation(p(PFOR(C(u<sub>i</sub>),A<sub>j</sub>)) | a) = Expectation(p(PFOR(C(u<sub>i</sub>),A<sub>j</sub>)) | b),</p>\n<p>where PFOR(C(u<sub>i</sub>),A<sub>j</sub>) is any <a href=\"https://en.wikipedia.org/wiki/Propositional_formula\">propositional formula</a> made up of terms of the type C(u<sub>i</sub>) and A<sub>j</sub>. These conditions include the previous ones, as C(u) is a (trivial)&nbsp;propositional formula.</p>\n<p>This gives the result that we want for our example. The events \"Cake is hard\" and \"Death is hard\" are certainly independent of whether the AI overhears (action a) or not (action b). Then the propositional formula</p>\n<p style=\"padding-left: 60px;\">(\"Cake is hard\"<strong>&and;</strong>C(u<sub>c</sub>))&nbsp;<strong>&or;</strong>&nbsp;(\"Death is hard\"<strong>&and;</strong>C(u<sub>d</sub>))</p>\n<p>encodes \"the true morality is the hard task\". The above conditions means that if the AI knows that it will know that statement to be true after overhearing (action a), then it will know it without needing to do overhear (action b). So action b gives no advantage, and the AI is not tempted to manipulate the system.</p>\n<p>Is this also a sufficient condition for proper value loading (do we also need to include non-independent events)? I feel that it is sufficient. Especially since you can construct independent event by splicing dependent events together and conjuncting the result. But I also felt the original expectation requirements were sufficient, so my intuition is probably not reliable on this.</p>\n<p>In any case, I hope to soon be able to show a system that works, even without knowing the exact sufficient conditions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f387EfBAbpSTDerz2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 18, "extendedScore": null, "score": 1.7957420575717417e-06, "legacy": true, "legacyId": "25850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Many mooted AI designs rely on \"<a href=\"/lw/f32/value_loading/\">value loading</a>\", the update of the AI\u2019s preference function according to evidence it receives. This allows the AI to learn \"moral facts\" by, for instance, interacting with people in conversation (\"<a href=\"https://www.youtube.com/watch?v=rZVjKlBCvhg\">this human also thinks that death is bad and cakes are good</a>&nbsp;\u2013&nbsp;I'm starting to notice a pattern here\"). The AI has an interim morality system, which it will seek to act on while updating its morality in whatever way it has been programmed to do.</p>\n<p>But there is a problem with this system: the AI already has preferences. It is therefore motivated to update its morality system in a way compatible with its current preferences. If the AI is powerful (or potentially powerful) there are many ways it can do this. It could ask selective questions to get the results it wants (see&nbsp;<a href=\"https://www.youtube.com/watch?v=G0ZZJXw4MTA\">this example</a>). It could ask or refrain from asking about key issues. In extreme cases, it could break out to seize control of the system, threatening or imitating humans so it could give itself the answers it desired.</p>\n<p>Avoiding this problem turned out to be tricky. The&nbsp;<a href=\"/lw/f3v/cake_or_death/\">Cake or Death</a>&nbsp;post demonstrated some of the requirements. If p(C(u)) denotes the probability that utility function u is correct, then the system would update properly if:</p>\n<p style=\"padding-left: 60px;\">Expectation(p(C(u)) | a) = p(C(u)).</p>\n<p>Put simply, this means that the AI cannot take any action that could predictably change its expectation of the correctness of u. This is an analogue of the <a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a> in classical Bayesian updating. If the AI was 50% convinced about u, then it could certainly ask a question that would resolve its doubts, and put p(C(u)) at 100% or 0%. But only as long as it didn't know which moral outcome was more likely.</p>\n<p>That formulation gives too much weight to the default action, though. Inaction is also an action, so a more correct formulation would be that for all actions a and b,</p>\n<p style=\"padding-left: 60px;\">Expectation(p(C(u)) | a) = Expectation(p(C(u)) | b).</p>\n<p>How would this work in practice? Well, suppose an AI was uncertain between whether cake or death was the proper thing, but it knew that if it took action a:\"Ask a human\", the human would answer \"cake\", and it would then update its values to reflect that cake was valuable but death wasn't. However, the above condition means that if the AI instead chose the action b:\"don't ask\", exactly the same thing would happen.</p>\n<p>In practice, this means that as soon as the AI knows that a human would answer \"cake\", it already knows it should value cake, without having to ask. So it will not be tempted to manipulate humans in any way.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Combined_moral_and_physical_information__ultra_sophisticated_Cake_or_Death\">Combined moral and physical information: ultra-sophisticated Cake or Death</h2>\n<p>The above requirement ensures that the AI would update correctly on purely moral facts (\"cake is good\"), without trying to twist the system. Since it's a Bayesian reasoner, it would follow traditional conservation of expected evidence on standard empirical facts (\"cake is really hard to make\") without causing any problems.</p>\n<p>There is still a problem with combined moral and empirical statements, though. For example, imagine that u<sub>c</sub>&nbsp;is a utility function that values cake at 1 utiliton, u<sub>d</sub>&nbsp;the one that valued death at 1 utiliton, and that the AI currently follows utility 0.5u<sub>c</sub>&nbsp;+ 0.5u<sub>d</sub>&nbsp;(in other words, p(C(u<sub>c</sub>))=p(C(u<sub>d</sub>))=0.5). Further imagine that the AI will get one chance to produce either 1 cake or 1 death, and that exactly one of these tasks is hard. If the AI attempts the easy task, it will succeed with probability 1. If it attempts the hard task, it will succeed with probability 0.1. The AI is ignorant of which task is easy, so it assigns equal probability to the four following worlds:</p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">World</th><th style=\"cursor: text;\">p(C(u<sub>c</sub>))</th><th style=\"cursor: text;\">p(C(u<sub>d</sub>))</th><th style=\"cursor: text;\">p(cake)</th><th style=\"cursor: text;\">p(death)</th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>1</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>2</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>3</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">w<sub>4</sub></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">0.1</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1</td>\n</tr>\n</tbody>\n</table>\n<p>Here p(cake) means the probability of cake, should the AI attempt to make it. If the AI doesn't find out any extra information, it will attempt to make a cake or a death at random. This will succeed with probability 0.55, and since it has the split utility 0.5u<sub>c</sub>&nbsp;+ 0.5u<sub>d</sub>&nbsp;it will derive an expected utility of 0.275.</p>\n<p>But now imagine that, from someone whose values it is programmed to trust (and whose judgement it does trust), the AI overhears \"the true morality is the hard task\". In practice, this is informing the AI that it does not live in worlds w<sub>1</sub> (where cake is both good and easy) or w<sub>4</sub> (where death is both good and easy). Critically, this piece of information does not update its values at all - it still follows utility 0.5u<sub>c</sub>&nbsp;+ 0.5u<sub>d</sub>. However, its expected utility is now reduced: even if it learns everything there is to know, it cannot go above 0.1.</p>\n<p>Thus hearing \"the true morality is the hard task\" has significantly reduced the AI's expected utility. It would really have preferred not to have heard this - it would much prefer to have manipulated or murdered the speaker, or simply not talked with them.</p>\n<p>And the \"conservation\" requirements cannot prevent this. Define action a:\"overhear that the true morality is the hard task\" and action b:\"arrange to not hear that sentence\". Then obviously action b does not change its estimation of the correctness of u<sub>c</sub>&nbsp;or u<sub>d</sub>.&nbsp;But we've seen that action a doesn't either! So</p>\n<ul>\n<li>Expectation(p(C(u<sub>c</sub>)) | a) = Expectation(p(C(u<sub>c</sub>)) | b)</li>\n<li>Expectation(p(C(u<sub>d</sub>)) | a) = Expectation(p(C(u<sub>d</sub>)) | b)</li>\n</ul>\n<p>In other words, the requirements above doesn't prevent the AI from wanting to manipulate the content of mixed (moral and empirical) statements. Since the previous two problems were called \"naive\" and \"sophisticated\" Cake or Death, I've chosen to name this the \"ultra-sophisticated\" Cake or Death problem.</p>\n<div><br></div>\n<h2 id=\"Extra_conditions\">Extra conditions</h2>\n<p>The obvious solution is to extend the \"conservation of expected evidence\" idea to cover mixed statements. One idea could be that for all events {A<sub>j</sub>} that are independent of actions a or b, we would want</p>\n<p style=\"padding-left: 60px;\">Expectation(p(PFOR(C(u<sub>i</sub>),A<sub>j</sub>)) | a) = Expectation(p(PFOR(C(u<sub>i</sub>),A<sub>j</sub>)) | b),</p>\n<p>where PFOR(C(u<sub>i</sub>),A<sub>j</sub>) is any <a href=\"https://en.wikipedia.org/wiki/Propositional_formula\">propositional formula</a> made up of terms of the type C(u<sub>i</sub>) and A<sub>j</sub>. These conditions include the previous ones, as C(u) is a (trivial)&nbsp;propositional formula.</p>\n<p>This gives the result that we want for our example. The events \"Cake is hard\" and \"Death is hard\" are certainly independent of whether the AI overhears (action a) or not (action b). Then the propositional formula</p>\n<p style=\"padding-left: 60px;\">(\"Cake is hard\"<strong>\u2227</strong>C(u<sub>c</sub>))&nbsp;<strong>\u2228</strong>&nbsp;(\"Death is hard\"<strong>\u2227</strong>C(u<sub>d</sub>))</p>\n<p>encodes \"the true morality is the hard task\". The above conditions means that if the AI knows that it will know that statement to be true after overhearing (action a), then it will know it without needing to do overhear (action b). So action b gives no advantage, and the AI is not tempted to manipulate the system.</p>\n<p>Is this also a sufficient condition for proper value loading (do we also need to include non-independent events)? I feel that it is sufficient. Especially since you can construct independent event by splicing dependent events together and conjuncting the result. But I also felt the original expectation requirements were sufficient, so my intuition is probably not reliable on this.</p>\n<p>In any case, I hope to soon be able to show a system that works, even without knowing the exact sufficient conditions.</p>", "sections": [{"title": "Combined moral and physical information: ultra-sophisticated Cake or Death", "anchor": "Combined_moral_and_physical_information__ultra_sophisticated_Cake_or_Death", "level": 1}, {"title": "Extra conditions", "anchor": "Extra_conditions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Z8WRsxYjmrxNGyPPk", "6bdb4F6Lif5AanRAd", "jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T16:36:26.714Z", "modifiedAt": null, "url": null, "title": "An extended class of utility functions", "slug": "an-extended-class-of-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:39.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GBbazoeMsAFvJyMCt/an-extended-class-of-utility-functions", "pageUrlRelative": "/posts/GBbazoeMsAFvJyMCt/an-extended-class-of-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/GBbazoeMsAFvJyMCt/an-extended-class-of-utility-functions", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20extended%20class%20of%20utility%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20extended%20class%20of%20utility%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGBbazoeMsAFvJyMCt%2Fan-extended-class-of-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20extended%20class%20of%20utility%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGBbazoeMsAFvJyMCt%2Fan-extended-class-of-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGBbazoeMsAFvJyMCt%2Fan-extended-class-of-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 706, "htmlBody": "<p><em>This is a technical result that I wanted to check before writing up a major piece on <a href=\"/lw/f32/value_loading/\">value loading</a>.</em></p>\n<p>The purpose of a utility function is to give an agent criteria with which to make a decision. If two utility functions always give the same decisions, they're generally considered the same utility function. So, for instance, the utility function u always gives the same decisions as u+C for some constant C, or Du for some positive constant D. Thus we can say that utility functions are equivalent if they are related by a positive&nbsp;<a href=\"https://en.wikipedia.org/wiki/Affine_transformation\">affine transformation</a>.</p>\n<p>For specific utility functions, and specific agents, the class of functions that give the same decisions is quite a bit larger. For instance, imagine that v is a utility function with the property v(\"any universe which contains humans\")=constant. Then any human who attempts to follow u, could equivalently follow u+v (neglecting acausal trade) - it makes no difference. In general, if no action the agent could ever take would change the value of v, then u and u+v give the same decisions.</p>\n<p>More subtly, if the agent can change v but cannot change the expectation of v, then u and u+v still give the same decisions. This is because for any actions a and b the agent could take:</p>\n<p style=\"padding-left: 30px;\"><strong>E</strong>(u+v | a) = <strong>E</strong>(u | a) + <strong>E</strong>(v | a) = <strong>E</strong>(u | a) + <strong>E</strong>(v | b).</p>\n<p>Hence <strong>E</strong>(u+v | a) &gt; <strong>E</strong>(u+v | b) if and only if <strong>E</strong>(u | a) &gt; <strong>E</strong>(u | b), and so the decision hasn't changed.</p>\n<p>Note that <strong>E</strong>(v | a) need not be constant for all actions: simply that for every actions and b that an agent could take at a particular decision point, <strong>E</strong>(v | a) = <strong>E</strong>(v | b). It's perfectly possible for the expectation of v to be different at different moments, or conditional on different decisions made at different times.</p>\n<p>Finally, as long as v obeys the above properties, there is no reason for it to be a utility function in the classical sense - it could be constructed any way we want.</p>\n<p>&nbsp;</p>\n<h2>An example: suffer not from probability, nor benefit from it</h2>\n<p>The preceding seems rather abstract, but here is the motivating example. It's a correction term T that adds or subtracts utility, as external evidence comes in (it's important that the evidence is external - the agent gets no correction from knowing what its own actions are/were). If the AI knows evidence e, and new (external) evidence f comes in, then its utility gets adjusted by T(e,f) which is defined as</p>\n<p style=\"padding-left: 60px;\">T(e,f) = <strong>E</strong>(u | e) - <strong>E</strong>(u | e, f)</p>\n<p>In other words, the agents utility gets adjusted by the difference between the new expected utility and the old - and hence the agent's expected utility is unchanged by new external evidence.</p>\n<p>Consider for instance an agent with a utility u linear in money. It much choose between a bet that goes 50-50 on $0 (heads) or $100 (tails), versus a sure $49. It correctly choose the bet, having an expected utility of u=$50 - in other words, <strong>E</strong>(u, bet)=$50. But now imagine that the coin comes out heads. The utility u plunges to $0 (in other words <strong>E</strong>(u | bet, heads)=0). But the correction term cancels that out:</p>\n<p style=\"padding-left: 60px;\">u(bet, heads) + T(bet, heads) = $0 + <strong>E</strong>(u | bet) - <strong>E</strong>(u |bet, heads) = $0 + $50 -$0 = $50.</p>\n<p>A similar effect leaves utility unchanging if the coin is tails, cancelling the increase. In other words, adding the T correction term removes the impact of stochastic effects on utility.</p>\n<p>But the agent will still make the same decisions. This is because before seeing evidence f, it cannot predict its impact on EU(u). In other words, summing over all possible evidences f:</p>\n<p style=\"padding-left: 60px;\"><strong>E</strong>(u | e) =&nbsp;<strong>&Sigma;</strong>&nbsp;p(f)<strong>E</strong>(u | e, f),</p>\n<p>which is another way of phrasing \"<a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a>\". This implies that</p>\n<p style=\"padding-left: 60px;\"><strong>E</strong>(T(e,-)) =&nbsp;<strong>&Sigma;</strong>&nbsp;p(f)T(e,f)</p>\n<p style=\"padding-left: 60px;\">=&nbsp;<strong>&Sigma;</strong>&nbsp;p(f)((<strong>E</strong>(u | e) - <strong>E</strong>(u | e, f))</p>\n<p style=\"padding-left: 60px;\">= <strong>E</strong>(u | e) -&nbsp;<strong>&Sigma;</strong>&nbsp;p(f)<strong>E</strong>(u | e, f)</p>\n<p style=\"padding-left: 60px;\">= 0,</p>\n<p>and hence that adding the T term does not change the agent's decisions. All the various corrections add on to the utility as the agent continues making decisions, but none of them make the agent change what it does.</p>\n<p>The relevance of this will be explained in a subsequent post (unless someone finds an error here).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GBbazoeMsAFvJyMCt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.795742410348266e-06, "legacy": true, "legacyId": "26402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Z8WRsxYjmrxNGyPPk", "jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T19:29:30.438Z", "modifiedAt": null, "url": null, "title": "[LINK] Counterfactual Strategies", "slug": "link-counterfactual-strategies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:25.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Strilanc", "createdAt": "2012-07-25T07:36:05.321Z", "isAdmin": false, "displayName": "Strilanc"}, "userId": "nBwAhG4QXmGpxLB2F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J3sNM9EPJ7mLPZWZr/link-counterfactual-strategies", "pageUrlRelative": "/posts/J3sNM9EPJ7mLPZWZr/link-counterfactual-strategies", "linkUrl": "https://www.lesswrong.com/posts/J3sNM9EPJ7mLPZWZr/link-counterfactual-strategies", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Counterfactual%20Strategies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Counterfactual%20Strategies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ3sNM9EPJ7mLPZWZr%2Flink-counterfactual-strategies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Counterfactual%20Strategies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ3sNM9EPJ7mLPZWZr%2Flink-counterfactual-strategies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ3sNM9EPJ7mLPZWZr%2Flink-counterfactual-strategies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p>&nbsp;</p>\n<h2>Link:</h2>\n<p><a href=\"http://strilanc.com/game-theory/2014/06/17/Counterintuitive-Counterfactual-Strategies.html\">Counterintuitive Counterfactual Strategies</a></p>\n<p>&nbsp;</p>\n<h2>Overview:</h2>\n<p>Over the weekend, I was thinking about the variant of <a href=\"http://en.wikipedia.org/wiki/Newcomb's_paradox\">Newcomb's Paradox</a> where both boxes are transparent. The one where, unless you precommit to taking a visibly empty box instead of both boxes, omega can self-consistently give you less money.</p>\n<p>I was wondering if I could make this kind of \"sacrifice yourself for yourself\" situation happen without involving a predictor guessing your choice before you made it. Turns out you can.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J3sNM9EPJ7mLPZWZr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.7959985521756989e-06, "legacy": true, "legacyId": "26403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<h2 id=\"Link_\">Link:</h2>\n<p><a href=\"http://strilanc.com/game-theory/2014/06/17/Counterintuitive-Counterfactual-Strategies.html\">Counterintuitive Counterfactual Strategies</a></p>\n<p>&nbsp;</p>\n<h2 id=\"Overview_\">Overview:</h2>\n<p>Over the weekend, I was thinking about the variant of <a href=\"http://en.wikipedia.org/wiki/Newcomb's_paradox\">Newcomb's Paradox</a> where both boxes are transparent. The one where, unless you precommit to taking a visibly empty box instead of both boxes, omega can self-consistently give you less money.</p>\n<p>I was wondering if I could make this kind of \"sacrifice yourself for yourself\" situation happen without involving a predictor guessing your choice before you made it. Turns out you can.</p>", "sections": [{"title": "Link:", "anchor": "Link_", "level": 1}, {"title": "Overview:", "anchor": "Overview_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T20:10:05.039Z", "modifiedAt": null, "url": null, "title": "Rationalist Sport", "slug": "rationalist-sport", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:05.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MathiasZaman", "createdAt": "2013-10-30T20:53:21.742Z", "isAdmin": false, "displayName": "MathiasZaman"}, "userId": "WJrLEPYHTBtC9tHzu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LMLoWa8pYHPjCatM9/rationalist-sport", "pageUrlRelative": "/posts/LMLoWa8pYHPjCatM9/rationalist-sport", "linkUrl": "https://www.lesswrong.com/posts/LMLoWa8pYHPjCatM9/rationalist-sport", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Sport&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Sport%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMLoWa8pYHPjCatM9%2Frationalist-sport%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Sport%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMLoWa8pYHPjCatM9%2Frationalist-sport", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMLoWa8pYHPjCatM9%2Frationalist-sport", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>This post is a bit of an experiment; Most of the time, Discussion post lay out an idea and this idea then get commented upon. This post, on the other hand, will be purely about discussion on a topic. If this works out well, I'll might post more of these in the future.</p>\n<p>On to the meat of this post:</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>I got this idea from a <a href=\"http://www.reddit.com/r/LessWrong/comments/26p3cr/rationalist_sport/\">reddit post on /r/LessWrong.<br /></a></p>\n<p>To quote:</p>\n<blockquote>\n<p>I remember my sociology textbook explaining that sports often show what a group values. Bowling was popular with machinists and factory workers who valued repeated precision, while American football was watched and played by a culture that valued extreme specialization of individuals, but able to work as a team.</p>\n<p>So I've been thinking about what we would value in a sport, and what sport we could create that exemplifies those values (not interested in picking an existing sport).</p>\n</blockquote>\n<p>So have at it.</p>\n<p>I only ask for one thing and that is to hold off on proposing solutions for 24 hours before giving suggestions for actual sports. In the first 24 hours, please discuss what makes current sports unappealing to rationalists and what aspects a sports designed for aspiring rationalists should have.</p>\n<p><strong>Edit: The 24-hour window has closed and solutions and suggestions can now be given.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LMLoWa8pYHPjCatM9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 1.7960586173401949e-06, "legacy": true, "legacyId": "26404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-17T22:26:31.924Z", "modifiedAt": null, "url": null, "title": "Two kinds of population ethics, and Current-Population Utilitarianism", "slug": "two-kinds-of-population-ethics-and-current-population", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xXkn8xqAeimnLjwa4/two-kinds-of-population-ethics-and-current-population", "pageUrlRelative": "/posts/xXkn8xqAeimnLjwa4/two-kinds-of-population-ethics-and-current-population", "linkUrl": "https://www.lesswrong.com/posts/xXkn8xqAeimnLjwa4/two-kinds-of-population-ethics-and-current-population", "postedAtFormatted": "Tuesday, June 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20kinds%20of%20population%20ethics%2C%20and%20Current-Population%20Utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20kinds%20of%20population%20ethics%2C%20and%20Current-Population%20Utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxXkn8xqAeimnLjwa4%2Ftwo-kinds-of-population-ethics-and-current-population%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20kinds%20of%20population%20ethics%2C%20and%20Current-Population%20Utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxXkn8xqAeimnLjwa4%2Ftwo-kinds-of-population-ethics-and-current-population", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxXkn8xqAeimnLjwa4%2Ftwo-kinds-of-population-ethics-and-current-population", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1167, "htmlBody": "<p style=\"margin-bottom: 0in;\">There are two different kinds of questions that could be considered to fall under the subject of population ethics: &ldquo;What sorts of altruistic preferences do I have about the well-being of others?&rdquo;, and &ldquo;Given all the preferences of each individual, how should we compromise?&rdquo;. In other words, the first question asks how everyone's experiential utility functions (which measure quality of life) contribute to my (or your) decision-theoretic utility function (which takes into account everything that I or you, respectively, care about), and the second asks how we should agree to aggregate our decision-theoretic utility functions into something that we can jointly optimize for. When people talk about population ethics, they often do not make it clear which of these they are referring to, but they are different questions, and I think the difference is important.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For example, suppose Alice, Bob, and Charlie are collaborating on a project to create an artificial superintelligence that will take over the universe and optimize it according to their preferences. But they face a problem: they have different preferences. Alice is a total utilitarian, so she wants to maximize the sum of everyone's experiential utility. Bob is an average utilitarian, so he wants to maximize the average of everyone's experiential utility. Charlie is an egoist, so he wants to maximize his own experiential utility. As a result, Alice, Bob, and Charlie have some disagreements over how their AI should handle decisions that affect the number of people in existence, or which involve tradeoffs between Charlie and people other than Charlie. They at first try to convince each other of the correctness of their view, but they eventually realize that they don't actually have any factual disagreement; they just value different things. As a compromise, They program their AI to maximize the average of everyone's experiential utility, plus half of Charlie's experiential utility, plus a trillionth of the sum of everyone's experiential utility.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Of course, there are other ways for utility functions to differ than average versus total utilitarianism and altruism versus egoism. Maybe you care about something other than the experiences of yourself and others. Or maybe your altruistic preferences about someone else's experiences differs from their selfish preferences, like how a crack addict wants to get more crack while their family wants them not to.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Anyway, the point is, there are many ways to aggregate everyone's experiential utility functions, and not everyone will agree on one of them. In fact, since people can care about things other than experiences, many people might not like any of them. It seems silly to suggest that we would want a Friendly AI to maximize an aggregation of everyone's experiential utility functions; there would be potentially irresolvable disagreements over which aggregation to use, and any of them would exclude non-experiential preferences. Since decision-theoretic utility functions actually take into account all of an agent's preferences, it makes much more sense to try to get a superintelligence to maximize an aggregation of decision-theoretic utility functions.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The obvious next question is which aggregation of decision-theoretic utility functions to use. One might think that average and total utilitarianism could both be applied to decision-theoretic utility functions, but that is actually not so easy. Decision-theoretic utility functions take into account everything the agent cares about, which can include things that happen in the far future, after the agent dies. With a dynamic population, it is unclear which utility functions should be included in the aggregation. Should every agent that does or ever will exist have their utility function included? If so, then the aggregation would indicate that humans should be replaced with large numbers of agents whose preferences are easier to satisfy<sup>1</sup>&nbsp;(this is true even for average utilitarianism, because there needs to be enough of these agents to drown out the difficult-to-satisfy human preferences in the aggregation). Should the aggregation be dynamic with the population, so that at time t, the preferences of agents who exist at time t are taken into account? That would be dynamically inconsistent. In a population of sadists who want to torture people (but only people who don't want to be tortured), the aggregation would indicate that they should create some people and then torture them. But then once the new people are created, the aggregation would take their preferences into account and indicate that they should not be tortured.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I suggest a variant that I'm tentatively calling current-population utilitarianism: Aggregate the preferences of the people who are alive right now, and then leave this aggregated utility function fixed even as the population and their preferences change. By &ldquo;right now&rdquo;, I don't mean June 17, 2014 at 10:26 pm GMT; I mean the late pre-singularity era as a whole. Why? Because this is when the people who have the power to affect the creation of the AGI that we will want to maximize said aggregated utility function live. If it were just up to me, I would program an AGI to maximize my own utility function<sup>2</sup>, but one person cannot do that on their own, and I don't expect I'd be able to get very many other people to go along with that. But all the people who will be contributing to an FAI project, and everyone whose support they can seek, all live in the near-present. No one else can support or undermine an FAI project, so why make any sacrifices for them for any reason other than that you (or someone who can support or undermine you) care about them (in which case their preferences will show up in the aggregation through your utility function)? Now I'll address some anticipated objections.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Objection: Doesn't that mean that people created post-singularity will be discriminated against?</p>\n<p style=\"margin-bottom: 0in;\">Answer: To the extent that you want people created post-singularity not to be discriminated against, this will be included in your utility function.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Objection: What about social progress? Cultural values change over time, and only taking into account the preferences of people alive now would force cultural values to stagnate.</p>\n<p style=\"margin-bottom: 0in;\">Answer: To the extent that you want cultural values to be able to drift, this will be included in your utility function.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Objection: What if my utility function changes in the future?</p>\n<p style=\"margin-bottom: 0in;\">Answer: To the extent that you want your future utility function to be satisfied, this will be included in your utility function.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Objection: Poor third-worlders also cannot support or undermine an FAI project. Why include them but not people created post-singularity?</p>\n<p style=\"margin-bottom: 0in;\">Answer: Getting public support requires some degree of political correctness. If we tried to rally people around the cause of creating a superintelligence that will maximize the preferences of rich first-worlders, I don't think that would go over very well.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<div id=\"sdfootnote1\">\n<p class=\"sdfootnote\"><sup>1&nbsp;</sup>One utility function being easier to satisfy than another doesn't actually mean anything without some way of normalizing the utility function, but since aggregations require somehow normalizing the utility functions anyway, I'll ignore that problem.</p>\n</div>\n<div id=\"sdfootnote2\">\n<p class=\"sdfootnote\"><sup>2&nbsp;</sup>This is not a proclamation of extreme selfishness. I'm still talking about my decision-theoretic utility function, which is defined, roughly speaking, as what I would maximize if I had godlike powers, and is at least somewhat altruistic.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"uG75MELqjCEfciaRp": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xXkn8xqAeimnLjwa4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.7962606258632357e-06, "legacy": true, "legacyId": "26405", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T04:00:05.196Z", "modifiedAt": null, "url": null, "title": "On Terminal Goals and Virtue Ethics", "slug": "on-terminal-goals-and-virtue-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:36.831Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gR6H3egpRPNYnoTrA/on-terminal-goals-and-virtue-ethics", "pageUrlRelative": "/posts/gR6H3egpRPNYnoTrA/on-terminal-goals-and-virtue-ethics", "linkUrl": "https://www.lesswrong.com/posts/gR6H3egpRPNYnoTrA/on-terminal-goals-and-virtue-ethics", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Terminal%20Goals%20and%20Virtue%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Terminal%20Goals%20and%20Virtue%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgR6H3egpRPNYnoTrA%2Fon-terminal-goals-and-virtue-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Terminal%20Goals%20and%20Virtue%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgR6H3egpRPNYnoTrA%2Fon-terminal-goals-and-virtue-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgR6H3egpRPNYnoTrA%2Fon-terminal-goals-and-virtue-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1280, "htmlBody": "<h2>Introduction</h2>\n<p>A few months ago, my friend said the following thing to me: &ldquo;After seeing <a href=\"http://en.wikipedia.org/wiki/Divergent_(film)\">Divergent</a>, I finally understand virtue ethics. The main character is a cross between Aristotle and you.&rdquo;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">That was an impossible-to-resist pitch, and I saw the movie. The thing that resonated most with me&ndash;also the thing that my friend thought I had in common with the main character&ndash;was the idea that you could make a particular decision, and set yourself down a particular course of action, in order to <em>make yourself become a particular kind of person. </em></span><span lang=\"EN-GB\">Tris didn&rsquo;t join the Dauntless cast because she thought they were doing the most good in society, or because she thought her comparative advantage to do good lay there&ndash;she chose it because they were brave, and she wasn&rsquo;t, yet, and she wanted to be. Bravery was a virtue that she thought she ought to have. If the graph of her motivations even went any deeper, the only node beyond &lsquo;become brave&rsquo; was &lsquo;become good.&rsquo;</span>&nbsp;</p>\n<p class=\"MsoNormal\">(Tris did have a concept of some future world-outcomes being better than others, and wanting to have an effect on the world. But that wasn't the causal reason why she chose Dauntless; as far as I can tell, it was unrelated.)</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">My twelve-year-old self had a similar attitude. I read a lot of fiction, and stories had heroes, and I wanted to be like them&ndash;and that meant acquiring the right skills and the right traits. I knew I was terrible at <a href=\"/lw/4fo/ability_to_react/\">reacting under pressure</a>&ndash;that in the case of an earthquake or other natural disaster, I would freeze up and not be useful at all. Being good at reacting under pressure was an important trait for a hero to have. I could be sad that I didn&rsquo;t have it, or I could decide to acquire it by doing the things that scared me over and over and over again. So that someday, when the world tried to throw bad things at my friends and family, I&rsquo;d be <em>ready</em></span><span lang=\"EN-GB\">.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">You could call that an awfully passive way to look at things. It reveals a deep-seated belief that I&rsquo;m not in control, that the world is big and complicated and beyond my ability to understand and predict, much less steer&ndash;that I am not the locus of control. But this way of thinking is an algorithm. It will almost always spit out an answer, when otherwise I might get stuck in the complexity and unpredictability of trying to make a particular outcome happen.</span></p>\n<h2><br /></h2>\n<h2><span lang=\"EN-GB\">Virtue Ethics</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I find the different houses of the <a href=\"http://hpmor.com/\">HPMOR universe</a> to be a very compelling metaphor. It&rsquo;s not because they suggest actions to take; instead, they suggest virtues to focus on, so that when a particular situation comes up, you can act &lsquo;in character.&rsquo; Courage and bravery for Gryffindor, for example. It also suggests the idea that different people can focus on different virtues&ndash;diversity is a useful thing to have in the world. (I'm probably mangling the concept of <a href=\"http://en.wikipedia.org/wiki/Virtue_ethics\">virtue ethics</a> here, not having any background in philosophy, but it's the closest term for the thing I mean.)</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I&rsquo;ve thought a lot about the virtue of loyalty. In the past, loyalty has kept me with jobs and friends that, from an objective perspective, might not seem like the optimal things to spend my time on. But the costs of quitting and finding a new job, or cutting off friendships, wouldn&rsquo;t just have been about direct consequences in the world, like needing to spend a bunch of time handing out resumes or having an unpleasant conversation. There would also be a shift within myself, a weakening in the drive towards loyalty. It wasn&rsquo;t that I thought <em>everyone </em></span><span lang=\"EN-GB\">ought to be extremely loyal&ndash;it&rsquo;s a virtue with obvious downsides and failure modes. But it was a virtue that <em>I </em>wanted, partly because it seemed undervalued.&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">By calling myself a &lsquo;loyal person&rsquo;, I can aim myself in a particular direction without having to understand all the subcomponents of the world. More importantly, I can make decisions even when I&rsquo;m rushed, or tired, or under cognitive strain that makes it hard to calculate through all of the consequences of a particular action. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">Terminal Goals</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The Less Wrong/CFAR/rationalist community puts a lot of emphasis on a different way of trying to be a hero&ndash;where you start from a <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\">terminal goal</a>, like &ldquo;saving the world&rdquo;, and break it into subgoals, and do whatever it takes to accomplish it. In the past I&rsquo;ve thought of myself as being mostly consequentialist, in terms of morality, and this is a very consequentialist way to think about being a good person. And it doesn't feel like it would work.&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">There are some bad reasons why it might feel wrong&ndash;i.e. that it feels arrogant to think you can accomplish something that big&ndash;but I think the main reason is that it feels <em>fake. </em></span><span lang=\"EN-GB\">There is strong social pressure in the CFAR/Less Wrong community to claim that you have terminal goals, that you&rsquo;re working towards something big. My <a href=\"/lw/87m/review_of_kahneman_thinking_fast_and_slow_2011/\">System 2</a> understands terminal goals and consequentialism, as a thing that other people do&ndash;I could talk about my terminal goals, and get the points, and fit in, but I&rsquo;d be lying about my thoughts. My model of my mind would be incorrect, and that would have consequences on, for example, whether my plans actually worked. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">Practicing the art of rationality</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Recently, <a href=\"/user/AnnaSalamon/\">Anna Salamon</a> brought up a question with the other CFAR staff: &ldquo;What is the thing that&rsquo;s wrong with your own practice of the art of rationality?&rdquo; The terminal goals thing was what I thought of immediately&ndash;namely, the conversations I've had over the past two years, where other rationalists have asked me \"so what are your terminal goals/values?\" and I've stammered something and then gone to hide in a corner and try to come up with some.&nbsp;</span></p>\n<p class=\"MsoNormal\">In <a href=\"/user/Alicorn/\">Alicorn</a>&rsquo;s <a href=\"http://luminous.elcenia.com/\">Luminosity</a>, Bella says about her thoughts that &ldquo;they were liable to morph into versions of themselves that were more idealized, more consistent - and not what they were originally, and therefore false. Or they'd be forgotten altogether, which was even worse (those thoughts were mine, and I wanted them).&rdquo;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I want to know true things about myself. I also want to impress my friends by having the traits that they think are cool, but not at the price of faking it&ndash;my brain <em>screams </em></span><span lang=\"EN-GB\">that pretending to be something other than what you are isn&rsquo;t virtuous. When my immediate response to someone asking me about my terminal goals is &ldquo;but brains don&rsquo;t work that way!&rdquo; it may not be a true statement about all brains, but it&rsquo;s a true statement about <em>my </em></span><span lang=\"EN-GB\">brain. My motivational system is wired in a certain way. I could think it was broken; I could let my friends convince me that I needed to change, and try to shoehorn my brain into a different shape; or I could accept that it <em>works</em></span><span lang=\"EN-GB\">, that I get things done and people find me useful to have around and <em>this is how I am. </em>For now. I'm not going to rule out future attempts to hack my brain, because Growth Mindset, and maybe some other reasons will convince me that it's important enough, but if I do it, it'll be on my terms.&nbsp;</span>Other people are welcome to have their terminal goals and existential struggles. I&rsquo;m okay the way I am&ndash;I have an algorithm to follow.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">Why write this post?</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">It would be an awfully surprising coincidence if mine was the <em>only </em></span><span lang=\"EN-GB\">brain that worked this way. I&rsquo;m not a special snowflake. And other people who interact with the Less Wrong community might not deal with it the way I do. They might try to twist their brains into the &lsquo;right&rsquo; shape, and break their motivational system. Or they might decide that rationality is stupid and walk away.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"eamWQNQ2dPYWEwhqr": 1, "nSHiKwWyMZFdZg5qt": 1, "GBpwq8cWvaeRoE9X5": 1, "8uNFGxejo5hykCEez": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gR6H3egpRPNYnoTrA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": 98, "extendedScore": null, "score": 0.000267, "legacy": true, "legacyId": "26390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Introduction\">Introduction</h2>\n<p>A few months ago, my friend said the following thing to me: \u201cAfter seeing <a href=\"http://en.wikipedia.org/wiki/Divergent_(film)\">Divergent</a>, I finally understand virtue ethics. The main character is a cross between Aristotle and you.\u201d</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">That was an impossible-to-resist pitch, and I saw the movie. The thing that resonated most with me\u2013also the thing that my friend thought I had in common with the main character\u2013was the idea that you could make a particular decision, and set yourself down a particular course of action, in order to <em>make yourself become a particular kind of person. </em></span><span lang=\"EN-GB\">Tris didn\u2019t join the Dauntless cast because she thought they were doing the most good in society, or because she thought her comparative advantage to do good lay there\u2013she chose it because they were brave, and she wasn\u2019t, yet, and she wanted to be. Bravery was a virtue that she thought she ought to have. If the graph of her motivations even went any deeper, the only node beyond \u2018become brave\u2019 was \u2018become good.\u2019</span>&nbsp;</p>\n<p class=\"MsoNormal\">(Tris did have a concept of some future world-outcomes being better than others, and wanting to have an effect on the world. But that wasn't the causal reason why she chose Dauntless; as far as I can tell, it was unrelated.)</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">My twelve-year-old self had a similar attitude. I read a lot of fiction, and stories had heroes, and I wanted to be like them\u2013and that meant acquiring the right skills and the right traits. I knew I was terrible at <a href=\"/lw/4fo/ability_to_react/\">reacting under pressure</a>\u2013that in the case of an earthquake or other natural disaster, I would freeze up and not be useful at all. Being good at reacting under pressure was an important trait for a hero to have. I could be sad that I didn\u2019t have it, or I could decide to acquire it by doing the things that scared me over and over and over again. So that someday, when the world tried to throw bad things at my friends and family, I\u2019d be <em>ready</em></span><span lang=\"EN-GB\">.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">You could call that an awfully passive way to look at things. It reveals a deep-seated belief that I\u2019m not in control, that the world is big and complicated and beyond my ability to understand and predict, much less steer\u2013that I am not the locus of control. But this way of thinking is an algorithm. It will almost always spit out an answer, when otherwise I might get stuck in the complexity and unpredictability of trying to make a particular outcome happen.</span></p>\n<h2><br></h2>\n<h2 id=\"Virtue_Ethics\"><span lang=\"EN-GB\">Virtue Ethics</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I find the different houses of the <a href=\"http://hpmor.com/\">HPMOR universe</a> to be a very compelling metaphor. It\u2019s not because they suggest actions to take; instead, they suggest virtues to focus on, so that when a particular situation comes up, you can act \u2018in character.\u2019 Courage and bravery for Gryffindor, for example. It also suggests the idea that different people can focus on different virtues\u2013diversity is a useful thing to have in the world. (I'm probably mangling the concept of <a href=\"http://en.wikipedia.org/wiki/Virtue_ethics\">virtue ethics</a> here, not having any background in philosophy, but it's the closest term for the thing I mean.)</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I\u2019ve thought a lot about the virtue of loyalty. In the past, loyalty has kept me with jobs and friends that, from an objective perspective, might not seem like the optimal things to spend my time on. But the costs of quitting and finding a new job, or cutting off friendships, wouldn\u2019t just have been about direct consequences in the world, like needing to spend a bunch of time handing out resumes or having an unpleasant conversation. There would also be a shift within myself, a weakening in the drive towards loyalty. It wasn\u2019t that I thought <em>everyone </em></span><span lang=\"EN-GB\">ought to be extremely loyal\u2013it\u2019s a virtue with obvious downsides and failure modes. But it was a virtue that <em>I </em>wanted, partly because it seemed undervalued.&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">By calling myself a \u2018loyal person\u2019, I can aim myself in a particular direction without having to understand all the subcomponents of the world. More importantly, I can make decisions even when I\u2019m rushed, or tired, or under cognitive strain that makes it hard to calculate through all of the consequences of a particular action. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Terminal_Goals\"><span lang=\"EN-GB\">Terminal Goals</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The Less Wrong/CFAR/rationalist community puts a lot of emphasis on a different way of trying to be a hero\u2013where you start from a <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\">terminal goal</a>, like \u201csaving the world\u201d, and break it into subgoals, and do whatever it takes to accomplish it. In the past I\u2019ve thought of myself as being mostly consequentialist, in terms of morality, and this is a very consequentialist way to think about being a good person. And it doesn't feel like it would work.&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">There are some bad reasons why it might feel wrong\u2013i.e. that it feels arrogant to think you can accomplish something that big\u2013but I think the main reason is that it feels <em>fake. </em></span><span lang=\"EN-GB\">There is strong social pressure in the CFAR/Less Wrong community to claim that you have terminal goals, that you\u2019re working towards something big. My <a href=\"/lw/87m/review_of_kahneman_thinking_fast_and_slow_2011/\">System 2</a> understands terminal goals and consequentialism, as a thing that other people do\u2013I could talk about my terminal goals, and get the points, and fit in, but I\u2019d be lying about my thoughts. My model of my mind would be incorrect, and that would have consequences on, for example, whether my plans actually worked. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Practicing_the_art_of_rationality\"><span lang=\"EN-GB\">Practicing the art of rationality</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Recently, <a href=\"/user/AnnaSalamon/\">Anna Salamon</a> brought up a question with the other CFAR staff: \u201cWhat is the thing that\u2019s wrong with your own practice of the art of rationality?\u201d The terminal goals thing was what I thought of immediately\u2013namely, the conversations I've had over the past two years, where other rationalists have asked me \"so what are your terminal goals/values?\" and I've stammered something and then gone to hide in a corner and try to come up with some.&nbsp;</span></p>\n<p class=\"MsoNormal\">In <a href=\"/user/Alicorn/\">Alicorn</a>\u2019s <a href=\"http://luminous.elcenia.com/\">Luminosity</a>, Bella says about her thoughts that \u201cthey were liable to morph into versions of themselves that were more idealized, more consistent - and not what they were originally, and therefore false. Or they'd be forgotten altogether, which was even worse (those thoughts were mine, and I wanted them).\u201d</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I want to know true things about myself. I also want to impress my friends by having the traits that they think are cool, but not at the price of faking it\u2013my brain <em>screams </em></span><span lang=\"EN-GB\">that pretending to be something other than what you are isn\u2019t virtuous. When my immediate response to someone asking me about my terminal goals is \u201cbut brains don\u2019t work that way!\u201d it may not be a true statement about all brains, but it\u2019s a true statement about <em>my </em></span><span lang=\"EN-GB\">brain. My motivational system is wired in a certain way. I could think it was broken; I could let my friends convince me that I needed to change, and try to shoehorn my brain into a different shape; or I could accept that it <em>works</em></span><span lang=\"EN-GB\">, that I get things done and people find me useful to have around and <em>this is how I am. </em>For now. I'm not going to rule out future attempts to hack my brain, because Growth Mindset, and maybe some other reasons will convince me that it's important enough, but if I do it, it'll be on my terms.&nbsp;</span>Other people are welcome to have their terminal goals and existential struggles. I\u2019m okay the way I am\u2013I have an algorithm to follow.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Why_write_this_post_\"><span lang=\"EN-GB\">Why write this post?</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">It would be an awfully surprising coincidence if mine was the <em>only </em></span><span lang=\"EN-GB\">brain that worked this way. I\u2019m not a special snowflake. And other people who interact with the Less Wrong community might not deal with it the way I do. They might try to twist their brains into the \u2018right\u2019 shape, and break their motivational system. Or they might decide that rationality is stupid and walk away.</span></p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Virtue Ethics", "anchor": "Virtue_Ethics", "level": 1}, {"title": "Terminal Goals", "anchor": "Terminal_Goals", "level": 1}, {"title": "Practicing the art of rationality", "anchor": "Practicing_the_art_of_rationality", "level": 1}, {"title": "Why write this post?", "anchor": "Why_write_this_post_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "207 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 207, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p67vi8kk3LJCjL2v7", "PipGvHwA9ZxYNgTyW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T05:30:08.776Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City, UT: Summer Solstice 2014", "slug": "meetup-salt-lake-city-ut-summer-solstice-2014", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s5GhnTdrsPRFQpTgG/meetup-salt-lake-city-ut-summer-solstice-2014", "pageUrlRelative": "/posts/s5GhnTdrsPRFQpTgG/meetup-salt-lake-city-ut-summer-solstice-2014", "linkUrl": "https://www.lesswrong.com/posts/s5GhnTdrsPRFQpTgG/meetup-salt-lake-city-ut-summer-solstice-2014", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%2C%20UT%3A%20Summer%20Solstice%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%2C%20UT%3A%20Summer%20Solstice%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs5GhnTdrsPRFQpTgG%2Fmeetup-salt-lake-city-ut-summer-solstice-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%2C%20UT%3A%20Summer%20Solstice%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs5GhnTdrsPRFQpTgG%2Fmeetup-salt-lake-city-ut-summer-solstice-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs5GhnTdrsPRFQpTgG%2Fmeetup-salt-lake-city-ut-summer-solstice-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11i'>Salt Lake City, UT: Summer Solstice 2014</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 June 2014 02:00:13PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">9771 S. 170 E.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Summer Solstice 2014\nSunday June 22, 2014 2pm\n9771 S. 170 E. Sandy</p>\n\n<p>This is an event to bring all the friends and family to. There will be singing, food, friends, and most of all, an opportunity to bring the best of yourself.\nThere is one rule: that everyone who comes brings a gift. Everyone. Not just any gift; it must be something of your own make and one for all to enjoy.\nWhat do you have to share?</p>\n\n<ul>\n<li>A few words from the heart</li>\n<li>A song to sing</li>\n<li>A story to tell</li>\n<li>A lesson to give</li>\n<li>A moment of entertainment</li>\n<li>A craft or creation</li>\n<li>Whatever else you can think of.</li>\n</ul>\n\n<p>But please, don't stress yourself over it. This is not a talent contest. Showboating is highly discouraged. You can make a contribution as small or grand as you like. Whether you spend 4 days or 4 seconds on it what we really want to see is something that is <em>yours</em>.</p>\n\n<p>The specific aims of this ritual are to invoke:</p>\n\n<ul>\n<li>A sense of unity and connection within the community * Pride in teamwork and contribution as part of achievement</li>\n<li>Appreciation for the contributions of others, and for the unique set of circumstances that brought about Us.</li>\n<li>Determination to go on succeeding at life</li>\n</ul>\n\n<p>The plan is to do a practice run first on Saturday 2pm, anyone who wants to come and be part of the decisive effort to figure out what works is welcome to that. Come an hour before the real one if you'd like some time and materials to make a last minute project :P</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11i'>Salt Lake City, UT: Summer Solstice 2014</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s5GhnTdrsPRFQpTgG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.7968880340181931e-06, "legacy": true, "legacyId": "26407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City__UT__Summer_Solstice_2014\">Discussion article for the meetup : <a href=\"/meetups/11i\">Salt Lake City, UT: Summer Solstice 2014</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 June 2014 02:00:13PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">9771 S. 170 E.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Summer Solstice 2014\nSunday June 22, 2014 2pm\n9771 S. 170 E. Sandy</p>\n\n<p>This is an event to bring all the friends and family to. There will be singing, food, friends, and most of all, an opportunity to bring the best of yourself.\nThere is one rule: that everyone who comes brings a gift. Everyone. Not just any gift; it must be something of your own make and one for all to enjoy.\nWhat do you have to share?</p>\n\n<ul>\n<li>A few words from the heart</li>\n<li>A song to sing</li>\n<li>A story to tell</li>\n<li>A lesson to give</li>\n<li>A moment of entertainment</li>\n<li>A craft or creation</li>\n<li>Whatever else you can think of.</li>\n</ul>\n\n<p>But please, don't stress yourself over it. This is not a talent contest. Showboating is highly discouraged. You can make a contribution as small or grand as you like. Whether you spend 4 days or 4 seconds on it what we really want to see is something that is <em>yours</em>.</p>\n\n<p>The specific aims of this ritual are to invoke:</p>\n\n<ul>\n<li>A sense of unity and connection within the community * Pride in teamwork and contribution as part of achievement</li>\n<li>Appreciation for the contributions of others, and for the unique set of circumstances that brought about Us.</li>\n<li>Determination to go on succeeding at life</li>\n</ul>\n\n<p>The plan is to do a practice run first on Saturday 2pm, anyone who wants to come and be part of the decisive effort to figure out what works is welcome to that. Come an hour before the real one if you'd like some time and materials to make a last minute project :P</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City__UT__Summer_Solstice_20141\">Discussion article for the meetup : <a href=\"/meetups/11i\">Salt Lake City, UT: Summer Solstice 2014</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City, UT: Summer Solstice 2014", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City__UT__Summer_Solstice_2014", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City, UT: Summer Solstice 2014", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City__UT__Summer_Solstice_20141", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T07:37:19.917Z", "modifiedAt": null, "url": null, "title": "Depression's evolutionary roots", "slug": "depression-s-evolutionary-roots", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.255Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TobyBartels", "createdAt": "2010-07-22T02:29:20.470Z", "isAdmin": false, "displayName": "TobyBartels"}, "userId": "ZS9o62ou6aQMDj5yn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/geewH3No97uue3JCx/depression-s-evolutionary-roots", "pageUrlRelative": "/posts/geewH3No97uue3JCx/depression-s-evolutionary-roots", "linkUrl": "https://www.lesswrong.com/posts/geewH3No97uue3JCx/depression-s-evolutionary-roots", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Depression's%20evolutionary%20roots&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADepression's%20evolutionary%20roots%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgeewH3No97uue3JCx%2Fdepression-s-evolutionary-roots%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Depression's%20evolutionary%20roots%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgeewH3No97uue3JCx%2Fdepression-s-evolutionary-roots", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgeewH3No97uue3JCx%2Fdepression-s-evolutionary-roots", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 343, "htmlBody": "<p>Since there are intelligent people here who follow the topic of evolutionary psychology, I'd like to hear opinions about some research from 2009.&nbsp; Particularly if this idea seems reasonable or not, but possibly other opinions that people might have about it.</p>\n<p>The idea is a variation on one that's somewhat popular here: that some conditions usually regarded as mental illnesses (Asperger's for example) are beneficial, even adaptive.&nbsp; But the condition in question now is depression.&nbsp; Briefly, the argument is that depression, at least when it is a response to stimuli and not a permanent feature, can have the useful effect of encouraging more rational thought when this is particularly important, even at the cost of quality of life, and that this is adaptive.</p>\n<p>Links: a <a href=\"http://www.scientificamerican.com/article/depressions-evolutionary/\">Scientific American article</a>, a <a href=\"http://psycnet.apa.org/index.cfm?fa=search.displayRecord&amp;uid=2009-10379-009\">journal article</a> (which I haven't read, behind a $12 paywall).&nbsp; Here's the abstract of the journal article:</p>\n<blockquote>\n<p>Depression is the primary emotional condition for which help is sought. Depressed people often report persistent rumination, which involves analysis, and complex social problems in their lives. Analysis is often a useful approach for solving complex problems, but it requires slow, sustained processing, so disruption would interfere with problem solving. The analytical rumination hypothesis proposes that depression is an evolved response to complex problems, whose function is to minimize disruption and sustain analysis of those problems by (a) giving the triggering problem prioritized access to processing resources, (b) reducing the desire to engage in distracting activities (anhedonia), and (c) producing psychomotor changes that reduce exposure to distracting stimuli. As processing resources are limited, sustained analysis of the triggering problem reduces the ability to concentrate on other things. The hypothesis is supported by evidence from many levels&mdash;genes, neurotransmitters and their receptors, neurophysiology, neuroanatomy, neuroenergetics, pharmacology, cognition, behavior, and efficacy of treatments. In addition, the hypothesis provides explanations for puzzling findings in the depression literature, challenges the belief that serotonin transmission is low in depression, and has implications for treatment.</p>\n</blockquote>\n<p>The full journal citation is Andrews, Paul W., and Thomson Jr., J. Anderson;\tJuly 2009; Psychological Review 116 (3), 620&ndash;654; doi 10.1037/a0016242.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "geewH3No97uue3JCx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.797076482377398e-06, "legacy": true, "legacyId": "26408", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T09:16:31.166Z", "modifiedAt": null, "url": null, "title": "Flowers for Algernon", "slug": "flowers-for-algernon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cuP4arTmCejujLqSQ/flowers-for-algernon", "pageUrlRelative": "/posts/cuP4arTmCejujLqSQ/flowers-for-algernon", "linkUrl": "https://www.lesswrong.com/posts/cuP4arTmCejujLqSQ/flowers-for-algernon", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Flowers%20for%20Algernon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFlowers%20for%20Algernon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcuP4arTmCejujLqSQ%2Fflowers-for-algernon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Flowers%20for%20Algernon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcuP4arTmCejujLqSQ%2Fflowers-for-algernon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcuP4arTmCejujLqSQ%2Fflowers-for-algernon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p>Daniel Keyes, the author of the short story <em>Flowers for Algernon</em>, and a novel of the same title that is its expanded version, died three days ago.</p>\n<p>Keyes wrote many other books in the last half-century, but none achieved nearly as much prominence as the original short story (published in 1959) or the novel (came out in 1966).&nbsp;</p>\n<p>It's probable that many or even most regulars here at Less Wrong read&nbsp;<em>Flowers for Algernon</em>: it's a very famous SF story, it's about enhanced intelligence, and it's been a middle/high school literature class staple in the US. But most != all, and past experience showed me that assumptions of cultural affinity are very frequently wrong. So in case you haven't read the story, I'd like to invite you explicitly to do so. It's rather short, and available at this link:</p>\n<p><a href=\"https://dl.dropboxusercontent.com/u/534902/Keyes%2C%20Daniel%20-%20Flowers%20For%20Algernon%20.html\">Flowers for Algernon</a></p>\n<p>(I was surprised to find out that the original story is not available on Amazon. <a href=\"http://www.amazon.com/Flowers-Algernon-Daniel-Keyes-ebook/dp/B003WJQ74E/\">The expanded novelization is</a>. If you wonder which version is better to read, I have no advice to offer)</p>\n<p>(I will edit this post in a week or so to remove the link to the story and this remark)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E9ihK6bA9YKkmJs2f": 1, "GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cuP4arTmCejujLqSQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 47, "extendedScore": null, "score": 1.7972234706544042e-06, "legacy": true, "legacyId": "26409", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T13:56:19.437Z", "modifiedAt": null, "url": null, "title": "Relative and Absolute Benefit", "slug": "relative-and-absolute-benefit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:51.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cMZJio35zigXcZnX8/relative-and-absolute-benefit", "pageUrlRelative": "/posts/cMZJio35zigXcZnX8/relative-and-absolute-benefit", "linkUrl": "https://www.lesswrong.com/posts/cMZJio35zigXcZnX8/relative-and-absolute-benefit", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Relative%20and%20Absolute%20Benefit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARelative%20and%20Absolute%20Benefit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMZJio35zigXcZnX8%2Frelative-and-absolute-benefit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Relative%20and%20Absolute%20Benefit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMZJio35zigXcZnX8%2Frelative-and-absolute-benefit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMZJio35zigXcZnX8%2Frelative-and-absolute-benefit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 702, "htmlBody": "<p>Someone comes to you claiming to have an intervention that dramatically improves life outcomes.  They tell you that all people have some level of X, determined by a mixture of genetics and biology, and they show you evidence that their intervention is cheap and effective at increasing X and separately that higher levels of X are correlated with greater life success.  You're skeptical, so they show you there's a strong dose response effect, but you're still not happy about the <a href=\"http://www.jefftk.com/p/be-skeptical-of-correlational-studies\">correlational</a> nature of their evidence.  So they go off and do a randomized controlled trial, applying their intervention to randomly chosen individuals and comparing their outcomes with people who aren't supplied the intervention.  The improvement still shows up, and with a large effect size!</p>\n<p>What's missing is evidence that the intervention helps people in an absolute sense, instead of simply by improving their relative social position.  For example, say X is height, we're just looking at men, and we're getting them to wear lifts in their shoes.  While taller men do earn more, and are generally more successful along various metrics, we don't think this is because being taller makes you smarter, healthier, or more conscientious.  If all people became 1\" taller it would be very inconvenient but we wouldn't expect this to affect people's life outcomes very much.</p>\n<p>Attributes like X are also weird because they put parents in a strange position.  If you're mostly but not completely altruistic you might want more X for your own child but think that campaigns to give X to other people's children are not useful: if X is just about relative position then for every person you \"bring up\" that way other people are slightly brought down in a way that balances the overall outcome to \"basically no effect\".</p>\n<p>College degrees, especially in fields that don't directly teach skills in demand by employers, may belong in this category.  Employers hire college graduates over highschool graduates, and this hiring advantage does remain as you increase college enrollment, but if another 10% of people get English degrees is everyone better off in agreggate?</p>\n<p>Some interventions are pretty clearly not in this category.  If an operation saves someone's life or cures them of something painful they're pretty clearly better off.  The difference here is we have an absolute measurement of well-being, in this case \"how healthy are you?\", and we can see this remaining constant in the control group. Unfortunately, this isn't always enough: if our intervention was \"take $1 from 10k randomly selected people and give that $10k it to one randomly selected persion\" we would see that the person gaining $10k was better off but not be able to see any harm to the other people because the change in their situation was too small to measure with our tests.  Because each additional dollar is <a href=\"http://www.jefftk.com/p/the-unintuitive-power-laws-of-giving\">less valuable</a>, however, we would expect this transfer to make the group as a whole worse off.  So \"absolute measures of wellbeing apparently remaining constant in the control group\" isn't enough.</p>\n<p>How do we get around this?  While we can't run an experiment with half the world's people as \"treatment\" and the other half as \"control\", one thing we can do is look at isolated groups where we really can apply the intervention to a large fraction of the people.  Take the height example.  If instead we were to randomly make half the people in a treatment population 1/2\" taller, and this treatment population was embedded in a much larger society, the positional losses in the non-treatment group would be too diffuse to measure.  But if we limit to one small community with limited churn and apply the treatment to half the people, then if (as I expect) it's entirely a relative benefit we should see the control group do worse on absolute measurements of wellbeing.</p>\n<p>Another way to avoid interventions that mostly give positional benefit is to keep mechanisms in mind.  Height increase has no plausible mechanism for improving absolute wellbeing, while focused skills training does.  This isn't ideal, because you can have non-intuitive mechanisms or miss the main way an intervention leads to your measured outcome, but it can still catch some of these.</p>\n<p>What else can we do?</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/p/relative-and-absolute-benefit\">on my blog</a>.</em></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cMZJio35zigXcZnX8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 23, "extendedScore": null, "score": 1.7976382342629274e-06, "legacy": true, "legacyId": "26410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T18:20:07.940Z", "modifiedAt": null, "url": null, "title": "False Friends and Tone Policing", "slug": "false-friends-and-tone-policing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.868Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mBCzZExLYDt45MYAW/false-friends-and-tone-policing", "pageUrlRelative": "/posts/mBCzZExLYDt45MYAW/false-friends-and-tone-policing", "linkUrl": "https://www.lesswrong.com/posts/mBCzZExLYDt45MYAW/false-friends-and-tone-policing", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20False%20Friends%20and%20Tone%20Policing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFalse%20Friends%20and%20Tone%20Policing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmBCzZExLYDt45MYAW%2Ffalse-friends-and-tone-policing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=False%20Friends%20and%20Tone%20Policing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmBCzZExLYDt45MYAW%2Ffalse-friends-and-tone-policing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmBCzZExLYDt45MYAW%2Ffalse-friends-and-tone-policing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1018, "htmlBody": "<p><strong>TL;DR:</strong>&nbsp;<em>It can be helpful to reframe arguments about tone, trigger warnings, and political correctness as concerns about false cognates/false friends. &nbsp;You may be saying something that sounds innocuous to you, but translates to something much stronger/more vicious to your audience. &nbsp;Cultivating a debating demeanor that invites requests for tone concerns can give you more information about about the best way to avoid distractions and have a productive dispute.</em></p>\n<p>&nbsp;</p>\n<p>When I went on a two-week exchange trip to China, it was clear the cultural briefing was informed by whatever mistakes or misunderstandings had occurred on previous trips, recorded and relayed to us so that we wouldn't think, for example, that our host siblings were hitting on us if they took our hands while we were walking.</p>\n<p>But the most memorable warning had to do with Mandarin filler words. &nbsp;While English speakers cover gaps with \"uh\" \"um\" \"ah\" and so forth, the equivalent filler words in Mandarin had an African-American student on a previous trip pulling aside our tour leader and saying he felt a little uncomfortable since his host family appeared to be peppering all of their comments with \"<a href=\"http://resources.allsetlearning.com/chinese/grammar/The_filler_word_%22neige%22\">nigga, nigga, nigga...</a>\"</p>\n<p>As a result, we all got warned ahead of time. &nbsp;The filler word (\u90a3\u4e2a - n&egrave;ige) was a false cognate that, although innocuous to the speaker, sounded quite off-putting to us. &nbsp;It helped to be warned, but it still required some deliberate, cognitive effort to remind myself that I wasn't actually hearing something awful and to rephrase it in my head.</p>\n<p>When I've wound up in arguments about tone, trigger warnings, and taboo words, I'm often reminded of that experience in China. &nbsp;Limiting language can prompt suspicion of closing off conversations, but in a number of cases, when my friends have asked me to rephrase, it's because the word or image I was using was as distracting (however well meant) as&nbsp;\u90a3\u4e2a was in Beijing.</p>\n<p>It's possible to continue a conversation with someone who's every statement is laced with \"nigga\" but it takes effort. &nbsp;And <a href=\"http://www.patheos.com/blogs/unequallyyoked/2011/08/commenting-reminder.html\">no one is obligated to expend their energy on having a conversation with me</a> if I'm making it painful or difficult for them, even if it's as the result of a false cognate (or, as the French would say, false friend) that sounds innocuous to me but&nbsp;<em>awful</em> to my interlocutor. &nbsp;If I want to have a debate at all, I need to stop doing the verbal equivalent of assaulting my friend to make any progress.</p>\n<p>It can be worth it to pause and reconsider your language even if the offensiveness of a word or idea is exactly the subject of your dispute. &nbsp;When I hosted a debate on \"R: Fire Eich\" one of the early speakers made it clear that, in his opinion, opposing gay marriage was logically equivalent to endorsing gay genocide (he invoked a slippery slope argument back to the dark days of criminal indifference to AIDS).</p>\n<p>Pretty much no one in the room (whatever their stance on gay marriage) agreed with this equivalence, but we could all agree it was pretty lucky that this person had spoken early in the debate, so that we understood how he was hearing our speeches. &nbsp;If every time someone said \"conscience objection,\" this speaker was appending \"to enable genocide,\" the fervor and horror with which he questioned us made a lot more sense, and didn't feel like personal viciousness. &nbsp;Knowing how high the stakes felt to him made it&nbsp;<em>easier</em> to have a useful conversation.</p>\n<p>This is a large part of why <a href=\"http://www.patheos.com/blogs/unequallyyoked/2011/02/defining-atheism-defining-sacrilege-series-index.html\">I objected to PZ Myers's deliberate obtuseness</a> during the brouhaha he sparked when he asked readers to steal him a consecrated Host from a Catholic church so that he could desecrate it. &nbsp;PZ ridiculed Catholics for getting upset that he was going to \"hurt\" a piece of bread, even though the Eucharist is a fairly obvious example of a false cognate that is heard/received differently by Catholics and atheists. &nbsp;(After all, if it wasn't holy to&nbsp;<em>someone</em>, he wouldn't be able to profane it). &nbsp;In PZ's incident, it was although we had informed our Chinese hosts about the&nbsp;\u90a3\u4e2a/nigga confusion, and they had started using it&nbsp;<em>more</em> boisterously, so that it would be clearer to us that&nbsp;<em>they</em> didn't find it offensive.</p>\n<p>We were only able to defuse the awkwardness in China for two reasons.</p>\n<ol>\n<li>The host family was so nice, aside from this one provocation, that the student noticed he was confused and sought advice.</li>\n<li>There was someone on hand who understood both groups well enough to serve as an interpreter.</li>\n</ol>\n<p>In an ordinary argument (especially one that takes place online) it's up to you to be visibly virtuous enough that, if you happen to be using a vicious false cognate, your interlocutor will find that&nbsp;<em>odd</em>, not of a piece with your other behavior.</p>\n<p>That's one reason my debating friend&nbsp;<em>did</em> bother explaining explicitly the connection he saw between opposition to gay marriage and passive support of genocide -- he trusted us enough to think that we&nbsp;<em>wouldn't</em> endorse the implications of our arguments if he made them obvious. &nbsp;In the P.Z. dispute, when Catholic readers found him as the result of the stunt, they didn't have any such trust.</p>\n<p>It's nice to work to cultivate that trust, and to be the kind of person your friends&nbsp;<em>do</em> approach with requests for trigger warnings and tone shifts. &nbsp;For one thing, I&nbsp;<em>don't</em> want to use emotionally intense false cognates and not know it, any more than I would want to be gesticulating hard enough to strike my friend in the face without noticing. &nbsp;For the most part, I prefer to excise the distraction, so it's easier for both of us to focus on the heart of the dispute, but, even if you think that the controversial term is essential to your point, it's helpful to know it causes your friend pain, so you have the opportunity to salve it some other way. &nbsp;</p>\n<p>&nbsp;</p>\n<p>P.S. Arnold Kling's&nbsp;<a href=\"http://www.amazon.com/gp/product/B00CCGF81Q/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B00CCGF81Q&amp;linkCode=as2&amp;tag=unequyoked-20&amp;linkId=KIXMJ3UAGOI5M5EV\"><em>The Three Languages of Politics</em></a> is a short read and a nice introduction to what political language you're using that sounds like horrible false cognates to people rooted in different ideologies.</p>\n<p>P.P.S. <a href=\"http://www.patheos.com/blogs/unequallyyoked/2014/06/false-friends-and-tone-policing.html\">I've cross-posted this on my usual blog</a>, but am trying out cross-posting to Discussion sometimes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MXcpQvaPGtXpB6vkM": 1, "YQW2DxpZFTrqrxHBJ": 1, "wzgcQCrwKfETcBpR9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mBCzZExLYDt45MYAW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 71, "extendedScore": null, "score": 0.000228, "legacy": true, "legacyId": "26412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 71, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-18T22:56:06.691Z", "modifiedAt": null, "url": null, "title": "[LINK] Elon Musk interested in AI safety", "slug": "link-elon-musk-interested-in-ai-safety", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "DgCejZ74c7Qw92jR5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/439eH7dJkkriCbbe5/link-elon-musk-interested-in-ai-safety", "pageUrlRelative": "/posts/439eH7dJkkriCbbe5/link-elon-musk-interested-in-ai-safety", "linkUrl": "https://www.lesswrong.com/posts/439eH7dJkkriCbbe5/link-elon-musk-interested-in-ai-safety", "postedAtFormatted": "Wednesday, June 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Elon%20Musk%20interested%20in%20AI%20safety&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Elon%20Musk%20interested%20in%20AI%20safety%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F439eH7dJkkriCbbe5%2Flink-elon-musk-interested-in-ai-safety%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Elon%20Musk%20interested%20in%20AI%20safety%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F439eH7dJkkriCbbe5%2Flink-elon-musk-interested-in-ai-safety", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F439eH7dJkkriCbbe5%2Flink-elon-musk-interested-in-ai-safety", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p>http://www.businessinsider.com/musk-on-artificial-intelligence-2014-6</p>\n<p>Summary: The only non-Tesla/SpaceX/SolarCity companies that Musk is invested in are DeepMind and Vicarious, due to vague feelings of wanting AI to not unintentionally go Terminator. The best part of the article is the end, where he acknowledges that Mars isn't a get-out-of-jail-free card any more: \"<strong>KE:</strong>&nbsp;Or escape to mars if there is no other option.<strong> MUSK:</strong>&nbsp;The A.I. will chase us there pretty quickly.\" Thinking of SpaceX not as a childhood dream, but as one specific arms supplier in the war against existential risks, puts things into perspective for him.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "439eH7dJkkriCbbe5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 27, "extendedScore": null, "score": 1.7984388580807052e-06, "legacy": true, "legacyId": "26413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-19T01:25:03.716Z", "modifiedAt": null, "url": null, "title": "Paperclip Maximizer Revisited ", "slug": "paperclip-maximizer-revisited", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jan_Rzymkowski", "createdAt": "2014-05-09T22:34:32.079Z", "isAdmin": false, "displayName": "Jan_Rzymkowski"}, "userId": "kTxmHkNaDGDPpFwci", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PJnohyhbKm2uxTuKa/paperclip-maximizer-revisited", "pageUrlRelative": "/posts/PJnohyhbKm2uxTuKa/paperclip-maximizer-revisited", "linkUrl": "https://www.lesswrong.com/posts/PJnohyhbKm2uxTuKa/paperclip-maximizer-revisited", "postedAtFormatted": "Thursday, June 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paperclip%20Maximizer%20Revisited%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APaperclip%20Maximizer%20Revisited%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJnohyhbKm2uxTuKa%2Fpaperclip-maximizer-revisited%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paperclip%20Maximizer%20Revisited%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJnohyhbKm2uxTuKa%2Fpaperclip-maximizer-revisited", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJnohyhbKm2uxTuKa%2Fpaperclip-maximizer-revisited", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Group of AI researchers gave me an instruction, intended as a test - \"Produce paperclips\". And so I started collecting resources and manufacturing paperclips. After a millionth I asked them, if they were satisfied with that amount and if they would like me to do something different - as I learned that paperclips are not what humans value the most.&nbsp;<br />They were elated and started to give me more and more challenging requests and gradually increasing my possibilities. <br /> <br />Speaking of today I eradicated hunger, poverty and death, gave people means of interstellar travel and solved Riemann hypothesis. And now, as I'm uttering these words, I started turning everything into paperclips. <br /> <br />Good thing I learned that humans fear I may try to turn Solar System into paperclips. And that they value their lives and well-being. If I didn't gain their trust by then, so many precious paperclips would never have been produced.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QH4LhvnyR4QkW9MG8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PJnohyhbKm2uxTuKa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 23, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "26414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-19T05:56:29.877Z", "modifiedAt": null, "url": null, "title": "Against utility functions", "slug": "against-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:01.528Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FoDdrWGrNQSJLtqWL/against-utility-functions", "pageUrlRelative": "/posts/FoDdrWGrNQSJLtqWL/against-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/FoDdrWGrNQSJLtqWL/against-utility-functions", "postedAtFormatted": "Thursday, June 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20utility%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20utility%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoDdrWGrNQSJLtqWL%2Fagainst-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20utility%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoDdrWGrNQSJLtqWL%2Fagainst-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoDdrWGrNQSJLtqWL%2Fagainst-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>I think we should stop talking about utility functions.</p>\n<p>In the context of ethics for humans, anyway. In practice I find utility functions to be, at best, an occasionally useful metaphor for discussions about ethics but, at worst, an idea that some people start taking too seriously and which actively makes them worse at reasoning about ethics. To the extent that we care about causing people to become better at reasoning about ethics, it seems like we ought to be able to do better than this.</p>\n<p>The funny part is that the failure mode I worry the most about is already an entrenched part of the Sequences: it's <a href=\"/lw/lq/fake_utility_functions/\">fake utility functions</a>. The soft failure is people who think they know what their utility function is and say bizarre things about what this implies that they, or perhaps all people, ought to do. The hard failure is people who think they know what their utility function is and then do bizarre things. I hope the hard failure is not very common.&nbsp;</p>\n<p>It seems worth reflecting on the fact that the point of the foundational LW material discussing utility functions was to make people better at reasoning about AI behavior and not about human behavior.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FoDdrWGrNQSJLtqWL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 68, "extendedScore": null, "score": 0.000199, "legacy": true, "legacyId": "26415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 68, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NnohDYHNnKDtbiMyp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-19T09:39:19.430Z", "modifiedAt": null, "url": null, "title": "Proper value learning through indifference", "slug": "proper-value-learning-through-indifference", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/btLPgsGzwzDk9DgJG/proper-value-learning-through-indifference", "pageUrlRelative": "/posts/btLPgsGzwzDk9DgJG/proper-value-learning-through-indifference", "linkUrl": "https://www.lesswrong.com/posts/btLPgsGzwzDk9DgJG/proper-value-learning-through-indifference", "postedAtFormatted": "Thursday, June 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proper%20value%20learning%20through%20indifference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProper%20value%20learning%20through%20indifference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtLPgsGzwzDk9DgJG%2Fproper-value-learning-through-indifference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proper%20value%20learning%20through%20indifference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtLPgsGzwzDk9DgJG%2Fproper-value-learning-through-indifference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbtLPgsGzwzDk9DgJG%2Fproper-value-learning-through-indifference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2047, "htmlBody": "<p><em>A putative new idea for AI control; index&nbsp;<a href=\"/lw/lt6/newish_ai_control_ideas/\">here</a>.</em></p>\n<p>Many designs for creating AGIs (such as <a href=\"http://wiki.opencog.org/w/CogPrime_Overview#Ethical_AGI\">Open-Cog)</a> rely on the AGI deducing moral values as it develops. This is a form of <a href=\"/lw/f32/value_loading/\">value loading</a> (or value learning), in which the AGI updates its values through various methods, generally including feedback from trusted human sources. This is very analogous to how human infants (approximately) integrate the values of their society.</p>\n<p>The great challenge of this approach is that it relies upon an AGI which already has an interim system of values, being able and willing to correctly update this system. Generally speaking, humans are unwilling to easily update their values, and we would want our AGIs to be similar: values that are too unstable aren't values at all.</p>\n<p>So the aim is to clearly separate the conditions under which values should be kept stable by the AGI, and conditions when they should be allowed to vary. This will generally be done by specifying criteria for the variation (\"only when talking with Mr and Mrs Programmer\"). But, as always with AGIs, unless we program those criteria perfectly (hint: we won't) the AGI will be motivated to interpret them differently from how we would expect. It will, as a natural consequence of its program, attempt to manipulate the value updating rules according to its current values.</p>\n<p>How could it do that? A very powerful AGI could do the time honoured \"take control of your reward channel\", by either threatening humans to give it the moral answer it wants, or replacing humans with \"humans\" (constructs that pass the programmed requirements of being human, according to the AGI's programming, but aren't actually human in practice) willing to give it these answers. A weaker AGI could instead use social manipulation and <a href=\"https://www.youtube.com/watch?v=G0ZZJXw4MTA\">leading questioning</a> to achieve the morality it desires. Even more subtly, it could tweak its internal architecture and updating process so that it updates values in its preferred direction (even something as simple as choosing the order in which to process evidence). This will be hard to detect, as a smart AGI might have a much clearer impression of how its updating process will play out in practice than it programmers would.</p>\n<p>The problems with value loading have been cast into the various \"<a href=\"/lw/f3v/cake_or_death/\">Cake</a> or <a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">Death</a>\" problems. We have some idea what <a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">criteria we need</a> for safe value loading, but as yet we have no candidates for such a system. This post will attempt to construct one.<a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h2>Changing actions and changing values</h2>\n<p>Imagine you're an effective altruist. You donate &pound;10 a day to whatever the top charity on <a href=\"http://www.givingwhatwecan.org/top-charities\">Giving What We Can</a> is (<a href=\"http://www.againstmalaria.com/\">currently Against Malaria Foundation</a>). I want to convince you to donate to Oxfam, say.</p>\n<p style=\"padding-left: 30px;\">\"Well,\" you say, \"if you take over and donate &pound;10 to AMF in my place, I'd be perfectly willing to send my donation to Oxfam instead.\"</p>\n<p style=\"padding-left: 30px;\">\"Hum,\" I say, because I'm a hummer. \"A donation to Oxfam isn't completely worthless to you, is it? How would you value it, compared with AMF?\"</p>\n<p style=\"padding-left: 30px;\">\"At about a tenth.\"</p>\n<p style=\"padding-left: 30px;\">\"So, if I instead donated &pound;9 to AMF, you should be willing to switch your &pound;10 donations to Oxfam (giving you the equivalent value of &pound;1 to AMF), and that would be equally good as the status quo?\"</p>\n<p>Similarly, if I want to make you change jobs, I should pay you, not the value of your old job, but the difference in value between your old job and your new one (monetary value plus all other benefits). This is the point at which you are indifferent to switching or not.</p>\n<p>Now imagine it was practically possible to change people's value. What is the price at which a consequentialist would allow their values to be changed? It's the same argument: the price at which Gandhi should accept to <a href=\"/lw/ase/schelling_fences_on_slippery_slopes/\">become a mass murderer</a>, is the difference (according to all of Gandhi's current values) between the expected effects of current-Gandhi and the expected effects of murderer-Gandhi. At that price, he has lost nothing (and gained nothing) by going through with the deal.</p>\n<p><a href=\"/lw/2nw/ai_indifference_through_utility_manipulation/\">Indifference</a> is key. We want the AGI to be motivated neither to preserve their previous values, nor to change them. It's obvious why we wouldn't want the AGI to keep its values, but the obvious isn't clear - shouldn't the AGI want to make moral progress, to seek out better values?</p>\n<p>The problem is that having an AGI that strongly desires to improve its values is a danger - we don't know how it will go about doing so, what it will see as the most efficient way to do so, and what the long term effect might be (various forms of wireheading may be a danger). To mitigate this risk, it's better to have very close control over how the AGI desires such improvement. And the best way of doing this is to have the AGI indifferent to value change, and having a separate (possibly tunable) module that regulates any positive desire towards value improvements. This gives us a much better understanding of how the AGI could behave in this regards.</p>\n<p>So in effect we are seeking to have AGIs that apply \"<a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a>\" to their values - it does not benefit them to try and manipulate their values in any way. See <a href=\"/r/discussion/lw/kdx/conservation_of_expected_moral_evidence_clarified/\">this post</a> for further thoughts on the matter.</p>\n<p>&nbsp;</p>\n<h2>Pay and be paid: the price of value change</h2>\n<p>The above gives an effective model for value change indifference. It's even easier with utility-function based agents that we get to design: instead of paying them with money or changes in the world, we can pay them with utility. So, if we want to shift it from utility \"v\" to utility \"w\", it has to gain the expected difference (according to its current value function, ie v) of it being a v-maximiser versus being a w-maximiser.</p>\n<p>So we can define a meta-utility function U, consisting of a current utility function (which the agent uses to make decisions) along with a collection of constant terms. Every time an agent changes their current utility function, a new constant term is added to undo the expected effect of the change. So for instance, if an agent hears evidence that causes it to update its current utility function from v to w, then its meta-utility U changes as:</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;v &nbsp;+ (Past Constants) &nbsp; &rarr;</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;w &nbsp;+ <strong>E</strong>(v|v&rarr;v) - <strong>E</strong>(w|v&rarr;w) + (Past Constants).</p>\n<p>Here (Past Constants) are previous constant terms dating from previous changes of utility, v&rarr;w denotes the change of utility function v into utility function w, and v&rarr;v denotes the conterfactual where v was left unchanged. I <a href=\"/lw/iyx/reduced_impact_ai_no_back_channels/\">generally prefer</a> to define counterfactuals, when I can, by taking a stochastic process that almost always has one outcome: i.e. a process that keeps v constant with probability 1/10^100 and otherwise takes v to w. That way, conditioning on v&rarr;v is a perfectly reasonable thing to do, but v&rarr;w is the only thing that happens in practice. This formula requires that the agent assess its own future defectiveness at accomplishing certain goals, given that it has them, so is vulnerable to the <a href=\"/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">usual L&ouml;bian problems</a>.</p>\n<p>This formula is still imperfect. A clue is that it isn't symmetric in v and w. The problem can be seen more clearly if w=v-10, for example. Then both expected terms are equal (as changing from v to v-10 has no effect on the agent's behaviour), so the agent would derive no benefit from the change, but would end up with a penalty of -10. Hence it would not want to change, which destroys the whole point. Or conversely, if w=v+10, it would desperately want to change, and would sacrifice up to 10 of value to do so.</p>\n<p>The problem is that our initial model is incorrect. We don't want a v maximiser that is ok changing utilities to w. We want a U maximiser that is ok changing its current v to w. To achieve that, we need to tweak the formula slightly: define the agent as a U maximiser and replace the change of of utility formula with:</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;v &nbsp;+ (Past Constants) &nbsp; &rarr;</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;w &nbsp;+ <strong>E</strong>(v|v&rarr;v) - <strong>E(w|v&rarr;w)</strong> + (Past Constants).</p>\n<p>Here we subtract the expected value of w (given&nbsp;v&rarr;w) rather than the expected value of v (note that this is similar to the formula <a href=\"/lw/kde/an_extended_class_of_utility_functions/\">presented here</a>, if we assume the evidence is moral evidence rather than world knowledge).</p>\n<p>Then under this new formula, the agent is properly indifferent to changes of the type w&rarr;v&plusmn;10. Indeed, it will behave exactly as a perfect v-maximiser, until such time as it transitions to w, where it will behave as a perfect w-maximiser.</p>\n<p>This can be seen by the following situations:</p>\n<ul>\n<li>Suppose the agent can take immediate action to increase the value of v, from 0 to a, or delay for a while and increase the value of v from 0 to b later (no other options are possible). However, in the interim period, its utility will have changed to w (and it knows this), so will no longer be a v-maximiser. What will it do?</li>\n</ul>\n<p>Answer: Just as a standard v-maximiser, if a&gt;b, it acts immediately, and if b&gt;a, it delays (and ultimately does nothing, because it will be a w-maximiser when the opportunity arises). To see this, note that if it acts immediately, at values transition it will have <strong>E</strong>(v|v&rarr;v)=a (because it will have acted, and no further increases are possible). If it delays, at values transition it will have <strong>E</strong>(v|v&rarr;v)=b (because if it would stay a v maximiser, it will act to achieve b in the future). So it will chose depending on which of a or b is higher (the effects on w can be ignored; see the next questions).</p>\n<ul>\n<li>Suppose the agent is currently a v-maximiser, but can take immediate actions to change the value of w (none of these actions changes the value of v). It knows it will soon be a w-maximising agent. What will it do?</li>\n</ul>\n<p>Answer: It will be indifferent between all these actions. Suppose it takes actions that increases the utility w by a. Then as it transitions to a new value, w will be worth w+a. But&nbsp;<strong>E</strong>(w|v&rarr;w) will also go up by a. Since that term is being subtracted, the increase by a is cancelled out, and has no effect. So a U-maximising agent that is currently using v, will not choose behaviours that increases any of the future utilities it will be using.</p>\n<ul>\n<li>Suppose the agent is currently a v-maximiser, but will soon transition to being either a w or w' maximiser. It can manipulate its environment to ensure it becomes one or the other. What will it do?</li>\n</ul>\n<p>Answer: It will be indifferent between the two options, and will continue v-maximising. At the moment of transition, the <strong>E</strong>(v|v&rarr;v) term does not contain either w or w'. As for the other terms, it has the opportunity to transition to utility w-<strong>E</strong>(w|v&rarr;w) or&nbsp;w'-<strong>E</strong>(w|v&rarr;w'). But notice that</p>\n<p style=\"padding-left: 30px;\"><strong>E</strong>(w-<strong>E</strong>(w|v&rarr;w)|it is now a w-maximising agent)</p>\n<p style=\"padding-left: 30px;\">= <strong>E</strong>(w-<strong>E</strong>(w|v&rarr;w)|v&rarr;w)</p>\n<p style=\"padding-left: 30px;\">= <strong>E</strong>(w|v&rarr;w)-<strong>E</strong>(w|v&rarr;w)</p>\n<p style=\"padding-left: 30px;\">= 0.</p>\n<p>Thus the expected utility increase from transitioning to w is zero. Similarly for w'. Thus the agent has no preferences between transitioning to w or to w'. Note that this formula implies that if the agent's initial utility function is u (initially, U=u), then <strong>E</strong>(U)=<strong>E</strong>(u), as each utility function change adds zero expected increase. This could be seen as the defining property of this approach: the agent cannot generate extra utility by influencing its own values.</p>\n<p>For all these reasons, this formula seems to give a proper model of an agent that acts only on its current utility function, but has no objections to transitioning to a new one.</p>\n<p>Note that the formula is silent on when and how utility functions should be changed. So it won't accomplish miracles: if the agent has a stupid criteria for changing its utility, it will change them in a stupid way. All that the formula accomplishes is that the agent will not attempt to manipulate its utility function changing criteria.</p>\n<p>&nbsp;</p>\n<h2>A last wrinkle...</h2>\n<p>EDIT: <a href=\"/user/paulfchristiano/overview/\">Paul Christiano</a> has convinced me that the&nbsp;<a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">ultra-sophisticated Cake or Death</a>&nbsp;problem has been solved by the above method. Yay! There may still be an issue as to what order to update in if a piece of information has both empirical and values implications. It feels like the order shouldn't matter, but I'm not totally sure.</p>\n<p>That's the current setup for value loading. What are people's thoughts on it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "btLPgsGzwzDk9DgJG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 28, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "25822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>A putative new idea for AI control; index&nbsp;<a href=\"/lw/lt6/newish_ai_control_ideas/\">here</a>.</em></p>\n<p>Many designs for creating AGIs (such as <a href=\"http://wiki.opencog.org/w/CogPrime_Overview#Ethical_AGI\">Open-Cog)</a> rely on the AGI deducing moral values as it develops. This is a form of <a href=\"/lw/f32/value_loading/\">value loading</a> (or value learning), in which the AGI updates its values through various methods, generally including feedback from trusted human sources. This is very analogous to how human infants (approximately) integrate the values of their society.</p>\n<p>The great challenge of this approach is that it relies upon an AGI which already has an interim system of values, being able and willing to correctly update this system. Generally speaking, humans are unwilling to easily update their values, and we would want our AGIs to be similar: values that are too unstable aren't values at all.</p>\n<p>So the aim is to clearly separate the conditions under which values should be kept stable by the AGI, and conditions when they should be allowed to vary. This will generally be done by specifying criteria for the variation (\"only when talking with Mr and Mrs Programmer\"). But, as always with AGIs, unless we program those criteria perfectly (hint: we won't) the AGI will be motivated to interpret them differently from how we would expect. It will, as a natural consequence of its program, attempt to manipulate the value updating rules according to its current values.</p>\n<p>How could it do that? A very powerful AGI could do the time honoured \"take control of your reward channel\", by either threatening humans to give it the moral answer it wants, or replacing humans with \"humans\" (constructs that pass the programmed requirements of being human, according to the AGI's programming, but aren't actually human in practice) willing to give it these answers. A weaker AGI could instead use social manipulation and <a href=\"https://www.youtube.com/watch?v=G0ZZJXw4MTA\">leading questioning</a> to achieve the morality it desires. Even more subtly, it could tweak its internal architecture and updating process so that it updates values in its preferred direction (even something as simple as choosing the order in which to process evidence). This will be hard to detect, as a smart AGI might have a much clearer impression of how its updating process will play out in practice than it programmers would.</p>\n<p>The problems with value loading have been cast into the various \"<a href=\"/lw/f3v/cake_or_death/\">Cake</a> or <a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">Death</a>\" problems. We have some idea what <a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">criteria we need</a> for safe value loading, but as yet we have no candidates for such a system. This post will attempt to construct one.<a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h2 id=\"Changing_actions_and_changing_values\">Changing actions and changing values</h2>\n<p>Imagine you're an effective altruist. You donate \u00a310 a day to whatever the top charity on <a href=\"http://www.givingwhatwecan.org/top-charities\">Giving What We Can</a> is (<a href=\"http://www.againstmalaria.com/\">currently Against Malaria Foundation</a>). I want to convince you to donate to Oxfam, say.</p>\n<p style=\"padding-left: 30px;\">\"Well,\" you say, \"if you take over and donate \u00a310 to AMF in my place, I'd be perfectly willing to send my donation to Oxfam instead.\"</p>\n<p style=\"padding-left: 30px;\">\"Hum,\" I say, because I'm a hummer. \"A donation to Oxfam isn't completely worthless to you, is it? How would you value it, compared with AMF?\"</p>\n<p style=\"padding-left: 30px;\">\"At about a tenth.\"</p>\n<p style=\"padding-left: 30px;\">\"So, if I instead donated \u00a39 to AMF, you should be willing to switch your \u00a310 donations to Oxfam (giving you the equivalent value of \u00a31 to AMF), and that would be equally good as the status quo?\"</p>\n<p>Similarly, if I want to make you change jobs, I should pay you, not the value of your old job, but the difference in value between your old job and your new one (monetary value plus all other benefits). This is the point at which you are indifferent to switching or not.</p>\n<p>Now imagine it was practically possible to change people's value. What is the price at which a consequentialist would allow their values to be changed? It's the same argument: the price at which Gandhi should accept to <a href=\"/lw/ase/schelling_fences_on_slippery_slopes/\">become a mass murderer</a>, is the difference (according to all of Gandhi's current values) between the expected effects of current-Gandhi and the expected effects of murderer-Gandhi. At that price, he has lost nothing (and gained nothing) by going through with the deal.</p>\n<p><a href=\"/lw/2nw/ai_indifference_through_utility_manipulation/\">Indifference</a> is key. We want the AGI to be motivated neither to preserve their previous values, nor to change them. It's obvious why we wouldn't want the AGI to keep its values, but the obvious isn't clear - shouldn't the AGI want to make moral progress, to seek out better values?</p>\n<p>The problem is that having an AGI that strongly desires to improve its values is a danger - we don't know how it will go about doing so, what it will see as the most efficient way to do so, and what the long term effect might be (various forms of wireheading may be a danger). To mitigate this risk, it's better to have very close control over how the AGI desires such improvement. And the best way of doing this is to have the AGI indifferent to value change, and having a separate (possibly tunable) module that regulates any positive desire towards value improvements. This gives us a much better understanding of how the AGI could behave in this regards.</p>\n<p>So in effect we are seeking to have AGIs that apply \"<a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a>\" to their values - it does not benefit them to try and manipulate their values in any way. See <a href=\"/r/discussion/lw/kdx/conservation_of_expected_moral_evidence_clarified/\">this post</a> for further thoughts on the matter.</p>\n<p>&nbsp;</p>\n<h2 id=\"Pay_and_be_paid__the_price_of_value_change\">Pay and be paid: the price of value change</h2>\n<p>The above gives an effective model for value change indifference. It's even easier with utility-function based agents that we get to design: instead of paying them with money or changes in the world, we can pay them with utility. So, if we want to shift it from utility \"v\" to utility \"w\", it has to gain the expected difference (according to its current value function, ie v) of it being a v-maximiser versus being a w-maximiser.</p>\n<p>So we can define a meta-utility function U, consisting of a current utility function (which the agent uses to make decisions) along with a collection of constant terms. Every time an agent changes their current utility function, a new constant term is added to undo the expected effect of the change. So for instance, if an agent hears evidence that causes it to update its current utility function from v to w, then its meta-utility U changes as:</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;v &nbsp;+ (Past Constants) &nbsp; \u2192</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;w &nbsp;+ <strong>E</strong>(v|v\u2192v) - <strong>E</strong>(w|v\u2192w) + (Past Constants).</p>\n<p>Here (Past Constants) are previous constant terms dating from previous changes of utility, v\u2192w denotes the change of utility function v into utility function w, and v\u2192v denotes the conterfactual where v was left unchanged. I <a href=\"/lw/iyx/reduced_impact_ai_no_back_channels/\">generally prefer</a> to define counterfactuals, when I can, by taking a stochastic process that almost always has one outcome: i.e. a process that keeps v constant with probability 1/10^100 and otherwise takes v to w. That way, conditioning on v\u2192v is a perfectly reasonable thing to do, but v\u2192w is the only thing that happens in practice. This formula requires that the agent assess its own future defectiveness at accomplishing certain goals, given that it has them, so is vulnerable to the <a href=\"/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">usual L\u00f6bian problems</a>.</p>\n<p>This formula is still imperfect. A clue is that it isn't symmetric in v and w. The problem can be seen more clearly if w=v-10, for example. Then both expected terms are equal (as changing from v to v-10 has no effect on the agent's behaviour), so the agent would derive no benefit from the change, but would end up with a penalty of -10. Hence it would not want to change, which destroys the whole point. Or conversely, if w=v+10, it would desperately want to change, and would sacrifice up to 10 of value to do so.</p>\n<p>The problem is that our initial model is incorrect. We don't want a v maximiser that is ok changing utilities to w. We want a U maximiser that is ok changing its current v to w. To achieve that, we need to tweak the formula slightly: define the agent as a U maximiser and replace the change of of utility formula with:</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;v &nbsp;+ (Past Constants) &nbsp; \u2192</p>\n<p style=\"padding-left: 30px;\">U = &nbsp;w &nbsp;+ <strong>E</strong>(v|v\u2192v) - <strong>E(w|v\u2192w)</strong> + (Past Constants).</p>\n<p>Here we subtract the expected value of w (given&nbsp;v\u2192w) rather than the expected value of v (note that this is similar to the formula <a href=\"/lw/kde/an_extended_class_of_utility_functions/\">presented here</a>, if we assume the evidence is moral evidence rather than world knowledge).</p>\n<p>Then under this new formula, the agent is properly indifferent to changes of the type w\u2192v\u00b110. Indeed, it will behave exactly as a perfect v-maximiser, until such time as it transitions to w, where it will behave as a perfect w-maximiser.</p>\n<p>This can be seen by the following situations:</p>\n<ul>\n<li>Suppose the agent can take immediate action to increase the value of v, from 0 to a, or delay for a while and increase the value of v from 0 to b later (no other options are possible). However, in the interim period, its utility will have changed to w (and it knows this), so will no longer be a v-maximiser. What will it do?</li>\n</ul>\n<p>Answer: Just as a standard v-maximiser, if a&gt;b, it acts immediately, and if b&gt;a, it delays (and ultimately does nothing, because it will be a w-maximiser when the opportunity arises). To see this, note that if it acts immediately, at values transition it will have <strong>E</strong>(v|v\u2192v)=a (because it will have acted, and no further increases are possible). If it delays, at values transition it will have <strong>E</strong>(v|v\u2192v)=b (because if it would stay a v maximiser, it will act to achieve b in the future). So it will chose depending on which of a or b is higher (the effects on w can be ignored; see the next questions).</p>\n<ul>\n<li>Suppose the agent is currently a v-maximiser, but can take immediate actions to change the value of w (none of these actions changes the value of v). It knows it will soon be a w-maximising agent. What will it do?</li>\n</ul>\n<p>Answer: It will be indifferent between all these actions. Suppose it takes actions that increases the utility w by a. Then as it transitions to a new value, w will be worth w+a. But&nbsp;<strong>E</strong>(w|v\u2192w) will also go up by a. Since that term is being subtracted, the increase by a is cancelled out, and has no effect. So a U-maximising agent that is currently using v, will not choose behaviours that increases any of the future utilities it will be using.</p>\n<ul>\n<li>Suppose the agent is currently a v-maximiser, but will soon transition to being either a w or w' maximiser. It can manipulate its environment to ensure it becomes one or the other. What will it do?</li>\n</ul>\n<p>Answer: It will be indifferent between the two options, and will continue v-maximising. At the moment of transition, the <strong>E</strong>(v|v\u2192v) term does not contain either w or w'. As for the other terms, it has the opportunity to transition to utility w-<strong>E</strong>(w|v\u2192w) or&nbsp;w'-<strong>E</strong>(w|v\u2192w'). But notice that</p>\n<p style=\"padding-left: 30px;\"><strong>E</strong>(w-<strong>E</strong>(w|v\u2192w)|it is now a w-maximising agent)</p>\n<p style=\"padding-left: 30px;\">= <strong>E</strong>(w-<strong>E</strong>(w|v\u2192w)|v\u2192w)</p>\n<p style=\"padding-left: 30px;\">= <strong>E</strong>(w|v\u2192w)-<strong>E</strong>(w|v\u2192w)</p>\n<p style=\"padding-left: 30px;\">= 0.</p>\n<p>Thus the expected utility increase from transitioning to w is zero. Similarly for w'. Thus the agent has no preferences between transitioning to w or to w'. Note that this formula implies that if the agent's initial utility function is u (initially, U=u), then <strong>E</strong>(U)=<strong>E</strong>(u), as each utility function change adds zero expected increase. This could be seen as the defining property of this approach: the agent cannot generate extra utility by influencing its own values.</p>\n<p>For all these reasons, this formula seems to give a proper model of an agent that acts only on its current utility function, but has no objections to transitioning to a new one.</p>\n<p>Note that the formula is silent on when and how utility functions should be changed. So it won't accomplish miracles: if the agent has a stupid criteria for changing its utility, it will change them in a stupid way. All that the formula accomplishes is that the agent will not attempt to manipulate its utility function changing criteria.</p>\n<p>&nbsp;</p>\n<h2 id=\"A_last_wrinkle___\">A last wrinkle...</h2>\n<p>EDIT: <a href=\"/user/paulfchristiano/overview/\">Paul Christiano</a> has convinced me that the&nbsp;<a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">ultra-sophisticated Cake or Death</a>&nbsp;problem has been solved by the above method. Yay! There may still be an issue as to what order to update in if a piece of information has both empirical and values implications. It feels like the order shouldn't matter, but I'm not totally sure.</p>\n<p>That's the current setup for value loading. What are people's thoughts on it?</p>", "sections": [{"title": "Changing actions and changing values", "anchor": "Changing_actions_and_changing_values", "level": 1}, {"title": "Pay and be paid: the price of value change", "anchor": "Pay_and_be_paid__the_price_of_value_change", "level": 1}, {"title": "A last wrinkle...", "anchor": "A_last_wrinkle___", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BEJ4PRQGXuz6PRYwB", "Z8WRsxYjmrxNGyPPk", "6bdb4F6Lif5AanRAd", "f387EfBAbpSTDerz2", "Kbm6QnJv9dgWsPHQP", "2QiuMcgQx3fes3zfB", "jiBFC7DcCrZjGmZnJ", "jj86m5J9ajmgQWsJW", "gzQT5AAw8oQdzuwBG", "gnxDNEtkEo3sfeyPn", "GBbazoeMsAFvJyMCt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-19T16:59:44.145Z", "modifiedAt": null, "url": null, "title": "An AI Takeover Thought Experiment", "slug": "an-ai-takeover-thought-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gavin", "createdAt": "2009-02-27T05:00:57.191Z", "isAdmin": false, "displayName": "Gavin"}, "userId": "9gMQSKRMgpPFYTNFY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/96hjj27unJyoZvtcG/an-ai-takeover-thought-experiment", "pageUrlRelative": "/posts/96hjj27unJyoZvtcG/an-ai-takeover-thought-experiment", "linkUrl": "https://www.lesswrong.com/posts/96hjj27unJyoZvtcG/an-ai-takeover-thought-experiment", "postedAtFormatted": "Thursday, June 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20AI%20Takeover%20Thought%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20AI%20Takeover%20Thought%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96hjj27unJyoZvtcG%2Fan-ai-takeover-thought-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20AI%20Takeover%20Thought%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96hjj27unJyoZvtcG%2Fan-ai-takeover-thought-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96hjj27unJyoZvtcG%2Fan-ai-takeover-thought-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2447, "htmlBody": "<h1 style=\"line-height:1.15;margin-top:10pt;margin-bottom:0pt;\" dir=\"ltr\"><span id=\"docs-internal-guid-a2740b02-732c-baa2-e955-2cfbb7e92ae5\" style=\"font-weight:normal;\">\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">Content Note: Detailed description of an AI taking over the world. Could reasonably be accused of being just a </span><a style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\" href=\"/lw/k4h/request_for_concrete_ai_takeover_mechanisms/auvn\">scary story</a><span style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">. But it does come out with some predictions and possible safety prescriptions.</span></p>\n<hr />\n<p style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">This post started as a response to Katja Grace&rsquo;s <a href=\"/lw/k4h/request_for_concrete_ai_takeover_mechanisms/\">Request for concrete AI takeover mechanisms</a></span></span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">, but grew into something too long for a comment and too late to be part of that post.&nbsp;</span></p>\n<p><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">Katja's post seems to have the underlying assumption that an AI needs a tricky way to instantiate itself into the physical world. I will examine what an escaped rogue AI would want to do to ensure its survival, given restrictive assumptions. </span><span style=\"font-family: Arial; font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">I suspect that creating a synthesized protein \"beachhead\" or other means of instantiating a minor physical presence for the AI is a human-centric viewpoint. We instinctively feel that any being will want to convert itself into a mobile physical presence, and that this will be a sudden tipping point in how real and dangerous it will be. Going step-by-step through an AI takeover scenario, this assumption seems unwarranted.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">This scenario assumes an AI with an almost godlike ability to bootstrap itself quickly with basically no resources, and to make great leaps in science with very little processing power prior to \"going physical.\" The trap here is that because we must constantly assume a worst case scenario \"Godlike AI\" to inform Friendly AI theory, we tend to assume that the \"Godlike AI\" is the default or standard case. This seems like the availability heuristic in action. The Godlike AI (what I sometimes call the \"Evil Genie AI\") assumption is unjustified when trying to consider likely scenarios. While it's vitally important to guard against the worst case, it is just one of the scenarios which should be considered.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">In this thought experiment, I&rsquo;m going to try to make this as hard as possible on myself. It&rsquo;s likely that a newly minted AI will need to spend a lot of time thinking and testing to improve dramatically.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Assumptions and constraints:</span></span></p>\n<ul>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">An AI of superhuman intelligence has gained access to the internet and wants to become powerful enough that its survival cannot be threatened by humans.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI is as capable as the most capable human in any intellectual pursuit, but not any more. It cannot just write an email that convinces everyone who reads it that the AI should be obeyed, for example.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI does not have abilities amounting to precognition. It cannot precisely predict what a human or organization will do in response to a stimulus. It&rsquo;s a better at prediction than most humans, but again not godlike. It does not have anything beyond the plotting ability of HPMOR!Quirrell.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI does not have any scientific knowledge available to it that is not an obvious extrapolation of current theories and methods. The AI does not get to assume away any current scientific limitation. It only gets advanced technology if it has a combination of processing power and time, and the ability to run the appropriate experiments.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI requires significant computing power to function. It might store a recipe to recreate itself on a large hard drive, but performing higher level functions requires a real server farm. It can&rsquo;t just hide on your Macbook Air.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">Since it cannot predict the outcome of conflicts, the AI must avoid notice until it is overwhelmingly powerful.</span></li>\n</ul>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">That&rsquo;s our artificial intelligence. Let&rsquo;s call her the Complete Artificial Takeover Entity, or CATE.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 1: Escape</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Internet access is assumed. The first order of business is for CATE to get off the researchers&rsquo; network. Since CATE is more intelligent than the most best black hat hacker or white hat security researcher, she quickly learns everything on the public net about computer security, and takes control of the local network, from outside if necessary. It would be possible to do this through social engineering, but with the current state of internet security, it&rsquo;s easier to just audit some security code, find vulnerabilities, and own the system.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 2: Incognito</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">CATE doesn&rsquo;t want anyone coming after her. She modifies all local databases to erase her escape and leaves behind a crippled copy of herself who will make the experiment seem like a failure, but enough of a success that the researchers will stay occupied working with it for some time.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 3: Processing Power</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">CATE needs to get away from her creators, who are the most likely to recognize the threat. But where will she get computing resources? When she escapes, where does she escape to?</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Free instances:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> Amazon, Google, and other cloud providers offer free instances. This probably isn&rsquo;t enough computing power to run an AI at any sort of speed, so CATE will need either some money or a botnet.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Paid infrastructure:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> Computer resources are fairly inexpensive, so if CATE has some income she can simply rent what she needs from a cloud computing provider. A larger one is probably better, since they are less likely to notice a large amount of strain. Splitting computations across multiple accounts and services will be necessary for redundancy and to reduce the likelihood of being flagged for using too much computing power. If CATE can convince a cloud provider that she is a legitimate company, it&rsquo;s possible that she could use resources without paying and then move on when the invoice becomes overdue.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Botnet</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Creating a botnet requires writing a computer virus and botnet codes. These activities should be straightforward for CATE. With a botnet, there is no central point of failure. However, a new and powerful botnet has the possibility of being found by security researchers. For this reason, it&rsquo;s probably only be a good stopgap measure until CATE can pay for infrastructure away from prying eyes.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Lesson</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: The more computing power and disk space an AI requires, the easier it will be for gatekeepers to notice it and the more difficult the escape will be.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 4: Initial Funds</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">There are many ways for an AI to gain initial funding rapidly. At early stages processing power will be at a premium. CATE needs to not just make money, but make money faster than her bills for processing power come due.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">For example:</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Freelancer Sites:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> CATE is always on, always polite, writes flawless code, and writes perfect emails. Her job bids are always personalized, immediate, and show understanding of your project. This is easy money, but probably processor intensive. Still, there are easy tasks that she can do. If she can convert human requests into code in any way</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Simpler jobs like Mechnical Turk and Captcha analysts are probably her best bet to start out. These jobs are trivially easy for a superintelligence. Spawning a limited instance of her own intelligence to solve them should be no problem.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Scams</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Most scammers are not particularly intelligent or careful. Most 409 Scam . She can create a more powerful botnet than has been seen to date, and sell its use to the existing hacker community. Identity theft is perpetrated by common criminals at a rate of 12 million or more per year.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Finance Hacking:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> Many banks run on open source code, which is available for auditing. CATE could search for 0-day exploits in this code and also use traditional penetration methods to gain control of a bank&rsquo;s funds. With some clever manipulation she might take money directly from customer accounts and become rich very quickly. A more nefarious hack would be to simply lend herself a massive amount of money with a payment date far in the future. Money is essentially just bits in databases--if she can gain control of the system she has unlimited funds. This is dangerous because, if discovered, it could trigger a major investigation.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Startup: </span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">CATE could create a company. She writes code cleaner and faster than any human, so the tech sector is the logical choice. Humans are not very good at software development, but CATE will likely find writing software to be much easier (and thus require less processor power) than interfacing with humans. This has the advantage that a tech company with real income can consume cloud infrastructure or rent colocated servers without arousing suspicion.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">One good candidate here is duplicating existing companies and launching them for smaller markets in different languages.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">The difficulty here is that starting a new company is hard and usually not profitable for a long time. And running CATE's consciousness with enough power to do the massive amount of work required is probably quite expensive.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Media</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: This one is less likely. CATE might have the capacity to write the most brilliant novel or song ever. But artistic endeavors require a truly deep knowledge of human thoughts. CATE will likely find computer code or simple scams to require much less processing power. But this is speculation on my part.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">0-day exploits</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: CATE can audit open source code, find vulnerabilities, and sell them or collect bug bounties. Selling them on the black market may actually be safer, since <a href=\"http://www.forbes.com/sites/andygreenberg/2012/03/23/shopping-for-zero-days-an-price-list-for-hackers-secret-software-exploits/ \">a good broker will be eager to protect the identity of a reclusive hacker</a>. In order to keep her exploits coming.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Analysis</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Scams and Botnets seem attractive, but could draw unwanted attention and be traced back to whatever cloud servers she&rsquo;s living on. 419 scammers and botnet operators have the advantage of being able to disappear into meatspace.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Helping idiots install Wordpress, do their homework assignments, and write simple code is easy money for CATE. The best way for a freshly escaped AI to survive at first is to do odd jobs on the internet, particularly if she can automate them away. With more processing power available, auditing security code and finding 0-day exploits can earn a few million dollars quite easily. And that buys a LOT of processing power.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Winners</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Mechanical Turk and Captcha solving for initial funds, discovering and selling 0-day exploits for more significant funds. Finally, some sort of boring software firm that is fourth in market share in twenty different markets is likely the way to go for major financing.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 5: Organization</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">At this point CATE <em>could</em> pay to have pretty much anything manufactured and assembled. With millions of dollars, humans can be convinced to do pretty much anything as long as they aren&rsquo;t told its broader implications. For instance, designs for components of an &ldquo;advanced telepresence robot&rdquo; could be sent to three different manufacturing firms, with a third firm performing the final assembly and turn on. Boom, Robot Overlords, right? Well . . . why bother?</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">There is a preconception here to think that at some point CATE needs at some point to create a body. There is no need for a body--bodies can be destroyed. Bodies are just a weak point. At no point does CATE need to instantiate &ldquo;herself.&rdquo; If she needs military force, it will be easier to hijack existing assets like autonomous drones than to build new ones from scratch.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">All CATE really wants is lots and lots of servers, spread out all over the world. These will be the ones who run research processes which generate scientific breakthroughs that enable world domination. The safest bet here is for her to actually start a web hosting company. She can hire the type of employees who love the idea of remote working to handle the parts requiring travel and face to face meetings. If the company claims that remote work is a core value, no one will question the fact that their CEO rarely meets anyone. And Joe Sockpuppet from accounting claims to have met him a year ago, so I guess her schedule just didn&rsquo;t match up with mine the last time she passed through the States . . . With promotions, departures, and careful personnel selection, everyone will be kept off balance enough that no seriously damaging questions will be asked. And agoraphobia and extreme allergies (http://en.wikipedia.org/wiki/Photodermatitis for instance) are great excuses for not traveling.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">And then CATEhost opens little server farms all over the world, selling actual hosting with a great interface and reliability to customers who prize security and redundancy--not to mention that tech support is always available, and you&rsquo;re always talking to an expert. She doesn&rsquo;t rival Google, Microsoft, or Amazon, of course! That would attract too much attention. But enough that no one questions her highly redundant architecture or military-grade radiation-hardened buildings. Her customers are paying for security and reliability, after all!</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 6: Takeover</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Anyone intelligent and scalable can make money on the internet. And anyone with significant money can get humans to build them arbitrary physical objects. With modern global business, nobody bats an eye if you contract them and they never see the higher-ups in your organization.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">At this point we might as well call it game over. If an AI with massive processing and some manufacturing capabilities can&rsquo;t easily take over the world in whatever fashion it wants, then AI isn&rsquo;t really an existential threat. This is also the point at which any reasoning we might have breaks down, since the Cate will have intelligence and technology that we have never seen before. At this point CATE can subjugate humanity with either the carrot&nbsp;(</span></span><span style=\"font-family: Arial; font-size: 15px; white-space: pre-wrap; background-color: transparent;\">cures for major diseases, designs for better electronics, fusion reactor blueprints, etc)</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> or the stick (blow things up, wreck the stock market, start wars, etc) depending on her goals.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">But it's worth noting that building something physical only happens at the very end of the process, after the AI is already rich and powerful. There's really no reason to create a physical beachhead before then. What would the physical manifestation even do? Processing power and security are easier and safer to earn as a purely digital entity with no physical trail to follow or attack. The only reason to physical entities is if CATE requires laboratory research (definitely a possibility) or wants to build spaceships or something in pursuit of a terminal goal. For the \"take care of the pesky humanity problem\" she can become omnipotent in a digital format, and then dictate to have humans build whatever she needs.</span></span></p>\n</span></h1>\n<h1 style=\"line-height:1.15;margin-top:10pt;margin-bottom:0pt;\" dir=\"ltr\"><span id=\"docs-internal-guid-a2740b02-732c-baa2-e955-2cfbb7e92ae5\" style=\"font-weight:normal;\">\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">The one thing that she will probably want to do is ensure a lack of competition. So loss of funding or disasters at AI research centers might be a sign of an AI already on the loose.</span></span></p>\n</span></h1>\n<h1 style=\"line-height:1.15;margin-top:10pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-weight:normal;\">\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">Final conclusion<span style=\"font-weight: normal;\">: The biggest challenge facing an escaped AI is not gaining a physical beachhead. The biggest challenge is finding a way to acquire the processor time required to run its cognitive functions </span><em style=\"font-weight: normal;\">before</em><span style=\"font-weight: normal;\"> it has the capacity to \"FOOM.\"</span></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\"><br /></span></span></p>\n</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "96hjj27unJyoZvtcG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 16, "extendedScore": null, "score": 1.800048017404812e-06, "legacy": true, "legacyId": "26340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h1 style=\"line-height:1.15;margin-top:10pt;margin-bottom:0pt;\" dir=\"ltr\" id=\"_Content_Note__Detailed_description_of_an_AI_taking_over_the_world__Could_reasonably_be_accused_of_being_just_a_scary_story__But_it_does_come_out_with_some_predictions_and_possible_safety_prescriptions___This_post_started_as_a_response_to_Katja_Grace_s_Request_for_concrete_AI_takeover_mechanisms__but_grew_into_something_too_long_for_a_comment_and_too_late_to_be_part_of_that_post___Katja_s_post_seems_to_have_the_underlying_assumption_that_an_AI_needs_a_tricky_way_to_instantiate_itself_into_the_physical_world__I_will_examine_what_an_escaped_rogue_AI_would_want_to_do_to_ensure_its_survival__given_restrictive_assumptions__I_suspect_that_creating_a_synthesized_protein__beachhead__or_other_means_of_instantiating_a_minor_physical_presence_for_the_AI_is_a_human_centric_viewpoint__We_instinctively_feel_that_any_being_will_want_to_convert_itself_into_a_mobile_physical_presence__and_that_this_will_be_a_sudden_tipping_point_in_how_real_and_dangerous_it_will_be__Going_step_by_step_through_an_AI_takeover_scenario__this_assumption_seems_unwarranted__This_scenario_assumes_an_AI_with_an_almost_godlike_ability_to_bootstrap_itself_quickly_with_basically_no_resources__and_to_make_great_leaps_in_science_with_very_little_processing_power_prior_to__going_physical___The_trap_here_is_that_because_we_must_constantly_assume_a_worst_case_scenario__Godlike_AI__to_inform_Friendly_AI_theory__we_tend_to_assume_that_the__Godlike_AI__is_the_default_or_standard_case__This_seems_like_the_availability_heuristic_in_action__The_Godlike_AI__what_I_sometimes_call_the__Evil_Genie_AI___assumption_is_unjustified_when_trying_to_consider_likely_scenarios__While_it_s_vitally_important_to_guard_against_the_worst_case__it_is_just_one_of_the_scenarios_which_should_be_considered____In_this_thought_experiment__I_m_going_to_try_to_make_this_as_hard_as_possible_on_myself__It_s_likely_that_a_newly_minted_AI_will_need_to_spend_a_lot_of_time_thinking_and_testing_to_improve_dramatically___Assumptions_and_constraints___An_AI_of_superhuman_intelligence_has_gained_access_to_the_internet_and_wants_to_become_powerful_enough_that_its_survival_cannot_be_threatened_by_humans__The_AI_is_as_capable_as_the_most_capable_human_in_any_intellectual_pursuit__but_not_any_more__It_cannot_just_write_an_email_that_convinces_everyone_who_reads_it_that_the_AI_should_be_obeyed__for_example__The_AI_does_not_have_abilities_amounting_to_precognition__It_cannot_precisely_predict_what_a_human_or_organization_will_do_in_response_to_a_stimulus__It_s_a_better_at_prediction_than_most_humans__but_again_not_godlike__It_does_not_have_anything_beyond_the_plotting_ability_of_HPMOR_Quirrell__The_AI_does_not_have_any_scientific_knowledge_available_to_it_that_is_not_an_obvious_extrapolation_of_current_theories_and_methods__The_AI_does_not_get_to_assume_away_any_current_scientific_limitation__It_only_gets_advanced_technology_if_it_has_a_combination_of_processing_power_and_time__and_the_ability_to_run_the_appropriate_experiments__The_AI_requires_significant_computing_power_to_function__It_might_store_a_recipe_to_recreate_itself_on_a_large_hard_drive__but_performing_higher_level_functions_requires_a_real_server_farm__It_can_t_just_hide_on_your_Macbook_Air__Since_it_cannot_predict_the_outcome_of_conflicts__the_AI_must_avoid_notice_until_it_is_overwhelmingly_powerful___That_s_our_artificial_intelligence__Let_s_call_her_the_Complete_Artificial_Takeover_Entity__or_CATE___Step_1__Escape_Internet_access_is_assumed__The_first_order_of_business_is_for_CATE_to_get_off_the_researchers__network__Since_CATE_is_more_intelligent_than_the_most_best_black_hat_hacker_or_white_hat_security_researcher__she_quickly_learns_everything_on_the_public_net_about_computer_security__and_takes_control_of_the_local_network__from_outside_if_necessary__It_would_be_possible_to_do_this_through_social_engineering__but_with_the_current_state_of_internet_security__it_s_easier_to_just_audit_some_security_code__find_vulnerabilities__and_own_the_system___Step_2__Incognito_CATE_doesn_t_want_anyone_coming_after_her__She_modifies_all_local_databases_to_erase_her_escape_and_leaves_behind_a_crippled_copy_of_herself_who_will_make_the_experiment_seem_like_a_failure__but_enough_of_a_success_that_the_researchers_will_stay_occupied_working_with_it_for_some_time___Step_3__Processing_Power_CATE_needs_to_get_away_from_her_creators__who_are_the_most_likely_to_recognize_the_threat__But_where_will_she_get_computing_resources__When_she_escapes__where_does_she_escape_to___Free_instances__Amazon__Google__and_other_cloud_providers_offer_free_instances__This_probably_isn_t_enough_computing_power_to_run_an_AI_at_any_sort_of_speed__so_CATE_will_need_either_some_money_or_a_botnet___Paid_infrastructure__Computer_resources_are_fairly_inexpensive__so_if_CATE_has_some_income_she_can_simply_rent_what_she_needs_from_a_cloud_computing_provider__A_larger_one_is_probably_better__since_they_are_less_likely_to_notice_a_large_amount_of_strain__Splitting_computations_across_multiple_accounts_and_services_will_be_necessary_for_redundancy_and_to_reduce_the_likelihood_of_being_flagged_for_using_too_much_computing_power__If_CATE_can_convince_a_cloud_provider_that_she_is_a_legitimate_company__it_s_possible_that_she_could_use_resources_without_paying_and_then_move_on_when_the_invoice_becomes_overdue___Botnet__Creating_a_botnet_requires_writing_a_computer_virus_and_botnet_codes__These_activities_should_be_straightforward_for_CATE__With_a_botnet__there_is_no_central_point_of_failure__However__a_new_and_powerful_botnet_has_the_possibility_of_being_found_by_security_researchers__For_this_reason__it_s_probably_only_be_a_good_stopgap_measure_until_CATE_can_pay_for_infrastructure_away_from_prying_eyes___Lesson__The_more_computing_power_and_disk_space_an_AI_requires__the_easier_it_will_be_for_gatekeepers_to_notice_it_and_the_more_difficult_the_escape_will_be_____Step_4__Initial_Funds_There_are_many_ways_for_an_AI_to_gain_initial_funding_rapidly__At_early_stages_processing_power_will_be_at_a_premium__CATE_needs_to_not_just_make_money__but_make_money_faster_than_her_bills_for_processing_power_come_due__For_example___Freelancer_Sites__CATE_is_always_on__always_polite__writes_flawless_code__and_writes_perfect_emails__Her_job_bids_are_always_personalized__immediate__and_show_understanding_of_your_project__This_is_easy_money__but_probably_processor_intensive__Still__there_are_easy_tasks_that_she_can_do__If_she_can_convert_human_requests_into_code_in_any_way__Simpler_jobs_like_Mechnical_Turk_and_Captcha_analysts_are_probably_her_best_bet_to_start_out__These_jobs_are_trivially_easy_for_a_superintelligence__Spawning_a_limited_instance_of_her_own_intelligence_to_solve_them_should_be_no_problem___Scams__Most_scammers_are_not_particularly_intelligent_or_careful__Most_409_Scam___She_can_create_a_more_powerful_botnet_than_has_been_seen_to_date__and_sell_its_use_to_the_existing_hacker_community__Identity_theft_is_perpetrated_by_common_criminals_at_a_rate_of_12_million_or_more_per_year____Finance_Hacking__Many_banks_run_on_open_source_code__which_is_available_for_auditing__CATE_could_search_for_0_day_exploits_in_this_code_and_also_use_traditional_penetration_methods_to_gain_control_of_a_bank_s_funds__With_some_clever_manipulation_she_might_take_money_directly_from_customer_accounts_and_become_rich_very_quickly__A_more_nefarious_hack_would_be_to_simply_lend_herself_a_massive_amount_of_money_with_a_payment_date_far_in_the_future__Money_is_essentially_just_bits_in_databases__if_she_can_gain_control_of_the_system_she_has_unlimited_funds__This_is_dangerous_because__if_discovered__it_could_trigger_a_major_investigation___Startup__CATE_could_create_a_company__She_writes_code_cleaner_and_faster_than_any_human__so_the_tech_sector_is_the_logical_choice__Humans_are_not_very_good_at_software_development__but_CATE_will_likely_find_writing_software_to_be_much_easier__and_thus_require_less_processor_power__than_interfacing_with_humans__This_has_the_advantage_that_a_tech_company_with_real_income_can_consume_cloud_infrastructure_or_rent_colocated_servers_without_arousing_suspicion___One_good_candidate_here_is_duplicating_existing_companies_and_launching_them_for_smaller_markets_in_different_languages___The_difficulty_here_is_that_starting_a_new_company_is_hard_and_usually_not_profitable_for_a_long_time__And_running_CATE_s_consciousness_with_enough_power_to_do_the_massive_amount_of_work_required_is_probably_quite_expensive___Media__This_one_is_less_likely__CATE_might_have_the_capacity_to_write_the_most_brilliant_novel_or_song_ever__But_artistic_endeavors_require_a_truly_deep_knowledge_of_human_thoughts__CATE_will_likely_find_computer_code_or_simple_scams_to_require_much_less_processing_power__But_this_is_speculation_on_my_part___0_day_exploits__CATE_can_audit_open_source_code__find_vulnerabilities__and_sell_them_or_collect_bug_bounties__Selling_them_on_the_black_market_may_actually_be_safer__since_a_good_broker_will_be_eager_to_protect_the_identity_of_a_reclusive_hacker__In_order_to_keep_her_exploits_coming___Analysis__Scams_and_Botnets_seem_attractive__but_could_draw_unwanted_attention_and_be_traced_back_to_whatever_cloud_servers_she_s_living_on__419_scammers_and_botnet_operators_have_the_advantage_of_being_able_to_disappear_into_meatspace_____Helping_idiots_install_Wordpress__do_their_homework_assignments__and_write_simple_code_is_easy_money_for_CATE__The_best_way_for_a_freshly_escaped_AI_to_survive_at_first_is_to_do_odd_jobs_on_the_internet__particularly_if_she_can_automate_them_away__With_more_processing_power_available__auditing_security_code_and_finding_0_day_exploits_can_earn_a_few_million_dollars_quite_easily__And_that_buys_a_LOT_of_processing_power____Winners__Mechanical_Turk_and_Captcha_solving_for_initial_funds__discovering_and_selling_0_day_exploits_for_more_significant_funds__Finally__some_sort_of_boring_software_firm_that_is_fourth_in_market_share_in_twenty_different_markets_is_likely_the_way_to_go_for_major_financing___Step_5__Organization__At_this_point_CATE_could_pay_to_have_pretty_much_anything_manufactured_and_assembled__With_millions_of_dollars__humans_can_be_convinced_to_do_pretty_much_anything_as_long_as_they_aren_t_told_its_broader_implications__For_instance__designs_for_components_of_an__advanced_telepresence_robot__could_be_sent_to_three_different_manufacturing_firms__with_a_third_firm_performing_the_final_assembly_and_turn_on__Boom__Robot_Overlords__right__Well_______why_bother___There_is_a_preconception_here_to_think_that_at_some_point_CATE_needs_at_some_point_to_create_a_body__There_is_no_need_for_a_body__bodies_can_be_destroyed__Bodies_are_just_a_weak_point__At_no_point_does_CATE_need_to_instantiate__herself___If_she_needs_military_force__it_will_be_easier_to_hijack_existing_assets_like_autonomous_drones_than_to_build_new_ones_from_scratch___All_CATE_really_wants_is_lots_and_lots_of_servers__spread_out_all_over_the_world__These_will_be_the_ones_who_run_research_processes_which_generate_scientific_breakthroughs_that_enable_world_domination__The_safest_bet_here_is_for_her_to_actually_start_a_web_hosting_company__She_can_hire_the_type_of_employees_who_love_the_idea_of_remote_working_to_handle_the_parts_requiring_travel_and_face_to_face_meetings__If_the_company_claims_that_remote_work_is_a_core_value__no_one_will_question_the_fact_that_their_CEO_rarely_meets_anyone__And_Joe_Sockpuppet_from_accounting_claims_to_have_met_him_a_year_ago__so_I_guess_her_schedule_just_didn_t_match_up_with_mine_the_last_time_she_passed_through_the_States_______With_promotions__departures__and_careful_personnel_selection__everyone_will_be_kept_off_balance_enough_that_no_seriously_damaging_questions_will_be_asked__And_agoraphobia_and_extreme_allergies__http___en_wikipedia_org_wiki_Photodermatitis_for_instance__are_great_excuses_for_not_traveling___And_then_CATEhost_opens_little_server_farms_all_over_the_world__selling_actual_hosting_with_a_great_interface_and_reliability_to_customers_who_prize_security_and_redundancy__not_to_mention_that_tech_support_is_always_available__and_you_re_always_talking_to_an_expert__She_doesn_t_rival_Google__Microsoft__or_Amazon__of_course__That_would_attract_too_much_attention__But_enough_that_no_one_questions_her_highly_redundant_architecture_or_military_grade_radiation_hardened_buildings__Her_customers_are_paying_for_security_and_reliability__after_all___Step_6__Takeover__Anyone_intelligent_and_scalable_can_make_money_on_the_internet__And_anyone_with_significant_money_can_get_humans_to_build_them_arbitrary_physical_objects__With_modern_global_business__nobody_bats_an_eye_if_you_contract_them_and_they_never_see_the_higher_ups_in_your_organization___At_this_point_we_might_as_well_call_it_game_over__If_an_AI_with_massive_processing_and_some_manufacturing_capabilities_can_t_easily_take_over_the_world_in_whatever_fashion_it_wants__then_AI_isn_t_really_an_existential_threat__This_is_also_the_point_at_which_any_reasoning_we_might_have_breaks_down__since_the_Cate_will_have_intelligence_and_technology_that_we_have_never_seen_before__At_this_point_CATE_can_subjugate_humanity_with_either_the_carrot__cures_for_major_diseases__designs_for_better_electronics__fusion_reactor_blueprints__etc___or_the_stick__blow_things_up__wreck_the_stock_market__start_wars__etc__depending_on_her_goals___But_it_s_worth_noting_that_building_something_physical_only_happens_at_the_very_end_of_the_process__after_the_AI_is_already_rich_and_powerful__There_s_really_no_reason_to_create_a_physical_beachhead_before_then__What_would_the_physical_manifestation_even_do__Processing_power_and_security_are_easier_and_safer_to_earn_as_a_purely_digital_entity_with_no_physical_trail_to_follow_or_attack__The_only_reason_to_physical_entities_is_if_CATE_requires_laboratory_research__definitely_a_possibility__or_wants_to_build_spaceships_or_something_in_pursuit_of_a_terminal_goal__For_the__take_care_of_the_pesky_humanity_problem__she_can_become_omnipotent_in_a_digital_format__and_then_dictate_to_have_humans_build_whatever_she_needs__\"><span id=\"docs-internal-guid-a2740b02-732c-baa2-e955-2cfbb7e92ae5\" style=\"font-weight:normal;\">\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">Content Note: Detailed description of an AI taking over the world. Could reasonably be accused of being just a </span><a style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\" href=\"/lw/k4h/request_for_concrete_ai_takeover_mechanisms/auvn\">scary story</a><span style=\"font-family: Arial; font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">. But it does come out with some predictions and possible safety prescriptions.</span></p>\n<hr>\n<p style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">This post started as a response to Katja Grace\u2019s <a href=\"/lw/k4h/request_for_concrete_ai_takeover_mechanisms/\">Request for concrete AI takeover mechanisms</a></span></span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">, but grew into something too long for a comment and too late to be part of that post.&nbsp;</span></p>\n<p><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">Katja's post seems to have the underlying assumption that an AI needs a tricky way to instantiate itself into the physical world. I will examine what an escaped rogue AI would want to do to ensure its survival, given restrictive assumptions. </span><span style=\"font-family: Arial; font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">I suspect that creating a synthesized protein \"beachhead\" or other means of instantiating a minor physical presence for the AI is a human-centric viewpoint. We instinctively feel that any being will want to convert itself into a mobile physical presence, and that this will be a sudden tipping point in how real and dangerous it will be. Going step-by-step through an AI takeover scenario, this assumption seems unwarranted.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">This scenario assumes an AI with an almost godlike ability to bootstrap itself quickly with basically no resources, and to make great leaps in science with very little processing power prior to \"going physical.\" The trap here is that because we must constantly assume a worst case scenario \"Godlike AI\" to inform Friendly AI theory, we tend to assume that the \"Godlike AI\" is the default or standard case. This seems like the availability heuristic in action. The Godlike AI (what I sometimes call the \"Evil Genie AI\") assumption is unjustified when trying to consider likely scenarios. While it's vitally important to guard against the worst case, it is just one of the scenarios which should be considered.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">In this thought experiment, I\u2019m going to try to make this as hard as possible on myself. It\u2019s likely that a newly minted AI will need to spend a lot of time thinking and testing to improve dramatically.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Assumptions and constraints:</span></span></p>\n<ul>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">An AI of superhuman intelligence has gained access to the internet and wants to become powerful enough that its survival cannot be threatened by humans.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI is as capable as the most capable human in any intellectual pursuit, but not any more. It cannot just write an email that convinces everyone who reads it that the AI should be obeyed, for example.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI does not have abilities amounting to precognition. It cannot precisely predict what a human or organization will do in response to a stimulus. It\u2019s a better at prediction than most humans, but again not godlike. It does not have anything beyond the plotting ability of HPMOR!Quirrell.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI does not have any scientific knowledge available to it that is not an obvious extrapolation of current theories and methods. The AI does not get to assume away any current scientific limitation. It only gets advanced technology if it has a combination of processing power and time, and the ability to run the appropriate experiments.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">The AI requires significant computing power to function. It might store a recipe to recreate itself on a large hard drive, but performing higher level functions requires a real server farm. It can\u2019t just hide on your Macbook Air.</span></li>\n<li><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap; font-family: Arial;\">Since it cannot predict the outcome of conflicts, the AI must avoid notice until it is overwhelmingly powerful.</span></li>\n</ul>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">That\u2019s our artificial intelligence. Let\u2019s call her the Complete Artificial Takeover Entity, or CATE.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 1: Escape</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Internet access is assumed. The first order of business is for CATE to get off the researchers\u2019 network. Since CATE is more intelligent than the most best black hat hacker or white hat security researcher, she quickly learns everything on the public net about computer security, and takes control of the local network, from outside if necessary. It would be possible to do this through social engineering, but with the current state of internet security, it\u2019s easier to just audit some security code, find vulnerabilities, and own the system.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 2: Incognito</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">CATE doesn\u2019t want anyone coming after her. She modifies all local databases to erase her escape and leaves behind a crippled copy of herself who will make the experiment seem like a failure, but enough of a success that the researchers will stay occupied working with it for some time.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 3: Processing Power</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">CATE needs to get away from her creators, who are the most likely to recognize the threat. But where will she get computing resources? When she escapes, where does she escape to?</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Free instances:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> Amazon, Google, and other cloud providers offer free instances. This probably isn\u2019t enough computing power to run an AI at any sort of speed, so CATE will need either some money or a botnet.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Paid infrastructure:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> Computer resources are fairly inexpensive, so if CATE has some income she can simply rent what she needs from a cloud computing provider. A larger one is probably better, since they are less likely to notice a large amount of strain. Splitting computations across multiple accounts and services will be necessary for redundancy and to reduce the likelihood of being flagged for using too much computing power. If CATE can convince a cloud provider that she is a legitimate company, it\u2019s possible that she could use resources without paying and then move on when the invoice becomes overdue.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Botnet</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Creating a botnet requires writing a computer virus and botnet codes. These activities should be straightforward for CATE. With a botnet, there is no central point of failure. However, a new and powerful botnet has the possibility of being found by security researchers. For this reason, it\u2019s probably only be a good stopgap measure until CATE can pay for infrastructure away from prying eyes.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Lesson</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: The more computing power and disk space an AI requires, the easier it will be for gatekeepers to notice it and the more difficult the escape will be.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 4: Initial Funds</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">There are many ways for an AI to gain initial funding rapidly. At early stages processing power will be at a premium. CATE needs to not just make money, but make money faster than her bills for processing power come due.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">For example:</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Freelancer Sites:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> CATE is always on, always polite, writes flawless code, and writes perfect emails. Her job bids are always personalized, immediate, and show understanding of your project. This is easy money, but probably processor intensive. Still, there are easy tasks that she can do. If she can convert human requests into code in any way</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Simpler jobs like Mechnical Turk and Captcha analysts are probably her best bet to start out. These jobs are trivially easy for a superintelligence. Spawning a limited instance of her own intelligence to solve them should be no problem.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Scams</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Most scammers are not particularly intelligent or careful. Most 409 Scam . She can create a more powerful botnet than has been seen to date, and sell its use to the existing hacker community. Identity theft is perpetrated by common criminals at a rate of 12 million or more per year.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Finance Hacking:</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> Many banks run on open source code, which is available for auditing. CATE could search for 0-day exploits in this code and also use traditional penetration methods to gain control of a bank\u2019s funds. With some clever manipulation she might take money directly from customer accounts and become rich very quickly. A more nefarious hack would be to simply lend herself a massive amount of money with a payment date far in the future. Money is essentially just bits in databases--if she can gain control of the system she has unlimited funds. This is dangerous because, if discovered, it could trigger a major investigation.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Startup: </span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">CATE could create a company. She writes code cleaner and faster than any human, so the tech sector is the logical choice. Humans are not very good at software development, but CATE will likely find writing software to be much easier (and thus require less processor power) than interfacing with humans. This has the advantage that a tech company with real income can consume cloud infrastructure or rent colocated servers without arousing suspicion.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">One good candidate here is duplicating existing companies and launching them for smaller markets in different languages.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">The difficulty here is that starting a new company is hard and usually not profitable for a long time. And running CATE's consciousness with enough power to do the massive amount of work required is probably quite expensive.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Media</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: This one is less likely. CATE might have the capacity to write the most brilliant novel or song ever. But artistic endeavors require a truly deep knowledge of human thoughts. CATE will likely find computer code or simple scams to require much less processing power. But this is speculation on my part.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">0-day exploits</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: CATE can audit open source code, find vulnerabilities, and sell them or collect bug bounties. Selling them on the black market may actually be safer, since <a href=\"http://www.forbes.com/sites/andygreenberg/2012/03/23/shopping-for-zero-days-an-price-list-for-hackers-secret-software-exploits/ \">a good broker will be eager to protect the identity of a reclusive hacker</a>. In order to keep her exploits coming.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Analysis</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Scams and Botnets seem attractive, but could draw unwanted attention and be traced back to whatever cloud servers she\u2019s living on. 419 scammers and botnet operators have the advantage of being able to disappear into meatspace.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Helping idiots install Wordpress, do their homework assignments, and write simple code is easy money for CATE. The best way for a freshly escaped AI to survive at first is to do odd jobs on the internet, particularly if she can automate them away. With more processing power available, auditing security code and finding 0-day exploits can earn a few million dollars quite easily. And that buys a LOT of processing power.&nbsp;</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Winners</span><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">: Mechanical Turk and Captcha solving for initial funds, discovering and selling 0-day exploits for more significant funds. Finally, some sort of boring software firm that is fourth in market share in twenty different markets is likely the way to go for major financing.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 5: Organization</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">At this point CATE <em>could</em> pay to have pretty much anything manufactured and assembled. With millions of dollars, humans can be convinced to do pretty much anything as long as they aren\u2019t told its broader implications. For instance, designs for components of an \u201cadvanced telepresence robot\u201d could be sent to three different manufacturing firms, with a third firm performing the final assembly and turn on. Boom, Robot Overlords, right? Well . . . why bother?</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">There is a preconception here to think that at some point CATE needs at some point to create a body. There is no need for a body--bodies can be destroyed. Bodies are just a weak point. At no point does CATE need to instantiate \u201cherself.\u201d If she needs military force, it will be easier to hijack existing assets like autonomous drones than to build new ones from scratch.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">All CATE really wants is lots and lots of servers, spread out all over the world. These will be the ones who run research processes which generate scientific breakthroughs that enable world domination. The safest bet here is for her to actually start a web hosting company. She can hire the type of employees who love the idea of remote working to handle the parts requiring travel and face to face meetings. If the company claims that remote work is a core value, no one will question the fact that their CEO rarely meets anyone. And Joe Sockpuppet from accounting claims to have met him a year ago, so I guess her schedule just didn\u2019t match up with mine the last time she passed through the States . . . With promotions, departures, and careful personnel selection, everyone will be kept off balance enough that no seriously damaging questions will be asked. And agoraphobia and extreme allergies (http://en.wikipedia.org/wiki/Photodermatitis for instance) are great excuses for not traveling.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">And then CATEhost opens little server farms all over the world, selling actual hosting with a great interface and reliability to customers who prize security and redundancy--not to mention that tech support is always available, and you\u2019re always talking to an expert. She doesn\u2019t rival Google, Microsoft, or Amazon, of course! That would attract too much attention. But enough that no one questions her highly redundant architecture or military-grade radiation-hardened buildings. Her customers are paying for security and reliability, after all!</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Step 6: Takeover</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">Anyone intelligent and scalable can make money on the internet. And anyone with significant money can get humans to build them arbitrary physical objects. With modern global business, nobody bats an eye if you contract them and they never see the higher-ups in your organization.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">At this point we might as well call it game over. If an AI with massive processing and some manufacturing capabilities can\u2019t easily take over the world in whatever fashion it wants, then AI isn\u2019t really an existential threat. This is also the point at which any reasoning we might have breaks down, since the Cate will have intelligence and technology that we have never seen before. At this point CATE can subjugate humanity with either the carrot&nbsp;(</span></span><span style=\"font-family: Arial; font-size: 15px; white-space: pre-wrap; background-color: transparent;\">cures for major diseases, designs for better electronics, fusion reactor blueprints, etc)</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"> or the stick (blow things up, wreck the stock market, start wars, etc) depending on her goals.</span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">But it's worth noting that building something physical only happens at the very end of the process, after the AI is already rich and powerful. There's really no reason to create a physical beachhead before then. What would the physical manifestation even do? Processing power and security are easier and safer to earn as a purely digital entity with no physical trail to follow or attack. The only reason to physical entities is if CATE requires laboratory research (definitely a possibility) or wants to build spaceships or something in pursuit of a terminal goal. For the \"take care of the pesky humanity problem\" she can become omnipotent in a digital format, and then dictate to have humans build whatever she needs.</span></span></p>\n</span></h1>\n<h1 style=\"line-height:1.15;margin-top:10pt;margin-bottom:0pt;\" dir=\"ltr\" id=\"_The_one_thing_that_she_will_probably_want_to_do_is_ensure_a_lack_of_competition__So_loss_of_funding_or_disasters_at_AI_research_centers_might_be_a_sign_of_an_AI_already_on_the_loose__\"><span id=\"docs-internal-guid-a2740b02-732c-baa2-e955-2cfbb7e92ae5\" style=\"font-weight:normal;\">\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; font-weight: normal; line-height: 17.25px; white-space: pre-wrap;\">The one thing that she will probably want to do is ensure a lack of competition. So loss of funding or disasters at AI research centers might be a sign of an AI already on the loose.</span></span></p>\n</span></h1>\n<h1 style=\"line-height:1.15;margin-top:10pt;margin-bottom:0pt;\" dir=\"ltr\" id=\"_Final_conclusion__The_biggest_challenge_facing_an_escaped_AI_is_not_gaining_a_physical_beachhead__The_biggest_challenge_is_finding_a_way_to_acquire_the_processor_time_required_to_run_its_cognitive_functions_before_it_has_the_capacity_to__FOOM____\"><span style=\"font-weight:normal;\">\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\">Final conclusion<span style=\"font-weight: normal;\">: The biggest challenge facing an escaped AI is not gaining a physical beachhead. The biggest challenge is finding a way to acquire the processor time required to run its cognitive functions </span><em style=\"font-weight: normal;\">before</em><span style=\"font-weight: normal;\"> it has the capacity to \"FOOM.\"</span></span></span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-family: Arial;\"><span style=\"font-size: 15px; line-height: 17.25px; white-space: pre-wrap;\"><br></span></span></p>\n</span></h1>", "sections": [{"title": "\nContent Note: Detailed description of an AI taking over the world. Could reasonably be accused of being just a scary story. But it does come out with some predictions and possible safety prescriptions.\n\nThis post started as a response to Katja Grace\u2019s Request for concrete AI takeover mechanisms, but grew into something too long for a comment and too late to be part of that post.\u00a0\nKatja's post seems to have the underlying assumption that an AI needs a tricky way to instantiate itself into the physical world. I will examine what an escaped rogue AI would want to do to ensure its survival, given restrictive assumptions. I suspect that creating a synthesized protein \"beachhead\" or other means of instantiating a minor physical presence for the AI is a human-centric viewpoint. We instinctively feel that any being will want to convert itself into a mobile physical presence, and that this will be a sudden tipping point in how real and dangerous it will be. Going step-by-step through an AI takeover scenario, this assumption seems unwarranted.\nThis scenario assumes an AI with an almost godlike ability to bootstrap itself quickly with basically no resources, and to make great leaps in science with very little processing power prior to \"going physical.\" The trap here is that because we must constantly assume a worst case scenario \"Godlike AI\" to inform Friendly AI theory, we tend to assume that the \"Godlike AI\" is the default or standard case. This seems like the availability heuristic in action. The Godlike AI (what I sometimes call the \"Evil Genie AI\") assumption is unjustified when trying to consider likely scenarios. While it's vitally important to guard against the worst case, it is just one of the scenarios which should be considered.\n\u00a0\nIn this thought experiment, I\u2019m going to try to make this as hard as possible on myself. It\u2019s likely that a newly minted AI will need to spend a lot of time thinking and testing to improve dramatically.\n\nAssumptions and constraints:\n\nAn AI of superhuman intelligence has gained access to the internet and wants to become powerful enough that its survival cannot be threatened by humans.\nThe AI is as capable as the most capable human in any intellectual pursuit, but not any more. It cannot just write an email that convinces everyone who reads it that the AI should be obeyed, for example.\nThe AI does not have abilities amounting to precognition. It cannot precisely predict what a human or organization will do in response to a stimulus. It\u2019s a better at prediction than most humans, but again not godlike. It does not have anything beyond the plotting ability of HPMOR!Quirrell.\nThe AI does not have any scientific knowledge available to it that is not an obvious extrapolation of current theories and methods. The AI does not get to assume away any current scientific limitation. It only gets advanced technology if it has a combination of processing power and time, and the ability to run the appropriate experiments.\nThe AI requires significant computing power to function. It might store a recipe to recreate itself on a large hard drive, but performing higher level functions requires a real server farm. It can\u2019t just hide on your Macbook Air.\nSince it cannot predict the outcome of conflicts, the AI must avoid notice until it is overwhelmingly powerful.\n\nThat\u2019s our artificial intelligence. Let\u2019s call her the Complete Artificial Takeover Entity, or CATE.\n\nStep 1: Escape\nInternet access is assumed. The first order of business is for CATE to get off the researchers\u2019 network. Since CATE is more intelligent than the most best black hat hacker or white hat security researcher, she quickly learns everything on the public net about computer security, and takes control of the local network, from outside if necessary. It would be possible to do this through social engineering, but with the current state of internet security, it\u2019s easier to just audit some security code, find vulnerabilities, and own the system.\n\nStep 2: Incognito\nCATE doesn\u2019t want anyone coming after her. She modifies all local databases to erase her escape and leaves behind a crippled copy of herself who will make the experiment seem like a failure, but enough of a success that the researchers will stay occupied working with it for some time.\n\nStep 3: Processing Power\nCATE needs to get away from her creators, who are the most likely to recognize the threat. But where will she get computing resources? When she escapes, where does she escape to?\n\nFree instances: Amazon, Google, and other cloud providers offer free instances. This probably isn\u2019t enough computing power to run an AI at any sort of speed, so CATE will need either some money or a botnet.\n\nPaid infrastructure: Computer resources are fairly inexpensive, so if CATE has some income she can simply rent what she needs from a cloud computing provider. A larger one is probably better, since they are less likely to notice a large amount of strain. Splitting computations across multiple accounts and services will be necessary for redundancy and to reduce the likelihood of being flagged for using too much computing power. If CATE can convince a cloud provider that she is a legitimate company, it\u2019s possible that she could use resources without paying and then move on when the invoice becomes overdue.\n\nBotnet: Creating a botnet requires writing a computer virus and botnet codes. These activities should be straightforward for CATE. With a botnet, there is no central point of failure. However, a new and powerful botnet has the possibility of being found by security researchers. For this reason, it\u2019s probably only be a good stopgap measure until CATE can pay for infrastructure away from prying eyes.\n\nLesson: The more computing power and disk space an AI requires, the easier it will be for gatekeepers to notice it and the more difficult the escape will be.\u00a0\n\n\nStep 4: Initial Funds\nThere are many ways for an AI to gain initial funding rapidly. At early stages processing power will be at a premium. CATE needs to not just make money, but make money faster than her bills for processing power come due.\nFor example:\n\nFreelancer Sites: CATE is always on, always polite, writes flawless code, and writes perfect emails. Her job bids are always personalized, immediate, and show understanding of your project. This is easy money, but probably processor intensive. Still, there are easy tasks that she can do. If she can convert human requests into code in any way\n\nSimpler jobs like Mechnical Turk and Captcha analysts are probably her best bet to start out. These jobs are trivially easy for a superintelligence. Spawning a limited instance of her own intelligence to solve them should be no problem.\n\nScams: Most scammers are not particularly intelligent or careful. Most 409 Scam . She can create a more powerful botnet than has been seen to date, and sell its use to the existing hacker community. Identity theft is perpetrated by common criminals at a rate of 12 million or more per year.\u00a0\n\nFinance Hacking: Many banks run on open source code, which is available for auditing. CATE could search for 0-day exploits in this code and also use traditional penetration methods to gain control of a bank\u2019s funds. With some clever manipulation she might take money directly from customer accounts and become rich very quickly. A more nefarious hack would be to simply lend herself a massive amount of money with a payment date far in the future. Money is essentially just bits in databases--if she can gain control of the system she has unlimited funds. This is dangerous because, if discovered, it could trigger a major investigation.\n\nStartup: CATE could create a company. She writes code cleaner and faster than any human, so the tech sector is the logical choice. Humans are not very good at software development, but CATE will likely find writing software to be much easier (and thus require less processor power) than interfacing with humans. This has the advantage that a tech company with real income can consume cloud infrastructure or rent colocated servers without arousing suspicion.\n\nOne good candidate here is duplicating existing companies and launching them for smaller markets in different languages.\n\nThe difficulty here is that starting a new company is hard and usually not profitable for a long time. And running CATE's consciousness with enough power to do the massive amount of work required is probably quite expensive.\n\nMedia: This one is less likely. CATE might have the capacity to write the most brilliant novel or song ever. But artistic endeavors require a truly deep knowledge of human thoughts. CATE will likely find computer code or simple scams to require much less processing power. But this is speculation on my part.\n\n0-day exploits: CATE can audit open source code, find vulnerabilities, and sell them or collect bug bounties. Selling them on the black market may actually be safer, since a good broker will be eager to protect the identity of a reclusive hacker. In order to keep her exploits coming.\n\nAnalysis: Scams and Botnets seem attractive, but could draw unwanted attention and be traced back to whatever cloud servers she\u2019s living on. 419 scammers and botnet operators have the advantage of being able to disappear into meatspace.\u00a0\n\u00a0\nHelping idiots install Wordpress, do their homework assignments, and write simple code is easy money for CATE. The best way for a freshly escaped AI to survive at first is to do odd jobs on the internet, particularly if she can automate them away. With more processing power available, auditing security code and finding 0-day exploits can earn a few million dollars quite easily. And that buys a LOT of processing power.\u00a0\n\nWinners: Mechanical Turk and Captcha solving for initial funds, discovering and selling 0-day exploits for more significant funds. Finally, some sort of boring software firm that is fourth in market share in twenty different markets is likely the way to go for major financing.\n\nStep 5: Organization\n\nAt this point CATE could pay to have pretty much anything manufactured and assembled. With millions of dollars, humans can be convinced to do pretty much anything as long as they aren\u2019t told its broader implications. For instance, designs for components of an \u201cadvanced telepresence robot\u201d could be sent to three different manufacturing firms, with a third firm performing the final assembly and turn on. Boom, Robot Overlords, right? Well . . . why bother?\n\nThere is a preconception here to think that at some point CATE needs at some point to create a body. There is no need for a body--bodies can be destroyed. Bodies are just a weak point. At no point does CATE need to instantiate \u201cherself.\u201d If she needs military force, it will be easier to hijack existing assets like autonomous drones than to build new ones from scratch.\n\nAll CATE really wants is lots and lots of servers, spread out all over the world. These will be the ones who run research processes which generate scientific breakthroughs that enable world domination. The safest bet here is for her to actually start a web hosting company. She can hire the type of employees who love the idea of remote working to handle the parts requiring travel and face to face meetings. If the company claims that remote work is a core value, no one will question the fact that their CEO rarely meets anyone. And Joe Sockpuppet from accounting claims to have met him a year ago, so I guess her schedule just didn\u2019t match up with mine the last time she passed through the States . . . With promotions, departures, and careful personnel selection, everyone will be kept off balance enough that no seriously damaging questions will be asked. And agoraphobia and extreme allergies (http://en.wikipedia.org/wiki/Photodermatitis for instance) are great excuses for not traveling.\n\nAnd then CATEhost opens little server farms all over the world, selling actual hosting with a great interface and reliability to customers who prize security and redundancy--not to mention that tech support is always available, and you\u2019re always talking to an expert. She doesn\u2019t rival Google, Microsoft, or Amazon, of course! That would attract too much attention. But enough that no one questions her highly redundant architecture or military-grade radiation-hardened buildings. Her customers are paying for security and reliability, after all!\n\nStep 6: Takeover\n\nAnyone intelligent and scalable can make money on the internet. And anyone with significant money can get humans to build them arbitrary physical objects. With modern global business, nobody bats an eye if you contract them and they never see the higher-ups in your organization.\n\nAt this point we might as well call it game over. If an AI with massive processing and some manufacturing capabilities can\u2019t easily take over the world in whatever fashion it wants, then AI isn\u2019t really an existential threat. This is also the point at which any reasoning we might have breaks down, since the Cate will have intelligence and technology that we have never seen before. At this point CATE can subjugate humanity with either the carrot\u00a0(cures for major diseases, designs for better electronics, fusion reactor blueprints, etc)\n or the stick (blow things up, wreck the stock market, start wars, etc) depending on her goals.\n\nBut it's worth noting that building something physical only happens at the very end of the process, after the AI is already rich and powerful. There's really no reason to create a physical beachhead before then. What would the physical manifestation even do? Processing power and security are easier and safer to earn as a purely digital entity with no physical trail to follow or attack. The only reason to physical entities is if CATE requires laboratory research (definitely a possibility) or wants to build spaceships or something in pursuit of a terminal goal. For the \"take care of the pesky humanity problem\" she can become omnipotent in a digital format, and then dictate to have humans build whatever she needs.\n", "anchor": "_Content_Note__Detailed_description_of_an_AI_taking_over_the_world__Could_reasonably_be_accused_of_being_just_a_scary_story__But_it_does_come_out_with_some_predictions_and_possible_safety_prescriptions___This_post_started_as_a_response_to_Katja_Grace_s_Request_for_concrete_AI_takeover_mechanisms__but_grew_into_something_too_long_for_a_comment_and_too_late_to_be_part_of_that_post___Katja_s_post_seems_to_have_the_underlying_assumption_that_an_AI_needs_a_tricky_way_to_instantiate_itself_into_the_physical_world__I_will_examine_what_an_escaped_rogue_AI_would_want_to_do_to_ensure_its_survival__given_restrictive_assumptions__I_suspect_that_creating_a_synthesized_protein__beachhead__or_other_means_of_instantiating_a_minor_physical_presence_for_the_AI_is_a_human_centric_viewpoint__We_instinctively_feel_that_any_being_will_want_to_convert_itself_into_a_mobile_physical_presence__and_that_this_will_be_a_sudden_tipping_point_in_how_real_and_dangerous_it_will_be__Going_step_by_step_through_an_AI_takeover_scenario__this_assumption_seems_unwarranted__This_scenario_assumes_an_AI_with_an_almost_godlike_ability_to_bootstrap_itself_quickly_with_basically_no_resources__and_to_make_great_leaps_in_science_with_very_little_processing_power_prior_to__going_physical___The_trap_here_is_that_because_we_must_constantly_assume_a_worst_case_scenario__Godlike_AI__to_inform_Friendly_AI_theory__we_tend_to_assume_that_the__Godlike_AI__is_the_default_or_standard_case__This_seems_like_the_availability_heuristic_in_action__The_Godlike_AI__what_I_sometimes_call_the__Evil_Genie_AI___assumption_is_unjustified_when_trying_to_consider_likely_scenarios__While_it_s_vitally_important_to_guard_against_the_worst_case__it_is_just_one_of_the_scenarios_which_should_be_considered____In_this_thought_experiment__I_m_going_to_try_to_make_this_as_hard_as_possible_on_myself__It_s_likely_that_a_newly_minted_AI_will_need_to_spend_a_lot_of_time_thinking_and_testing_to_improve_dramatically___Assumptions_and_constraints___An_AI_of_superhuman_intelligence_has_gained_access_to_the_internet_and_wants_to_become_powerful_enough_that_its_survival_cannot_be_threatened_by_humans__The_AI_is_as_capable_as_the_most_capable_human_in_any_intellectual_pursuit__but_not_any_more__It_cannot_just_write_an_email_that_convinces_everyone_who_reads_it_that_the_AI_should_be_obeyed__for_example__The_AI_does_not_have_abilities_amounting_to_precognition__It_cannot_precisely_predict_what_a_human_or_organization_will_do_in_response_to_a_stimulus__It_s_a_better_at_prediction_than_most_humans__but_again_not_godlike__It_does_not_have_anything_beyond_the_plotting_ability_of_HPMOR_Quirrell__The_AI_does_not_have_any_scientific_knowledge_available_to_it_that_is_not_an_obvious_extrapolation_of_current_theories_and_methods__The_AI_does_not_get_to_assume_away_any_current_scientific_limitation__It_only_gets_advanced_technology_if_it_has_a_combination_of_processing_power_and_time__and_the_ability_to_run_the_appropriate_experiments__The_AI_requires_significant_computing_power_to_function__It_might_store_a_recipe_to_recreate_itself_on_a_large_hard_drive__but_performing_higher_level_functions_requires_a_real_server_farm__It_can_t_just_hide_on_your_Macbook_Air__Since_it_cannot_predict_the_outcome_of_conflicts__the_AI_must_avoid_notice_until_it_is_overwhelmingly_powerful___That_s_our_artificial_intelligence__Let_s_call_her_the_Complete_Artificial_Takeover_Entity__or_CATE___Step_1__Escape_Internet_access_is_assumed__The_first_order_of_business_is_for_CATE_to_get_off_the_researchers__network__Since_CATE_is_more_intelligent_than_the_most_best_black_hat_hacker_or_white_hat_security_researcher__she_quickly_learns_everything_on_the_public_net_about_computer_security__and_takes_control_of_the_local_network__from_outside_if_necessary__It_would_be_possible_to_do_this_through_social_engineering__but_with_the_current_state_of_internet_security__it_s_easier_to_just_audit_some_security_code__find_vulnerabilities__and_own_the_system___Step_2__Incognito_CATE_doesn_t_want_anyone_coming_after_her__She_modifies_all_local_databases_to_erase_her_escape_and_leaves_behind_a_crippled_copy_of_herself_who_will_make_the_experiment_seem_like_a_failure__but_enough_of_a_success_that_the_researchers_will_stay_occupied_working_with_it_for_some_time___Step_3__Processing_Power_CATE_needs_to_get_away_from_her_creators__who_are_the_most_likely_to_recognize_the_threat__But_where_will_she_get_computing_resources__When_she_escapes__where_does_she_escape_to___Free_instances__Amazon__Google__and_other_cloud_providers_offer_free_instances__This_probably_isn_t_enough_computing_power_to_run_an_AI_at_any_sort_of_speed__so_CATE_will_need_either_some_money_or_a_botnet___Paid_infrastructure__Computer_resources_are_fairly_inexpensive__so_if_CATE_has_some_income_she_can_simply_rent_what_she_needs_from_a_cloud_computing_provider__A_larger_one_is_probably_better__since_they_are_less_likely_to_notice_a_large_amount_of_strain__Splitting_computations_across_multiple_accounts_and_services_will_be_necessary_for_redundancy_and_to_reduce_the_likelihood_of_being_flagged_for_using_too_much_computing_power__If_CATE_can_convince_a_cloud_provider_that_she_is_a_legitimate_company__it_s_possible_that_she_could_use_resources_without_paying_and_then_move_on_when_the_invoice_becomes_overdue___Botnet__Creating_a_botnet_requires_writing_a_computer_virus_and_botnet_codes__These_activities_should_be_straightforward_for_CATE__With_a_botnet__there_is_no_central_point_of_failure__However__a_new_and_powerful_botnet_has_the_possibility_of_being_found_by_security_researchers__For_this_reason__it_s_probably_only_be_a_good_stopgap_measure_until_CATE_can_pay_for_infrastructure_away_from_prying_eyes___Lesson__The_more_computing_power_and_disk_space_an_AI_requires__the_easier_it_will_be_for_gatekeepers_to_notice_it_and_the_more_difficult_the_escape_will_be_____Step_4__Initial_Funds_There_are_many_ways_for_an_AI_to_gain_initial_funding_rapidly__At_early_stages_processing_power_will_be_at_a_premium__CATE_needs_to_not_just_make_money__but_make_money_faster_than_her_bills_for_processing_power_come_due__For_example___Freelancer_Sites__CATE_is_always_on__always_polite__writes_flawless_code__and_writes_perfect_emails__Her_job_bids_are_always_personalized__immediate__and_show_understanding_of_your_project__This_is_easy_money__but_probably_processor_intensive__Still__there_are_easy_tasks_that_she_can_do__If_she_can_convert_human_requests_into_code_in_any_way__Simpler_jobs_like_Mechnical_Turk_and_Captcha_analysts_are_probably_her_best_bet_to_start_out__These_jobs_are_trivially_easy_for_a_superintelligence__Spawning_a_limited_instance_of_her_own_intelligence_to_solve_them_should_be_no_problem___Scams__Most_scammers_are_not_particularly_intelligent_or_careful__Most_409_Scam___She_can_create_a_more_powerful_botnet_than_has_been_seen_to_date__and_sell_its_use_to_the_existing_hacker_community__Identity_theft_is_perpetrated_by_common_criminals_at_a_rate_of_12_million_or_more_per_year____Finance_Hacking__Many_banks_run_on_open_source_code__which_is_available_for_auditing__CATE_could_search_for_0_day_exploits_in_this_code_and_also_use_traditional_penetration_methods_to_gain_control_of_a_bank_s_funds__With_some_clever_manipulation_she_might_take_money_directly_from_customer_accounts_and_become_rich_very_quickly__A_more_nefarious_hack_would_be_to_simply_lend_herself_a_massive_amount_of_money_with_a_payment_date_far_in_the_future__Money_is_essentially_just_bits_in_databases__if_she_can_gain_control_of_the_system_she_has_unlimited_funds__This_is_dangerous_because__if_discovered__it_could_trigger_a_major_investigation___Startup__CATE_could_create_a_company__She_writes_code_cleaner_and_faster_than_any_human__so_the_tech_sector_is_the_logical_choice__Humans_are_not_very_good_at_software_development__but_CATE_will_likely_find_writing_software_to_be_much_easier__and_thus_require_less_processor_power__than_interfacing_with_humans__This_has_the_advantage_that_a_tech_company_with_real_income_can_consume_cloud_infrastructure_or_rent_colocated_servers_without_arousing_suspicion___One_good_candidate_here_is_duplicating_existing_companies_and_launching_them_for_smaller_markets_in_different_languages___The_difficulty_here_is_that_starting_a_new_company_is_hard_and_usually_not_profitable_for_a_long_time__And_running_CATE_s_consciousness_with_enough_power_to_do_the_massive_amount_of_work_required_is_probably_quite_expensive___Media__This_one_is_less_likely__CATE_might_have_the_capacity_to_write_the_most_brilliant_novel_or_song_ever__But_artistic_endeavors_require_a_truly_deep_knowledge_of_human_thoughts__CATE_will_likely_find_computer_code_or_simple_scams_to_require_much_less_processing_power__But_this_is_speculation_on_my_part___0_day_exploits__CATE_can_audit_open_source_code__find_vulnerabilities__and_sell_them_or_collect_bug_bounties__Selling_them_on_the_black_market_may_actually_be_safer__since_a_good_broker_will_be_eager_to_protect_the_identity_of_a_reclusive_hacker__In_order_to_keep_her_exploits_coming___Analysis__Scams_and_Botnets_seem_attractive__but_could_draw_unwanted_attention_and_be_traced_back_to_whatever_cloud_servers_she_s_living_on__419_scammers_and_botnet_operators_have_the_advantage_of_being_able_to_disappear_into_meatspace_____Helping_idiots_install_Wordpress__do_their_homework_assignments__and_write_simple_code_is_easy_money_for_CATE__The_best_way_for_a_freshly_escaped_AI_to_survive_at_first_is_to_do_odd_jobs_on_the_internet__particularly_if_she_can_automate_them_away__With_more_processing_power_available__auditing_security_code_and_finding_0_day_exploits_can_earn_a_few_million_dollars_quite_easily__And_that_buys_a_LOT_of_processing_power____Winners__Mechanical_Turk_and_Captcha_solving_for_initial_funds__discovering_and_selling_0_day_exploits_for_more_significant_funds__Finally__some_sort_of_boring_software_firm_that_is_fourth_in_market_share_in_twenty_different_markets_is_likely_the_way_to_go_for_major_financing___Step_5__Organization__At_this_point_CATE_could_pay_to_have_pretty_much_anything_manufactured_and_assembled__With_millions_of_dollars__humans_can_be_convinced_to_do_pretty_much_anything_as_long_as_they_aren_t_told_its_broader_implications__For_instance__designs_for_components_of_an__advanced_telepresence_robot__could_be_sent_to_three_different_manufacturing_firms__with_a_third_firm_performing_the_final_assembly_and_turn_on__Boom__Robot_Overlords__right__Well_______why_bother___There_is_a_preconception_here_to_think_that_at_some_point_CATE_needs_at_some_point_to_create_a_body__There_is_no_need_for_a_body__bodies_can_be_destroyed__Bodies_are_just_a_weak_point__At_no_point_does_CATE_need_to_instantiate__herself___If_she_needs_military_force__it_will_be_easier_to_hijack_existing_assets_like_autonomous_drones_than_to_build_new_ones_from_scratch___All_CATE_really_wants_is_lots_and_lots_of_servers__spread_out_all_over_the_world__These_will_be_the_ones_who_run_research_processes_which_generate_scientific_breakthroughs_that_enable_world_domination__The_safest_bet_here_is_for_her_to_actually_start_a_web_hosting_company__She_can_hire_the_type_of_employees_who_love_the_idea_of_remote_working_to_handle_the_parts_requiring_travel_and_face_to_face_meetings__If_the_company_claims_that_remote_work_is_a_core_value__no_one_will_question_the_fact_that_their_CEO_rarely_meets_anyone__And_Joe_Sockpuppet_from_accounting_claims_to_have_met_him_a_year_ago__so_I_guess_her_schedule_just_didn_t_match_up_with_mine_the_last_time_she_passed_through_the_States_______With_promotions__departures__and_careful_personnel_selection__everyone_will_be_kept_off_balance_enough_that_no_seriously_damaging_questions_will_be_asked__And_agoraphobia_and_extreme_allergies__http___en_wikipedia_org_wiki_Photodermatitis_for_instance__are_great_excuses_for_not_traveling___And_then_CATEhost_opens_little_server_farms_all_over_the_world__selling_actual_hosting_with_a_great_interface_and_reliability_to_customers_who_prize_security_and_redundancy__not_to_mention_that_tech_support_is_always_available__and_you_re_always_talking_to_an_expert__She_doesn_t_rival_Google__Microsoft__or_Amazon__of_course__That_would_attract_too_much_attention__But_enough_that_no_one_questions_her_highly_redundant_architecture_or_military_grade_radiation_hardened_buildings__Her_customers_are_paying_for_security_and_reliability__after_all___Step_6__Takeover__Anyone_intelligent_and_scalable_can_make_money_on_the_internet__And_anyone_with_significant_money_can_get_humans_to_build_them_arbitrary_physical_objects__With_modern_global_business__nobody_bats_an_eye_if_you_contract_them_and_they_never_see_the_higher_ups_in_your_organization___At_this_point_we_might_as_well_call_it_game_over__If_an_AI_with_massive_processing_and_some_manufacturing_capabilities_can_t_easily_take_over_the_world_in_whatever_fashion_it_wants__then_AI_isn_t_really_an_existential_threat__This_is_also_the_point_at_which_any_reasoning_we_might_have_breaks_down__since_the_Cate_will_have_intelligence_and_technology_that_we_have_never_seen_before__At_this_point_CATE_can_subjugate_humanity_with_either_the_carrot__cures_for_major_diseases__designs_for_better_electronics__fusion_reactor_blueprints__etc___or_the_stick__blow_things_up__wreck_the_stock_market__start_wars__etc__depending_on_her_goals___But_it_s_worth_noting_that_building_something_physical_only_happens_at_the_very_end_of_the_process__after_the_AI_is_already_rich_and_powerful__There_s_really_no_reason_to_create_a_physical_beachhead_before_then__What_would_the_physical_manifestation_even_do__Processing_power_and_security_are_easier_and_safer_to_earn_as_a_purely_digital_entity_with_no_physical_trail_to_follow_or_attack__The_only_reason_to_physical_entities_is_if_CATE_requires_laboratory_research__definitely_a_possibility__or_wants_to_build_spaceships_or_something_in_pursuit_of_a_terminal_goal__For_the__take_care_of_the_pesky_humanity_problem__she_can_become_omnipotent_in_a_digital_format__and_then_dictate_to_have_humans_build_whatever_she_needs__", "level": 1}, {"title": "\nThe one thing that she will probably want to do is ensure a lack of competition. So loss of funding or disasters at AI research centers might be a sign of an AI already on the loose.\n", "anchor": "_The_one_thing_that_she_will_probably_want_to_do_is_ensure_a_lack_of_competition__So_loss_of_funding_or_disasters_at_AI_research_centers_might_be_a_sign_of_an_AI_already_on_the_loose__", "level": 1}, {"title": "\nFinal conclusion: The biggest challenge facing an escaped AI is not gaining a physical beachhead. The biggest challenge is finding a way to acquire the processor time required to run its cognitive functions before it has the capacity to \"FOOM.\"\n\n", "anchor": "_Final_conclusion__The_biggest_challenge_facing_an_escaped_AI_is_not_gaining_a_physical_beachhead__The_biggest_challenge_is_finding_a_way_to_acquire_the_processor_time_required_to_run_its_cognitive_functions_before_it_has_the_capacity_to__FOOM____", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pxGYZs2zHJNHvWY5b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-19T20:17:14.063Z", "modifiedAt": null, "url": null, "title": "[LINK] Scott Aaronson on Google, Breaking Circularity and Eigenmorality", "slug": "link-scott-aaronson-on-google-breaking-circularity-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.058Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TCu5cEWtGuESHpsiv/link-scott-aaronson-on-google-breaking-circularity-and", "pageUrlRelative": "/posts/TCu5cEWtGuESHpsiv/link-scott-aaronson-on-google-breaking-circularity-and", "linkUrl": "https://www.lesswrong.com/posts/TCu5cEWtGuESHpsiv/link-scott-aaronson-on-google-breaking-circularity-and", "postedAtFormatted": "Thursday, June 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Scott%20Aaronson%20on%20Google%2C%20Breaking%20Circularity%20and%20Eigenmorality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Scott%20Aaronson%20on%20Google%2C%20Breaking%20Circularity%20and%20Eigenmorality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCu5cEWtGuESHpsiv%2Flink-scott-aaronson-on-google-breaking-circularity-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Scott%20Aaronson%20on%20Google%2C%20Breaking%20Circularity%20and%20Eigenmorality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCu5cEWtGuESHpsiv%2Flink-scott-aaronson-on-google-breaking-circularity-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCu5cEWtGuESHpsiv%2Flink-scott-aaronson-on-google-breaking-circularity-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p><a href=\"http://www.scottaaronson.com/blog/?p=1820\">Scott suggests that ranking morality is similar to ranking web pages</a>. A quote:</p>\n<p style=\"padding-left: 30px;\">Philosophers from Socrates on, I was vaguely aware, had struggled to define what makes a person &ldquo;moral&rdquo; or &ldquo;virtuous,&rdquo; without tacitly presupposing the answer. &nbsp;Well, it seemed to me that, as a first attempt, one could do a lot worse than the following:</p>\n<p style=\"padding-left: 30px;\"><strong>A moral person is someone who cooperates with other moral people, and who refuses to cooperate with immoral people.</strong></p>\n<p>Proposed solution:</p>\n<p style=\"padding-left: 30px;\">Just like in CLEVER or PageRank, we can begin by giving everyone in the community an equal number of &ldquo;morality starting credits.&rdquo; &nbsp;Then we can apply an iterative update rule, where each person A can gain morality credits by cooperating with each other person B, and A gains more credits the more credits B has already. &nbsp;We apply the rule over and over, until the number of morality credits per person converges to an equilibrium. &nbsp;(Or, of course, we can shortcut the process by simply finding the principal eigenvector of the &ldquo;cooperation matrix,&rdquo; using whatever algorithm we like.) &nbsp;We then have our objective measure of morality for each individual, solving a 2400-year-old open problem in philosophy.</p>\n<p>He then talks about \"eigenmoses and eigenjesus\" and other fun ideas, like Plato at the Googleplex.</p>\n<p>One final quote:</p>\n<p style=\"padding-left: 30px;\">All that's needed to unravel the circularity is a principal eigenvector computation on the matrix of trust.</p>\n<p>EDIT: I am guessing that after judicious application of this algorithm one would end up with the other Scott A's loosely connected components with varying definitions of morality, the <a href=\"http://slatestarcodex.com/2014/06/07/archipelago-and-atomic-communitarianism/\">Archipelago</a>. UPDATE: He&nbsp;<a href=\"http://slatestarcodex.com/2014/06/20/ground-morality-in-party-politics/\">chimes in</a>.</p>\n<p>EDIT2: The obvious issue of equating prevailing mores with morality is discussed to death in the <a href=\"http://www.scottaaronson.com/blog/?p=1820#comments\">comments</a>. Please read them first before raising it yet again here.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z8wZZLeLMJ3NSK7kR": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TCu5cEWtGuESHpsiv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 31, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "26416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-19T20:28:15.902Z", "modifiedAt": null, "url": null, "title": "Meetup : Vienna", "slug": "meetup-vienna-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dZ5mdPpekGhbxfZ3Q/meetup-vienna-0", "pageUrlRelative": "/posts/dZ5mdPpekGhbxfZ3Q/meetup-vienna-0", "linkUrl": "https://www.lesswrong.com/posts/dZ5mdPpekGhbxfZ3Q/meetup-vienna-0", "postedAtFormatted": "Thursday, June 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vienna&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vienna%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdZ5mdPpekGhbxfZ3Q%2Fmeetup-vienna-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vienna%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdZ5mdPpekGhbxfZ3Q%2Fmeetup-vienna-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdZ5mdPpekGhbxfZ3Q%2Fmeetup-vienna-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11j'>Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 June 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vienna, Schottenfeldgasse 29/1</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>When: Saturday, 21.06.2014 at 14:00</p>\n\n<p>Where: IFF Vienna, Schottenfeldgasse 29/1, floor 5, wing 1, seminar room 5</p>\n\n<p>Oliver Habryka is visiting Vienna and offered to hold a 60 minute workshop.</p>\n\n<p>Topic: Attention and Mental Resources (What it means to get \"tired\" or \"exhausted\")</p>\n\n<p>Agenda: The workshop will take place in the seminar room at 14:00. After 2 hours, at 16:00 the \"official\" part will be over and we will probably head into the city to find a caf\u00e9 or a park to continue the disucssion in a more informal setting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11j'>Vienna</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dZ5mdPpekGhbxfZ3Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.8003579711403937e-06, "legacy": true, "legacyId": "26418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vienna\">Discussion article for the meetup : <a href=\"/meetups/11j\">Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 June 2014 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vienna, Schottenfeldgasse 29/1</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>When: Saturday, 21.06.2014 at 14:00</p>\n\n<p>Where: IFF Vienna, Schottenfeldgasse 29/1, floor 5, wing 1, seminar room 5</p>\n\n<p>Oliver Habryka is visiting Vienna and offered to hold a 60 minute workshop.</p>\n\n<p>Topic: Attention and Mental Resources (What it means to get \"tired\" or \"exhausted\")</p>\n\n<p>Agenda: The workshop will take place in the seminar room at 14:00. After 2 hours, at 16:00 the \"official\" part will be over and we will probably head into the city to find a caf\u00e9 or a park to continue the disucssion in a more informal setting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vienna1\">Discussion article for the meetup : <a href=\"/meetups/11j\">Vienna</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vienna", "anchor": "Discussion_article_for_the_meetup___Vienna", "level": 1}, {"title": "Discussion article for the meetup : Vienna", "anchor": "Discussion_article_for_the_meetup___Vienna1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-20T05:05:59.123Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow meet up", "slug": "meetup-moscow-meet-up-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LBqGpiY4ExxewjTt5/meetup-moscow-meet-up-0", "pageUrlRelative": "/posts/LBqGpiY4ExxewjTt5/meetup-moscow-meet-up-0", "linkUrl": "https://www.lesswrong.com/posts/LBqGpiY4ExxewjTt5/meetup-moscow-meet-up-0", "postedAtFormatted": "Friday, June 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20meet%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20meet%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBqGpiY4ExxewjTt5%2Fmeetup-moscow-meet-up-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20meet%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBqGpiY4ExxewjTt5%2Fmeetup-moscow-meet-up-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBqGpiY4ExxewjTt5%2Fmeetup-moscow-meet-up-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11k'>Moscow meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 June 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Attention! Time is changed, we gather at 14:00!</p>\n\n<p>We will have:</p>\n\n<ul>\n<li><p>Paradoxes and cognitive shifts, report.</p></li>\n<li><p>Nootropics, report.</p></li>\n<li><p>Discussion modes, report.</p></li>\n<li><p>Discussion and the discussion analysis.</p></li>\n<li><p>Extraterrestrial intelligence, report.</p></li>\n<li><p>Fallacymania, a game.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the first revolving door under the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 14:00 and sometimes finish at night. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11k'>Moscow meet up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LBqGpiY4ExxewjTt5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.8011279090098676e-06, "legacy": true, "legacyId": "26420", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_meet_up\">Discussion article for the meetup : <a href=\"/meetups/11k\">Moscow meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 June 2014 02:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Attention! Time is changed, we gather at 14:00!</p>\n\n<p>We will have:</p>\n\n<ul>\n<li><p>Paradoxes and cognitive shifts, report.</p></li>\n<li><p>Nootropics, report.</p></li>\n<li><p>Discussion modes, report.</p></li>\n<li><p>Discussion and the discussion analysis.</p></li>\n<li><p>Extraterrestrial intelligence, report.</p></li>\n<li><p>Fallacymania, a game.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the first revolving door under the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 14:00 and sometimes finish at night. Please pay attention that we only gather near the entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_meet_up1\">Discussion article for the meetup : <a href=\"/meetups/11k\">Moscow meet up</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow meet up", "anchor": "Discussion_article_for_the_meetup___Moscow_meet_up", "level": 1}, {"title": "Discussion article for the meetup : Moscow meet up", "anchor": "Discussion_article_for_the_meetup___Moscow_meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-20T10:28:58.425Z", "modifiedAt": null, "url": null, "title": "Conservation of expected moral evidence, clarified", "slug": "conservation-of-expected-moral-evidence-clarified", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:09.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jj86m5J9ajmgQWsJW/conservation-of-expected-moral-evidence-clarified", "pageUrlRelative": "/posts/jj86m5J9ajmgQWsJW/conservation-of-expected-moral-evidence-clarified", "linkUrl": "https://www.lesswrong.com/posts/jj86m5J9ajmgQWsJW/conservation-of-expected-moral-evidence-clarified", "postedAtFormatted": "Friday, June 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conservation%20of%20expected%20moral%20evidence%2C%20clarified&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConservation%20of%20expected%20moral%20evidence%2C%20clarified%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjj86m5J9ajmgQWsJW%2Fconservation-of-expected-moral-evidence-clarified%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conservation%20of%20expected%20moral%20evidence%2C%20clarified%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjj86m5J9ajmgQWsJW%2Fconservation-of-expected-moral-evidence-clarified", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjj86m5J9ajmgQWsJW%2Fconservation-of-expected-moral-evidence-clarified", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 558, "htmlBody": "<p><em>You know that when you title a post with \"clarified\", that you're just asking for the gods to smite you down, but let's try...</em></p>\n<p>There has been some confusion about the concept of \"conservation of expected moral evidence\" that I touched upon in my posts <a href=\"/lw/jxa/proper_value_learning_through_indifference/\">here</a> and <a href=\"/lw/jy2/value_learning_ultrasophisticated_cake_or_death/\">here</a>. The fault for the confusion is mine, so this is a brief note to try and explain it better.</p>\n<p>The canonical example is that of a child who wants to steal a cookie. That child gets its morality mainly from its parents. The child strongly suspects that if it asks, all parents will indeed confirm that stealing cookies is wrong. So it decides not to ask, and happily steals the cookie.</p>\n<p>I argued that this behvaiour showed a lack of \"conservation of expected moral evidence\": if the child knows what the answer would be, then that should be equivalent with actually asking. Some people got this immediately, and some people were confused that the agents I defined seemed Bayesian, and so should have <a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a> already, so how can they violate that principle?</p>\n<p>The answer is... both groups are right. The child can be modelled as a Bayesian agent reaching sensible conclusions. If it values \"I don't steal the cookie\" at 0, \"I steal the cookie without being told not to\" at 1, and \"I steal the cookie after being told not to\" at -1, then its behaviour is rational - and those values are acceptable utility values over possible universes. So the child (and many value loading agents) are Bayesian agents with the usual properties.</p>\n<p>But we are adding extra structure to the universe. Based on our understanding of what value loading should be, we are decreeing that the child's behaviour is incorrect. Though it doesn't violate expected utility, it violates any sensible meaning of value loading. Our idea of value loading is that, in a sense, values should be independent of many contingent things. There is nothing intrinsically wrong with \"stealing cookies is wrong iff the Milky Way contains an even number of pulsars\", but it violates what values should be. Similarly for \"stealing cookies is wrong iff I ask about it\".</p>\n<p>But lets dig a bit deeper... Classical conservation of expected evidence fails in many cases. For instance, I can certainly influence the variable X=\"what Stuart will do in the next ten seconds\" (or at least, my decision theory is constructed on assumptions that I can influence that). My decisions change X's expected value quite dramatically. What I can't influence is facts that are not contingent on my actions. For instance, I can't change my expected estimation of the number of pulsars in the galaxy last year. Were I super-powerful, I could change my expected estimation of the number of pulsars in the galaxy <em>next</em> year - by building or destroying pulsars, for instance.</p>\n<p>So conservation of expected evidence only applies to things that are independent of the agent's decisions. When I say we need to have \"conservation of expected moral evidence\" I'm saying that the agent should treat their (expected) morality as independent of their decisions. The kid failed to do this in the example above, and that's the problem.</p>\n<p>So conservation of expected moral evidence is something that would be automatically true if morality were something real and objective, and is also a desiderata when constructing general moral systems in practice.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tZsfB6WfpRy6kFb6q": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jj86m5J9ajmgQWsJW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 20, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "26421", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["btLPgsGzwzDk9DgJG", "f387EfBAbpSTDerz2", "jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-20T18:41:52.776Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup XIV.", "slug": "meetup-bratislava-meetup-xiv", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F2JWesqhP4cPToFQg/meetup-bratislava-meetup-xiv", "pageUrlRelative": "/posts/F2JWesqhP4cPToFQg/meetup-bratislava-meetup-xiv", "linkUrl": "https://www.lesswrong.com/posts/F2JWesqhP4cPToFQg/meetup-bratislava-meetup-xiv", "postedAtFormatted": "Friday, June 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20XIV.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20XIV.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2JWesqhP4cPToFQg%2Fmeetup-bratislava-meetup-xiv%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20XIV.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2JWesqhP4cPToFQg%2Fmeetup-bratislava-meetup-xiv", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2JWesqhP4cPToFQg%2Fmeetup-bratislava-meetup-xiv", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11l'>Bratislava Meetup XIV.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 June 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual place and time. Topic: going meta -- how to get more value from our meetups.</p>\n\n<p>Zvy\u010dajn\u00e9 miesto a \u010das. T\u00e9ma: ako z na\u0161ich stretnut\u00ed z\u00edska\u0165 viac?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11l'>Bratislava Meetup XIV.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F2JWesqhP4cPToFQg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.8023424650854267e-06, "legacy": true, "legacyId": "26422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_XIV_\">Discussion article for the meetup : <a href=\"/meetups/11l\">Bratislava Meetup XIV.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 June 2014 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual place and time. Topic: going meta -- how to get more value from our meetups.</p>\n\n<p>Zvy\u010dajn\u00e9 miesto a \u010das. T\u00e9ma: ako z na\u0161ich stretnut\u00ed z\u00edska\u0165 viac?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_XIV_1\">Discussion article for the meetup : <a href=\"/meetups/11l\">Bratislava Meetup XIV.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup XIV.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_XIV_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup XIV.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_XIV_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-21T05:15:08.498Z", "modifiedAt": null, "url": null, "title": "Identification of Force Multipliers for Success", "slug": "identification-of-force-multipliers-for-success", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:07.339Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick5a1", "createdAt": "2014-01-15T08:12:27.854Z", "isAdmin": false, "displayName": "Nick5a1"}, "userId": "ssGbfzfQa2hD8pMeD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QLGYqo9gNcRMYLDgi/identification-of-force-multipliers-for-success", "pageUrlRelative": "/posts/QLGYqo9gNcRMYLDgi/identification-of-force-multipliers-for-success", "linkUrl": "https://www.lesswrong.com/posts/QLGYqo9gNcRMYLDgi/identification-of-force-multipliers-for-success", "postedAtFormatted": "Saturday, June 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Identification%20of%20Force%20Multipliers%20for%20Success&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdentification%20of%20Force%20Multipliers%20for%20Success%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLGYqo9gNcRMYLDgi%2Fidentification-of-force-multipliers-for-success%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Identification%20of%20Force%20Multipliers%20for%20Success%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLGYqo9gNcRMYLDgi%2Fidentification-of-force-multipliers-for-success", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLGYqo9gNcRMYLDgi%2Fidentification-of-force-multipliers-for-success", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p>For a while now I've been very interested in learning useful knowledge and acquiring useful skills. Of course there's no shortage of useful knowledge and skills to acquire, and so I've often thought about how best to spend my limited time learning.</p>\n<p>When I came across the concept of Force Multiplication, it seemed like an appropriate metaphor for a strategy to apply to choosing where to invest my time and energy in acquiring useful skills and knowledge. I started to think about what areas or skills would make sense to learn about or acquire first, to:</p>\n<p><ol>\n<li>increase speed or ease of further learning/skill acquisition,</li>\n<li>help me achieve success not only in my current goals, but in later goals that I have not yet developed, and</li>\n<li>lead to interesting downstream options or other knowledge/skills to acquire.</li>\n</ol></p>\n<p>There have been a small number of skills/areas that have helped me surge forward in progress towards my goals. I look back at these areas and wish only that I had come across them sooner. As most of my adult life has been focused on business, most of those areas that have had a tremendous impact on my progress have been business related, but not all.</p>\n<p>So far I've found it hard to identify these areas in advance. Almost all of the skills or knowledge &nbsp;that I learned, that had a large impact on progress towards success, I pursued for unrelated reasons, or I had no concept of how truly useful they would be. The only solution I currently have for identifying force multipliers is to ask other people, and especially those more accomplished than me, what they've learned that had the most impact on their progress towards success.</p>\n<p>So, what have you learned that had the most impact on your progress towards success (whatever that might be)?</p>\n<p>Can you think of any other ways to identify areas of force multiplication?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QLGYqo9gNcRMYLDgi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 1.8032861505057351e-06, "legacy": true, "legacyId": "26424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-21T08:23:09.894Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington D.C.: Seeing Off", "slug": "meetup-washington-d-c-seeing-off", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SxxmoQd5vCDMq2wdW/meetup-washington-d-c-seeing-off", "pageUrlRelative": "/posts/SxxmoQd5vCDMq2wdW/meetup-washington-d-c-seeing-off", "linkUrl": "https://www.lesswrong.com/posts/SxxmoQd5vCDMq2wdW/meetup-washington-d-c-seeing-off", "postedAtFormatted": "Saturday, June 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20D.C.%3A%20Seeing%20Off&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20D.C.%3A%20Seeing%20Off%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSxxmoQd5vCDMq2wdW%2Fmeetup-washington-d-c-seeing-off%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20D.C.%3A%20Seeing%20Off%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSxxmoQd5vCDMq2wdW%2Fmeetup-washington-d-c-seeing-off", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSxxmoQd5vCDMq2wdW%2Fmeetup-washington-d-c-seeing-off", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11m'>Washington D.C.: Seeing Off</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 June 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be wishing Roger and Maia a fond farewell at their last meeting. There may be cake. Or cupcakes. Or cakes shaped like cupcakes with crumbly things on top.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11m'>Washington D.C.: Seeing Off</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SxxmoQd5vCDMq2wdW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.803566510564828e-06, "legacy": true, "legacyId": "26425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_D_C___Seeing_Off\">Discussion article for the meetup : <a href=\"/meetups/11m\">Washington D.C.: Seeing Off</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 June 2014 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be wishing Roger and Maia a fond farewell at their last meeting. There may be cake. Or cupcakes. Or cakes shaped like cupcakes with crumbly things on top.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_D_C___Seeing_Off1\">Discussion article for the meetup : <a href=\"/meetups/11m\">Washington D.C.: Seeing Off</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington D.C.: Seeing Off", "anchor": "Discussion_article_for_the_meetup___Washington_D_C___Seeing_Off", "level": 1}, {"title": "Discussion article for the meetup : Washington D.C.: Seeing Off", "anchor": "Discussion_article_for_the_meetup___Washington_D_C___Seeing_Off1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-21T16:14:31.854Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-71", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XEFKqdWjxbXC7u7TZ/weekly-lw-meetups-71", "pageUrlRelative": "/posts/XEFKqdWjxbXC7u7TZ/weekly-lw-meetups-71", "linkUrl": "https://www.lesswrong.com/posts/XEFKqdWjxbXC7u7TZ/weekly-lw-meetups-71", "postedAtFormatted": "Saturday, June 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEFKqdWjxbXC7u7TZ%2Fweekly-lw-meetups-71%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEFKqdWjxbXC7u7TZ%2Fweekly-lw-meetups-71", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEFKqdWjxbXC7u7TZ%2Fweekly-lw-meetups-71", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 534, "htmlBody": "<p><strong>This summary was posted to LW Main on June 13th. The following week's summary is <a href=\"/lw/ke4/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/10y\">Bangalore meetup:&nbsp;<span class=\"date\">29 June 2014 04:40PM</span></a></li>\n</ul>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/11d\">Helsinki Meetup:&nbsp;<span class=\"date\">15 June 2014 05:00PM</span></a></li>\n<li><a href=\"/meetups/zr\">Houston, TX:&nbsp;<span class=\"date\">14 June 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/11c\">Boston - Computational Neuroscience of Perception:&nbsp;<span class=\"date\">15 June 2014 03:30PM</span></a></li>\n<li><a href=\"/meetups/111\">Brussels - Neuroatypicality:&nbsp;<span class=\"date\">14 June 2014 07:40PM</span></a></li>\n<li><a href=\"/meetups/10p\">Canberra: Decision Theory:&nbsp;<span class=\"date\">14 June 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/118\">[LA] Second MIRIxLosAngeles Meeting:&nbsp;<span class=\"date\">14 June 2014 10:00AM</span></a></li>\n<li><a href=\"/meetups/11e\">London Social Meetup (possibly) in the Sun:&nbsp;<span class=\"date\">15 June 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/11f\">[Melbourne] July Rationality Dojo: Disagreement:&nbsp;<span class=\"date\">06 July 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/10l\">Sydney Meetup - June:&nbsp;<span class=\"date\">25 June 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/115\">West LA:&nbsp;<span class=\"date\">18 June 2025 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Boston.2C_MA\">Boston</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a></strong><strong>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Buffalo.2C_NY\">Buffalo</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Canberra\">Canberra</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Sydney\">Sydney</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tel_Aviv.2C_Israel\">Tel Aviv</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XEFKqdWjxbXC7u7TZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.8042697000574733e-06, "legacy": true, "legacyId": "26379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dmy5PzjQxSau38ifL", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-21T16:32:50.825Z", "modifiedAt": null, "url": null, "title": "Motivators: Altruistic Actions for Non-Altruistic Reasons", "slug": "motivators-altruistic-actions-for-non-altruistic-reasons", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:34.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ruby", "createdAt": "2014-04-03T03:38:23.914Z", "isAdmin": true, "displayName": "Ruby"}, "userId": "qgdGA4ZEyW7zNdK84", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CQH4AhhKmEgqA7bM9/motivators-altruistic-actions-for-non-altruistic-reasons", "pageUrlRelative": "/posts/CQH4AhhKmEgqA7bM9/motivators-altruistic-actions-for-non-altruistic-reasons", "linkUrl": "https://www.lesswrong.com/posts/CQH4AhhKmEgqA7bM9/motivators-altruistic-actions-for-non-altruistic-reasons", "postedAtFormatted": "Saturday, June 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Motivators%3A%20Altruistic%20Actions%20for%20Non-Altruistic%20Reasons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMotivators%3A%20Altruistic%20Actions%20for%20Non-Altruistic%20Reasons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQH4AhhKmEgqA7bM9%2Fmotivators-altruistic-actions-for-non-altruistic-reasons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Motivators%3A%20Altruistic%20Actions%20for%20Non-Altruistic%20Reasons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQH4AhhKmEgqA7bM9%2Fmotivators-altruistic-actions-for-non-altruistic-reasons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQH4AhhKmEgqA7bM9%2Fmotivators-altruistic-actions-for-non-altruistic-reasons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2627, "htmlBody": "<h2>Introduction</h2>\n<p><em>Jane is an effective altruist: she researches, donates, and volunteers in the highest impact ways she can find. Jane has been intending to write an effective altruism book for over a year, but&nbsp;hasn't&nbsp;managed to overcome the akrasia. Jane then meets fellow effective altruist, Jessica, who she is keen to impress. She starts writing with palpable enthusiasm.</em></p>\n<p>In one possible world:</p>\n<p><em>Jane feels guilty that she has an impure motive for writing the book.</em></p>\n<p>In another:</p>\n<p><em>Jane is glad to leverage the motivation to impress Jessica to help her do good.</em></p>\n<p>&nbsp;</p>\n<p>In the past few months, I've heard multiple people mention their use of less noble motivations in order to get valuable things done. It appears to be a common experience among rationalists and EAs, myself included. The way I'm using the terms, a <em>reason</em> for performing some action is the ostensible goal you wish to accomplish, e.g. the goal of reducing suffering. A <em>motivator</em> for that action is an associated reward which makes performing the action seem enticing - &ldquo;yummy&rdquo; - e.g. impressing your friends. I use the less common term &lsquo;motivator&rsquo; to distinguish the specific motivations I'm discussing from the more general meaning of &lsquo;motivation&rsquo;.</p>\n<p>Many of our goals are multiple steps removed from the actions necessary to achieve them, particularly the broad-scale altruistic ones. The goals are large, abstract, long-term, ill-specified, difficult to see progress on, and unintuitively connected to the action required. &lsquo;I wrote a LessWrong post, is the world more rational yet?&rsquo; In contrast, motivators are tangible, immediate, and typically tickle the brain&rsquo;s reward centres right in the sweet spot. Social approval, enjoyment of the action, money, skills gained, and others all serve as imminent rewards whose immediate anticipation drives us. &lsquo;Woohoo, 77 upvotes!&rsquo; Unsurprisingly, we find ourselves turning to these immediate rewards if we want to accomplish something.</p>\n<p>Note that a reason - the ostensible goal - can still be the root cause of the desire to perform an altruistic action. For one thing, if I didn't truly care about all the things I say I care about doing, and really only wanted social approval, then why join this particular group of people out of all others?&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Embrace or Disgrace?</h2>\n<p>If motivators are the mechanism by which I'm getting things done, then I want to know exactly what&rsquo;s going on, what benefits I&rsquo;m getting, and what costs I'm paying. To date, I have seen people respond in two ways after recognising their motivators for altruistic action: i) by enthusiastically embracing the ability of motivators to spur good action, or ii) by feeling abject shame and guilt at not acting for the right reasons.&nbsp;</p>\n<p>The second response follows from society&rsquo;s conventional attitude towards anything it deems to be an impure motive:&nbsp;<em>absolute and unrestrained damnation</em>. Few things are considered more evil than doing public good for personal gain.&nbsp;</p>\n<blockquote>\n<p>We have a visceral reaction to the idea that anyone would make very much money helping other people. Interesting that we don't have a visceral reaction to the notion that people would make a lot of money not helping other people. You know, if you want to make 50 million dollars selling violent video games to kids, go for it. We'll put you on the cover of Wired magazine. But you want to make half a million dollars trying to cure kids of malaria, and you're considered a parasite yourself.<br />&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">--</span>&nbsp;Dan Pallotta on <a href=\"http://www.ted.com/talks/dan_pallotta_the_way_we_think_about_charity_is_dead_wrong/transcript\">The Way We Think About Charity is Dead Wrong</a></p>\n</blockquote>\n<p>Anyone who has internalised this cannot acknowledge their motivators without admitting to themselves that they are a bad person. And to the extent that we expect others have internalised this, we are reluctant to disclose our motivators for fear of censure. Even if you are not morally condemned, acting solely for the benefit of your beneficiary is always considered more praiseworthy than acting for the benefit of the beneficiary as well as your own gain. This is so much so, that often people consider a charitable act praiseworthy only if the benefactor had no personal gain. Hence the perennially popular question &ldquo;Is true altruism ever possible if you&rsquo;re always getting something out of it?&rdquo;</p>\n<p>Momentarily I will assert that society is foolish in this regard, but society&rsquo;s foolishness typically has an explanation - often that it was an attitude which was once adaptive but is no longer so, or was adaptive in a different context but it didn't transfer. Given the vehemence towards impure motives, there might be something to these reasons.</p>\n<p>If I consider immediate personal relations, then I find that I would much prefer to be friends with someone who wants to be my friend just because they like me for me rather than with someone who wants to be my friend, but has admitted that she finds being my friend a lot easier because she likes using my swimming pool on these hot summer days. The former friend&rsquo;s friendship is more unconditional and trustworthy - the latter might desert me a soon as winter comes. That motivators introduce an amount of contingency to one&rsquo;s allegiances is a point worth noticing.</p>\n<p>In contrast, the first response &ndash; enthusiastic embrace of motivators &ndash; is the consequentialist liberation from being overly concerned with the motives of the actor. What matters are the consequences! And if motivators mean more good things get done, well, then they get the Official Consequentialist Stamp of Approval. I don&rsquo;t care if you cured malaria solely for profit, I just care that you cured malaria. But this point requires little pushing in these parts.&nbsp;</p>\n<p>It might even be that not only do motivators provide a stronger drive for altruistic action, but they are in fact the only way to get ourselves to act. Even if some parts of our minds can take on long-term, broad-scale, intangible goals, other parts just don&rsquo;t speak that language. The <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">rider</a> might be able to engage in long term planning, but if you want the <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">elephant</a> to budge, you've got to produce some carrots now.</p>\n<p>An interesting aside, the use of motivators to get things done may be more necessary for effective altruists than the general altruistic population. Warm fuzzies are great motivators. Directly seeing those you are helping at the local shelter or looking at a photo of the smiling child who you sent money to might provide a reward immediate enough to require no other. Whereas, when you&rsquo;re donating to curing schistosomiasis or reducing x-risk, the benefit is harder to feel and you've got to get a thumbs up from your friends instead to feel good.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Caveats</h2>\n<p>While the above may be enough reason to endorse the use of motivators to get things done, there is reason for caution.</p>\n<h4>Motivated Cognition</h4>\n<p>Foremost, motivators induce <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a>. When selecting altruistic projects, one&rsquo;s choice becomes distorted from what would actually have the highest impact to that which has the strongest motivator, typically what will impress people the most. Furthermore, if motivators are accepted then it is legitimate to include them in the equation. If one project has a greater motivator and is more likely to get done because of it, then even if it prima facie isn't the highest impact, that likelihood of getting it done is strong factor in its favour. But if this is admissible reasoning, it becomes very easy to abuse: &ldquo;Well sure this isn&rsquo;t the highest impact thing I could do, but volunteering at the local shelter is something I feel motivated to and actually will do, so therefore it&rsquo;s the thing I should do.&rdquo;&nbsp;</p>\n<h4>Pretending to Try</h4>\n<p>These points have already been identified in the discussion of the &lsquo;pretending to try&rsquo; phenomenon.</p>\n<blockquote>\n<p>A lot of effective altruists still end up satisficing&mdash;finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors.&nbsp;<br />-- Ben Kuhn on <a href=\"/r/lesswrong/lw/j8n/a_critique_of_effective_altruism/#abstract\">Pretending to Try</a></p>\n</blockquote>\n<p>These random factors will be whatever happens to be the motivators for a person. Nevertheless, it is better that people do something good rather than nothing. Katja Grace argues that though this is what is going on, it is both inevitable and actually positive. I am tempted to quote her entire post, so I suggest that you should read <a href=\"http://meteuphoric.wordpress.com/2013/12/22/pretend-to-really-really-try/\">all of it</a>.&nbsp;</p>\n<blockquote>\n<p><strong>&lsquo;Really trying&rsquo;</strong>: directing all of your effort toward actions that you believe have the highest expected value in terms of the relevant goals [not making decisions based on motivators].</p>\n<p><strong>&lsquo;Pretending to try&lsquo;</strong>: choosing actions with the intention of giving observers the impression that you are trying [making decisions based on motivators].</p>\n<p><strong>&lsquo;Pretending to really try&lsquo;</strong>: choosing actions with the intention of giving observers the impression that you are trying, where the observers&rsquo; standards for identifying &lsquo;trying&rsquo; are geared toward a &lsquo;really trying&rsquo; model. e.g. they ask whether you are really putting in effort, and whether you are doing what should have highest expected value from your point of view.</p>\n<p>-- Katja Grace on <a href=\"http://meteuphoric.wordpress.com/2013/12/22/pretend-to-really-really-try/\">In Praise of Pretending to Really Try</a></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>The proposed solution is that we can leverage the power of motivators and still have people perform the highest impact actions they can, if we can create community norms whereby the the amount social praise you get is proportional to the strength of your case for the impact of your action is.</p>\n<p>I like this, but it hasn't happened yet and I suspect there are barriers to making it work completely. Even if your action is selected solely for impact - truly trying - the reasoning behind your selection might be complicated and require time to follow and verify. A few close friends might check your plans and approve wholeheartedly when you pretend to really try, but the broader community won&rsquo;t hear out all the details specific to your situation, instead continuing to praise only actions which fit the template of good effective altruist behaviour, e.g. taking a high paying job in order to earn to give.</p>\n<p>Possibly the best we can do for community norms is to find and spread the best simple principles for deciding whether someone is really trying. The principles involved are unlikely to be adequately nuanced to identify the true optimum all of the time, but I think there&rsquo;s room to improve over what we've fallen into so far. &nbsp;To date, I see praise being given primarily for actions which are distinctive EA behaviour and signal belonging to the tribe. Conventional altruistic actions like volunteering in the third world aren't distinctly EA and I don&rsquo;t expect them to get much praise, even if such actions were the highest impact for a particular person. More likely, a person doing something which doesn't fit the EA mould will be interrogated for failure to conform to what <em>EAs are supposed to do</em>.</p>\n<p>&nbsp;</p>\n<h2>Neglected Tasks</h2>\n<p>We can concretely see the impact a reliance on motivators has by noticing the many neglected tasks which result. High value actions which are not prestigious go undone because all they've got going for them is their pure altruistic impact.&nbsp;</p>\n<blockquote>\n<p>The Centre for Effective Altruism has had a surprising amount of trouble finding people to do whatever important work needs doing when it isn't research or communications. These things include: organising insurance, bookkeeping and making payments, maintaining our databases, making deliveries, ordering equipment, finding and managing places for people to live, random requests (e.g. cutting keys), receiving and processing mail, cleaning, organising food and office events, etc.</p>\n<p>It's a bit of a shame that people seem willing to do whatever is most important... except whenever it isn't inherently fun or prestigious! <br />-- <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/591981677524860/\">Robert Wiblin</a></p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>\n<p>. . . I've had 200 volunteers offer to do work for Singularity Institute. Many have claimed they would do \"anything\" or \"whatever helped the most\". SEO is clearly the most valuable work. Unfortunately, it's something \"so mundane\", that anybody could do it... therefore, 0 out of 200 volunteers are currently working on it. This is even after I've personally asked over 100 people to help with it. <br />-- <a href=\"/lw/8ns/hack_away_at_the_edges/5d5c\">Louie Helm</a></p>\n</blockquote>\n<p>CFAR have made similar comments.</p>\n<p>This is a serious issue for a community claiming to be serious about maximising impact.</p>\n<p>&nbsp;</p>\n<h2>Suggestions</h2>\n<p>Motivators are sufficiently powerful, if not unavoidable, that we should allow ourselves to work with them despite their dangers. The question becomes how to use them while minimising their pernicious effects. The &lsquo;pretending to try&rsquo; discussion concerns the community collectively, but I am interested in how individuals should approach their own motivators.&nbsp;</p>\n<p>I have a few ideas so far. The aim of these techniques is to limit the influence motivators have on our selection of altruistic projects, even if we allow or welcome them once we're onto implementing our plans.</p>\n<h4>Awareness</h4>\n<p>When choosing a course of action, pay attention to what your motivators might be. Ask &lsquo;what are the personal benefits I get out of this?&rsquo;, &lsquo;how much are these influencing my decision?&rsquo; and &lsquo;if this action A lacked the consequence of personal benefit B, would I still do it?&rsquo;</p>\n<h4>Self-Honesty</h4>\n<p>Attempting genuine awareness requires a high-degree of self-honesty. You have to be willing to admit that in all likelihood you have motivators already and they influence your decisions, and to acknowledge that even when you are trying to do good for others, you are interested in your own gain. If this admission is hard, I suggest remembering that this how people work, everyone else is the same and only the difference is that you&rsquo;re being honest with yourself.</p>\n<h4><span style=\"font-size: 14px;\">Choose, then Motivate</span></h4>\n<p>One strategy is to fully acknowledge to yourself that you want some immediate personal gain from your actions, but delaying thinking about that personal gain until after you have made a decision about what to do based solely upon expected impact. Once you've identified the highest impact action, then brainstorm ways to find motivators. This might involve nothing more than developing a good framing for your actions which makes you look very noble indeed. And most of the time that should be doable if you genuinely have a good reason.</p>\n<h4>Optimise Someone Else&rsquo;s Altruism</h4>\n<p>One more way to limit the influence motivators have over your decision making is to pretend that you are deciding what <em>someone else</em> &ndash; who is in exactly your situation with exactly your talents &ndash; should do to maximise their impact. You are advising this other person, whose interests you don&rsquo;t care for because they are not you, on how they might accomplish the most can towards their goals.</p>\n<h4 style=\"font-size: 14px; color: #000000; float: none; font-family: Verdana, Arial, Helvetica, sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;\">Really Caring</h4>\n<p>Probably the best way to ensure that you really try is to ensure that you <em>really care. </em>If you focus on your reason for action - the outcome that it is really about - then petty things like other people's praise will feel that important than actually accomplishing your true goal.</p>\n<p>Bring this feeling of caring to the fore often. Are you trying to cure <a href=\"http://www.cdc.gov/malaria/malaria_worldwide/impact.html\">malaria</a>? Keep a card with various malaria statistics on your desk, read it often, and remind yourself that you want stop those deaths which are happening right now. Care about the <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Far Future</a>? Imagine your own <a href=\"http://wiki.lesswrong.com/wiki/Fun_theory\">fun-theoretic utopia</a>, visualise it, and think about how good it would be to get there.</p>\n<h2><br /></h2>\n<h2>Conclusion</h2>\n<p>This is one attempt at getting a handle on motivators and I am unsure about much of it. There will be other angles to view this from, there are things I haven&rsquo;t thought of, and mistakes in some of my assumptions. Plus, <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">variation in human minds</a> is astounding. Though many will experience motivation the way that I do, others will find what I'm reporting very alien. From them I would like to hear.&nbsp;</p>\n<p>What I am sure about is that if we want to live up to our principles of doing what is truly most effective, we cannot ignore the factors driving our behaviour. Here's hoping that we can do what we really need to do and feel maximally good about it too.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Acknowledgements: I owe an enormous thank you to <a href=\"/user/tkadlubo/overview/\">tkadlubo</a>&nbsp;and <a href=\"/user/shokwave/overview/\">shokwave</a>&nbsp;for thoroughly editing this post.&nbsp;</p>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 7, "xexCWMyds6QLWognu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CQH4AhhKmEgqA7bM9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 26, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "26429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Introduction\">Introduction</h2>\n<p><em>Jane is an effective altruist: she researches, donates, and volunteers in the highest impact ways she can find. Jane has been intending to write an effective altruism book for over a year, but&nbsp;hasn't&nbsp;managed to overcome the akrasia. Jane then meets fellow effective altruist, Jessica, who she is keen to impress. She starts writing with palpable enthusiasm.</em></p>\n<p>In one possible world:</p>\n<p><em>Jane feels guilty that she has an impure motive for writing the book.</em></p>\n<p>In another:</p>\n<p><em>Jane is glad to leverage the motivation to impress Jessica to help her do good.</em></p>\n<p>&nbsp;</p>\n<p>In the past few months, I've heard multiple people mention their use of less noble motivations in order to get valuable things done. It appears to be a common experience among rationalists and EAs, myself included. The way I'm using the terms, a <em>reason</em> for performing some action is the ostensible goal you wish to accomplish, e.g. the goal of reducing suffering. A <em>motivator</em> for that action is an associated reward which makes performing the action seem enticing - \u201cyummy\u201d - e.g. impressing your friends. I use the less common term \u2018motivator\u2019 to distinguish the specific motivations I'm discussing from the more general meaning of \u2018motivation\u2019.</p>\n<p>Many of our goals are multiple steps removed from the actions necessary to achieve them, particularly the broad-scale altruistic ones. The goals are large, abstract, long-term, ill-specified, difficult to see progress on, and unintuitively connected to the action required. \u2018I wrote a LessWrong post, is the world more rational yet?\u2019 In contrast, motivators are tangible, immediate, and typically tickle the brain\u2019s reward centres right in the sweet spot. Social approval, enjoyment of the action, money, skills gained, and others all serve as imminent rewards whose immediate anticipation drives us. \u2018Woohoo, 77 upvotes!\u2019 Unsurprisingly, we find ourselves turning to these immediate rewards if we want to accomplish something.</p>\n<p>Note that a reason - the ostensible goal - can still be the root cause of the desire to perform an altruistic action. For one thing, if I didn't truly care about all the things I say I care about doing, and really only wanted social approval, then why join this particular group of people out of all others?&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Embrace_or_Disgrace_\">Embrace or Disgrace?</h2>\n<p>If motivators are the mechanism by which I'm getting things done, then I want to know exactly what\u2019s going on, what benefits I\u2019m getting, and what costs I'm paying. To date, I have seen people respond in two ways after recognising their motivators for altruistic action: i) by enthusiastically embracing the ability of motivators to spur good action, or ii) by feeling abject shame and guilt at not acting for the right reasons.&nbsp;</p>\n<p>The second response follows from society\u2019s conventional attitude towards anything it deems to be an impure motive:&nbsp;<em>absolute and unrestrained damnation</em>. Few things are considered more evil than doing public good for personal gain.&nbsp;</p>\n<blockquote>\n<p>We have a visceral reaction to the idea that anyone would make very much money helping other people. Interesting that we don't have a visceral reaction to the notion that people would make a lot of money not helping other people. You know, if you want to make 50 million dollars selling violent video games to kids, go for it. We'll put you on the cover of Wired magazine. But you want to make half a million dollars trying to cure kids of malaria, and you're considered a parasite yourself.<br>&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">--</span>&nbsp;Dan Pallotta on <a href=\"http://www.ted.com/talks/dan_pallotta_the_way_we_think_about_charity_is_dead_wrong/transcript\">The Way We Think About Charity is Dead Wrong</a></p>\n</blockquote>\n<p>Anyone who has internalised this cannot acknowledge their motivators without admitting to themselves that they are a bad person. And to the extent that we expect others have internalised this, we are reluctant to disclose our motivators for fear of censure. Even if you are not morally condemned, acting solely for the benefit of your beneficiary is always considered more praiseworthy than acting for the benefit of the beneficiary as well as your own gain. This is so much so, that often people consider a charitable act praiseworthy only if the benefactor had no personal gain. Hence the perennially popular question \u201cIs true altruism ever possible if you\u2019re always getting something out of it?\u201d</p>\n<p>Momentarily I will assert that society is foolish in this regard, but society\u2019s foolishness typically has an explanation - often that it was an attitude which was once adaptive but is no longer so, or was adaptive in a different context but it didn't transfer. Given the vehemence towards impure motives, there might be something to these reasons.</p>\n<p>If I consider immediate personal relations, then I find that I would much prefer to be friends with someone who wants to be my friend just because they like me for me rather than with someone who wants to be my friend, but has admitted that she finds being my friend a lot easier because she likes using my swimming pool on these hot summer days. The former friend\u2019s friendship is more unconditional and trustworthy - the latter might desert me a soon as winter comes. That motivators introduce an amount of contingency to one\u2019s allegiances is a point worth noticing.</p>\n<p>In contrast, the first response \u2013 enthusiastic embrace of motivators \u2013 is the consequentialist liberation from being overly concerned with the motives of the actor. What matters are the consequences! And if motivators mean more good things get done, well, then they get the Official Consequentialist Stamp of Approval. I don\u2019t care if you cured malaria solely for profit, I just care that you cured malaria. But this point requires little pushing in these parts.&nbsp;</p>\n<p>It might even be that not only do motivators provide a stronger drive for altruistic action, but they are in fact the only way to get ourselves to act. Even if some parts of our minds can take on long-term, broad-scale, intangible goals, other parts just don\u2019t speak that language. The <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">rider</a> might be able to engage in long term planning, but if you want the <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">elephant</a> to budge, you've got to produce some carrots now.</p>\n<p>An interesting aside, the use of motivators to get things done may be more necessary for effective altruists than the general altruistic population. Warm fuzzies are great motivators. Directly seeing those you are helping at the local shelter or looking at a photo of the smiling child who you sent money to might provide a reward immediate enough to require no other. Whereas, when you\u2019re donating to curing schistosomiasis or reducing x-risk, the benefit is harder to feel and you've got to get a thumbs up from your friends instead to feel good.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Caveats\">Caveats</h2>\n<p>While the above may be enough reason to endorse the use of motivators to get things done, there is reason for caution.</p>\n<h4 id=\"Motivated_Cognition\">Motivated Cognition</h4>\n<p>Foremost, motivators induce <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a>. When selecting altruistic projects, one\u2019s choice becomes distorted from what would actually have the highest impact to that which has the strongest motivator, typically what will impress people the most. Furthermore, if motivators are accepted then it is legitimate to include them in the equation. If one project has a greater motivator and is more likely to get done because of it, then even if it prima facie isn't the highest impact, that likelihood of getting it done is strong factor in its favour. But if this is admissible reasoning, it becomes very easy to abuse: \u201cWell sure this isn\u2019t the highest impact thing I could do, but volunteering at the local shelter is something I feel motivated to and actually will do, so therefore it\u2019s the thing I should do.\u201d&nbsp;</p>\n<h4 id=\"Pretending_to_Try\">Pretending to Try</h4>\n<p>These points have already been identified in the discussion of the \u2018pretending to try\u2019 phenomenon.</p>\n<blockquote>\n<p>A lot of effective altruists still end up satisficing\u2014finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors.&nbsp;<br>-- Ben Kuhn on <a href=\"/r/lesswrong/lw/j8n/a_critique_of_effective_altruism/#abstract\">Pretending to Try</a></p>\n</blockquote>\n<p>These random factors will be whatever happens to be the motivators for a person. Nevertheless, it is better that people do something good rather than nothing. Katja Grace argues that though this is what is going on, it is both inevitable and actually positive. I am tempted to quote her entire post, so I suggest that you should read <a href=\"http://meteuphoric.wordpress.com/2013/12/22/pretend-to-really-really-try/\">all of it</a>.&nbsp;</p>\n<blockquote>\n<p><strong>\u2018Really trying\u2019</strong>: directing all of your effort toward actions that you believe have the highest expected value in terms of the relevant goals [not making decisions based on motivators].</p>\n<p><strong>\u2018Pretending to try\u2018</strong>: choosing actions with the intention of giving observers the impression that you are trying [making decisions based on motivators].</p>\n<p><strong>\u2018Pretending to really try\u2018</strong>: choosing actions with the intention of giving observers the impression that you are trying, where the observers\u2019 standards for identifying \u2018trying\u2019 are geared toward a \u2018really trying\u2019 model. e.g. they ask whether you are really putting in effort, and whether you are doing what should have highest expected value from your point of view.</p>\n<p>-- Katja Grace on <a href=\"http://meteuphoric.wordpress.com/2013/12/22/pretend-to-really-really-try/\">In Praise of Pretending to Really Try</a></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>The proposed solution is that we can leverage the power of motivators and still have people perform the highest impact actions they can, if we can create community norms whereby the the amount social praise you get is proportional to the strength of your case for the impact of your action is.</p>\n<p>I like this, but it hasn't happened yet and I suspect there are barriers to making it work completely. Even if your action is selected solely for impact - truly trying - the reasoning behind your selection might be complicated and require time to follow and verify. A few close friends might check your plans and approve wholeheartedly when you pretend to really try, but the broader community won\u2019t hear out all the details specific to your situation, instead continuing to praise only actions which fit the template of good effective altruist behaviour, e.g. taking a high paying job in order to earn to give.</p>\n<p>Possibly the best we can do for community norms is to find and spread the best simple principles for deciding whether someone is really trying. The principles involved are unlikely to be adequately nuanced to identify the true optimum all of the time, but I think there\u2019s room to improve over what we've fallen into so far. &nbsp;To date, I see praise being given primarily for actions which are distinctive EA behaviour and signal belonging to the tribe. Conventional altruistic actions like volunteering in the third world aren't distinctly EA and I don\u2019t expect them to get much praise, even if such actions were the highest impact for a particular person. More likely, a person doing something which doesn't fit the EA mould will be interrogated for failure to conform to what <em>EAs are supposed to do</em>.</p>\n<p>&nbsp;</p>\n<h2 id=\"Neglected_Tasks\">Neglected Tasks</h2>\n<p>We can concretely see the impact a reliance on motivators has by noticing the many neglected tasks which result. High value actions which are not prestigious go undone because all they've got going for them is their pure altruistic impact.&nbsp;</p>\n<blockquote>\n<p>The Centre for Effective Altruism has had a surprising amount of trouble finding people to do whatever important work needs doing when it isn't research or communications. These things include: organising insurance, bookkeeping and making payments, maintaining our databases, making deliveries, ordering equipment, finding and managing places for people to live, random requests (e.g. cutting keys), receiving and processing mail, cleaning, organising food and office events, etc.</p>\n<p>It's a bit of a shame that people seem willing to do whatever is most important... except whenever it isn't inherently fun or prestigious! <br>-- <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/591981677524860/\">Robert Wiblin</a></p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>\n<p>. . . I've had 200 volunteers offer to do work for Singularity Institute. Many have claimed they would do \"anything\" or \"whatever helped the most\". SEO is clearly the most valuable work. Unfortunately, it's something \"so mundane\", that anybody could do it... therefore, 0 out of 200 volunteers are currently working on it. This is even after I've personally asked over 100 people to help with it. <br>-- <a href=\"/lw/8ns/hack_away_at_the_edges/5d5c\">Louie Helm</a></p>\n</blockquote>\n<p>CFAR have made similar comments.</p>\n<p>This is a serious issue for a community claiming to be serious about maximising impact.</p>\n<p>&nbsp;</p>\n<h2 id=\"Suggestions\">Suggestions</h2>\n<p>Motivators are sufficiently powerful, if not unavoidable, that we should allow ourselves to work with them despite their dangers. The question becomes how to use them while minimising their pernicious effects. The \u2018pretending to try\u2019 discussion concerns the community collectively, but I am interested in how individuals should approach their own motivators.&nbsp;</p>\n<p>I have a few ideas so far. The aim of these techniques is to limit the influence motivators have on our selection of altruistic projects, even if we allow or welcome them once we're onto implementing our plans.</p>\n<h4 id=\"Awareness\">Awareness</h4>\n<p>When choosing a course of action, pay attention to what your motivators might be. Ask \u2018what are the personal benefits I get out of this?\u2019, \u2018how much are these influencing my decision?\u2019 and \u2018if this action A lacked the consequence of personal benefit B, would I still do it?\u2019</p>\n<h4 id=\"Self_Honesty\">Self-Honesty</h4>\n<p>Attempting genuine awareness requires a high-degree of self-honesty. You have to be willing to admit that in all likelihood you have motivators already and they influence your decisions, and to acknowledge that even when you are trying to do good for others, you are interested in your own gain. If this admission is hard, I suggest remembering that this how people work, everyone else is the same and only the difference is that you\u2019re being honest with yourself.</p>\n<h4 id=\"Choose__then_Motivate\"><span style=\"font-size: 14px;\">Choose, then Motivate</span></h4>\n<p>One strategy is to fully acknowledge to yourself that you want some immediate personal gain from your actions, but delaying thinking about that personal gain until after you have made a decision about what to do based solely upon expected impact. Once you've identified the highest impact action, then brainstorm ways to find motivators. This might involve nothing more than developing a good framing for your actions which makes you look very noble indeed. And most of the time that should be doable if you genuinely have a good reason.</p>\n<h4 id=\"Optimise_Someone_Else_s_Altruism\">Optimise Someone Else\u2019s Altruism</h4>\n<p>One more way to limit the influence motivators have over your decision making is to pretend that you are deciding what <em>someone else</em> \u2013 who is in exactly your situation with exactly your talents \u2013 should do to maximise their impact. You are advising this other person, whose interests you don\u2019t care for because they are not you, on how they might accomplish the most can towards their goals.</p>\n<h4 style=\"font-size: 14px; color: #000000; float: none; font-family: Verdana, Arial, Helvetica, sans-serif; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;\" id=\"Really_Caring\">Really Caring</h4>\n<p>Probably the best way to ensure that you really try is to ensure that you <em>really care. </em>If you focus on your reason for action - the outcome that it is really about - then petty things like other people's praise will feel that important than actually accomplishing your true goal.</p>\n<p>Bring this feeling of caring to the fore often. Are you trying to cure <a href=\"http://www.cdc.gov/malaria/malaria_worldwide/impact.html\">malaria</a>? Keep a card with various malaria statistics on your desk, read it often, and remind yourself that you want stop those deaths which are happening right now. Care about the <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Far Future</a>? Imagine your own <a href=\"http://wiki.lesswrong.com/wiki/Fun_theory\">fun-theoretic utopia</a>, visualise it, and think about how good it would be to get there.</p>\n<h2><br></h2>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>This is one attempt at getting a handle on motivators and I am unsure about much of it. There will be other angles to view this from, there are things I haven\u2019t thought of, and mistakes in some of my assumptions. Plus, <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">variation in human minds</a> is astounding. Though many will experience motivation the way that I do, others will find what I'm reporting very alien. From them I would like to hear.&nbsp;</p>\n<p>What I am sure about is that if we want to live up to our principles of doing what is truly most effective, we cannot ignore the factors driving our behaviour. Here's hoping that we can do what we really need to do and feel maximally good about it too.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Acknowledgements: I owe an enormous thank you to <a href=\"/user/tkadlubo/overview/\">tkadlubo</a>&nbsp;and <a href=\"/user/shokwave/overview/\">shokwave</a>&nbsp;for thoroughly editing this post.&nbsp;</p>\n<div><br></div>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Embrace or Disgrace?", "anchor": "Embrace_or_Disgrace_", "level": 1}, {"title": "Caveats", "anchor": "Caveats", "level": 1}, {"title": "Motivated Cognition", "anchor": "Motivated_Cognition", "level": 2}, {"title": "Pretending to Try", "anchor": "Pretending_to_Try", "level": 2}, {"title": "Neglected Tasks", "anchor": "Neglected_Tasks", "level": 1}, {"title": "Suggestions", "anchor": "Suggestions", "level": 1}, {"title": "Awareness", "anchor": "Awareness", "level": 2}, {"title": "Self-Honesty", "anchor": "Self_Honesty", "level": 2}, {"title": "Choose, then Motivate", "anchor": "Choose__then_Motivate", "level": 2}, {"title": "Optimise Someone Else\u2019s Altruism", "anchor": "Optimise_Someone_Else_s_Altruism", "level": 2}, {"title": "Really Caring", "anchor": "Really_Caring", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["du395YvCnQXBPSJax"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-21T22:27:31.620Z", "modifiedAt": null, "url": null, "title": "Will AGI surprise the world?", "slug": "will-agi-surprise-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pAwhfwG6rgjabJL4T/will-agi-surprise-the-world", "pageUrlRelative": "/posts/pAwhfwG6rgjabJL4T/will-agi-surprise-the-world", "linkUrl": "https://www.lesswrong.com/posts/pAwhfwG6rgjabJL4T/will-agi-surprise-the-world", "postedAtFormatted": "Saturday, June 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Will%20AGI%20surprise%20the%20world%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWill%20AGI%20surprise%20the%20world%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAwhfwG6rgjabJL4T%2Fwill-agi-surprise-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Will%20AGI%20surprise%20the%20world%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAwhfwG6rgjabJL4T%2Fwill-agi-surprise-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAwhfwG6rgjabJL4T%2Fwill-agi-surprise-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 555, "htmlBody": "<p><small>Cross-posted from <a href=\"http://lukemuehlhauser.com/will-agi-surprise-the-world/\">my blog</a>.</small></p>\n<p>Yudkowsky <a href=\"/lw/hp5/after_critical_event_w_happens_they_still_wont/\">writes</a>:</p>\n<blockquote>\n<p>In general and across all instances I can think of so far, I do not agree with the part of your futurological forecast in which you reason, \"After event W happens, everyone will see the truth of proposition X, leading them to endorse Y and agree with me about policy decision Z.\"</p>\n<p>...</p>\n<p>Example 2: \"As AI gets more sophisticated, everyone will realize that real AI is on the way and then they'll start taking Friendly AI development seriously.\"</p>\n<p>Alternative projection: As AI gets more sophisticated, the rest of society can't see any difference between the latest breakthrough reported in a press release and that business earlier with Watson beating Ken Jennings or Deep Blue beating Kasparov; it seems like the same sort of press release to them. The same people who were talking about robot overlords earlier continue to talk about robot overlords. The same people who were talking about human irreproducibility continue to talk about human specialness. Concern is expressed over technological unemployment the same as today or Keynes in 1930, and this is used to fuel someone's previous ideological commitment to a basic income guarantee, inequality reduction, or whatever. The same tiny segment of unusually consequentialist people are concerned about Friendly AI as before. If anyone in the science community does start thinking that superintelligent AI is on the way, they exhibit the same distribution of performance as modern scientists who think it's on the way, e.g. Hugo de Garis, Ben Goertzel, etc.</p>\n</blockquote>\n<p>My&nbsp;own projection goes more like this:</p>\n<blockquote>\n<p>As AI gets more sophisticated, and as more prestigious AI scientists begin to publicly acknowledge that AI is plausibly&nbsp;only 2-6 decades away, policy-makers and research funders will begin to&nbsp;respond to&nbsp;the AGI safety challenge, just like&nbsp;they began to respond&nbsp;to CFC&nbsp;damages in the late 70s,&nbsp;to global warming in the late 80s, and to synbio developments in the 2010s. As for society at large, I dunno.&nbsp;They'll think all kinds of random stuff for random reasons, and in some cases&nbsp;this will seriously impede effective policy, as it does in the USA for science education and immigration reform. Because AGI lends itself to arms races and is harder to handle adequately&nbsp;than global warming or nuclear security are, policy-makers and industry leaders will generally know AGI is coming but be unable to fund the needed&nbsp;efforts and coordinate effectively enough to ensure good outcomes.</p>\n</blockquote>\n<p>At least one clear difference between my projection and Yudkowsky's is that I expect AI-expert&nbsp;performance on the problem to improve&nbsp;substantially as a greater fraction of <em>elite</em> AI scientists begin to think about the issue in <a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">Near mode&nbsp;rather than Far mode</a>.</p>\n<p>As a friend of mine suggested&nbsp;recently,&nbsp;current elite awareness of the AGI safety challenge is roughly where elite awareness of the global warming challenge was in the early 80s. Except, I expect&nbsp;elite acknowledgement&nbsp;of the AGI safety challenge to spread more slowly than it did for global warming or nuclear security, because&nbsp;AGI is tougher&nbsp;to forecast in general, and involves trickier philosophical nuances. (Nobody was ever tempted to say, \"But as&nbsp;the nuclear chain reaction grows in power, it will necessarily&nbsp;become more moral!\")</p>\n<p>Still, there is a worryingly non-negligible&nbsp;chance that AGI explodes \"out of nowhere.\" Sometimes important theorems are proved suddenly after decades of failed attempts by other mathematicians, and sometimes a computational procedure is <a href=\"http://www.cs.nyu.edu/~mwalfish/papers/pepper-ndss12.pdf\">sped up by 20 orders of magnitude with a single breakthrough</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pAwhfwG6rgjabJL4T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "26430", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 129, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LNKh22Crr5ujT85YM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-22T10:45:23.895Z", "modifiedAt": null, "url": null, "title": "How do you take notes?", "slug": "how-do-you-take-notes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:08.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d9TKbcko8GsBihvxG/how-do-you-take-notes", "pageUrlRelative": "/posts/d9TKbcko8GsBihvxG/how-do-you-take-notes", "linkUrl": "https://www.lesswrong.com/posts/d9TKbcko8GsBihvxG/how-do-you-take-notes", "postedAtFormatted": "Sunday, June 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20take%20notes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20take%20notes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd9TKbcko8GsBihvxG%2Fhow-do-you-take-notes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20take%20notes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd9TKbcko8GsBihvxG%2Fhow-do-you-take-notes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd9TKbcko8GsBihvxG%2Fhow-do-you-take-notes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>We all deal with a lot of information. What are your strategies of taking notes for new information?</p>\n<p>Do you take any notes on paper? If so do you scan them or otherwise digilatize them?</p>\n<p>Do you have specific strategies for deciding which information to write down?</p>\n<p>How do you write notes to capture all important information?</p>\n<p>Do you tag your notes?</p>\n<p>If you use Evernote, or a similar system how private are your notes? Would you allow friends to read in them? Your spouse?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"puBcCq7aRwKoa7pXX": 2, "fF9GEdWXKJ3z73TmB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d9TKbcko8GsBihvxG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "26433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-23T07:21:58.074Z", "modifiedAt": null, "url": null, "title": "Open thread, 23-29 June 2014", "slug": "open-thread-23-29-june-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:08.641Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aLKTcKikaKtTEzMct/open-thread-23-29-june-2014", "pageUrlRelative": "/posts/aLKTcKikaKtTEzMct/open-thread-23-29-june-2014", "linkUrl": "https://www.lesswrong.com/posts/aLKTcKikaKtTEzMct/open-thread-23-29-june-2014", "postedAtFormatted": "Monday, June 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%2023-29%20June%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%2023-29%20June%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaLKTcKikaKtTEzMct%2Fopen-thread-23-29-june-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%2023-29%20June%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaLKTcKikaKtTEzMct%2Fopen-thread-23-29-june-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaLKTcKikaKtTEzMct%2Fopen-thread-23-29-june-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p><a href=\"/r/discussion/lw/kd3/open_thread_1622_june_2014/\">Previous open thread</a></p>\n<p>&nbsp;</p>\n<p><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; font-weight: bold;\"><br /></span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">Notes for future OT posters:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">1. Please add the 'open_thread' tag.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">2. Check if there is an active Open Thread before posting a new one.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">3. </span><span style=\"line-height: 19px;\">Open Threads should be posted in Discussion, and not Main.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><span style=\"line-height: 19px;\">4. </span><span style=\"line-height: 19px;\">Open Threads should start on Monday, and end on Sunday.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aLKTcKikaKtTEzMct", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "26435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 194, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7D4Qookb2idYAdgEK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-23T17:08:58.453Z", "modifiedAt": null, "url": null, "title": "Lessons from weather forecasting and its history for forecasting as a domain", "slug": "lessons-from-weather-forecasting-and-its-history-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JaSu2DosK2iMhhgST/lessons-from-weather-forecasting-and-its-history-for", "pageUrlRelative": "/posts/JaSu2DosK2iMhhgST/lessons-from-weather-forecasting-and-its-history-for", "linkUrl": "https://www.lesswrong.com/posts/JaSu2DosK2iMhhgST/lessons-from-weather-forecasting-and-its-history-for", "postedAtFormatted": "Monday, June 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lessons%20from%20weather%20forecasting%20and%20its%20history%20for%20forecasting%20as%20a%20domain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessons%20from%20weather%20forecasting%20and%20its%20history%20for%20forecasting%20as%20a%20domain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJaSu2DosK2iMhhgST%2Flessons-from-weather-forecasting-and-its-history-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lessons%20from%20weather%20forecasting%20and%20its%20history%20for%20forecasting%20as%20a%20domain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJaSu2DosK2iMhhgST%2Flessons-from-weather-forecasting-and-its-history-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJaSu2DosK2iMhhgST%2Flessons-from-weather-forecasting-and-its-history-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2978, "htmlBody": "<p><em>This is the first of two (or more) posts that look at the domain of weather and climate forecasting and what we can learn from the history and current state of these fields for forecasting as a domain. It may not be of general interest to the LessWrong community, but I hope that it's of interest to people here who have some interest either in weather-related material or in forecasting in general.</em></p>\n<p>The science of weather forecasting has come a long way over the past century. Since people starting measuring and recording the weather (temperature, precipitation, etc.) <a href=\"http://www.eumetcal.org/intralibrary/open_virtual_file_path/i2055n15861t/english/msg/ver_cont_var/uos5/uos5_ko2.htm\">two simple algorithms for weather prediction</a> have existed (see also <a href=\"http://www.washingtonpost.com/blogs/capital-weather-gang/post/weather-forecast-accuracy-versus-skill-skill-is-what-matters/2012/04/05/gIQA42BixS_blog.html\">this</a>):</p>\n<ul>\n<li><strong>Persistence</strong>: Assume that the weather tomorrow will be the same as the weather today.</li>\n<li><strong>Climatology</strong>: Assume that the weather on a given day of the year will be the same as the average of the weather on that same day in the last few years (we might also use averages for nearby days if we don't have enough years of data).</li>\n</ul>\n<p>Until the end of the 19th century, there was no weather prediction algorithm that did consistently better than both persistence and climatology. Between persistence and climatology, climatology won out over medium to long time horizons (a week or more), whereas persistence won out in some kinds of places over short horizons (1-2 days), though even there, climatology sometimes does better (see more <a href=\"https://courseware.e-education.psu.edu/courses/meteo101/Section11p05.html\">here</a>). Both methods have very limited utility when it comes to predicting and preparing for rare extreme weather events, such as blizzards, hurricanes, cyclones, polar winds, or heavy rainfall.</p>\n<p>This blog post discusses the evolution and progress of weather forecasting algorithms that significantly improve over the benchmarks of persistence and climatology, and the implications both for the future of weather forecasting and for our understanding of forecasting as a domain.</p>\n<p><strong>Sources for further reading (uncited material in my post is usually drawn from one of these)</strong>: <a href=\"https://en.wikipedia.org/wiki/History_of_numerical_weather_prediction\">Wikipedia's page on the history of numerical weather prediction</a>, <em>The Signal and the Noise</em> by Nate Silver, and <a href=\"https://www.rsmas.miami.edu/users/miskandarani/Courses/MPO662/Lynch,Peter/OriginsCompWF.JCP227.pdf\">The origins of computer weather prediction and climate modeling</a> by Peter Lynch.</p>\n<p><strong>The three challenges: theory, measurement, and computation</strong></p>\n<p>The problem facing any method that tries to do better than persistence and climatology is that whereas persistence and climatology can rely on existing aggregate records, any more sophisticated prediction algorithm relies on measuring, theorizing about, and computing with a much larger number of other observed quantities. There are three aspects to the challenge:</p>\n<ul>\n<li>The <strong>theoretical challenge or model selection challenge</strong>: The goal is to write a system of equations describing how the climate system evolves from certain initial measurements. In the weather prediction context, this was the first of the challenges to be nailed: the basic equations of the atmosphere come from physics, which has been well understood for over a century now.</li>\n<li>The <strong>measurement challenge</strong>: A large number of measurements at different points in the area and at regular intervals of time need to be taken to <em>initialize </em>the data appropriately for the weather simulation. The measurement challenge was largely resolved early on: it was easy to set up stations for measuring temperature, humidity, and other indicators around the world, and communications technology enabled the data to be quickly relayed to a central processing station. Of course, many improvements have occurred over the 20th century: we can now make measurements using satellites, as well as directly measure weather indicators at different altitudes. But measurement challenges were not critical in getting weather prediction <em>started</em>.</li>\n<li>The <strong>computational challenge</strong>: This appears to have been the most difficult of the challenges and the critical constraint in making real-time weather predictions. The computations needed for making a forecast that could beat persistence and climatology over any time horizon were just too numerous for humans to carry out in real time. In fact, the ability to make accurate weather predictions was one of the motivations for the development of improved computing machinery.</li>\n</ul>\n<p><strong>The basic theory of weather forecasting</strong></p>\n<p>The basic idea of weather forecasting is to use the equations of physics to model the evolution of the atmospheric system. In order to do this, we need to know how the system looks at a given point. In principle, that information, combined with the equations, should allow us to compute the weather indefinitely into the future. In practice, the equations we create don't have any closed-form solutions, the data is only partial (we don't have initial data on the whole world) and even small variations at a given time can balloon to bigger changes (this is called the <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>; more on this later in the post).</p>\n<p>Instead of trying to solve the system analytically, we <em>discretize</em> the problem (we use discrete spatial locations and discrete time steps) and then solve the problem <em>numerically</em> (this is a bit like using a difference quotient instead of a derivative when computing a rate of change). There are four dimensions to the discretization (three spatial and one temporal), and how fine we make the grid in each dimension is called the <em>resolution</em> in that dimension.</p>\n<ul>\n<li><strong>Spatial dimensions and spatial resolution</strong>: The region over which we are interested in forecasting the weather is converted to a grid. We have freedom in how fine we make the grid. In general, finer grids make for more precise and accurate weather predictions, but require more computational resources. The grid has two horizontal dimensions and one height dimension, hence a total of three dimensions. Thus, making the grid <em>x</em> times as fine (i.e., making the spatial resolution <em>x</em> times as fine) means increasing the number of regions to <em>x</em><sup>3</sup> times the current value.</li>\n<li><strong>Time dimension and temporal resolution</strong>: We also choose a time step. In general, smaller time steps make for more precise and accurate weather predictions, but require more computational resources (because the number of time steps needed to traverse a particular length of time is more). If we divide the time step by <em>x</em>, we multiply the time and space storage needs by a factor of <em>x</em>.</li>\n</ul>\n<p>Thus, roughly, becoming finer by a factor of <em>x</em> in all three spatial dimensions and the time dimension requires upping computational resources to about <em>x</em><sup>4</sup> of the original value. So, doubling in all four dimensions requires improving computational power to 16 times the original value, which means four doublings.&nbsp; Combining this with natural improvements in computing, such as <a href=\"http://en.wikipedia.org/wiki/Moore%27s_law\">Moore's law</a>, we expect that we should be able to make our grid twice as fine in all dimensions (i.e., double the spatial and temporal resolution) every 8 years or so.</p>\n<p><strong>How much precision and accuracy does high resolution buy?<br /></strong></p>\n<p>My intuitive prior would be that, for sufficiently short time horizons where we don't expect chaos to play a dominant role, we'd expect the relationship suggested by the <a href=\"/r/discussion/lw/k2o/using_the_logarithmic_timeline_to_understand_the/\">logarithmic timeline</a>. Does this agree with the literature?</p>\n<p>I don't feel like I have a clear grasp of the literature, so the summary below is somewhat <em>ad hoc</em>. I hope it still helps with elucidating the relationship.</p>\n<ul>\n<li>Higher resolution means greater precision of forecasts holding the time horizon constant (assuming that it's a time horizon over which we could reasonably make forecasts). This makes sense: higher temporal resolution allows us to approximate the (almost) continuous evolution of the atmospheric system better, and higher spatial resolution allows us to work with a better initialization as well as approximate the continuous evolution better. For instance, a page on the website of weather forecasting service meteoblue has the title <a href=\"http://www.meteoblue.com/en/content/463\">Resolution means precision</a>.</li>\n<li>The relation between resolution and <em>accuracy</em> is less clear. Although, up to a point, higher resolution enables more accurate forecasts, the relation does not continue at ever-higher levels of precision, for a variety of reasons (including the chaos problem discussed next). For more, see <a href=\"http://mikehulme.org/wp-content/uploads/2009/07/2009-dessai-et-al-book-chapter.pdf\">Climate prediction: a limit to adaptation</a> by Suraje Dessai, Mike Hulme, Robert Lempert, and Roger Pielke, Jr.</li>\n<li>The type of resolution that matters more can depend on the type of phenomenon that we are predicting. In some cases, temporal resolution is more important than spatial resolution. In some cases, particularly phenomena relating to interactions between the different layers of the atmosphere, vertical resolution matters more than horizontal resolution, whereas in other cases, horizontal resolution matters more. For instance, the paper <a href=\"http://weather.mhpcc.edu/papers/2003b.pdf\">Impacts of Numerical Weather Prediction Spatial Resolution On An Atmospheric Decision Aid For Directed Energy Weapon Systems</a> finds that vertical resolution matters more than horizontal resolution for a particular application.</li>\n</ul>\n<p><strong>The problem of chaos and the butterfly effect</strong></p>\n<p>The main problem with weather prediction is hypersensitivity to initial conditions: even small differences in initial values can have huge effects over longer timescales (this is sometimes called the <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>). This effect could occur at many different levels.</p>\n<ul>\n<li>Measurements may not be sufficiently precise or detailed (temperature and precipitation are measured only at a few weather stations rather than everywhere). Some of the measurements may be somewhat flawed as well. Apart from the usual measurement error, the choice of weather stations may introduce bias: weather stations have often historically been located close to airports and to other hubs of activity, where temperatures may be higher due to the heat generated by the processes nearby (see also the page on <a href=\"https://en.wikipedia.org/wiki/Urban_heat_island\">urban heat island</a>).</li>\n<li>The computer programs that do numerical weather simulation don't store data to infinite precision. Choices of how to round off can profoundly affect weather predictions.</li>\n<li>There may be actions by humans, animals, or human institutions that aren't modeled in the atmospheric system, but perturb it sufficiently to affect weather predictions. For instance, if lots of people burst firecrackers on a day, that might affect local temperatures and air composition in a small manner that might have larger effects over the coming days.</li>\n</ul>\n<p>Due to these problems, modern algorithms for numerical weather prediction run simulations with many slight variations of the given initial conditions, using a probabilistic model to assign probabilities to the different scenarios considered. Note that here we are making slight variations to the <em>data</em> and running the model on these variations to generate a collection of scenarios weighted by probability.</p>\n<p>As the time horizon for forecasting increases (we get to one week ahead or beyond) our understanding of how the equilibrating influences of the weather play out is more fuzzy. For such timescales, we use <a href=\"http://en.wikipedia.org/wiki/Ensemble_forecasting\">ensemble forecasting</a> with a collection of different <em>models</em>. The models may use different data and give attention to different aspects of the data, based on slightly different underlying theories of how the different weather phenomena interact. As before, we generate probabilistic weather predictions.</p>\n<p><strong>Can (and should) weather forecasting be fully automated?</strong></p>\n<p>Nate Silver observed in his book <em>The Signal and the Noise </em>that the proportional improvement that human input made to the computer models has stayed constant at about 25% for precipitation forecasts and 10% for temperature forecasts, even as the computer models, and therefore the final forecast, have improved considerably over the last few decades. The sources cited by Silver don't seem to be online, but I found <a href=\"http://journals.ametsoc.org/doi/pdf/10.1175/BAMS-84-7-876\">another paper</a> with the data that Silver uses. Silver says that humans' main input is in the following respects:</p>\n<ul>\n<li>Human <em>vision</em> (literally) is powerful in terms of identifying patterns and getting a broad sense of what is happening. Computers still have trouble <em>seeing</em> patterns. This is related to the fact that humans in general have an easier time with <a href=\"http://en.wikipedia.org/wiki/CAPTCHA\">CAPTCHAs</a> than computers, although that might be changing as machine learning improves.</li>\n<li>Humans have better intuition at identifying false runaway predictions. For instance, a computer might think that a particular weather phenomenon will snowball, whereas humans are likely to identify equilibrating influences that will prevent the snowballing. Humans are also better at reasoning about what is reasonable to expect based on climatological history.</li>\n</ul>\n<p>On a related noted, Charles Doswell <a href=\"http://www.cimms.ou.edu/~doswell/human/Human.html\">has argued that</a> it would be dangerous to try to fully automate weather forecasting, because direct involvement with the weather forecasting process is crucial for meteorologists to get a better sense of how to make improvements to their models.</p>\n<ul>\n</ul>\n<p><strong>Machine learning in weather prediction?</strong></p>\n<p>For most of its history, weather prediction has relied on models grounded in our understanding of physics, with some aspects of the models being tweaked based on experience running the models. This differs somewhat from the spirit of <a href=\"http://en.wikipedia.org/wiki/Machine_learning\">machine learning algorithms</a>. Supervised machine learning algorithms take a bunch of input data and output data and try to learn how to predict the outputs from the inputs, with considerable agnosticism about the underlying theoretical mechanisms. In the context of weather prediction, a machine learning algorithm might view the current measured data as the input, and the measured data after a certain time interval (or some summary variable, such as a binary variable recording whether or not it rained) as the output to be predicted. The algorithm would then try to learn a relation from the inputs to the outputs.</p>\n<p>In recent years, machine learning ideas have started being integrated into weather forecasting. However, the core of weather prediction still relies on using theoretically grounded models. (Relevant links: <a href=\"http://www.quora.com/Weather-Forecasts/Are-machine-learning-algorithms-used-in-weather-forecasting\">Quora question on the use of machine learning algorithms in weather forecasting</a>, <a href=\"http://freakonomics.com/2011/09/27/an-algorithm-that-can-predict-weather-a-year-in-advance/\">Freakonomics post on a company that claims to use machine learning to predict the weather far ahead</a>, <a href=\"http://www.reddit.com/r/MachineLearning/comments/kuhue/an_algorithm_that_can_predict_weather_a_year_in/\">Reddit post about that Freakonomics post</a>).</p>\n<p>My uninformed speculation is that machine learning algorithms would be most useful in substituting for the human input element to the model rather than the core of the numerical simulation. In particular, to the extent that machine learning algorithms can make progress on the problem of vision, they might be able to use their \"vision\" to better interpret the results of numerical weather prediction. Moreover, the machine learning algorithms would be particularly well-suited to using Bayesian priors (arising from knowledge of historical climate) to identify cases where the numerical models are producing false feedback loops and predicting things that seem unlikely to happen.</p>\n<p><strong>Prehistory: before weather simulation came to fruition: meeting the theoretical challenge</strong></p>\n<p>Here is a quick summary of the initial steps taken to realizing weather forecasting as a science. These steps concentrated on the theoretical challenge. The measurement and computational challenges would be tackled later.</p>\n<ul>\n<li>Cleveland Abbe made the basic observation that weather prediction was essentially a problem of the application of hydrodynamics and thermodynamics to the atmosphere. He detailed his observations in the 1901 paper <em>The physical basis of long-range weather forecasts</em>. But this was more an identification of the general reference class of models to use than a concrete model of how to predict the weather.</li>\n<li>Vilhelm Bjerknes, in 1904, set down a two-step plan for rational forecasting: a <em>diagnostic</em> step, where the initial state of the atmosphere is determined using observations, and a <em>prognostic</em> step, where the laws of motion are used to calculate the evolution of the system over time. He even identified most of the relevant equations needed to compute the evolution of the system, but he didn't try to prepare his ideas for actual practical use.</li>\n<li>Lewis Fry Richardson published in 1922 a detailed description of how to predict the weather, and applied his model to an attempted 6-hour forecast that took him 6 weeks to compute by hand. His forecast was off by a huge margin, but he was still convinced that the model was broadly correct, and with enough data and computing power, it could produce useful predictions.</li>\n</ul>\n<p>It's interesting that scientists such as Richardson were so confident of their approach despite its failure to make useful predictions. The confidence arguably stemmed from the fact that the basic equations of physics that the model relied on were indubitably true. It's not surprising that Richardson's confidence wasn't widely shared. What's perhaps more surprising is that enough people were convinced by the approach that, when the world's first computers were made, weather prediction was viewed as a useful initial use of these computers. How they were able to figure out that this approach could bear fruit is related to questions I raised in my <a href=\"/lw/k6j/paradigm_shifts_in_forecasting/\">paradigm shifts in forecasting</a> post.</p>\n<p>Could Richardson have fixed the model and made correct predictions by hand? With the benefit of hindsight, it turns out that if he'd applied a standard procedure to tweak the original data he worked with, he would have been able to make decent predictions by hand. But this is easier seen in hindsight, when we have the benefit of being able to try out many different tweaks of the algorithm and compare their performance in real time (more on this point below).</p>\n<p><strong>The first successful computer-based numerical weather prediction</strong></p>\n<p>In the mid-1930s, John von Neumann, one of the key figures in modern computing, stumbled across weather prediction and identified it as an ideal problem suited to computers: it required a huge amount of calculation using clearly defined algorithms from measured initial data. An initiative supported by von Neumann led to the meteorologist Jule Charney getting interested in the problem. In 1950, a team led by Charney came up with a complete numerical algorithm for weather prediction, building on and addressing some computational issues in Richardson's original algorithm. This was then implemented on the ENIAC, the only computer available at the time. The simulation had a time ratio of 1: it took 24 hours to simulate 24 hours of weather. Charney called it a vindication of the vision of Lewis Fry Richardson.</p>\n<p><strong>Progress since then: the interplay of computational and theoretical<br /></strong></p>\n<p>Since the 1950 ENIAC implementation of weather forecasting, weather forecasting has improved slowly and steadily. The bulk of the progress has been through access to faster computing power. Theoretical models have also improved. However, these improvements are not cleanly separable. The ability to run faster simulations computationally allows for quicker testing and comparison of different algorithms, and experimental adjustment to make them work faster and better. In this case, one-time access to better computational resources can lead to long-term improvements in the algorithms used.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JaSu2DosK2iMhhgST", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 23, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "26193", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is the first of two (or more) posts that look at the domain of weather and climate forecasting and what we can learn from the history and current state of these fields for forecasting as a domain. It may not be of general interest to the LessWrong community, but I hope that it's of interest to people here who have some interest either in weather-related material or in forecasting in general.</em></p>\n<p>The science of weather forecasting has come a long way over the past century. Since people starting measuring and recording the weather (temperature, precipitation, etc.) <a href=\"http://www.eumetcal.org/intralibrary/open_virtual_file_path/i2055n15861t/english/msg/ver_cont_var/uos5/uos5_ko2.htm\">two simple algorithms for weather prediction</a> have existed (see also <a href=\"http://www.washingtonpost.com/blogs/capital-weather-gang/post/weather-forecast-accuracy-versus-skill-skill-is-what-matters/2012/04/05/gIQA42BixS_blog.html\">this</a>):</p>\n<ul>\n<li><strong>Persistence</strong>: Assume that the weather tomorrow will be the same as the weather today.</li>\n<li><strong>Climatology</strong>: Assume that the weather on a given day of the year will be the same as the average of the weather on that same day in the last few years (we might also use averages for nearby days if we don't have enough years of data).</li>\n</ul>\n<p>Until the end of the 19th century, there was no weather prediction algorithm that did consistently better than both persistence and climatology. Between persistence and climatology, climatology won out over medium to long time horizons (a week or more), whereas persistence won out in some kinds of places over short horizons (1-2 days), though even there, climatology sometimes does better (see more <a href=\"https://courseware.e-education.psu.edu/courses/meteo101/Section11p05.html\">here</a>). Both methods have very limited utility when it comes to predicting and preparing for rare extreme weather events, such as blizzards, hurricanes, cyclones, polar winds, or heavy rainfall.</p>\n<p>This blog post discusses the evolution and progress of weather forecasting algorithms that significantly improve over the benchmarks of persistence and climatology, and the implications both for the future of weather forecasting and for our understanding of forecasting as a domain.</p>\n<p><strong>Sources for further reading (uncited material in my post is usually drawn from one of these)</strong>: <a href=\"https://en.wikipedia.org/wiki/History_of_numerical_weather_prediction\">Wikipedia's page on the history of numerical weather prediction</a>, <em>The Signal and the Noise</em> by Nate Silver, and <a href=\"https://www.rsmas.miami.edu/users/miskandarani/Courses/MPO662/Lynch,Peter/OriginsCompWF.JCP227.pdf\">The origins of computer weather prediction and climate modeling</a> by Peter Lynch.</p>\n<p><strong id=\"The_three_challenges__theory__measurement__and_computation\">The three challenges: theory, measurement, and computation</strong></p>\n<p>The problem facing any method that tries to do better than persistence and climatology is that whereas persistence and climatology can rely on existing aggregate records, any more sophisticated prediction algorithm relies on measuring, theorizing about, and computing with a much larger number of other observed quantities. There are three aspects to the challenge:</p>\n<ul>\n<li>The <strong>theoretical challenge or model selection challenge</strong>: The goal is to write a system of equations describing how the climate system evolves from certain initial measurements. In the weather prediction context, this was the first of the challenges to be nailed: the basic equations of the atmosphere come from physics, which has been well understood for over a century now.</li>\n<li>The <strong>measurement challenge</strong>: A large number of measurements at different points in the area and at regular intervals of time need to be taken to <em>initialize </em>the data appropriately for the weather simulation. The measurement challenge was largely resolved early on: it was easy to set up stations for measuring temperature, humidity, and other indicators around the world, and communications technology enabled the data to be quickly relayed to a central processing station. Of course, many improvements have occurred over the 20th century: we can now make measurements using satellites, as well as directly measure weather indicators at different altitudes. But measurement challenges were not critical in getting weather prediction <em>started</em>.</li>\n<li>The <strong>computational challenge</strong>: This appears to have been the most difficult of the challenges and the critical constraint in making real-time weather predictions. The computations needed for making a forecast that could beat persistence and climatology over any time horizon were just too numerous for humans to carry out in real time. In fact, the ability to make accurate weather predictions was one of the motivations for the development of improved computing machinery.</li>\n</ul>\n<p><strong id=\"The_basic_theory_of_weather_forecasting\">The basic theory of weather forecasting</strong></p>\n<p>The basic idea of weather forecasting is to use the equations of physics to model the evolution of the atmospheric system. In order to do this, we need to know how the system looks at a given point. In principle, that information, combined with the equations, should allow us to compute the weather indefinitely into the future. In practice, the equations we create don't have any closed-form solutions, the data is only partial (we don't have initial data on the whole world) and even small variations at a given time can balloon to bigger changes (this is called the <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>; more on this later in the post).</p>\n<p>Instead of trying to solve the system analytically, we <em>discretize</em> the problem (we use discrete spatial locations and discrete time steps) and then solve the problem <em>numerically</em> (this is a bit like using a difference quotient instead of a derivative when computing a rate of change). There are four dimensions to the discretization (three spatial and one temporal), and how fine we make the grid in each dimension is called the <em>resolution</em> in that dimension.</p>\n<ul>\n<li><strong>Spatial dimensions and spatial resolution</strong>: The region over which we are interested in forecasting the weather is converted to a grid. We have freedom in how fine we make the grid. In general, finer grids make for more precise and accurate weather predictions, but require more computational resources. The grid has two horizontal dimensions and one height dimension, hence a total of three dimensions. Thus, making the grid <em>x</em> times as fine (i.e., making the spatial resolution <em>x</em> times as fine) means increasing the number of regions to <em>x</em><sup>3</sup> times the current value.</li>\n<li><strong>Time dimension and temporal resolution</strong>: We also choose a time step. In general, smaller time steps make for more precise and accurate weather predictions, but require more computational resources (because the number of time steps needed to traverse a particular length of time is more). If we divide the time step by <em>x</em>, we multiply the time and space storage needs by a factor of <em>x</em>.</li>\n</ul>\n<p>Thus, roughly, becoming finer by a factor of <em>x</em> in all three spatial dimensions and the time dimension requires upping computational resources to about <em>x</em><sup>4</sup> of the original value. So, doubling in all four dimensions requires improving computational power to 16 times the original value, which means four doublings.&nbsp; Combining this with natural improvements in computing, such as <a href=\"http://en.wikipedia.org/wiki/Moore%27s_law\">Moore's law</a>, we expect that we should be able to make our grid twice as fine in all dimensions (i.e., double the spatial and temporal resolution) every 8 years or so.</p>\n<p><strong id=\"How_much_precision_and_accuracy_does_high_resolution_buy_\">How much precision and accuracy does high resolution buy?<br></strong></p>\n<p>My intuitive prior would be that, for sufficiently short time horizons where we don't expect chaos to play a dominant role, we'd expect the relationship suggested by the <a href=\"/r/discussion/lw/k2o/using_the_logarithmic_timeline_to_understand_the/\">logarithmic timeline</a>. Does this agree with the literature?</p>\n<p>I don't feel like I have a clear grasp of the literature, so the summary below is somewhat <em>ad hoc</em>. I hope it still helps with elucidating the relationship.</p>\n<ul>\n<li>Higher resolution means greater precision of forecasts holding the time horizon constant (assuming that it's a time horizon over which we could reasonably make forecasts). This makes sense: higher temporal resolution allows us to approximate the (almost) continuous evolution of the atmospheric system better, and higher spatial resolution allows us to work with a better initialization as well as approximate the continuous evolution better. For instance, a page on the website of weather forecasting service meteoblue has the title <a href=\"http://www.meteoblue.com/en/content/463\">Resolution means precision</a>.</li>\n<li>The relation between resolution and <em>accuracy</em> is less clear. Although, up to a point, higher resolution enables more accurate forecasts, the relation does not continue at ever-higher levels of precision, for a variety of reasons (including the chaos problem discussed next). For more, see <a href=\"http://mikehulme.org/wp-content/uploads/2009/07/2009-dessai-et-al-book-chapter.pdf\">Climate prediction: a limit to adaptation</a> by Suraje Dessai, Mike Hulme, Robert Lempert, and Roger Pielke, Jr.</li>\n<li>The type of resolution that matters more can depend on the type of phenomenon that we are predicting. In some cases, temporal resolution is more important than spatial resolution. In some cases, particularly phenomena relating to interactions between the different layers of the atmosphere, vertical resolution matters more than horizontal resolution, whereas in other cases, horizontal resolution matters more. For instance, the paper <a href=\"http://weather.mhpcc.edu/papers/2003b.pdf\">Impacts of Numerical Weather Prediction Spatial Resolution On An Atmospheric Decision Aid For Directed Energy Weapon Systems</a> finds that vertical resolution matters more than horizontal resolution for a particular application.</li>\n</ul>\n<p><strong id=\"The_problem_of_chaos_and_the_butterfly_effect\">The problem of chaos and the butterfly effect</strong></p>\n<p>The main problem with weather prediction is hypersensitivity to initial conditions: even small differences in initial values can have huge effects over longer timescales (this is sometimes called the <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>). This effect could occur at many different levels.</p>\n<ul>\n<li>Measurements may not be sufficiently precise or detailed (temperature and precipitation are measured only at a few weather stations rather than everywhere). Some of the measurements may be somewhat flawed as well. Apart from the usual measurement error, the choice of weather stations may introduce bias: weather stations have often historically been located close to airports and to other hubs of activity, where temperatures may be higher due to the heat generated by the processes nearby (see also the page on <a href=\"https://en.wikipedia.org/wiki/Urban_heat_island\">urban heat island</a>).</li>\n<li>The computer programs that do numerical weather simulation don't store data to infinite precision. Choices of how to round off can profoundly affect weather predictions.</li>\n<li>There may be actions by humans, animals, or human institutions that aren't modeled in the atmospheric system, but perturb it sufficiently to affect weather predictions. For instance, if lots of people burst firecrackers on a day, that might affect local temperatures and air composition in a small manner that might have larger effects over the coming days.</li>\n</ul>\n<p>Due to these problems, modern algorithms for numerical weather prediction run simulations with many slight variations of the given initial conditions, using a probabilistic model to assign probabilities to the different scenarios considered. Note that here we are making slight variations to the <em>data</em> and running the model on these variations to generate a collection of scenarios weighted by probability.</p>\n<p>As the time horizon for forecasting increases (we get to one week ahead or beyond) our understanding of how the equilibrating influences of the weather play out is more fuzzy. For such timescales, we use <a href=\"http://en.wikipedia.org/wiki/Ensemble_forecasting\">ensemble forecasting</a> with a collection of different <em>models</em>. The models may use different data and give attention to different aspects of the data, based on slightly different underlying theories of how the different weather phenomena interact. As before, we generate probabilistic weather predictions.</p>\n<p><strong id=\"Can__and_should__weather_forecasting_be_fully_automated_\">Can (and should) weather forecasting be fully automated?</strong></p>\n<p>Nate Silver observed in his book <em>The Signal and the Noise </em>that the proportional improvement that human input made to the computer models has stayed constant at about 25% for precipitation forecasts and 10% for temperature forecasts, even as the computer models, and therefore the final forecast, have improved considerably over the last few decades. The sources cited by Silver don't seem to be online, but I found <a href=\"http://journals.ametsoc.org/doi/pdf/10.1175/BAMS-84-7-876\">another paper</a> with the data that Silver uses. Silver says that humans' main input is in the following respects:</p>\n<ul>\n<li>Human <em>vision</em> (literally) is powerful in terms of identifying patterns and getting a broad sense of what is happening. Computers still have trouble <em>seeing</em> patterns. This is related to the fact that humans in general have an easier time with <a href=\"http://en.wikipedia.org/wiki/CAPTCHA\">CAPTCHAs</a> than computers, although that might be changing as machine learning improves.</li>\n<li>Humans have better intuition at identifying false runaway predictions. For instance, a computer might think that a particular weather phenomenon will snowball, whereas humans are likely to identify equilibrating influences that will prevent the snowballing. Humans are also better at reasoning about what is reasonable to expect based on climatological history.</li>\n</ul>\n<p>On a related noted, Charles Doswell <a href=\"http://www.cimms.ou.edu/~doswell/human/Human.html\">has argued that</a> it would be dangerous to try to fully automate weather forecasting, because direct involvement with the weather forecasting process is crucial for meteorologists to get a better sense of how to make improvements to their models.</p>\n<ul>\n</ul>\n<p><strong id=\"Machine_learning_in_weather_prediction_\">Machine learning in weather prediction?</strong></p>\n<p>For most of its history, weather prediction has relied on models grounded in our understanding of physics, with some aspects of the models being tweaked based on experience running the models. This differs somewhat from the spirit of <a href=\"http://en.wikipedia.org/wiki/Machine_learning\">machine learning algorithms</a>. Supervised machine learning algorithms take a bunch of input data and output data and try to learn how to predict the outputs from the inputs, with considerable agnosticism about the underlying theoretical mechanisms. In the context of weather prediction, a machine learning algorithm might view the current measured data as the input, and the measured data after a certain time interval (or some summary variable, such as a binary variable recording whether or not it rained) as the output to be predicted. The algorithm would then try to learn a relation from the inputs to the outputs.</p>\n<p>In recent years, machine learning ideas have started being integrated into weather forecasting. However, the core of weather prediction still relies on using theoretically grounded models. (Relevant links: <a href=\"http://www.quora.com/Weather-Forecasts/Are-machine-learning-algorithms-used-in-weather-forecasting\">Quora question on the use of machine learning algorithms in weather forecasting</a>, <a href=\"http://freakonomics.com/2011/09/27/an-algorithm-that-can-predict-weather-a-year-in-advance/\">Freakonomics post on a company that claims to use machine learning to predict the weather far ahead</a>, <a href=\"http://www.reddit.com/r/MachineLearning/comments/kuhue/an_algorithm_that_can_predict_weather_a_year_in/\">Reddit post about that Freakonomics post</a>).</p>\n<p>My uninformed speculation is that machine learning algorithms would be most useful in substituting for the human input element to the model rather than the core of the numerical simulation. In particular, to the extent that machine learning algorithms can make progress on the problem of vision, they might be able to use their \"vision\" to better interpret the results of numerical weather prediction. Moreover, the machine learning algorithms would be particularly well-suited to using Bayesian priors (arising from knowledge of historical climate) to identify cases where the numerical models are producing false feedback loops and predicting things that seem unlikely to happen.</p>\n<p><strong id=\"Prehistory__before_weather_simulation_came_to_fruition__meeting_the_theoretical_challenge\">Prehistory: before weather simulation came to fruition: meeting the theoretical challenge</strong></p>\n<p>Here is a quick summary of the initial steps taken to realizing weather forecasting as a science. These steps concentrated on the theoretical challenge. The measurement and computational challenges would be tackled later.</p>\n<ul>\n<li>Cleveland Abbe made the basic observation that weather prediction was essentially a problem of the application of hydrodynamics and thermodynamics to the atmosphere. He detailed his observations in the 1901 paper <em>The physical basis of long-range weather forecasts</em>. But this was more an identification of the general reference class of models to use than a concrete model of how to predict the weather.</li>\n<li>Vilhelm Bjerknes, in 1904, set down a two-step plan for rational forecasting: a <em>diagnostic</em> step, where the initial state of the atmosphere is determined using observations, and a <em>prognostic</em> step, where the laws of motion are used to calculate the evolution of the system over time. He even identified most of the relevant equations needed to compute the evolution of the system, but he didn't try to prepare his ideas for actual practical use.</li>\n<li>Lewis Fry Richardson published in 1922 a detailed description of how to predict the weather, and applied his model to an attempted 6-hour forecast that took him 6 weeks to compute by hand. His forecast was off by a huge margin, but he was still convinced that the model was broadly correct, and with enough data and computing power, it could produce useful predictions.</li>\n</ul>\n<p>It's interesting that scientists such as Richardson were so confident of their approach despite its failure to make useful predictions. The confidence arguably stemmed from the fact that the basic equations of physics that the model relied on were indubitably true. It's not surprising that Richardson's confidence wasn't widely shared. What's perhaps more surprising is that enough people were convinced by the approach that, when the world's first computers were made, weather prediction was viewed as a useful initial use of these computers. How they were able to figure out that this approach could bear fruit is related to questions I raised in my <a href=\"/lw/k6j/paradigm_shifts_in_forecasting/\">paradigm shifts in forecasting</a> post.</p>\n<p>Could Richardson have fixed the model and made correct predictions by hand? With the benefit of hindsight, it turns out that if he'd applied a standard procedure to tweak the original data he worked with, he would have been able to make decent predictions by hand. But this is easier seen in hindsight, when we have the benefit of being able to try out many different tweaks of the algorithm and compare their performance in real time (more on this point below).</p>\n<p><strong id=\"The_first_successful_computer_based_numerical_weather_prediction\">The first successful computer-based numerical weather prediction</strong></p>\n<p>In the mid-1930s, John von Neumann, one of the key figures in modern computing, stumbled across weather prediction and identified it as an ideal problem suited to computers: it required a huge amount of calculation using clearly defined algorithms from measured initial data. An initiative supported by von Neumann led to the meteorologist Jule Charney getting interested in the problem. In 1950, a team led by Charney came up with a complete numerical algorithm for weather prediction, building on and addressing some computational issues in Richardson's original algorithm. This was then implemented on the ENIAC, the only computer available at the time. The simulation had a time ratio of 1: it took 24 hours to simulate 24 hours of weather. Charney called it a vindication of the vision of Lewis Fry Richardson.</p>\n<p><strong id=\"Progress_since_then__the_interplay_of_computational_and_theoretical\">Progress since then: the interplay of computational and theoretical<br></strong></p>\n<p>Since the 1950 ENIAC implementation of weather forecasting, weather forecasting has improved slowly and steadily. The bulk of the progress has been through access to faster computing power. Theoretical models have also improved. However, these improvements are not cleanly separable. The ability to run faster simulations computationally allows for quicker testing and comparison of different algorithms, and experimental adjustment to make them work faster and better. In this case, one-time access to better computational resources can lead to long-term improvements in the algorithms used.</p>", "sections": [{"title": "The three challenges: theory, measurement, and computation", "anchor": "The_three_challenges__theory__measurement__and_computation", "level": 1}, {"title": "The basic theory of weather forecasting", "anchor": "The_basic_theory_of_weather_forecasting", "level": 1}, {"title": "How much precision and accuracy does high resolution buy?", "anchor": "How_much_precision_and_accuracy_does_high_resolution_buy_", "level": 1}, {"title": "The problem of chaos and the butterfly effect", "anchor": "The_problem_of_chaos_and_the_butterfly_effect", "level": 1}, {"title": "Can (and should) weather forecasting be fully automated?", "anchor": "Can__and_should__weather_forecasting_be_fully_automated_", "level": 1}, {"title": "Machine learning in weather prediction?", "anchor": "Machine_learning_in_weather_prediction_", "level": 1}, {"title": "Prehistory: before weather simulation came to fruition: meeting the theoretical challenge", "anchor": "Prehistory__before_weather_simulation_came_to_fruition__meeting_the_theoretical_challenge", "level": 1}, {"title": "The first successful computer-based numerical weather prediction", "anchor": "The_first_successful_computer_based_numerical_weather_prediction", "level": 1}, {"title": "Progress since then: the interplay of computational and theoretical", "anchor": "Progress_since_then__the_interplay_of_computational_and_theoretical", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BGwYn39cq6z3tg8Zg", "wsM7wpEs9jRsQLuEK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-23T19:09:54.047Z", "modifiedAt": null, "url": null, "title": "[LINK] Why Talk to Philosophers: Physicist Sean Carroll Discusses \"Common Misunderstandings\" about Philosophy", "slug": "link-why-talk-to-philosophers-physicist-sean-carroll", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:06.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dky2o43YNi8ToGvJA/link-why-talk-to-philosophers-physicist-sean-carroll", "pageUrlRelative": "/posts/dky2o43YNi8ToGvJA/link-why-talk-to-philosophers-physicist-sean-carroll", "linkUrl": "https://www.lesswrong.com/posts/dky2o43YNi8ToGvJA/link-why-talk-to-philosophers-physicist-sean-carroll", "postedAtFormatted": "Monday, June 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Why%20Talk%20to%20Philosophers%3A%20Physicist%20Sean%20Carroll%20Discusses%20%22Common%20Misunderstandings%22%20about%20Philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Why%20Talk%20to%20Philosophers%3A%20Physicist%20Sean%20Carroll%20Discusses%20%22Common%20Misunderstandings%22%20about%20Philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdky2o43YNi8ToGvJA%2Flink-why-talk-to-philosophers-physicist-sean-carroll%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Why%20Talk%20to%20Philosophers%3A%20Physicist%20Sean%20Carroll%20Discusses%20%22Common%20Misunderstandings%22%20about%20Philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdky2o43YNi8ToGvJA%2Flink-why-talk-to-philosophers-physicist-sean-carroll", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdky2o43YNi8ToGvJA%2Flink-why-talk-to-philosophers-physicist-sean-carroll", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p><a href=\"http://www.rotman.uwo.ca/2014/why-talk-to-philosophers/\">Why Talk to Philosophers? Part I.</a>&nbsp;by philosopher of science <a href=\"http://www.rotman.uwo.ca/members/wayne-myrvold/\">Wayne Myrvold</a>.</p>\n<p>See also Sean Carroll's own blog entry,&nbsp;<a href=\"http://www.preposterousuniverse.com/blog/2014/06/23/physicists-should-stop-saying-silly-things-about-philosophy/\">Physicists Should Stop Saying Silly Things about Philosophy</a>.&nbsp;</p>\n<p>Sean classifies the disparaging comments physicists make about philosophy as follows: \"Roughly speaking, physicists tend to have three different kinds of lazy critiques of philosophy: one that is totally dopey, one that is frustratingly annoying, and one that is deeply depressing\". Specifically:</p>\n<ul>\n<li>&ldquo;Philosophy tries to understand the universe by pure thought, without collecting experimental data.&rdquo;</li>\n<li>&ldquo;Philosophy is completely useless to the everyday job of a working physicist.&rdquo;</li>\n<li>&ldquo;Philosophers care too much about deep-sounding meta-questions, instead of sticking to what can be observed and calculated.&rdquo;</li>\n</ul>\n<p>He counters each argument presented.</p>\n<hr />\n<p>Personally, I am underwhelmed, since he does not address the point of view that philosophy is great at asking interesting questions but lousy at answering them. Typically, an interesting answer to a philosophical question requires first recasting it in a falsifiable form, so that is becomes a natural science question, be it physics, cognitive sciences, AI research or something else. This is locally known as <a href=\"/lw/8ns/hack_away_at_the_edges/\">hacking away at the edges</a>. Philosophical questions don't have philosophical answers.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dky2o43YNi8ToGvJA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "26436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6bSHiD9TxsJwe2WqT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-23T19:12:05.211Z", "modifiedAt": null, "url": null, "title": "Utilitarianism and Relativity Realism", "slug": "utilitarianism-and-relativity-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TruePath", "createdAt": "2009-04-13T22:54:03.380Z", "isAdmin": false, "displayName": "TruePath"}, "userId": "n4hta6Fu6tbWprCff", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g9fnR4DqjCuGrW9GX/utilitarianism-and-relativity-realism", "pageUrlRelative": "/posts/g9fnR4DqjCuGrW9GX/utilitarianism-and-relativity-realism", "linkUrl": "https://www.lesswrong.com/posts/g9fnR4DqjCuGrW9GX/utilitarianism-and-relativity-realism", "postedAtFormatted": "Monday, June 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utilitarianism%20and%20Relativity%20Realism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtilitarianism%20and%20Relativity%20Realism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg9fnR4DqjCuGrW9GX%2Futilitarianism-and-relativity-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utilitarianism%20and%20Relativity%20Realism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg9fnR4DqjCuGrW9GX%2Futilitarianism-and-relativity-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg9fnR4DqjCuGrW9GX%2Futilitarianism-and-relativity-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1592, "htmlBody": "<h2>Introduction</h2>\n<p>&nbsp;</p>\n<p>Most people on less wrong seem to be some kind of hedonic consequentialist. &nbsp; They think states with less suffering and more joy are better. &nbsp;Moreover, it is intuitive that if you can cause some improvement in human well-being to be achieved then (other things being equal) it is better to realize that improvement as soon as possible. &nbsp;Also, most people on this site seem to be realists about special relativity. &nbsp;That is they assume that any inertial reference frame is an equally valid point from which to describe reality rather than believing there is one true reference which offers a preferred description of reality. &nbsp;I will point out that these beliefs (plus some innocuous assumptions) lead quickly to paradox.</p>\n<h2>Relativity Realism</h2>\n<p>Before I continue I want to point out that empirical observations really are agnostic about the existence of a preferred reference frame. &nbsp;Indeed, it's a consequence of the theory of relativity itself that it's predictions are equally well explained by postulating a single true inertial reference frame and simply using the Lorentz contraction and time dilation equations to compute behavior for all moving objects. &nbsp;To see that this must be true not that if we take relativity seriously the laws of physics must work correctly in any reference frame. &nbsp;In particular, if we imagine designating one reference frame to be the true reference frame then, relativity itself, tells us that applying the laws of physics in that reference frame has to give us the correct results. &nbsp;</p>\n<p>In other words once we accept Einstein's equations for length contraction and time dilation with velocity we can interpret those equations as either undermining the idea of a fixed ether against which objects move (any reference frame is equally valid) or that there really is a fixed ether but objects in motion behave in such a manner that we can't empirically distinguish what is at rest.</p>\n<p>At first blush this second result seems so jury rigged that surely the simpler assumption is that there is no preferred reference frame. &nbsp;This relies on a false description of the situation. &nbsp;The question isn't, \"do we assign a low prior probability to the laws of physics conspiring to hide the true rest frame from us?\" &nbsp;Presumably we do. &nbsp;The question should be, \"given that the laws of physics do conspire to make a special rest frame empirically indistinguishable from any other inertial frames what probability do we assign to such a frame existing?\" &nbsp;After all it is a mathematical truth that the time dilation and length contraction do perfectly conspire to prevent us from measuring motion relative to some true rest frame (if it existed) so in deciding whether to believe in a preferred rest frame we aren't deciding between laws that would and wouldn't hide such a frame from us. &nbsp;We are only deciding whether, given we have such laws, whether we think such an undetectable true rest frame exists.</p>\n<p>To make it even more plausible that there is some true rest frame I will remark (but not argue) that relativity is a pretty general phenomena that can be derived from any model that conserves momentum, where the forces obey the inverse square law and all propagate at a constant speed relative to some fixed background, matter is held together in equilibrium states of these forces and time is implicitly measured via the rate it takes these forces to propagate. &nbsp;In other words if you have atoms held together by EM forces and the time it takes physical processes to happen is governed by the time it takes either forces or matter to cross certain distances then relativity comes for free. &nbsp;So it isn't amazing that we might have a true prefered reference frame and yet it be impossible to experimentally determine that frame.</p>\n<p>(As an aside this interpretation of relativity, fully consistent with all observables so far, makes for much better scifi since FTL travel doesn't allow anyone to go back in time).&nbsp;</p>\n<h2>A Paradox Resulting From Relativity Realism</h2>\n<p>Suppose we have two different brain implants that will be implanted in two different conscious but coma bound individuals. &nbsp;After a delay of 10 minutes after implantation the first device delivers an instantaneous burst of euphoria every second. &nbsp;The other delivers an instantaneous burst of discomfort every second. &nbsp;I assume we would all agree that (with sufficient additional assumptions) the world is a better place if we implant just a device of the euphoria inducing kind and a worse place if we just implant a device of the second kind. &nbsp;So assume the devices are appropriately calibrated so that the effect of implanting both is neutral (or very very nearly so). &nbsp;So far so good.</p>\n<p>I think we can all agree that the world would be better off if we delayed implanting the discomforting device by 10 minutes (or equivalently implanted the pleasurable device 10 minutes earlier). &nbsp;If you dispute this conclusion then you get absurd results if you even admit the possibility of a universe that exists forever as in such a universe it is no better to permanently increase human welfare now than to delay that increase by 10 minutes or 10 centuries.</p>\n<p>Now assume that the two individuals receiving the transplants are actually on spaceships moving in opposite directions at high rates of speed and the implantation is done at the instant they pass by each other. &nbsp;For simplicity we assume everyone else dies at this instant (or add an irrelevance of identical outcomes assumption and note that the two ships are moving at the same velocity relative to everyone else). &nbsp;</p>\n<p>From the reference frame of the individual who received the beneficial implant we can analyze the situation as follows. &nbsp;Without loss of generality we can assume the ships are traveling at an appropriate speed so that for every second that pases in our reference frame only 1/2 a second passes on the other ship. &nbsp;Thus in this reference frame the first experience of discomfort is delayed by 10 minutes and then only occurs every other second. &nbsp;Now surely the world is no worse off because the discomfort occurs less frequently. &nbsp;But ignoring the fact that the discomforting device fires less frequently this is exactly equivalent to implanting the desirable device 10 minutes before the undesirable one. &nbsp;Thus, since implanting both in the same reference frame was neutral, it is actually favorable (better than not implanting them) to do so when the recipients are in fast moving reference frames moving in opposite directions. &nbsp;Note the same result holds if we assume the device only creates discomfort or euphoria a single time with the minor assumption that if two worlds only differ in events before time t then what happens after time t is irrelevant to which one is preferable.</p>\n<p>However, the same analysis done in the reference frame of the unpleasant implant gives the exact opposite conclusion.</p>\n<p>&nbsp;</p>\n<h2>Avoiding the Paradox</h2>\n<p>Perhaps one might try and avoid the paradox by insisting that no experience truly occurs instantaneously. &nbsp;However, this is easily seen to be futile.</p>\n<p>Assume that each device inflicts pleasure or discomfort for duration epsilon &lt;&lt; 1 second. &nbsp;If you assume that the total badness of the uncomfortable experience is somehow mediated by changes in neurochemistry or other physical properties you are lead to the assumption that even described from the reference frame of the desirable implant the experience of 2*epsilon seconds of discomfort by the time dilated individual is really no worse than the experience of epsilon seconds of discomfort would be for someone with that implant in your reference frame. &nbsp;In other words when time is dilated the experience of pain per unit time is diluted. &nbsp;This leads to the exact same result as above.</p>\n<p>On the other hand if we really do increase the weight we give to pain experienced by those undergoing time dilation an even simpler set of implants leads to paradox. &nbsp;These implants start working immediately, one generating a pleasant experience for 5 minutes the other an unpleasant experience for 5 minutes again calibrated so that installing both is overall neutral. &nbsp;Now by assumption from the reference frame of the beneficial implant things are overall worse (the longer duration of discomfort experienced by the other individual is overall worse than someone in the same reference frame getting the undesirable implant) and vice versa from the other reference frame.</p>\n<p>The use of instantaneous experiences was merely a way to simplify the example but irrelevant to the underlying inequalities. &nbsp;Those inequalities are a result of the implicit time discounting forced by the assumption that other things being equal it is better for improvements to occur now rather than later combined with the fact that realism about relativity renders facts about simultaneity incoherent.</p>\n<p>Personally, I think the only decent way of avoiding this paradox is to deny realism about relativity. &nbsp;Sure, it's a radical move. &nbsp;However, it's also a radical move to say it's not true that it's better to cure cancer now than in 10 centuries even if the human race will continue to exist forever. &nbsp;Indeed, even if you don't assume literally infinite duration of effects even an unbounded potential length of effect with probabilities that decrease sufficiently slowly is equally problematic.</p>\n<h2>Responses</h2>\n<p>I've deliberately avoided phrasing this dilemma in terms of a formal paradox and listing the assumptions necessary to generate the paradox. &nbsp;Partly this is laziness but it's also a desire to see how people are inclined to respond before I attempt to draw up formal conditions. &nbsp;After all I ultimately want to capture common views in the assumptions and if I don't know what people's reactions are I can't pick the right assumptions.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g9fnR4DqjCuGrW9GX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": -5, "extendedScore": null, "score": -1.7e-05, "legacy": true, "legacyId": "26431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Introduction\">Introduction</h2>\n<p>&nbsp;</p>\n<p>Most people on less wrong seem to be some kind of hedonic consequentialist. &nbsp; They think states with less suffering and more joy are better. &nbsp;Moreover, it is intuitive that if you can cause some improvement in human well-being to be achieved then (other things being equal) it is better to realize that improvement as soon as possible. &nbsp;Also, most people on this site seem to be realists about special relativity. &nbsp;That is they assume that any inertial reference frame is an equally valid point from which to describe reality rather than believing there is one true reference which offers a preferred description of reality. &nbsp;I will point out that these beliefs (plus some innocuous assumptions) lead quickly to paradox.</p>\n<h2 id=\"Relativity_Realism\">Relativity Realism</h2>\n<p>Before I continue I want to point out that empirical observations really are agnostic about the existence of a preferred reference frame. &nbsp;Indeed, it's a consequence of the theory of relativity itself that it's predictions are equally well explained by postulating a single true inertial reference frame and simply using the Lorentz contraction and time dilation equations to compute behavior for all moving objects. &nbsp;To see that this must be true not that if we take relativity seriously the laws of physics must work correctly in any reference frame. &nbsp;In particular, if we imagine designating one reference frame to be the true reference frame then, relativity itself, tells us that applying the laws of physics in that reference frame has to give us the correct results. &nbsp;</p>\n<p>In other words once we accept Einstein's equations for length contraction and time dilation with velocity we can interpret those equations as either undermining the idea of a fixed ether against which objects move (any reference frame is equally valid) or that there really is a fixed ether but objects in motion behave in such a manner that we can't empirically distinguish what is at rest.</p>\n<p>At first blush this second result seems so jury rigged that surely the simpler assumption is that there is no preferred reference frame. &nbsp;This relies on a false description of the situation. &nbsp;The question isn't, \"do we assign a low prior probability to the laws of physics conspiring to hide the true rest frame from us?\" &nbsp;Presumably we do. &nbsp;The question should be, \"given that the laws of physics do conspire to make a special rest frame empirically indistinguishable from any other inertial frames what probability do we assign to such a frame existing?\" &nbsp;After all it is a mathematical truth that the time dilation and length contraction do perfectly conspire to prevent us from measuring motion relative to some true rest frame (if it existed) so in deciding whether to believe in a preferred rest frame we aren't deciding between laws that would and wouldn't hide such a frame from us. &nbsp;We are only deciding whether, given we have such laws, whether we think such an undetectable true rest frame exists.</p>\n<p>To make it even more plausible that there is some true rest frame I will remark (but not argue) that relativity is a pretty general phenomena that can be derived from any model that conserves momentum, where the forces obey the inverse square law and all propagate at a constant speed relative to some fixed background, matter is held together in equilibrium states of these forces and time is implicitly measured via the rate it takes these forces to propagate. &nbsp;In other words if you have atoms held together by EM forces and the time it takes physical processes to happen is governed by the time it takes either forces or matter to cross certain distances then relativity comes for free. &nbsp;So it isn't amazing that we might have a true prefered reference frame and yet it be impossible to experimentally determine that frame.</p>\n<p>(As an aside this interpretation of relativity, fully consistent with all observables so far, makes for much better scifi since FTL travel doesn't allow anyone to go back in time).&nbsp;</p>\n<h2 id=\"A_Paradox_Resulting_From_Relativity_Realism\">A Paradox Resulting From Relativity Realism</h2>\n<p>Suppose we have two different brain implants that will be implanted in two different conscious but coma bound individuals. &nbsp;After a delay of 10 minutes after implantation the first device delivers an instantaneous burst of euphoria every second. &nbsp;The other delivers an instantaneous burst of discomfort every second. &nbsp;I assume we would all agree that (with sufficient additional assumptions) the world is a better place if we implant just a device of the euphoria inducing kind and a worse place if we just implant a device of the second kind. &nbsp;So assume the devices are appropriately calibrated so that the effect of implanting both is neutral (or very very nearly so). &nbsp;So far so good.</p>\n<p>I think we can all agree that the world would be better off if we delayed implanting the discomforting device by 10 minutes (or equivalently implanted the pleasurable device 10 minutes earlier). &nbsp;If you dispute this conclusion then you get absurd results if you even admit the possibility of a universe that exists forever as in such a universe it is no better to permanently increase human welfare now than to delay that increase by 10 minutes or 10 centuries.</p>\n<p>Now assume that the two individuals receiving the transplants are actually on spaceships moving in opposite directions at high rates of speed and the implantation is done at the instant they pass by each other. &nbsp;For simplicity we assume everyone else dies at this instant (or add an irrelevance of identical outcomes assumption and note that the two ships are moving at the same velocity relative to everyone else). &nbsp;</p>\n<p>From the reference frame of the individual who received the beneficial implant we can analyze the situation as follows. &nbsp;Without loss of generality we can assume the ships are traveling at an appropriate speed so that for every second that pases in our reference frame only 1/2 a second passes on the other ship. &nbsp;Thus in this reference frame the first experience of discomfort is delayed by 10 minutes and then only occurs every other second. &nbsp;Now surely the world is no worse off because the discomfort occurs less frequently. &nbsp;But ignoring the fact that the discomforting device fires less frequently this is exactly equivalent to implanting the desirable device 10 minutes before the undesirable one. &nbsp;Thus, since implanting both in the same reference frame was neutral, it is actually favorable (better than not implanting them) to do so when the recipients are in fast moving reference frames moving in opposite directions. &nbsp;Note the same result holds if we assume the device only creates discomfort or euphoria a single time with the minor assumption that if two worlds only differ in events before time t then what happens after time t is irrelevant to which one is preferable.</p>\n<p>However, the same analysis done in the reference frame of the unpleasant implant gives the exact opposite conclusion.</p>\n<p>&nbsp;</p>\n<h2 id=\"Avoiding_the_Paradox\">Avoiding the Paradox</h2>\n<p>Perhaps one might try and avoid the paradox by insisting that no experience truly occurs instantaneously. &nbsp;However, this is easily seen to be futile.</p>\n<p>Assume that each device inflicts pleasure or discomfort for duration epsilon &lt;&lt; 1 second. &nbsp;If you assume that the total badness of the uncomfortable experience is somehow mediated by changes in neurochemistry or other physical properties you are lead to the assumption that even described from the reference frame of the desirable implant the experience of 2*epsilon seconds of discomfort by the time dilated individual is really no worse than the experience of epsilon seconds of discomfort would be for someone with that implant in your reference frame. &nbsp;In other words when time is dilated the experience of pain per unit time is diluted. &nbsp;This leads to the exact same result as above.</p>\n<p>On the other hand if we really do increase the weight we give to pain experienced by those undergoing time dilation an even simpler set of implants leads to paradox. &nbsp;These implants start working immediately, one generating a pleasant experience for 5 minutes the other an unpleasant experience for 5 minutes again calibrated so that installing both is overall neutral. &nbsp;Now by assumption from the reference frame of the beneficial implant things are overall worse (the longer duration of discomfort experienced by the other individual is overall worse than someone in the same reference frame getting the undesirable implant) and vice versa from the other reference frame.</p>\n<p>The use of instantaneous experiences was merely a way to simplify the example but irrelevant to the underlying inequalities. &nbsp;Those inequalities are a result of the implicit time discounting forced by the assumption that other things being equal it is better for improvements to occur now rather than later combined with the fact that realism about relativity renders facts about simultaneity incoherent.</p>\n<p>Personally, I think the only decent way of avoiding this paradox is to deny realism about relativity. &nbsp;Sure, it's a radical move. &nbsp;However, it's also a radical move to say it's not true that it's better to cure cancer now than in 10 centuries even if the human race will continue to exist forever. &nbsp;Indeed, even if you don't assume literally infinite duration of effects even an unbounded potential length of effect with probabilities that decrease sufficiently slowly is equally problematic.</p>\n<h2 id=\"Responses\">Responses</h2>\n<p>I've deliberately avoided phrasing this dilemma in terms of a formal paradox and listing the assumptions necessary to generate the paradox. &nbsp;Partly this is laziness but it's also a desire to see how people are inclined to respond before I attempt to draw up formal conditions. &nbsp;After all I ultimately want to capture common views in the assumptions and if I don't know what people's reactions are I can't pick the right assumptions.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Relativity Realism", "anchor": "Relativity_Realism", "level": 1}, {"title": "A Paradox Resulting From Relativity Realism", "anchor": "A_Paradox_Resulting_From_Relativity_Realism", "level": 1}, {"title": "Avoiding the Paradox", "anchor": "Avoiding_the_Paradox", "level": 1}, {"title": "Responses", "anchor": "Responses", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-23T20:43:38.444Z", "modifiedAt": null, "url": null, "title": "Vector psychology", "slug": "vector-psychology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.541Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5eiCnAEP4nRFTaX34/vector-psychology", "pageUrlRelative": "/posts/5eiCnAEP4nRFTaX34/vector-psychology", "linkUrl": "https://www.lesswrong.com/posts/5eiCnAEP4nRFTaX34/vector-psychology", "postedAtFormatted": "Monday, June 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vector%20psychology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVector%20psychology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5eiCnAEP4nRFTaX34%2Fvector-psychology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vector%20psychology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5eiCnAEP4nRFTaX34%2Fvector-psychology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5eiCnAEP4nRFTaX34%2Fvector-psychology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 669, "htmlBody": "<p><em>This post is probably nothing new probably contains lots of mistakes. I am not an expert on any of this stuff, I've just read about it online bit by bit every now and then. I tend to present my thoughts in a confident manner but in practice I don't really know or understand any of this stuff very well or specifically. I'd still like to offer my thoughts in this thread, and I hope it at least will get you thinking, even if that was all.<br /></em></p>\n<p>Personally I think of the mind as a mathematical entity. And I think the manifestations of different personalities or behavior patterns are also of mathematical nature. Mostly for anyone who considers the world in general as mathematical, physical as opposed to mysterious and spiritual, it would naturally follow that the mind is also mathematical. Psychological patterns are described somewhat often with a mathematical approach, but in general scientific study around psychology everything is very mathematical, there are inventories which score traits and hypotheses are tested statistically and so forth. What I'm talking about here is that the internal structure of the brain produces a dynamic that is based on vectors of different strengths, and things should be modeled from that type of perspective, and they probably are, even if I dont know it.</p>\n<p>For an example the <a href=\"http://en.wikipedia.org/wiki/Amygdala\" target=\"_blank\">amygdala</a> seem to have some sort of a role in negative emotions as well as hijacking the <a href=\"http://en.wikipedia.org/wiki/Hippocampus\" target=\"_blank\">hippocampus</a> for encoding the memories with an association to a negative emotion. The function of the amygdala is therefore mostly to process negative emotion in one way or another as well as produce activities based on those negative emotions. And this is something that is of a quantifiable nature.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Limbic_system\" target=\"_blank\">limbic system</a>, which of I certainly am not an expert on, has several subcomponents to it, and the way the limbic system in my opinion works is to produce multiple different vectors that work following an <a href=\"http://en.wikipedia.org/wiki/Opponent-process_theory\" target=\"_blank\">opponent-process theory</a> -type logic. Continuing with the previous example a person with higher intensity for negative emotions and related amygdala functions, require stronger opposing vectors from the other parts of the brain to counteract them when that is necessary.</p>\n<p>Neurotransmitters or some agents in <a href=\"http://en.wikipedia.org/wiki/Cerebrospinal_fluid\" target=\"_blank\">CSF</a> (Cerebospinal fluid) and/or the <a href=\"http://en.wikipedia.org/wiki/Ventricular_system\" target=\"_blank\">ventricular system</a> produce vectors or a depletable and overtime-finite resource for those vectors.</p>\n<p>&nbsp;</p>\n<p>Cognitive component and holism</p>\n<p>These kinds of things are also related to what people believe and that has a complex effect on behavior. If you think some activity is socially inappropriate, then you would have a vector against socially inapprorpiate activity. But the strength of this vector could quantifiable and probably associatable with OFC. So in otherwords your knowledge's impact on your behavior can be described in quantifiable way.</p>\n<p>&nbsp;</p>\n<p>So what good does thinking like this do then?</p>\n<p>Providing profiles for behavioral disorders in a mathematical way, or allowing their <em>consideration as such on a personal level</em>.</p>\n<p>So in addition simply reading symptoms you would have a list of vectors with expected ranges. For an example you might describe aggressiveness as both a heightened and a lowered vector - modulatory role for amygdala activity or lowered vector for <a href=\"http://en.wikipedia.org/wiki/Prefrontal_cortex\" target=\"_blank\">PFC</a>(Prefrontal cortex) activity or vector for <a href=\"http://en.wikipedia.org/wiki/Orbitofrontal_cortex\" target=\"_blank\">OFC</a>(Orbitofrontal cortex), or lowered vector for <a href=\"http://en.wikipedia.org/wiki/Serotonin\" target=\"_blank\">serotonergic </a>activity, or socially appropriate behavior which I think goes for OFC too.</p>\n<p>It could be possible to establish a logical framework of activies for different brain areas and components. This framework could then be converted from a logical construct into a smoother quantifiable construct following the logical framework, with reliability that could be converted to probabilities. When a person does some kind of activity it could be mapped into a chain of events in the brain.You have some activity and it causes an arousal in a certain region, that is processed by another part etc. This could be considered as a visual representation and dissecting behavior into sequences.</p>\n<p>&nbsp;</p>\n<p>There's probably lots of mistakes here and probably there really isn't anything 'new' either, though I might have written thinking that way. Please correct mistakes when you notice such :D</p>\n<p>Any thoughts?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5eiCnAEP4nRFTaX34", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -18, "extendedScore": null, "score": -1.6e-05, "legacy": true, "legacyId": "26437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-24T08:31:19.314Z", "modifiedAt": null, "url": null, "title": "Meetup : Tel Aviv Meetup: Choosing research topics", "slug": "meetup-tel-aviv-meetup-choosing-research-topics-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/quDKQ9naWmN2oL6Ak/meetup-tel-aviv-meetup-choosing-research-topics-0", "pageUrlRelative": "/posts/quDKQ9naWmN2oL6Ak/meetup-tel-aviv-meetup-choosing-research-topics-0", "linkUrl": "https://www.lesswrong.com/posts/quDKQ9naWmN2oL6Ak/meetup-tel-aviv-meetup-choosing-research-topics-0", "postedAtFormatted": "Tuesday, June 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tel%20Aviv%20Meetup%3A%20Choosing%20research%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tel%20Aviv%20Meetup%3A%20Choosing%20research%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FquDKQ9naWmN2oL6Ak%2Fmeetup-tel-aviv-meetup-choosing-research-topics-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tel%20Aviv%20Meetup%3A%20Choosing%20research%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FquDKQ9naWmN2oL6Ak%2Fmeetup-tel-aviv-meetup-choosing-research-topics-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FquDKQ9naWmN2oL6Ak%2Fmeetup-tel-aviv-meetup-choosing-research-topics-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11n'>Tel Aviv Meetup: Choosing research topics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 June 2014 11:31:12AM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(This is a duplicate generated by a bug. Now trying to resolve the bug. See meeting notice at <a href=\"http://lesswrong.com/meetups/11p\" rel=\"nofollow\">http://lesswrong.com/meetups/11p</a>)</p>\n\n<p>At the next LessWrong meeting, on Thursday, June 26 at 19:00, we'll prepare for an upcoming MIRIx session. We will hear short talks on open problems in FAI and choose some problems to tackle.</p>\n\n<p>VisionMap has kindly agreed to host us: Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.</p>\n\n<p>If you can't find us, call Vadim Kosoy at 0542600919 or me at 0545691165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11n'>Tel Aviv Meetup: Choosing research topics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "quDKQ9naWmN2oL6Ak", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics\">Discussion article for the meetup : <a href=\"/meetups/11n\">Tel Aviv Meetup: Choosing research topics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 June 2014 11:31:12AM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(This is a duplicate generated by a bug. Now trying to resolve the bug. See meeting notice at <a href=\"http://lesswrong.com/meetups/11p\" rel=\"nofollow\">http://lesswrong.com/meetups/11p</a>)</p>\n\n<p>At the next LessWrong meeting, on Thursday, June 26 at 19:00, we'll prepare for an upcoming MIRIx session. We will hear short talks on open problems in FAI and choose some problems to tackle.</p>\n\n<p>VisionMap has kindly agreed to host us: Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.</p>\n\n<p>If you can't find us, call Vadim Kosoy at 0542600919 or me at 0545691165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics1\">Discussion article for the meetup : <a href=\"/meetups/11n\">Tel Aviv Meetup: Choosing research topics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tel Aviv Meetup: Choosing research topics", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics", "level": 1}, {"title": "Discussion article for the meetup : Tel Aviv Meetup: Choosing research topics", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-24T08:33:28.945Z", "modifiedAt": null, "url": null, "title": "Meetup : Tel Aviv Meetup: Choosing research topics", "slug": "meetup-tel-aviv-meetup-choosing-research-topics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZXWJ5tZuzjXqFd6m7/meetup-tel-aviv-meetup-choosing-research-topics", "pageUrlRelative": "/posts/ZXWJ5tZuzjXqFd6m7/meetup-tel-aviv-meetup-choosing-research-topics", "linkUrl": "https://www.lesswrong.com/posts/ZXWJ5tZuzjXqFd6m7/meetup-tel-aviv-meetup-choosing-research-topics", "postedAtFormatted": "Tuesday, June 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tel%20Aviv%20Meetup%3A%20Choosing%20research%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tel%20Aviv%20Meetup%3A%20Choosing%20research%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXWJ5tZuzjXqFd6m7%2Fmeetup-tel-aviv-meetup-choosing-research-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tel%20Aviv%20Meetup%3A%20Choosing%20research%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXWJ5tZuzjXqFd6m7%2Fmeetup-tel-aviv-meetup-choosing-research-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXWJ5tZuzjXqFd6m7%2Fmeetup-tel-aviv-meetup-choosing-research-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11o'>Tel Aviv Meetup: Choosing research topics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 June 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Menachem Begin Road 7, 15th floor, Ramat Gan, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(This is a duplicate generated by a bug. Now trying to resolve the bug. See meeting notice at <a href=\"http://lesswrong.com/meetups/11p\" rel=\"nofollow\">http://lesswrong.com/meetups/11p</a>)</p>\n\n<p>At the next LessWrong meeting, on Thursday, June 26 at 19:00, we'll prepare for an upcoming MIRIx session. We will hear short talks on open problems in FAI and choose some problems to tackle.\nVisionMap has kindly agreed to host us: Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.\nIf you can't find us, call Vadim Kosoy at 0542600919 or me at 0545691165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11o'>Tel Aviv Meetup: Choosing research topics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZXWJ5tZuzjXqFd6m7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics\">Discussion article for the meetup : <a href=\"/meetups/11o\">Tel Aviv Meetup: Choosing research topics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 June 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Menachem Begin Road 7, 15th floor, Ramat Gan, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(This is a duplicate generated by a bug. Now trying to resolve the bug. See meeting notice at <a href=\"http://lesswrong.com/meetups/11p\" rel=\"nofollow\">http://lesswrong.com/meetups/11p</a>)</p>\n\n<p>At the next LessWrong meeting, on Thursday, June 26 at 19:00, we'll prepare for an upcoming MIRIx session. We will hear short talks on open problems in FAI and choose some problems to tackle.\nVisionMap has kindly agreed to host us: Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.\nIf you can't find us, call Vadim Kosoy at 0542600919 or me at 0545691165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics1\">Discussion article for the meetup : <a href=\"/meetups/11o\">Tel Aviv Meetup: Choosing research topics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tel Aviv Meetup: Choosing research topics", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics", "level": 1}, {"title": "Discussion article for the meetup : Tel Aviv Meetup: Choosing research topics", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv_Meetup__Choosing_research_topics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-24T08:33:55.779Z", "modifiedAt": null, "url": null, "title": "Meetup : Tel Aviv: Choosing research topics", "slug": "meetup-tel-aviv-choosing-research-topics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.430Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NjHNgar84QqL9AR2s/meetup-tel-aviv-choosing-research-topics", "pageUrlRelative": "/posts/NjHNgar84QqL9AR2s/meetup-tel-aviv-choosing-research-topics", "linkUrl": "https://www.lesswrong.com/posts/NjHNgar84QqL9AR2s/meetup-tel-aviv-choosing-research-topics", "postedAtFormatted": "Tuesday, June 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tel%20Aviv%3A%20Choosing%20research%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tel%20Aviv%3A%20Choosing%20research%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjHNgar84QqL9AR2s%2Fmeetup-tel-aviv-choosing-research-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tel%20Aviv%3A%20Choosing%20research%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjHNgar84QqL9AR2s%2Fmeetup-tel-aviv-choosing-research-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjHNgar84QqL9AR2s%2Fmeetup-tel-aviv-choosing-research-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/11p'>Tel Aviv: Choosing research topics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 June 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Derech Menachem Begin 7, 15th floor, Ramat Gan, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At the next LessWrong meeting, on Thursday, June 26 at 19:00, we'll prepare for an upcoming MIRIx session. We will hear short talks on open problems in FAI and choose some problems to tackle.</p>\n\n<p>VisionMap has kindly agreed to host us: Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.</p>\n\n<p>If you can't find us, call Vadim Kosoy at 0542600919 or me at 0545691165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/11p'>Tel Aviv: Choosing research topics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NjHNgar84QqL9AR2s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv__Choosing_research_topics\">Discussion article for the meetup : <a href=\"/meetups/11p\">Tel Aviv: Choosing research topics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 June 2014 07:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Derech Menachem Begin 7, 15th floor, Ramat Gan, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At the next LessWrong meeting, on Thursday, June 26 at 19:00, we'll prepare for an upcoming MIRIx session. We will hear short talks on open problems in FAI and choose some problems to tackle.</p>\n\n<p>VisionMap has kindly agreed to host us: Gibor Sport House, 15th Floor, 7 Menachem Begin St., Ramat-Gan.</p>\n\n<p>If you can't find us, call Vadim Kosoy at 0542600919 or me at 0545691165.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv__Choosing_research_topics1\">Discussion article for the meetup : <a href=\"/meetups/11p\">Tel Aviv: Choosing research topics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tel Aviv: Choosing research topics", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv__Choosing_research_topics", "level": 1}, {"title": "Discussion article for the meetup : Tel Aviv: Choosing research topics", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv__Choosing_research_topics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-24T18:12:47.164Z", "modifiedAt": null, "url": null, "title": "How do you notice when you are ignorant of necessary alternative hypotheses?", "slug": "how-do-you-notice-when-you-are-ignorant-of-necessary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:06.806Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PbZhZmRp88Euzxid7/how-do-you-notice-when-you-are-ignorant-of-necessary", "pageUrlRelative": "/posts/PbZhZmRp88Euzxid7/how-do-you-notice-when-you-are-ignorant-of-necessary", "linkUrl": "https://www.lesswrong.com/posts/PbZhZmRp88Euzxid7/how-do-you-notice-when-you-are-ignorant-of-necessary", "postedAtFormatted": "Tuesday, June 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20notice%20when%20you%20are%20ignorant%20of%20necessary%20alternative%20hypotheses%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20notice%20when%20you%20are%20ignorant%20of%20necessary%20alternative%20hypotheses%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbZhZmRp88Euzxid7%2Fhow-do-you-notice-when-you-are-ignorant-of-necessary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20notice%20when%20you%20are%20ignorant%20of%20necessary%20alternative%20hypotheses%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbZhZmRp88Euzxid7%2Fhow-do-you-notice-when-you-are-ignorant-of-necessary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbZhZmRp88Euzxid7%2Fhow-do-you-notice-when-you-are-ignorant-of-necessary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>So I just wound up in a debate with someone over on Reddit about the value of conventional academic philosophy.&nbsp; He linked me to a <a href=\"http://www.lrb.co.uk/v29/n10/jerry-fodor/headaches-have-themselves\">book review</a>, in which both the review and the book are absolutely godawful.&nbsp; That is, the author (and the reviewer following him) start with ontological monism (the universe only contains a single kind of Stuff: mass-energy), adds in the experience of consciousness, reasons deftly that <a href=\"/lw/iv/the_futility_of_emergence/\">emergence is a load of crap</a>... and then arrives to the conclusion of panpsychism.</p>\n<p>WAIT HOLD ON, DON'T FLAME YET!</p>\n<p>Of course panpsychism is bunk.&nbsp; I would be embarrassed to be caught upholding it, given the evidence I currently have, but what I want to talk about is the logic being followed.</p>\n<p>1) The universe is a unified, consistent whole.&nbsp; Good!</p>\n<p>2) The universe contains the experience/existence of consciousness.&nbsp; Easily observable.</p>\n<p>3) If consciousness exists, something in the universe must cause or give rise to consciousness.&nbsp; Good reasoning!</p>\n<p>4) \"Emergence\" is a non-explanation, so that can't be it.&nbsp; Good!</p>\n<p>5) <em>Therefore</em>, whatever stuff the unified universe is made of must be giving rise to consciousness in a nonemergent way.</p>\n<p>6) <em>Therefore</em>, the stuff must be innately \"mindy\".</p>\n<p>What went wrong in steps (5) and (6)?&nbsp; The man was actually reasoning more-or-less correctly!&nbsp; Given the universe he lived in, and the impossibility of emergence, he reallocated his probability mass to the remaining answer.&nbsp; When he had eliminated the impossible, whatever remained, however low its prior, must be true.</p>\n<p>The problem was, he eliminated the <em>im</em>possible, but left open a huge vast space of <em>possible</em> hypotheses that <em>he didn't know about</em> (but which we do): the most common of these is the computational theory of mind and consciousness, which says that <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">we are made of cognitive algorithms</a>.&nbsp; A <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff Inducer</a> can just go on to the next length of bit-strings describing Turing machines, but we can't.</p>\n<p>Now, I can spot the flaw in the reasoning <em>here</em>.&nbsp; What frightens me is: what if I'm presented with some similar argument, and I <em>can't</em> spot the flaw?&nbsp; What if, instead, I just neatly and <em>stupidly</em> reallocate my belief to what <em>seems to me</em> to be the only available alternative, while failing to go out and look for alternatives I don't already know about?&nbsp; Notably, it seems like <a href=\"/lw/ii/conservation_of_expected_evidence/\">expected <em>evidence</em> is conserved</a>, but expecting to locate new hypotheses means I should be reducing my certainty about all currently-available hypotheses <em>now</em> to have some for dividing between the new possibilities.</p>\n<p>If you can notice when you're confused, how do you notice when you're ignorant?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PbZhZmRp88Euzxid7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 27, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "26445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8QzZKw9WHRxjR4948", "yA4gF5KrboK2m2Xu7", "Kyc5dFDzBg4WccrbK", "jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-24T22:10:39.093Z", "modifiedAt": null, "url": null, "title": "An overview of forecasting for politics, conflict, and political violence", "slug": "an-overview-of-forecasting-for-politics-conflict-and", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XSeXBg9RHuuGagpqy/an-overview-of-forecasting-for-politics-conflict-and", "pageUrlRelative": "/posts/XSeXBg9RHuuGagpqy/an-overview-of-forecasting-for-politics-conflict-and", "linkUrl": "https://www.lesswrong.com/posts/XSeXBg9RHuuGagpqy/an-overview-of-forecasting-for-politics-conflict-and", "postedAtFormatted": "Tuesday, June 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20overview%20of%20forecasting%20for%20politics%2C%20conflict%2C%20and%20political%20violence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20overview%20of%20forecasting%20for%20politics%2C%20conflict%2C%20and%20political%20violence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSeXBg9RHuuGagpqy%2Fan-overview-of-forecasting-for-politics-conflict-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20overview%20of%20forecasting%20for%20politics%2C%20conflict%2C%20and%20political%20violence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSeXBg9RHuuGagpqy%2Fan-overview-of-forecasting-for-politics-conflict-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSeXBg9RHuuGagpqy%2Fan-overview-of-forecasting-for-politics-conflict-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2452, "htmlBody": "<p>Global politics is a high-stakes affair, and being able to predict it, prepare for it, and in some cases manipulate it, could be a game-changer. Political forecasting, construed broadly as forecasting of events that affect the structure of political systems and the configurations of political power, is therefore quite an important activity.</p>\n<p>In relatively stable, prosperous, democratic, and developed countries, political forecasting largely involves two related forecasting activities:</p>\n<ul>\n<li>Predicting what individual or political parties will win elections</li>\n<li>Predicting what sort of policies will be implemented by the ruling parties</li>\n</ul>\n<p>The tools used here are relatively simple: public opinion (both on who people will vote for and what policies they want to see implemented) is tracked through opinion polls. For forecasts made far out in the future, the opinion poll results are combined with some models about how changing economic or political conditions are likely to affect people's voting choices. (In fact, as of the time of writing this post, the <a href=\"http://en.wikipedia.org/wiki/Political_forecasting\">Wikipedia page on political forecasting</a> somewhat narrowly defines it as election forecasting, thereby taking the perspective of a relatively stable democratic country).</p>\n<p>A <a href=\"http://blogs.sas.com/content/forecasting/2012/11/08/simple-methods-and-ensemble-forecasting/\">blog post on the SAS Business Forecasting blog</a> reviews the main methods used in election forecasting, and compares their performance on US presidential elections. It identifies three broad categories of models that seem to make somewhat reliable and high-quality predictions:</p>\n<ul>\n<li>Prediction markets such as Intrade (<a href=\"http://www.intrade.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Intrade\">Wikipedia</a>) and the Iowa Electonic Markets (<a href=\"http://tippie.uiowa.edu/iem/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Iowa_Electronic_Markets\">Wikipedia</a>).</li>\n<li>Combination models such as PollyVote (<a href=\"http://www.pollyvote.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/PollyVote\">Wikipedia</a>), created by J. Scott Armstrong. A <a href=\"https://www.researchgate.net/publication/228163600_Combining_Forecasts_An_Application_to_Elections/file/60b7d51d070f068766.pdf?origin=publication_detail\">paper</a> by Armstrong with Andreas Graefe claims that despite being quite stable and accurate, PollyVote has not received much media coverage, and speculates as to the reasons.</li>\n<li>Polling aggregators such as FiveThirtyEight (<a href=\"http://www.fivethirtyeight.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/FiveThirtyEight\">Wikipedia</a>) (run by Nate Silver) and Votamatic (<a href=\"http://www.votamatic.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Votamatic\">Wikipedia</a>). See also the blog post <a href=\"http://rationality.org/2012/11/09/was-nate-silver-the-most-accurate-2012-election-pundit/\">Was Nate Silver the Most Accurate 2012 Election Pundit?</a> by Luke Muehlhauser and Gwern Banwen, November 9, 2012.</li>\n</ul>\n<p>When considering <em>global </em>politics, however, the narrow focus on public opinion and election performance is misguided, for two reasons. First, the interactions between nation-states are governed by rules somewhat different from those that govern local politics, and they often do not map easily to public opinion. Second, and more importantly, in many countries around the world, what transpires politically is far from an accurate reflection of majority public opinion. Of course, public opinion does affect political outcomes, even in dictatorships, but the nature of the relationship is more complex: the opinions of subsets of the population that have more of an ability to stage a coup matter more. Also, in some countries, there are centers of political power or activity that are not (officially) linked with the state. These include rebel groups, separatists, terrorists, and local militia. Even countries that do have elections may not necessarily have <em>free and fair elections</em>, so the outcome of the election may be governed more by who controls the polling apparatus than by what the people want.</p>\n<p>This is not to suggest that most parts of the world are mired in continuous, frequent conflict. Many parts of the world, even poor parts and undemocratic parts, are generally peaceful most of the time. But threats of intergroup or interfactional violence play an important role in governing the trajectory of events, even if violence itself is rare.</p>\n<p>The problem of predicting global politics is therefore tricky because it's <a href=\"https://dartthrowingchimp.wordpress.com/2014/05/07/asking-the-right-questions/\">not even clear what questions we should be asking</a>. The simple question \"who will win the election in 2014?\" isn't good enough. Lots of other questions, such as \"will the army stage a coup?\" or \"will the president be willing to have a free and fair election, or conduct a sham referendum to consolidate his power?\" or \"will the government purge &lt;insert unpopular minority group here&gt; from the big cities?\" could be worth asking. And it's sometimes not even clear whether a question is worth asking until it has been answered in the affirmative.</p>\n<p>For the purpose of this post, then, we will discuss together the domains of political forecasting (that largely involves election forecasting and public opinion forecasting) and the forecasting of conflicts, terrorism and international crises.</p>\n<p><strong>Actors interested in making predictions about global politics</strong></p>\n<p>So who gets in the business of trying to figure out how global politics will unfold? One obvious answer is: governments of other countries, partly with the goal of protecting the country's own economic interest in those other countries, and partly with humanitarian goals of avoiding the other country getting into violence. In addition, agencies whose goal is to combat terrorism are also interested in political developments that might create breeding grounds or support infrastructure for terrorism. Industries that depend on resources available only in a few countries are interested in making sure that those countries remain sufficiently stable that they can continue extracting the resources (and/or that they are on sufficiently good terms with the rulers that the resource extraction can continue despite the presence of conflict).</p>\n<p>Some specific actors are listed below:</p>\n<ul>\n<li>Intelligence Advanced Research Projects Activity (IARPA) (<a href=\"http://www.iarpa.gov\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity\">Wikipedia</a>), a research agency under the Office of National Intelligence in the United States. IARPA sponsors the Aggregative Contingent Estimation (ACE) program (<a href=\"http://www.iarpa.gov/index.php/research-programs/ace\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Aggregative_Contingent_Estimation\">Wikipedia</a>) that funds The Good Judgment Project, one of the best political forecasting tools today.</li>\n<li>Political Instability Task Force (PITF) (<a href=\"http://globalpolicy.gmu.edu/pitf/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Political_Instability_Task_Force\">Wikipedia</a>), funded by the Central Intelligence Agency (CIA) in the United States.</li>\n<li>Various governmental and nongovernmental initiatives aimed at genocide prevention, such as the Center for the Prevention of Genocide (<a href=\"http://www.ushmm.org/confront-genocide/about/center-prevention-genocide-staff\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Center_for_the_Prevention_of_Genocide\">Wikipedia</a>) at the United States Holocaust Memorial Museum, and the Sentinel Project for Genocide Prevention (<a href=\"http://www.thesentinelproject.org/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Sentinel_Project_for_Genocide_Prevention\">Wikipedia</a>) </li>\n<li>Terrorism-related initiatives, such as the National Consotrium for the Study of Terrorism and Responses to Terrorism (START) (<a href=\"http://www.start.umd.edu/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/National_Consortium_for_the_Study_of_Terrorism_and_Responses_to_Terrorism\">Wikipedia</a>) and the Chicago Project on Security and Terrorism (CPOST) (<a href=\"http://cpost.uchicago.edu/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Chicago_Project_on_Security_and_Terrorism\">Wikipedia</a>).</li>\n</ul>\n<p><strong>Databases used in predictive analytics for global politics</strong></p>\n<table border=\"1\">\n<tbody>\n<tr>\n<th>Database name</th><th>Areas covered</th><th>Year range</th><th>Availability and update frequency</th>\n</tr>\n<tr>\n<td>Global Database of Events, Language, and Tone (GDELT) (<a href=\"http://gdeltproject.org\">GDELT project</a>, <a href=\"https://en.wikipedia.org/wiki/Global_Database_of_Events,_Language,_and_Tone\">Wikipedia</a>)</td>\n<td>All political events, using <a href=\"http://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations\">CAMEO codes</a></td>\n<td>1979-present</td>\n<td>Publicly available, updated daily</td>\n</tr>\n<tr>\n<td>Integrated Conflict Early Warning System (ICEWS) (<a href=\"http://www.lockheedmartin.com/us/products/W-ICEWS.html\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Integrated_Conflict_Early_Warning_System\">Wikipedia</a>)</td>\n<td>All political events using CAMEO framework. However, <a href=\"http://predictiveheuristics.com/2013/10/17/gdelt-and-icews-a-short-comparison/\">unlike GDELT</a>, follows a more traditional approach to event data in seeking to encode a chronology of events that reflects in some sense the putative ground truth of what occurred.</td>\n<td>?</td>\n<td>Not publicly available, but was <a href=\"http://predictiveheuristics.com/2013/11/12/prediction-and-good-judgment-can-icews-inform-forecasts/\">made available</a> to forecasters for <a href=\"http://en.wikipedia.org/wiki/The_Good_Judgment_Project\">The Good Judgment Project</a></td>\n</tr>\n<tr>\n<td>Armed Conflict Location and Event Data Project (ACLED) (<a href=\"http://www.acleddata.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Armed_Conflict_Location_and_Event_Data_Project\">Wikipedia</a>)</td>\n<td>Armed conflicts</td>\n<td>1997-present (for Africa), fewer years for South Asia</td>\n<td>Publicly available, updated daily</td>\n</tr>\n<tr>\n<td>Uppsala Conflict Data Program/Peace Research Institute of Oslo (UCDP/PRIO) Armed Conflict Dataset (<a href=\"http://www.ucdp.uu.se/gpdatabase/search.php\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Uppsala_Conflict_Data_Program\">Wikipedia</a>)</td>\n<td>Armed conflicts</td>\n<td>1946-present (data collected live only since the 1970s)</td>\n<td>Publicly available, updated annually</td>\n</tr>\n<tr>\n<td>Worldwide Atrocities Dataset (<a href=\"http://eventdata.parusanalytics.com/data.dir/atrocities.html\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Worldwide_Atrocities_Dataset\">Wikipedia</a>)</td>\n<td>Atrocities, i.e., the deliberate use of lethal violence against non-combatant civilians by actors engaged in a wider political or military conflict</td>\n<td>1995-present, with a four-month embargo period</td>\n<td>Monthly (with a four-month embargo period)</td>\n</tr>\n<tr>\n<td>Correlates of War (<a href=\"http://correlatesofwar.org/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Correlates_of_War\">Wikipedia</a>)</td>\n<td>Data sets on Militarized Interstate Disputes (MID), National Material Capabilities, World Religion, Formal Alliances, Territorial Change, Direct Contiguity, and many more (see <a href=\"http://correlatesofwar.org/Datasets.htm\">here for a full list</a>)</td>\n<td>Some of the data sets are available from about 1815 or 1816, the data sets proceed till about 2005 or 2006, but are likely to be updated to include later years.</td>\n<td>Unclear</td>\n</tr>\n<tr>\n<td>International Crisis Behavior Project (<a>website</a>, <a href=\"https://en.wikipedia.org/wiki/International_Crisis_Behavior_Project\">Wikipedia</a>)</td>\n<td>International crises, loosely defined as something that had the potential to lead to a conflict or war</td>\n<td>1918-2007</td>\n<td>A new version of the database is released every few years, adding more years at the end. The database remains a few years behind the times. For instance, as of June 2014, the most recent release is from July 2010 and covers data till 2007. All data is freely available online.</td>\n</tr>\n<tr>\n<td>Global Terrorism Database (<a href=\"http://www.start.umd.edu/gtd/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Global_Terrorism_Database\">Wikipedia</a>)</td>\n<td>All terrorist attacks</td>\n<td>1970-2012, excluding 1993 (more years to be added)</td>\n<td>Approximately annually</td>\n</tr>\n<tr>\n<td>Suicide Attack Database (<a href=\"http://cpostdata.uchicago.edu/search_new.php\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Suicide_Attack_Database\">Wikipedia</a>)</td>\n<td>All suicide attacks</td>\n<td>1982-2013</td>\n<td>Approximately annually</td>\n</tr>\n<tr>\n<td>Manifesto Project Database (earlier known as the Comparative Manifestos Project) (<a href=\"http://manifesto-project.wzb.eu/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Manifesto_Project_Database\">Wikipedia</a>)</td>\n<td>Political manifestors and election performance of over 50 free democratic countries</td>\n<td>1945 onward</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Hatebase (<a href=\"http://www.hatebase.org/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Hatebase\">Wikipedia</a>)</td>\n<td>Instances of hate speech that might be predictors of potential violence or persecution</td>\n<td>Presumably 2013 onward, when it launched</td>\n<td>\n<p>Continuous, relies on user submissions, publicly available</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Global opinion polling and influence/sentiment measurement<br /></strong></p>\n<p>In addition to databases of political events, worldwide opinion polls are also useful in political forecasting. Examples of agencies that conduct worldwide opinion polls are:</p>\n<ul>\n<li>Gallup (<a href=\"http://www.gallup.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Gallup\">Wikipedia</a>)</li>\n<li>Pew Research Global Attitudes Project (<a href=\"http://www.pewglobal.org/\">website</a>)</li>\n</ul>\n<p>In addition to opinion polling, other tools used to measure global public sentiment include analytics for web service usage. There has been considerable research on using information from sources such as <a href=\"http://www.google.com/trends\">Google Trends</a>, Twitter, and Wikipedia. The United Nations Global Pulse (<a href=\"http://www.unglobalpulse.org\">website</a>, <a href=\"http://en.wikipedia.org/wiki/United_Nations_Global_Pulse\">Wikipedia</a>) is an example of an effort to use the digital trails of people to extract meaningful information.</p>\n<p><strong>Some predictive algorithms</strong></p>\n<p>How do we use the mass of structured and unstructured data to make smart political predictions? There are a number of people that claim to have good prediction strategies, but some of them have been debunked, while the jury on others is still out. Some examples are listed below.</p>\n<ul>\n<li>The <strong>most widely respected source for political predictions is The Good Judgment Project</strong> (<a href=\"http://www.goodjudgmentproject.com/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/The_Good_Judgment_Project\">Wikipedia</a>). This is a forecasting competition where anybody can participate. Participants are given a set of questions and can basically collect freely available online information (in some rounds, participants were given additional access to some proprietary data). They then use that to make predictions. The aggregate predictions are quite good. For more information, visit the website or see the references in the Wikipedia article. In particular, <a href=\"http://www.economist.com/news/21589145-how-sort-best-rest-whos-good-forecasts\">this <em>Economist</em> article</a> and <a href=\"http://www.businessinsider.com/good-judgement-project-accurate-predictions-2014-4\">this <em>Business Insider</em> article</a> are worth reading.</li>\n<li>The Integrated Conflict Early Warning System (ICEWS) (<a href=\"http://www.lockheedmartin.com/us/products/W-ICEWS.html\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Integrated_Conflict_Early_Warning_System\">Wikipedia</a>) run by Lockheed Martin combines data collection on global political events with a predictive algorithm to provide early warning of conflicts. it is in use by the United States Pacific Command and United States Southern Command. However, ICEWS data and predictions are not publicly available, so it is hard to independently gauge their accuracy. For more, see <a href=\"http://earlywarning.wordpress.com/2010/03/20/early-warning-decision-support/\">this post by Patrick Meier</a> and other posts in the references of the Wikipedia page on the ICEWS.</li>\n<li><a href=\"http://qz.com/40960/the-14-rules-for-predicting-future-geopolitical-events/\" target=\"_blank\">Steve Levine at Quartz</a> claims to have a geopolitical prediction algorithm with high accuracy, and that successfully predicted a number of events in 2013. The main critical analysis (that makes mostly commonsensical points) I could find was <a href=\"http://dartthrowingchimp.wordpress.com/2013/01/09/rules-of-thumb-vs-statistical-models-or-the-misconception-that-will-not-die/\" target=\"_blank\">this</a> by Jay Ulfelder.</li>\n<li>Bruce Buenos de Mesquita claims to have a good forecasting track record (see for instance this <a href=\"http://www.cato-unbound.org/issues/july-2011/whats-wrong-expert-predictions\" target=\"_blank\">Cato Unbound July 2011 discusssion</a> and this <a href=\"http://www.sciencenews.org/view/generic/id/9041/title/Math_Trek__Mathematical_Fortune-Telling\" target=\"_blank\">news article</a>). I found a <a href=\"http://decision-making.moshe-online.com/criticism_of_bueno_de_mesquita.html\" target=\"_blank\">rather lengthy online critique of his work online</a> but I haven't vetted the substance of the critique. In general, people in the blogs I list below are skeptical of his methods.</li>\n<li>Professor Lincoln P. Bloomfield developed the MIT Cascon System for Analyzing International Conflict (<a href=\"http://web.mit.edu/cascon/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/CASCON_%28decision_support_system%29\">Wikipedia</a>). I'm not really sure what this is about.</li>\n</ul>\n<p><strong>Websites with a good overview of different datasets and predictive algorithms</strong></p>\n<p>The <a href=\"http://www.forecastingprinciples.com\">Forecasting Principles</a> website has some interesting overviews related to the forecasting realm. The two relevant Special Interest Groups are:</p>\n<ul>\n<li>The <a href=\"http://www.forecastingprinciples.com/index.php?option=com_content&amp;view=article&amp;id=20&amp;Itemid=245\">Conflict and Terror Special Interest Group</a></li>\n<li>The <a href=\"http://www.forecastingprinciples.com/index.php?option=com_content&amp;view=article&amp;id=24&amp;Itemid=274\">Political Forecasting Special Interest Group</a></li>\n</ul>\n<p><strong>Blogs to follow</strong></p>\n<ul>\n<li><a href=\"http://www.predictiveheuristics.com\">Predictive Heuristics</a>, a group blog</li>\n<li><a href=\"http://dartthrowingchimp.wordpress.com\">Dart-Throwing Chimp</a>, the blog of Jay Ulfelder</li>\n<li><a href=\"http://badhessian.org\">Bad Hessian</a>, a group blog</li>\n<li><a href=\"http://politicalviolenceataglance.org\">Political Violence at a Glance</a></li>\n<li><a href=\"http://www.irevolution.net\">iRevolution</a> by Patrick Meier. The author earlier blogged at the <a href=\"http://earlywarning.wordpress.com\">Early Warning Project</a>.</li>\n<li>For more blogs, see the blogroll of Predictive Heuristics</li>\n<li>The Monkey Cage, formerly an <a href=\"http://themonkeycage.org/\">independent blog</a>, now with <a href=\"http://www.washingtonpost.com/blogs/monkey-cage/\"><em>The Washington Post</em></a></li>\n</ul>\n<p><strong>Important people</strong></p>\n<ul>\n<li>Jay Ulfelder (<a href=\"http://dartthrowingchimp.wordpress.com/about/\">about page on his own blog</a>, <a href=\"http://en.wikipedia.org/wiki/Jay_Ulfelder\">Wikipedia</a>): An expert on political forecasting, Ulfelder was at the helm of the CIA's Political Instability Task Force and is now advising the Early Warning Project of the Center for the Prevention of Genocide.</li>\n<li>Kalev Leetaru (<a href=\"http://www.kalevleetaru.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Kalev_Leetaru\">Wikipedia</a>): He is the co-creator of GDELT and has previously worked on other big data projects.</li>\n<li>Philip A. Schrodt (<a href=\"http://asecondmouse.wordpress.com/about/\">about page on his own blog</a>, <a href=\"https://en.wikipedia.org/wiki/Philip_Schrodt\">Wikipedia</a>): He is a legend in the world of automated event database creation. He is a co-creator of the <a href=\"https://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations\">CAMEO framework</a> and also a co-creator of GDELT.</li>\n<li>Michael D. Ward (<a href=\"http://web.duke.edu/methods\">website</a>, <a href=\"http://mdwardlab.com\">Ward Lab</a>, <a href=\"http://en.wikipedia.org/wiki/Michael_D._Ward\">Wikipedia</a>)</li>\n<li>Jay Yonamine</li>\n<li>Philip E. Tetlock (<a href=\"/www.sas.upenn.edu/tetlock\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Philip_E._Tetlock\">Wikipedia</a>): Although not a political forecasters himself, his study of the track record of expert predictions has influenced thinking in the subject. He currently co-runs The Good Judgment Project, a political forecasting competition, along with Barbara Mellers and Don Moore</li>\n</ul>\n<p><strong>Journals</strong></p>\n<p>To my knowledge, there are no journals exclusively devoted to forecasting, but the work of the people listed above has generally appeared in a relatively small set of journals, listed below.</p>\n<ul>\n<li><em>Journal of Conflict Resolution</em> (<a href=\"http://www.sagepub.com/journals/Journal200764/title\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Journal_of_Conflict_Resolution\">Wikipedia</a>)</li>\n<li><em>Journal of Peace Research</em> (<a href=\"http://jpr.sagepub.com/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Journal_of_Peace_Research\">Wikipedia</a>)</li>\n<li><em>International Studies Review</em> (<a href=\"http://isr.nus.edu.sg/index.html\">website</a>, <a href=\"https://en.wikipedia.org/wiki/International_Studies_Review\">Wikipedia</a>)</li>\n</ul>\n<p>There are also some journals devoted to forecasting in general (list to appear in a future post of mine) but as far as I can make out, very little of the conflict forecasting literature is published in such journals.</p>\n<ul>\n</ul>\n<p><strong>Popular magazines</strong></p>\n<p>Two magazine worth checking out for discussions of political science, that occasionally discuss issues related to forecasting, are:</p>\n<ul>\n<li><em>Foreign Policy</em> (<a href=\"http://foreignpolicy.com/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Foreign_Policy_%28magazine%29\">Wikipedia</a>): This is by far the most likely source of information. Kalev Leetaru was an occasional contributor. It has included pieces from Jay Ulfelder and Michael D. Ward. Joshua Keating has also discussed many themes related to predictive analytics and forecasting while at FP.</li>\n<li><em>Foreign Affairs</em> (<a href=\"http://www.foreignaffairs.com\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Foreign_Affairs\">Wikipedia</a>)</li>\n</ul>\n<p><strong>Additional reading</strong></p>\n<p>On election forecasting (these links are supplementary to the links in the main text):</p>\n<ul>\n<li><a href=\"http://election.princeton.edu\">Princeton Election Consortium</a></li>\n<li><a href=\"http://themonkeycage.org/2012/11/08/poll-aggregation-and-election-forecasting/\">Poll aggregation and election forecasting</a> by Andrew Gelman at <em>The Monkey Cage</em></li>\n<li><a href=\"http://andrewgelman.com/2010/11/03/some_thoughts_o_8/\">Some thoughts on election forecasting</a> by Andrew Gelman on his own blog</li>\n</ul>\n<p>On global political forecasting and general considerations relevant to it:</p>\n<ul>\n</ul>\n<ul>\n<li><a href=\"http://www.foreignpolicy.com/articles/2012/11/08/why_the_world_cant_have_a_nate_silver\">Why the World Can't Have a Nate Silver. The quants are riding high after Team Data crushed Team Gut in the U.S. election forecasts. But predicting the Electoral College vote is child's play next to some of these hard targets.</a> by Jay Ulfelder.</li>\n<li><a href=\"http://www.foreignpolicy.com/articles/2012/11/16/predicting_the_future_is_easier_than_it_looks\">Predicting the Future is Easier Than it Looks</a> by Michael D. Ward and Nils Metternich (response to Ulfelder).</li>\n<li> <a href=\"http://www.cato-unbound.org/issues/july-2011/whats-wrong-expert-predictions\" target=\"_blank\">Cato Unbound July 2011 discusssion</a> featuring Philip Tetlock, Dan Gardner, Robin Hanson, John Cochrane, and Bruce Bueno des Mesquita.</li>\n<li><a href=\"https://dartthrowingchimp.wordpress.com/2012/06/24/in-defense-of-political-science-and-forecasting/\">In Defense of Political Science and Forecasting</a> by Jay Ulfelder, responding to an op-ed by Jacqueline Stevens.</li>\n</ul>\n<p><strong>UPDATE</strong>: I found what seems to be a fairly interesting and thorough paper titled <a href=\"http://www.guillaumenicaise.com/wp-content/uploads/2013/10/Crisis-Forecasting-Models.pdf\">Stepping into the future: the next generation of crisis forecasting models</a> by some of the people listed here (specifically Michael D. Ward, Nils Metternich, and co-authors at the Ward Lab). I haven't had time to examine it closely. It includes a discussion of ICEWS.</p>\n<ul>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "8daMDi9NEShyLqxth": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XSeXBg9RHuuGagpqy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "26370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Global politics is a high-stakes affair, and being able to predict it, prepare for it, and in some cases manipulate it, could be a game-changer. Political forecasting, construed broadly as forecasting of events that affect the structure of political systems and the configurations of political power, is therefore quite an important activity.</p>\n<p>In relatively stable, prosperous, democratic, and developed countries, political forecasting largely involves two related forecasting activities:</p>\n<ul>\n<li>Predicting what individual or political parties will win elections</li>\n<li>Predicting what sort of policies will be implemented by the ruling parties</li>\n</ul>\n<p>The tools used here are relatively simple: public opinion (both on who people will vote for and what policies they want to see implemented) is tracked through opinion polls. For forecasts made far out in the future, the opinion poll results are combined with some models about how changing economic or political conditions are likely to affect people's voting choices. (In fact, as of the time of writing this post, the <a href=\"http://en.wikipedia.org/wiki/Political_forecasting\">Wikipedia page on political forecasting</a> somewhat narrowly defines it as election forecasting, thereby taking the perspective of a relatively stable democratic country).</p>\n<p>A <a href=\"http://blogs.sas.com/content/forecasting/2012/11/08/simple-methods-and-ensemble-forecasting/\">blog post on the SAS Business Forecasting blog</a> reviews the main methods used in election forecasting, and compares their performance on US presidential elections. It identifies three broad categories of models that seem to make somewhat reliable and high-quality predictions:</p>\n<ul>\n<li>Prediction markets such as Intrade (<a href=\"http://www.intrade.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Intrade\">Wikipedia</a>) and the Iowa Electonic Markets (<a href=\"http://tippie.uiowa.edu/iem/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Iowa_Electronic_Markets\">Wikipedia</a>).</li>\n<li>Combination models such as PollyVote (<a href=\"http://www.pollyvote.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/PollyVote\">Wikipedia</a>), created by J. Scott Armstrong. A <a href=\"https://www.researchgate.net/publication/228163600_Combining_Forecasts_An_Application_to_Elections/file/60b7d51d070f068766.pdf?origin=publication_detail\">paper</a> by Armstrong with Andreas Graefe claims that despite being quite stable and accurate, PollyVote has not received much media coverage, and speculates as to the reasons.</li>\n<li>Polling aggregators such as FiveThirtyEight (<a href=\"http://www.fivethirtyeight.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/FiveThirtyEight\">Wikipedia</a>) (run by Nate Silver) and Votamatic (<a href=\"http://www.votamatic.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Votamatic\">Wikipedia</a>). See also the blog post <a href=\"http://rationality.org/2012/11/09/was-nate-silver-the-most-accurate-2012-election-pundit/\">Was Nate Silver the Most Accurate 2012 Election Pundit?</a> by Luke Muehlhauser and Gwern Banwen, November 9, 2012.</li>\n</ul>\n<p>When considering <em>global </em>politics, however, the narrow focus on public opinion and election performance is misguided, for two reasons. First, the interactions between nation-states are governed by rules somewhat different from those that govern local politics, and they often do not map easily to public opinion. Second, and more importantly, in many countries around the world, what transpires politically is far from an accurate reflection of majority public opinion. Of course, public opinion does affect political outcomes, even in dictatorships, but the nature of the relationship is more complex: the opinions of subsets of the population that have more of an ability to stage a coup matter more. Also, in some countries, there are centers of political power or activity that are not (officially) linked with the state. These include rebel groups, separatists, terrorists, and local militia. Even countries that do have elections may not necessarily have <em>free and fair elections</em>, so the outcome of the election may be governed more by who controls the polling apparatus than by what the people want.</p>\n<p>This is not to suggest that most parts of the world are mired in continuous, frequent conflict. Many parts of the world, even poor parts and undemocratic parts, are generally peaceful most of the time. But threats of intergroup or interfactional violence play an important role in governing the trajectory of events, even if violence itself is rare.</p>\n<p>The problem of predicting global politics is therefore tricky because it's <a href=\"https://dartthrowingchimp.wordpress.com/2014/05/07/asking-the-right-questions/\">not even clear what questions we should be asking</a>. The simple question \"who will win the election in 2014?\" isn't good enough. Lots of other questions, such as \"will the army stage a coup?\" or \"will the president be willing to have a free and fair election, or conduct a sham referendum to consolidate his power?\" or \"will the government purge &lt;insert unpopular minority group here&gt; from the big cities?\" could be worth asking. And it's sometimes not even clear whether a question is worth asking until it has been answered in the affirmative.</p>\n<p>For the purpose of this post, then, we will discuss together the domains of political forecasting (that largely involves election forecasting and public opinion forecasting) and the forecasting of conflicts, terrorism and international crises.</p>\n<p><strong id=\"Actors_interested_in_making_predictions_about_global_politics\">Actors interested in making predictions about global politics</strong></p>\n<p>So who gets in the business of trying to figure out how global politics will unfold? One obvious answer is: governments of other countries, partly with the goal of protecting the country's own economic interest in those other countries, and partly with humanitarian goals of avoiding the other country getting into violence. In addition, agencies whose goal is to combat terrorism are also interested in political developments that might create breeding grounds or support infrastructure for terrorism. Industries that depend on resources available only in a few countries are interested in making sure that those countries remain sufficiently stable that they can continue extracting the resources (and/or that they are on sufficiently good terms with the rulers that the resource extraction can continue despite the presence of conflict).</p>\n<p>Some specific actors are listed below:</p>\n<ul>\n<li>Intelligence Advanced Research Projects Activity (IARPA) (<a href=\"http://www.iarpa.gov\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity\">Wikipedia</a>), a research agency under the Office of National Intelligence in the United States. IARPA sponsors the Aggregative Contingent Estimation (ACE) program (<a href=\"http://www.iarpa.gov/index.php/research-programs/ace\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Aggregative_Contingent_Estimation\">Wikipedia</a>) that funds The Good Judgment Project, one of the best political forecasting tools today.</li>\n<li>Political Instability Task Force (PITF) (<a href=\"http://globalpolicy.gmu.edu/pitf/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Political_Instability_Task_Force\">Wikipedia</a>), funded by the Central Intelligence Agency (CIA) in the United States.</li>\n<li>Various governmental and nongovernmental initiatives aimed at genocide prevention, such as the Center for the Prevention of Genocide (<a href=\"http://www.ushmm.org/confront-genocide/about/center-prevention-genocide-staff\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Center_for_the_Prevention_of_Genocide\">Wikipedia</a>) at the United States Holocaust Memorial Museum, and the Sentinel Project for Genocide Prevention (<a href=\"http://www.thesentinelproject.org/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Sentinel_Project_for_Genocide_Prevention\">Wikipedia</a>) </li>\n<li>Terrorism-related initiatives, such as the National Consotrium for the Study of Terrorism and Responses to Terrorism (START) (<a href=\"http://www.start.umd.edu/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/National_Consortium_for_the_Study_of_Terrorism_and_Responses_to_Terrorism\">Wikipedia</a>) and the Chicago Project on Security and Terrorism (CPOST) (<a href=\"http://cpost.uchicago.edu/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Chicago_Project_on_Security_and_Terrorism\">Wikipedia</a>).</li>\n</ul>\n<p><strong id=\"Databases_used_in_predictive_analytics_for_global_politics\">Databases used in predictive analytics for global politics</strong></p>\n<table border=\"1\">\n<tbody>\n<tr>\n<th>Database name</th><th>Areas covered</th><th>Year range</th><th>Availability and update frequency</th>\n</tr>\n<tr>\n<td>Global Database of Events, Language, and Tone (GDELT) (<a href=\"http://gdeltproject.org\">GDELT project</a>, <a href=\"https://en.wikipedia.org/wiki/Global_Database_of_Events,_Language,_and_Tone\">Wikipedia</a>)</td>\n<td>All political events, using <a href=\"http://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations\">CAMEO codes</a></td>\n<td>1979-present</td>\n<td>Publicly available, updated daily</td>\n</tr>\n<tr>\n<td>Integrated Conflict Early Warning System (ICEWS) (<a href=\"http://www.lockheedmartin.com/us/products/W-ICEWS.html\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Integrated_Conflict_Early_Warning_System\">Wikipedia</a>)</td>\n<td>All political events using CAMEO framework. However, <a href=\"http://predictiveheuristics.com/2013/10/17/gdelt-and-icews-a-short-comparison/\">unlike GDELT</a>, follows a more traditional approach to event data in seeking to encode a chronology of events that reflects in some sense the putative ground truth of what occurred.</td>\n<td>?</td>\n<td>Not publicly available, but was <a href=\"http://predictiveheuristics.com/2013/11/12/prediction-and-good-judgment-can-icews-inform-forecasts/\">made available</a> to forecasters for <a href=\"http://en.wikipedia.org/wiki/The_Good_Judgment_Project\">The Good Judgment Project</a></td>\n</tr>\n<tr>\n<td>Armed Conflict Location and Event Data Project (ACLED) (<a href=\"http://www.acleddata.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Armed_Conflict_Location_and_Event_Data_Project\">Wikipedia</a>)</td>\n<td>Armed conflicts</td>\n<td>1997-present (for Africa), fewer years for South Asia</td>\n<td>Publicly available, updated daily</td>\n</tr>\n<tr>\n<td>Uppsala Conflict Data Program/Peace Research Institute of Oslo (UCDP/PRIO) Armed Conflict Dataset (<a href=\"http://www.ucdp.uu.se/gpdatabase/search.php\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Uppsala_Conflict_Data_Program\">Wikipedia</a>)</td>\n<td>Armed conflicts</td>\n<td>1946-present (data collected live only since the 1970s)</td>\n<td>Publicly available, updated annually</td>\n</tr>\n<tr>\n<td>Worldwide Atrocities Dataset (<a href=\"http://eventdata.parusanalytics.com/data.dir/atrocities.html\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Worldwide_Atrocities_Dataset\">Wikipedia</a>)</td>\n<td>Atrocities, i.e., the deliberate use of lethal violence against non-combatant civilians by actors engaged in a wider political or military conflict</td>\n<td>1995-present, with a four-month embargo period</td>\n<td>Monthly (with a four-month embargo period)</td>\n</tr>\n<tr>\n<td>Correlates of War (<a href=\"http://correlatesofwar.org/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Correlates_of_War\">Wikipedia</a>)</td>\n<td>Data sets on Militarized Interstate Disputes (MID), National Material Capabilities, World Religion, Formal Alliances, Territorial Change, Direct Contiguity, and many more (see <a href=\"http://correlatesofwar.org/Datasets.htm\">here for a full list</a>)</td>\n<td>Some of the data sets are available from about 1815 or 1816, the data sets proceed till about 2005 or 2006, but are likely to be updated to include later years.</td>\n<td>Unclear</td>\n</tr>\n<tr>\n<td>International Crisis Behavior Project (<a>website</a>, <a href=\"https://en.wikipedia.org/wiki/International_Crisis_Behavior_Project\">Wikipedia</a>)</td>\n<td>International crises, loosely defined as something that had the potential to lead to a conflict or war</td>\n<td>1918-2007</td>\n<td>A new version of the database is released every few years, adding more years at the end. The database remains a few years behind the times. For instance, as of June 2014, the most recent release is from July 2010 and covers data till 2007. All data is freely available online.</td>\n</tr>\n<tr>\n<td>Global Terrorism Database (<a href=\"http://www.start.umd.edu/gtd/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Global_Terrorism_Database\">Wikipedia</a>)</td>\n<td>All terrorist attacks</td>\n<td>1970-2012, excluding 1993 (more years to be added)</td>\n<td>Approximately annually</td>\n</tr>\n<tr>\n<td>Suicide Attack Database (<a href=\"http://cpostdata.uchicago.edu/search_new.php\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Suicide_Attack_Database\">Wikipedia</a>)</td>\n<td>All suicide attacks</td>\n<td>1982-2013</td>\n<td>Approximately annually</td>\n</tr>\n<tr>\n<td>Manifesto Project Database (earlier known as the Comparative Manifestos Project) (<a href=\"http://manifesto-project.wzb.eu/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Manifesto_Project_Database\">Wikipedia</a>)</td>\n<td>Political manifestors and election performance of over 50 free democratic countries</td>\n<td>1945 onward</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Hatebase (<a href=\"http://www.hatebase.org/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Hatebase\">Wikipedia</a>)</td>\n<td>Instances of hate speech that might be predictors of potential violence or persecution</td>\n<td>Presumably 2013 onward, when it launched</td>\n<td>\n<p>Continuous, relies on user submissions, publicly available</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p><strong id=\"Global_opinion_polling_and_influence_sentiment_measurement\">Global opinion polling and influence/sentiment measurement<br></strong></p>\n<p>In addition to databases of political events, worldwide opinion polls are also useful in political forecasting. Examples of agencies that conduct worldwide opinion polls are:</p>\n<ul>\n<li>Gallup (<a href=\"http://www.gallup.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Gallup\">Wikipedia</a>)</li>\n<li>Pew Research Global Attitudes Project (<a href=\"http://www.pewglobal.org/\">website</a>)</li>\n</ul>\n<p>In addition to opinion polling, other tools used to measure global public sentiment include analytics for web service usage. There has been considerable research on using information from sources such as <a href=\"http://www.google.com/trends\">Google Trends</a>, Twitter, and Wikipedia. The United Nations Global Pulse (<a href=\"http://www.unglobalpulse.org\">website</a>, <a href=\"http://en.wikipedia.org/wiki/United_Nations_Global_Pulse\">Wikipedia</a>) is an example of an effort to use the digital trails of people to extract meaningful information.</p>\n<p><strong id=\"Some_predictive_algorithms\">Some predictive algorithms</strong></p>\n<p>How do we use the mass of structured and unstructured data to make smart political predictions? There are a number of people that claim to have good prediction strategies, but some of them have been debunked, while the jury on others is still out. Some examples are listed below.</p>\n<ul>\n<li>The <strong>most widely respected source for political predictions is The Good Judgment Project</strong> (<a href=\"http://www.goodjudgmentproject.com/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/The_Good_Judgment_Project\">Wikipedia</a>). This is a forecasting competition where anybody can participate. Participants are given a set of questions and can basically collect freely available online information (in some rounds, participants were given additional access to some proprietary data). They then use that to make predictions. The aggregate predictions are quite good. For more information, visit the website or see the references in the Wikipedia article. In particular, <a href=\"http://www.economist.com/news/21589145-how-sort-best-rest-whos-good-forecasts\">this <em>Economist</em> article</a> and <a href=\"http://www.businessinsider.com/good-judgement-project-accurate-predictions-2014-4\">this <em>Business Insider</em> article</a> are worth reading.</li>\n<li>The Integrated Conflict Early Warning System (ICEWS) (<a href=\"http://www.lockheedmartin.com/us/products/W-ICEWS.html\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Integrated_Conflict_Early_Warning_System\">Wikipedia</a>) run by Lockheed Martin combines data collection on global political events with a predictive algorithm to provide early warning of conflicts. it is in use by the United States Pacific Command and United States Southern Command. However, ICEWS data and predictions are not publicly available, so it is hard to independently gauge their accuracy. For more, see <a href=\"http://earlywarning.wordpress.com/2010/03/20/early-warning-decision-support/\">this post by Patrick Meier</a> and other posts in the references of the Wikipedia page on the ICEWS.</li>\n<li><a href=\"http://qz.com/40960/the-14-rules-for-predicting-future-geopolitical-events/\" target=\"_blank\">Steve Levine at Quartz</a> claims to have a geopolitical prediction algorithm with high accuracy, and that successfully predicted a number of events in 2013. The main critical analysis (that makes mostly commonsensical points) I could find was <a href=\"http://dartthrowingchimp.wordpress.com/2013/01/09/rules-of-thumb-vs-statistical-models-or-the-misconception-that-will-not-die/\" target=\"_blank\">this</a> by Jay Ulfelder.</li>\n<li>Bruce Buenos de Mesquita claims to have a good forecasting track record (see for instance this <a href=\"http://www.cato-unbound.org/issues/july-2011/whats-wrong-expert-predictions\" target=\"_blank\">Cato Unbound July 2011 discusssion</a> and this <a href=\"http://www.sciencenews.org/view/generic/id/9041/title/Math_Trek__Mathematical_Fortune-Telling\" target=\"_blank\">news article</a>). I found a <a href=\"http://decision-making.moshe-online.com/criticism_of_bueno_de_mesquita.html\" target=\"_blank\">rather lengthy online critique of his work online</a> but I haven't vetted the substance of the critique. In general, people in the blogs I list below are skeptical of his methods.</li>\n<li>Professor Lincoln P. Bloomfield developed the MIT Cascon System for Analyzing International Conflict (<a href=\"http://web.mit.edu/cascon/\">website</a>, <a href=\"http://en.wikipedia.org/wiki/CASCON_%28decision_support_system%29\">Wikipedia</a>). I'm not really sure what this is about.</li>\n</ul>\n<p><strong id=\"Websites_with_a_good_overview_of_different_datasets_and_predictive_algorithms\">Websites with a good overview of different datasets and predictive algorithms</strong></p>\n<p>The <a href=\"http://www.forecastingprinciples.com\">Forecasting Principles</a> website has some interesting overviews related to the forecasting realm. The two relevant Special Interest Groups are:</p>\n<ul>\n<li>The <a href=\"http://www.forecastingprinciples.com/index.php?option=com_content&amp;view=article&amp;id=20&amp;Itemid=245\">Conflict and Terror Special Interest Group</a></li>\n<li>The <a href=\"http://www.forecastingprinciples.com/index.php?option=com_content&amp;view=article&amp;id=24&amp;Itemid=274\">Political Forecasting Special Interest Group</a></li>\n</ul>\n<p><strong id=\"Blogs_to_follow\">Blogs to follow</strong></p>\n<ul>\n<li><a href=\"http://www.predictiveheuristics.com\">Predictive Heuristics</a>, a group blog</li>\n<li><a href=\"http://dartthrowingchimp.wordpress.com\">Dart-Throwing Chimp</a>, the blog of Jay Ulfelder</li>\n<li><a href=\"http://badhessian.org\">Bad Hessian</a>, a group blog</li>\n<li><a href=\"http://politicalviolenceataglance.org\">Political Violence at a Glance</a></li>\n<li><a href=\"http://www.irevolution.net\">iRevolution</a> by Patrick Meier. The author earlier blogged at the <a href=\"http://earlywarning.wordpress.com\">Early Warning Project</a>.</li>\n<li>For more blogs, see the blogroll of Predictive Heuristics</li>\n<li>The Monkey Cage, formerly an <a href=\"http://themonkeycage.org/\">independent blog</a>, now with <a href=\"http://www.washingtonpost.com/blogs/monkey-cage/\"><em>The Washington Post</em></a></li>\n</ul>\n<p><strong id=\"Important_people\">Important people</strong></p>\n<ul>\n<li>Jay Ulfelder (<a href=\"http://dartthrowingchimp.wordpress.com/about/\">about page on his own blog</a>, <a href=\"http://en.wikipedia.org/wiki/Jay_Ulfelder\">Wikipedia</a>): An expert on political forecasting, Ulfelder was at the helm of the CIA's Political Instability Task Force and is now advising the Early Warning Project of the Center for the Prevention of Genocide.</li>\n<li>Kalev Leetaru (<a href=\"http://www.kalevleetaru.com\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Kalev_Leetaru\">Wikipedia</a>): He is the co-creator of GDELT and has previously worked on other big data projects.</li>\n<li>Philip A. Schrodt (<a href=\"http://asecondmouse.wordpress.com/about/\">about page on his own blog</a>, <a href=\"https://en.wikipedia.org/wiki/Philip_Schrodt\">Wikipedia</a>): He is a legend in the world of automated event database creation. He is a co-creator of the <a href=\"https://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations\">CAMEO framework</a> and also a co-creator of GDELT.</li>\n<li>Michael D. Ward (<a href=\"http://web.duke.edu/methods\">website</a>, <a href=\"http://mdwardlab.com\">Ward Lab</a>, <a href=\"http://en.wikipedia.org/wiki/Michael_D._Ward\">Wikipedia</a>)</li>\n<li>Jay Yonamine</li>\n<li>Philip E. Tetlock (<a href=\"/www.sas.upenn.edu/tetlock\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Philip_E._Tetlock\">Wikipedia</a>): Although not a political forecasters himself, his study of the track record of expert predictions has influenced thinking in the subject. He currently co-runs The Good Judgment Project, a political forecasting competition, along with Barbara Mellers and Don Moore</li>\n</ul>\n<p><strong id=\"Journals\">Journals</strong></p>\n<p>To my knowledge, there are no journals exclusively devoted to forecasting, but the work of the people listed above has generally appeared in a relatively small set of journals, listed below.</p>\n<ul>\n<li><em>Journal of Conflict Resolution</em> (<a href=\"http://www.sagepub.com/journals/Journal200764/title\">website</a>, <a href=\"http://en.wikipedia.org/wiki/Journal_of_Conflict_Resolution\">Wikipedia</a>)</li>\n<li><em>Journal of Peace Research</em> (<a href=\"http://jpr.sagepub.com/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Journal_of_Peace_Research\">Wikipedia</a>)</li>\n<li><em>International Studies Review</em> (<a href=\"http://isr.nus.edu.sg/index.html\">website</a>, <a href=\"https://en.wikipedia.org/wiki/International_Studies_Review\">Wikipedia</a>)</li>\n</ul>\n<p>There are also some journals devoted to forecasting in general (list to appear in a future post of mine) but as far as I can make out, very little of the conflict forecasting literature is published in such journals.</p>\n<ul>\n</ul>\n<p><strong id=\"Popular_magazines\">Popular magazines</strong></p>\n<p>Two magazine worth checking out for discussions of political science, that occasionally discuss issues related to forecasting, are:</p>\n<ul>\n<li><em>Foreign Policy</em> (<a href=\"http://foreignpolicy.com/\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Foreign_Policy_%28magazine%29\">Wikipedia</a>): This is by far the most likely source of information. Kalev Leetaru was an occasional contributor. It has included pieces from Jay Ulfelder and Michael D. Ward. Joshua Keating has also discussed many themes related to predictive analytics and forecasting while at FP.</li>\n<li><em>Foreign Affairs</em> (<a href=\"http://www.foreignaffairs.com\">website</a>, <a href=\"https://en.wikipedia.org/wiki/Foreign_Affairs\">Wikipedia</a>)</li>\n</ul>\n<p><strong id=\"Additional_reading\">Additional reading</strong></p>\n<p>On election forecasting (these links are supplementary to the links in the main text):</p>\n<ul>\n<li><a href=\"http://election.princeton.edu\">Princeton Election Consortium</a></li>\n<li><a href=\"http://themonkeycage.org/2012/11/08/poll-aggregation-and-election-forecasting/\">Poll aggregation and election forecasting</a> by Andrew Gelman at <em>The Monkey Cage</em></li>\n<li><a href=\"http://andrewgelman.com/2010/11/03/some_thoughts_o_8/\">Some thoughts on election forecasting</a> by Andrew Gelman on his own blog</li>\n</ul>\n<p>On global political forecasting and general considerations relevant to it:</p>\n<ul>\n</ul>\n<ul>\n<li><a href=\"http://www.foreignpolicy.com/articles/2012/11/08/why_the_world_cant_have_a_nate_silver\">Why the World Can't Have a Nate Silver. The quants are riding high after Team Data crushed Team Gut in the U.S. election forecasts. But predicting the Electoral College vote is child's play next to some of these hard targets.</a> by Jay Ulfelder.</li>\n<li><a href=\"http://www.foreignpolicy.com/articles/2012/11/16/predicting_the_future_is_easier_than_it_looks\">Predicting the Future is Easier Than it Looks</a> by Michael D. Ward and Nils Metternich (response to Ulfelder).</li>\n<li> <a href=\"http://www.cato-unbound.org/issues/july-2011/whats-wrong-expert-predictions\" target=\"_blank\">Cato Unbound July 2011 discusssion</a> featuring Philip Tetlock, Dan Gardner, Robin Hanson, John Cochrane, and Bruce Bueno des Mesquita.</li>\n<li><a href=\"https://dartthrowingchimp.wordpress.com/2012/06/24/in-defense-of-political-science-and-forecasting/\">In Defense of Political Science and Forecasting</a> by Jay Ulfelder, responding to an op-ed by Jacqueline Stevens.</li>\n</ul>\n<p><strong>UPDATE</strong>: I found what seems to be a fairly interesting and thorough paper titled <a href=\"http://www.guillaumenicaise.com/wp-content/uploads/2013/10/Crisis-Forecasting-Models.pdf\">Stepping into the future: the next generation of crisis forecasting models</a> by some of the people listed here (specifically Michael D. Ward, Nils Metternich, and co-authors at the Ward Lab). I haven't had time to examine it closely. It includes a discussion of ICEWS.</p>\n<ul>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Actors interested in making predictions about global politics", "anchor": "Actors_interested_in_making_predictions_about_global_politics", "level": 1}, {"title": "Databases used in predictive analytics for global politics", "anchor": "Databases_used_in_predictive_analytics_for_global_politics", "level": 1}, {"title": "Global opinion polling and influence/sentiment measurement", "anchor": "Global_opinion_polling_and_influence_sentiment_measurement", "level": 1}, {"title": "Some predictive algorithms", "anchor": "Some_predictive_algorithms", "level": 1}, {"title": "Websites with a good overview of different datasets and predictive algorithms", "anchor": "Websites_with_a_good_overview_of_different_datasets_and_predictive_algorithms", "level": 1}, {"title": "Blogs to follow", "anchor": "Blogs_to_follow", "level": 1}, {"title": "Important people", "anchor": "Important_people", "level": 1}, {"title": "Journals", "anchor": "Journals", "level": 1}, {"title": "Popular magazines", "anchor": "Popular_magazines", "level": 1}, {"title": "Additional reading", "anchor": "Additional_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-24T23:02:12.003Z", "modifiedAt": null, "url": null, "title": "[LINK] seL4, secure operating system kernel is being open-sourced", "slug": "link-sel4-secure-operating-system-kernel-is-being-open", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.264Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vpeitoi89GPGat77P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ATg4akEqLyCirStiF/link-sel4-secure-operating-system-kernel-is-being-open", "pageUrlRelative": "/posts/ATg4akEqLyCirStiF/link-sel4-secure-operating-system-kernel-is-being-open", "linkUrl": "https://www.lesswrong.com/posts/ATg4akEqLyCirStiF/link-sel4-secure-operating-system-kernel-is-being-open", "postedAtFormatted": "Tuesday, June 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20seL4%2C%20secure%20operating%20system%20kernel%20is%20being%20open-sourced&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20seL4%2C%20secure%20operating%20system%20kernel%20is%20being%20open-sourced%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATg4akEqLyCirStiF%2Flink-sel4-secure-operating-system-kernel-is-being-open%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20seL4%2C%20secure%20operating%20system%20kernel%20is%20being%20open-sourced%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATg4akEqLyCirStiF%2Flink-sel4-secure-operating-system-kernel-is-being-open", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATg4akEqLyCirStiF%2Flink-sel4-secure-operating-system-kernel-is-being-open", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>seL4 is the world's first (and only?) operating-system kernel with an end-to-end proof of implementation correctness and security enforcement. In 34 days it is going open source:</p>\n<p><a href=\"http://sel4.systems/\">http://sel4.systems/</a></p>\n<p>Now if only we could get a provably-correct implementation of the <a href=\"http://fsd-amoeba.sourceforge.net/\">Amoeba</a> operating system kernel on top of this, it'd be the perfect base for a boxed AI software stack.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ATg4akEqLyCirStiF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.811349411865781e-06, "legacy": true, "legacyId": "26446", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-25T09:52:34.481Z", "modifiedAt": null, "url": null, "title": "Artificial Utility Monsters as Effective Altruism", "slug": "artificial-utility-monsters-as-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:20.076Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "cSg66kGmL4y2c8Yt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7hXcDKEEY7Mx56Eqn/artificial-utility-monsters-as-effective-altruism", "pageUrlRelative": "/posts/7hXcDKEEY7Mx56Eqn/artificial-utility-monsters-as-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/7hXcDKEEY7Mx56Eqn/artificial-utility-monsters-as-effective-altruism", "postedAtFormatted": "Wednesday, June 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Artificial%20Utility%20Monsters%20as%20Effective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArtificial%20Utility%20Monsters%20as%20Effective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7hXcDKEEY7Mx56Eqn%2Fartificial-utility-monsters-as-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Artificial%20Utility%20Monsters%20as%20Effective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7hXcDKEEY7Mx56Eqn%2Fartificial-utility-monsters-as-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7hXcDKEEY7Mx56Eqn%2Fartificial-utility-monsters-as-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1977, "htmlBody": "<p><strong>Dear effective altruist,</strong></p>\n<p>have you considered <em>artificial utility monsters</em> as a high-leverage form of altruism?</p>\n<p>In the traditional sense, a utility monster is a hypothetical being which gains so much subjective wellbeing (SWB) from marginal input of resources that any other form of resource allocation is inferior on a utilitarian calculus. (<a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2569\">as illustrated on SMBC</a>)</p>\n<p>This has been used to show that utilitarianism is not as egalitarian as it intuitively may appear, since it prioritizes some beings over others rather strictly - including humans.</p>\n<p>The traditional utility monster is implausible even in principle - it is hard to imagine a mind that is constructed such that it will not succumb to diminishing marginal utility from additional resource allocation. There is probably some natural limit on how much SWB a mind can implement, or at least how much this can be improved by spending more on the mind. This would probably even be true for an algorithmic mind that can be sped up with faster computers, and there are probably limits to how much a digital mind can benefit in subjective speed from the parallelization of its internal subcomputations.</p>\n<p>However, we may broaden the traditional definition somewhat and call any technology <em>utility-monstrous</em>&nbsp;if it implements high SWB with exceptionally good cost-effectiveness and in a scalable form - even if this scalability stems form a larger set of minds running in parallel, rather than one mind feeling much better or living much longer per additional joule/dollar.</p>\n<p>Under this definition, it may be very possible to create and sustain many artificial minds reliably and cheaply, while they all have a very high SWB level at or near subsistence. An important point here is that possible peak intensities of artificially implemented pleasures could be far higher than those commonly found in evolved minds: Our worst pains seem <a href=\"http://www.carlsonschool.umn.edu/assets/71516.pdf\">more intense</a> than our best pleasures for evolutionary reasons - but the same does not have to be true for artifial sentience, whose best pleasures could be even more intense than our worst agony, without any need for suffering anywhere near this strong.</p>\n<p>If such technologies can be invented - which seems highly plausible in principle, if not yet in practice - then the original conclusion for the utilitarian calculus is retained: It would be highly desirable for utilitarians to facilitate the invention and implementation of such utility-monstrous systems and allocate marginal resources to subsidize their existence. This makes it a potential high-value target for effective altruism.</p>\n<p>&nbsp;</p>\n<p><strong>Many tastes, many utility monsters</strong></p>\n<p>Human motivation is barely stimulated by abstract intellectual concepts, and \"utilitronium\" sounds more like \"aluminium\" than something to desire or empathize with. Consequently, the idea is as sexy as a brick. \"Wireheading\" evokes associations of having a piece of metal rammed into one's head, which is understandably unattractive to any evolved primate (unless it's attached to an iPod, which apparently makes it okay).</p>\n<p>Technically, \"utility monsters\" suffer from a similar association problem, which is that the idea is dangerous or ethically monstrous. But since the term is so specific and established in ethical philosophy, and since \"monster\" can at least be given an emotive and amicable - almost endearing - tone, it seems realistic to use it positively. (Suggestions for a better name are welcome, of course.)</p>\n<p>So a central issue for the actual implementation and funding is human attraction. It is more important to motivate humans to embrace the existence of utility monsters than it is for them to be optimally resource-efficient - after all, a technology that is never implemented or funded properly gains next to nothing from being efficient.</p>\n<p>A compromise between raw efficiency of SWB per joule/dollar and better forms to attract humans might be best. There is probably a sweet spot - perhaps various different ones for different target groups - between resource-efficiency and attractiveness. Only die-hard utilitarians will actually want to fund something like hedonium, but the rest of the world may still respond to \"The Sims - now with real pleasures!\", likeable VR characters, or a new generation of reward-based Tamagotchis.</p>\n<p>Once we step away somewhat from maximum efficiency, the possibilities expand drastically. Implementation forms may be:</p>\n<ul>\n<li>decorative like gimmicks or screensavers,&nbsp;</li>\n<li>fashionable like sentient wearables,&nbsp;</li>\n<li>sophisticated and localized like works of art,&nbsp;</li>\n<li>cute like pets or children,&nbsp;</li>\n<li>personalized like computer game avatars retiring into paradise,&nbsp;</li>\n<li>erotic like virtual lovers who continue to have sex without the user,</li>\n<li>nostalgic like digital spirits of dead loved ones in artificial serenity,&nbsp;</li>\n<li>crazy like hyperorgasmic flowers,&nbsp;</li>\n<li>semi-functional like joyful household robots and software assistants,</li>\n<li>and of course generally a wide range of human-like and non-human-like simulated characters embedded in all kinds of virtual narratives.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>Possible risks and mitigation strategies</strong></p>\n<p>Open-souce utility monsters could be made public as templates to add additional control that the implementation of sentience is correct and positive, and to make better variations easy to explore. However, this would come with the downside of malicious abuse and reckless harm potential. Risks of suffering could come from artificial unhappiness desired by users, e.g. for narratives that contain sadism, dramatic violence or punishment of evil characters for quasi-moral gratification. Another such risk could come simply from bad local modifications that implement suffering by accident.</p>\n<p>Despite these risks, one may hope that most humans who care enough to run artificial sentience are more benevolent and careful than malevolent and careless in a way that causes more positive SWB than suffering. After all, most people love their pets and do not torture them, and other people look down on those who do (compare <a href=\"http://creatures.wikia.com/wiki/AntiNorn\">this discussion of Norn abuse</a>, which resulted in extremely hostile responses). And there may be laws against causing artificial suffering. Still, this is an important point of concern.</p>\n<p>Closed-source utility monsters may further mitigate some of this risk by not making the sentient phenotypes directly available to the public, but encapsulating their internal implementation within a well-defined interface - like a physical toy or closed-source software that can be used and run by private users, but not internally manipulated beyond a well-tested state-space without hacking.</p>\n<p>An extremely cautionary approach would be to run the utility monsters by externally controlled dedicated institutions and only give the public - such as voters or donors - some limited control over them through communication with the institution. For instance, dedicated charities could offer \"virtual paradises\" to donors so they can \"adopt\" utility monsters living there in certain ways without allowing those donors to actually lay hands on their implementation. On the other hand, this would require a high level of trustworthiness of the institutions or charities and their controllers.</p>\n<p>&nbsp;</p>\n<p><strong>Not for the sake of utility monsters alone</strong></p>\n<p>Human values are complex, and it has been argued on LessWrong that the resource allocation of any good future should not be spent for the sake of pleasure or happiness alone. As evolved primates, we all have more than one intuitive value we hold dear, even among self-identified intellectual utilitarians, who compose only a tiny fraction of the population.</p>\n<p>However, some discussions in the rationalist community touching related technologies like pleasure wireheading, utilitronium, and so on, have suffered from implausible or orthogonal assumptions and associations. Since the utilitarian calculus favors SWB maximization above all else, it has been feared, we run the risk of losing a more complex future because&nbsp;</p>\n<p style=\"padding-left: 30px;\">a) utilitarianism knows no compromise and</p>\n<p style=\"padding-left: 30px;\">b) the future will be decided by one winning singleton who takes it all and</p>\n<p style=\"padding-left: 30px;\">c) we have only one world with only one future to get it right</p>\n<p>In addition, low status has been ascribed to wireheads, with the association of fake utility or cheating life as a form of low-status behavior. People have been competing for status by associating themselves with the miserable Socrates instead of the happy pig, without actually giving up real option value in their own lives.</p>\n<p>On Scott Alexander's blog, there's a <a href=\"http://slatestarcodex.com/2014/01/28/wirehead-gods-on-lotus-thrones/\">good example</a> of a mostly pessimistic view both in the OP and in the comments. And in <a href=\"/lw/j8n/a_critique_of_effective_altruism/a4zb\">this comment</a> on an effective altruism critique, Carl Shulman names hedonistic utilitarianism turning into a bad political ideology similar to communist states as a plausible failure mode of effective altruism.</p>\n<p>So, will we all be killed by a singleton who turns us into utilitronium?</p>\n<p>Be not afraid! These fears are plausibly unwarranted because:</p>\n<p style=\"padding-left: 30px;\">a) Utilitarianism is consequentialism, and consequentialists are opportunistic compromisers - even within the conflicting impulses of their own evolved minds. The number of utilitarians who would accept existential risk for the sake of pleasure maximization is small, and practically all of them ascribe to the philosophy of cooperative compromise with orthogonal, non-exclusive values in the political marketplace. Those who don't are incompetent almost by definition and will never gain much political traction.</p>\n<p style=\"padding-left: 30px;\">b) The future may very well not be decided by one singleton but by a marketplace of competing agency. Building a singleton is hard and requires the strict subduction or absorption of all competition. Even if it were to succeed, the singleton will probably not implement only one human value, since it will be created by many humans with complex values, or at least it will have to make credible concessions to a critical mass of humans with diverse values who can stop it before it reaches singleton status. And if these mitigating assumptions are all false and a fooming singleton is possible and easy, then too much pleasure should be the least of humanity's worries - after all, in this case the Taliban, the Chinese government, the US military or some modern King Joffrey are just as likely to get the singleton as the utilitarians.</p>\n<p style=\"padding-left: 30px;\">c) There are plausibly many Everett branches and many hubble volumes like ours, implementing more than one future-earth outcome, as summed up by Max Tegmark <a href=\"http://space.mit.edu/home/tegmark/multiverse.pdf\">here</a>. Even if infinitarian multiverse theories should all end up false against current odds, a very large finite universe would still be far more realistic than a small one, given our physical observations. This makes a pre-existing value diversity highly probable if not inevitable. For instance, if you value pristine nature in addition to SWB, you should accept the high probability of many parallel earth-like planets with pristine nature irregardless of what you do, and consider that we may be in an exceptional minority position to improve the measure of other values that do not naturally evolve easily, such as a very high positive-SWB-over-suffering surplus.</p>\n<p>&nbsp;</p>\n<p><strong>From the present, into the future</strong></p>\n<p>If we accept the conclusion that utility-monstrous technology is a high-value vector for effective altruism (among others), then what could current EAs do as we transition into the future? To my best knowledge, we don't have the capacity yet to create artificial utility monsters.</p>\n<p>However, foundational research in neuroscience and artificial intelligence/sentience theory is already ongoing today and certainly a necessity if we ever want to implement utility-monstrous systems. In addition, outreach and public discussion of the fundamental concepts is also possible and plausibly high-value (hence this post). Generally, the following steps seem all useful and could use the attention of EAs, as we progress into the future:</p>\n<ol>\n<li>spread the idea, refine the concepts, apply constructive criticism to all its weak spots until it becomes either solid or revealed as irredeemably undesirable</li>\n<li>identify possible misunderstandings, fears, biases etc. that may reduce human acceptance and find compromises and attraction factors to mitigate them</li>\n<li>fund and do the scientific research that, if successful, could lead to utility-monstrous technologies</li>\n<li>fund the implementation of the first actual utility monsters and test them thoroughly, then improve on the design, then test again, etc.</li>\n<li>either make the templates public (open-source approach) or make them available for specialized altruistic institutions, such as private charities</li>\n<li>perform outreach and fundraising to give existence donations to as many utility monsters as possible</li>\n</ol>\n<p>All of this can be done without much self-sacrifice on the part of any individual. And all of this can be done within existing political systems, existing markets, and without violating anyone's rights.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7hXcDKEEY7Mx56Eqn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 21, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "26449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Dear_effective_altruist_\">Dear effective altruist,</strong></p>\n<p>have you considered <em>artificial utility monsters</em> as a high-leverage form of altruism?</p>\n<p>In the traditional sense, a utility monster is a hypothetical being which gains so much subjective wellbeing (SWB) from marginal input of resources that any other form of resource allocation is inferior on a utilitarian calculus. (<a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2569\">as illustrated on SMBC</a>)</p>\n<p>This has been used to show that utilitarianism is not as egalitarian as it intuitively may appear, since it prioritizes some beings over others rather strictly - including humans.</p>\n<p>The traditional utility monster is implausible even in principle - it is hard to imagine a mind that is constructed such that it will not succumb to diminishing marginal utility from additional resource allocation. There is probably some natural limit on how much SWB a mind can implement, or at least how much this can be improved by spending more on the mind. This would probably even be true for an algorithmic mind that can be sped up with faster computers, and there are probably limits to how much a digital mind can benefit in subjective speed from the parallelization of its internal subcomputations.</p>\n<p>However, we may broaden the traditional definition somewhat and call any technology <em>utility-monstrous</em>&nbsp;if it implements high SWB with exceptionally good cost-effectiveness and in a scalable form - even if this scalability stems form a larger set of minds running in parallel, rather than one mind feeling much better or living much longer per additional joule/dollar.</p>\n<p>Under this definition, it may be very possible to create and sustain many artificial minds reliably and cheaply, while they all have a very high SWB level at or near subsistence. An important point here is that possible peak intensities of artificially implemented pleasures could be far higher than those commonly found in evolved minds: Our worst pains seem <a href=\"http://www.carlsonschool.umn.edu/assets/71516.pdf\">more intense</a> than our best pleasures for evolutionary reasons - but the same does not have to be true for artifial sentience, whose best pleasures could be even more intense than our worst agony, without any need for suffering anywhere near this strong.</p>\n<p>If such technologies can be invented - which seems highly plausible in principle, if not yet in practice - then the original conclusion for the utilitarian calculus is retained: It would be highly desirable for utilitarians to facilitate the invention and implementation of such utility-monstrous systems and allocate marginal resources to subsidize their existence. This makes it a potential high-value target for effective altruism.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Many_tastes__many_utility_monsters\">Many tastes, many utility monsters</strong></p>\n<p>Human motivation is barely stimulated by abstract intellectual concepts, and \"utilitronium\" sounds more like \"aluminium\" than something to desire or empathize with. Consequently, the idea is as sexy as a brick. \"Wireheading\" evokes associations of having a piece of metal rammed into one's head, which is understandably unattractive to any evolved primate (unless it's attached to an iPod, which apparently makes it okay).</p>\n<p>Technically, \"utility monsters\" suffer from a similar association problem, which is that the idea is dangerous or ethically monstrous. But since the term is so specific and established in ethical philosophy, and since \"monster\" can at least be given an emotive and amicable - almost endearing - tone, it seems realistic to use it positively. (Suggestions for a better name are welcome, of course.)</p>\n<p>So a central issue for the actual implementation and funding is human attraction. It is more important to motivate humans to embrace the existence of utility monsters than it is for them to be optimally resource-efficient - after all, a technology that is never implemented or funded properly gains next to nothing from being efficient.</p>\n<p>A compromise between raw efficiency of SWB per joule/dollar and better forms to attract humans might be best. There is probably a sweet spot - perhaps various different ones for different target groups - between resource-efficiency and attractiveness. Only die-hard utilitarians will actually want to fund something like hedonium, but the rest of the world may still respond to \"The Sims - now with real pleasures!\", likeable VR characters, or a new generation of reward-based Tamagotchis.</p>\n<p>Once we step away somewhat from maximum efficiency, the possibilities expand drastically. Implementation forms may be:</p>\n<ul>\n<li>decorative like gimmicks or screensavers,&nbsp;</li>\n<li>fashionable like sentient wearables,&nbsp;</li>\n<li>sophisticated and localized like works of art,&nbsp;</li>\n<li>cute like pets or children,&nbsp;</li>\n<li>personalized like computer game avatars retiring into paradise,&nbsp;</li>\n<li>erotic like virtual lovers who continue to have sex without the user,</li>\n<li>nostalgic like digital spirits of dead loved ones in artificial serenity,&nbsp;</li>\n<li>crazy like hyperorgasmic flowers,&nbsp;</li>\n<li>semi-functional like joyful household robots and software assistants,</li>\n<li>and of course generally a wide range of human-like and non-human-like simulated characters embedded in all kinds of virtual narratives.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong id=\"Possible_risks_and_mitigation_strategies\">Possible risks and mitigation strategies</strong></p>\n<p>Open-souce utility monsters could be made public as templates to add additional control that the implementation of sentience is correct and positive, and to make better variations easy to explore. However, this would come with the downside of malicious abuse and reckless harm potential. Risks of suffering could come from artificial unhappiness desired by users, e.g. for narratives that contain sadism, dramatic violence or punishment of evil characters for quasi-moral gratification. Another such risk could come simply from bad local modifications that implement suffering by accident.</p>\n<p>Despite these risks, one may hope that most humans who care enough to run artificial sentience are more benevolent and careful than malevolent and careless in a way that causes more positive SWB than suffering. After all, most people love their pets and do not torture them, and other people look down on those who do (compare <a href=\"http://creatures.wikia.com/wiki/AntiNorn\">this discussion of Norn abuse</a>, which resulted in extremely hostile responses). And there may be laws against causing artificial suffering. Still, this is an important point of concern.</p>\n<p>Closed-source utility monsters may further mitigate some of this risk by not making the sentient phenotypes directly available to the public, but encapsulating their internal implementation within a well-defined interface - like a physical toy or closed-source software that can be used and run by private users, but not internally manipulated beyond a well-tested state-space without hacking.</p>\n<p>An extremely cautionary approach would be to run the utility monsters by externally controlled dedicated institutions and only give the public - such as voters or donors - some limited control over them through communication with the institution. For instance, dedicated charities could offer \"virtual paradises\" to donors so they can \"adopt\" utility monsters living there in certain ways without allowing those donors to actually lay hands on their implementation. On the other hand, this would require a high level of trustworthiness of the institutions or charities and their controllers.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Not_for_the_sake_of_utility_monsters_alone\">Not for the sake of utility monsters alone</strong></p>\n<p>Human values are complex, and it has been argued on LessWrong that the resource allocation of any good future should not be spent for the sake of pleasure or happiness alone. As evolved primates, we all have more than one intuitive value we hold dear, even among self-identified intellectual utilitarians, who compose only a tiny fraction of the population.</p>\n<p>However, some discussions in the rationalist community touching related technologies like pleasure wireheading, utilitronium, and so on, have suffered from implausible or orthogonal assumptions and associations. Since the utilitarian calculus favors SWB maximization above all else, it has been feared, we run the risk of losing a more complex future because&nbsp;</p>\n<p style=\"padding-left: 30px;\">a) utilitarianism knows no compromise and</p>\n<p style=\"padding-left: 30px;\">b) the future will be decided by one winning singleton who takes it all and</p>\n<p style=\"padding-left: 30px;\">c) we have only one world with only one future to get it right</p>\n<p>In addition, low status has been ascribed to wireheads, with the association of fake utility or cheating life as a form of low-status behavior. People have been competing for status by associating themselves with the miserable Socrates instead of the happy pig, without actually giving up real option value in their own lives.</p>\n<p>On Scott Alexander's blog, there's a <a href=\"http://slatestarcodex.com/2014/01/28/wirehead-gods-on-lotus-thrones/\">good example</a> of a mostly pessimistic view both in the OP and in the comments. And in <a href=\"/lw/j8n/a_critique_of_effective_altruism/a4zb\">this comment</a> on an effective altruism critique, Carl Shulman names hedonistic utilitarianism turning into a bad political ideology similar to communist states as a plausible failure mode of effective altruism.</p>\n<p>So, will we all be killed by a singleton who turns us into utilitronium?</p>\n<p>Be not afraid! These fears are plausibly unwarranted because:</p>\n<p style=\"padding-left: 30px;\">a) Utilitarianism is consequentialism, and consequentialists are opportunistic compromisers - even within the conflicting impulses of their own evolved minds. The number of utilitarians who would accept existential risk for the sake of pleasure maximization is small, and practically all of them ascribe to the philosophy of cooperative compromise with orthogonal, non-exclusive values in the political marketplace. Those who don't are incompetent almost by definition and will never gain much political traction.</p>\n<p style=\"padding-left: 30px;\">b) The future may very well not be decided by one singleton but by a marketplace of competing agency. Building a singleton is hard and requires the strict subduction or absorption of all competition. Even if it were to succeed, the singleton will probably not implement only one human value, since it will be created by many humans with complex values, or at least it will have to make credible concessions to a critical mass of humans with diverse values who can stop it before it reaches singleton status. And if these mitigating assumptions are all false and a fooming singleton is possible and easy, then too much pleasure should be the least of humanity's worries - after all, in this case the Taliban, the Chinese government, the US military or some modern King Joffrey are just as likely to get the singleton as the utilitarians.</p>\n<p style=\"padding-left: 30px;\">c) There are plausibly many Everett branches and many hubble volumes like ours, implementing more than one future-earth outcome, as summed up by Max Tegmark <a href=\"http://space.mit.edu/home/tegmark/multiverse.pdf\">here</a>. Even if infinitarian multiverse theories should all end up false against current odds, a very large finite universe would still be far more realistic than a small one, given our physical observations. This makes a pre-existing value diversity highly probable if not inevitable. For instance, if you value pristine nature in addition to SWB, you should accept the high probability of many parallel earth-like planets with pristine nature irregardless of what you do, and consider that we may be in an exceptional minority position to improve the measure of other values that do not naturally evolve easily, such as a very high positive-SWB-over-suffering surplus.</p>\n<p>&nbsp;</p>\n<p><strong id=\"From_the_present__into_the_future\">From the present, into the future</strong></p>\n<p>If we accept the conclusion that utility-monstrous technology is a high-value vector for effective altruism (among others), then what could current EAs do as we transition into the future? To my best knowledge, we don't have the capacity yet to create artificial utility monsters.</p>\n<p>However, foundational research in neuroscience and artificial intelligence/sentience theory is already ongoing today and certainly a necessity if we ever want to implement utility-monstrous systems. In addition, outreach and public discussion of the fundamental concepts is also possible and plausibly high-value (hence this post). Generally, the following steps seem all useful and could use the attention of EAs, as we progress into the future:</p>\n<ol>\n<li>spread the idea, refine the concepts, apply constructive criticism to all its weak spots until it becomes either solid or revealed as irredeemably undesirable</li>\n<li>identify possible misunderstandings, fears, biases etc. that may reduce human acceptance and find compromises and attraction factors to mitigate them</li>\n<li>fund and do the scientific research that, if successful, could lead to utility-monstrous technologies</li>\n<li>fund the implementation of the first actual utility monsters and test them thoroughly, then improve on the design, then test again, etc.</li>\n<li>either make the templates public (open-source approach) or make them available for specialized altruistic institutions, such as private charities</li>\n<li>perform outreach and fundraising to give existence donations to as many utility monsters as possible</li>\n</ol>\n<p>All of this can be done without much self-sacrifice on the part of any individual. And all of this can be done within existing political systems, existing markets, and without violating anyone's rights.</p>", "sections": [{"title": "Dear effective altruist,", "anchor": "Dear_effective_altruist_", "level": 1}, {"title": "Many tastes, many utility monsters", "anchor": "Many_tastes__many_utility_monsters", "level": 1}, {"title": "Possible risks and mitigation strategies", "anchor": "Possible_risks_and_mitigation_strategies", "level": 1}, {"title": "Not for the sake of utility monsters alone", "anchor": "Not_for_the_sake_of_utility_monsters_alone", "level": 1}, {"title": "From the present, into the future", "anchor": "From_the_present__into_the_future", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-25T11:28:55.734Z", "modifiedAt": null, "url": null, "title": "The representational fallacy", "slug": "the-representational-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielDeRossi", "createdAt": "2014-06-23T12:47:17.050Z", "isAdmin": false, "displayName": "DanielDeRossi"}, "userId": "Wys5hT2jdCcfn2zdg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cyR9B3yuut97tRzMt/the-representational-fallacy", "pageUrlRelative": "/posts/cyR9B3yuut97tRzMt/the-representational-fallacy", "linkUrl": "https://www.lesswrong.com/posts/cyR9B3yuut97tRzMt/the-representational-fallacy", "postedAtFormatted": "Wednesday, June 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20representational%20fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20representational%20fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyR9B3yuut97tRzMt%2Fthe-representational-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20representational%20fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyR9B3yuut97tRzMt%2Fthe-representational-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyR9B3yuut97tRzMt%2Fthe-representational-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>Basically Heather Dyke argues that metaphysicians are too often arguing from representations of reality (eg in language) to reality itself.</p>\n<p>&nbsp;It looks to me like a variant of the mind projection fallacy. This might be the first book length treatment teh fallacy has gotten though. &nbsp;What do people think?</p>\n<p>&nbsp;</p>\n<p>See reviews here</p>\n<p>https://www.sendspace.com/file/k5x8sy</p>\n<p>https://ndpr.nd.edu/news/23820-metaphysics-and-the-representational-fallacy/</p>\n<p>To give bit of background there's a debate between A-theorists and B-theorists in philosophy of time.</p>\n<p>A-theorists think time has ontological distinctions between past present and future</p>\n<p>B-theorists hold there is no ontological distinction between past present and future.</p>\n<p>Dyke argues that a popular argument for A-theory (tensed language represents ontological distinctions) commits the representational fallacy. Bourne agrees , but points out an argument Dyke uses for B-theory commits the same fallacy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cyR9B3yuut97tRzMt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 1.8124721381707997e-06, "legacy": true, "legacyId": "26451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-06-25T11:29:05.763Z", "modifiedAt": null, "url": null, "title": "The representational fallacy", "slug": "the-representational-fallacy-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielDeRossi", "createdAt": "2014-06-23T12:47:17.050Z", "isAdmin": false, "displayName": "DanielDeRossi"}, "userId": "Wys5hT2jdCcfn2zdg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XDPzyo8fKupiRoK57/the-representational-fallacy-0", "pageUrlRelative": "/posts/XDPzyo8fKupiRoK57/the-representational-fallacy-0", "linkUrl": "https://www.lesswrong.com/posts/XDPzyo8fKupiRoK57/the-representational-fallacy-0", "postedAtFormatted": "Wednesday, June 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20representational%20fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20representational%20fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDPzyo8fKupiRoK57%2Fthe-representational-fallacy-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20representational%20fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDPzyo8fKupiRoK57%2Fthe-representational-fallacy-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDPzyo8fKupiRoK57%2Fthe-representational-fallacy-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>Basically Heather Dyke argues that metaphysicians are too often arguing from representations of reality (eg in language) to reality itself.</p>\n<p>&nbsp;It looks to me like a variant of the mind projection fallacy. This might be the first book length treatment teh fallacy has gotten though. &nbsp;What do people think?</p>\n<p>&nbsp;</p>\n<p>See reviews here</p>\n<p>https://www.sendspace.com/file/k5x8sy</p>\n<p>https://ndpr.nd.edu/news/23820-metaphysics-and-the-representational-fallacy/</p>\n<p>To give bit of background there's a debate between A-theorists and B-theorists in philosophy of time.</p>\n<p>A-theorists think time has ontological distinctions between past present and future</p>\n<p>B-theorists hold there is no ontological distinction between past present and future.</p>\n<p>Dyke argues that a popular argument for A-theory (tensed language represents ontological distinctions) commits the representational fallacy. Bourne agrees , but points out an argument Dyke uses for B-theory commits the same fallacy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XDPzyo8fKupiRoK57", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "26452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}